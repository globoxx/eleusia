{"ast":null,"code":"import _createForOfIteratorHelper from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/createForOfIteratorHelper.js\";\nimport _toConsumableArray from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/toConsumableArray.js\";\nimport _regeneratorRuntime from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/regeneratorRuntime.js\";\nimport _asyncToGenerator from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/asyncToGenerator.js\";\nimport _classCallCheck from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/classCallCheck.js\";\nimport _createClass from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/createClass.js\";\n/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { KernelBackend } from './backends/backend';\nimport { Environment, setEnvironmentGlobal } from './environment';\nimport { getGlobalNamespace } from './global_util';\nimport { Add, Cast, Identity } from './kernel_names';\nimport { getGradient, getKernel, getKernelsForBackend } from './kernel_registry';\nimport * as log from './log';\nimport { Profiler } from './profiler';\nimport { backpropagateGradients, getFilteredNodesXToY } from './tape';\nimport { setTensorTracker, Tensor, Variable } from './tensor';\nimport { getTensorsInContainer } from './tensor_util';\nimport * as util from './util';\nimport { bytesFromStringArray, makeOnesTypedArray, now, sizeFromShape } from './util';\nfunction isRegisteredKernelInvocation(kernelInvocation) {\n  return kernelInvocation.kernelName != null;\n}\nvar EngineState = /*#__PURE__*/function () {\n  function EngineState() {\n    _classCallCheck(this, EngineState);\n    // Public since optimizers will use it.\n    this.registeredVariables = {};\n    this.nextTapeNodeId = 0;\n    this.numBytes = 0;\n    this.numTensors = 0;\n    this.numStringTensors = 0;\n    this.numDataBuffers = 0;\n    // Number of nested tf.grad() statements when computing higher-order\n    // gradients. E.g. `1` for first-order gradients and `2` for second-order\n    // gradients. Used to track if the tape should be removed after a backprop.\n    this.gradientDepth = 0;\n    // Number of nested kernel calls. When kernel depth is greater than 1, we turn\n    // off the tape.\n    this.kernelDepth = 0;\n    this.scopeStack = [];\n    /**\n     * Keeps track of the number of data moves during a kernel execution. We\n     * maintain a stack since kernels can call other kernels, recursively.\n     */\n    this.numDataMovesStack = [];\n    this.nextScopeId = 0;\n    this.tensorInfo = new WeakMap();\n    this.profiling = false;\n    this.activeProfile = {\n      newBytes: 0,\n      newTensors: 0,\n      peakBytes: 0,\n      kernels: [],\n      result: null,\n      get kernelNames() {\n        return Array.from(new Set(this.kernels.map(function (k) {\n          return k.name;\n        })));\n      }\n    };\n  }\n  _createClass(EngineState, [{\n    key: \"dispose\",\n    value: function dispose() {\n      for (var variableName in this.registeredVariables) {\n        this.registeredVariables[variableName].dispose();\n      }\n    }\n  }]);\n  return EngineState;\n}();\nexport var Engine = /*#__PURE__*/function () {\n  function Engine(ENV) {\n    _classCallCheck(this, Engine);\n    this.ENV = ENV;\n    this.registry = {};\n    this.registryFactory = {};\n    this.pendingBackendInitId = 0;\n    this.state = new EngineState();\n  }\n  _createClass(Engine, [{\n    key: \"ready\",\n    value: function () {\n      var _ready = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee() {\n        var sortedBackends, i, backendName, success;\n        return _regeneratorRuntime().wrap(function _callee$(_context) {\n          while (1) switch (_context.prev = _context.next) {\n            case 0:\n              if (!(this.pendingBackendInit != null)) {\n                _context.next = 2;\n                break;\n              }\n              return _context.abrupt(\"return\", this.pendingBackendInit.then(function () {}));\n            case 2:\n              if (!(this.backendInstance != null)) {\n                _context.next = 4;\n                break;\n              }\n              return _context.abrupt(\"return\");\n            case 4:\n              sortedBackends = this.getSortedBackends();\n              i = 0;\n            case 6:\n              if (!(i < sortedBackends.length)) {\n                _context.next = 18;\n                break;\n              }\n              backendName = sortedBackends[i];\n              _context.next = 10;\n              return this.initializeBackend(backendName).success;\n            case 10:\n              success = _context.sent;\n              if (!success) {\n                _context.next = 15;\n                break;\n              }\n              _context.next = 14;\n              return this.setBackend(backendName);\n            case 14:\n              return _context.abrupt(\"return\");\n            case 15:\n              i++;\n              _context.next = 6;\n              break;\n            case 18:\n              throw new Error(\"Could not initialize any backends, all backend initializations \" + \"failed.\");\n            case 19:\n            case \"end\":\n              return _context.stop();\n          }\n        }, _callee, this);\n      }));\n      function ready() {\n        return _ready.apply(this, arguments);\n      }\n      return ready;\n    }()\n  }, {\n    key: \"backend\",\n    get: function get() {\n      if (this.pendingBackendInit != null) {\n        throw new Error(\"Backend '\".concat(this.backendName, \"' has not yet been initialized. Make \") + \"sure to await tf.ready() or await tf.setBackend() before calling \" + \"other methods\");\n      }\n      if (this.backendInstance == null) {\n        var _this$initializeBacke = this.initializeBackendsAndReturnBest(),\n          name = _this$initializeBacke.name,\n          asyncInit = _this$initializeBacke.asyncInit;\n        if (asyncInit) {\n          throw new Error(\"The highest priority backend '\".concat(name, \"' has not yet been \") + \"initialized. Make sure to await tf.ready() or \" + \"await tf.setBackend() before calling other methods\");\n        }\n        this.setBackend(name);\n      }\n      return this.backendInstance;\n    }\n  }, {\n    key: \"backendNames\",\n    value: function backendNames() {\n      return Object.keys(this.registryFactory);\n    }\n  }, {\n    key: \"findBackend\",\n    value: function findBackend(backendName) {\n      if (!(backendName in this.registry)) {\n        // If the backend hasn't been initialized but we have a registry entry for\n        // it, initialize it and return it.\n        if (backendName in this.registryFactory) {\n          var _this$initializeBacke2 = this.initializeBackend(backendName),\n            asyncInit = _this$initializeBacke2.asyncInit;\n          if (asyncInit) {\n            // Backend is not ready yet.\n            return null;\n          }\n        } else {\n          return null;\n        }\n      }\n      return this.registry[backendName];\n    }\n  }, {\n    key: \"findBackendFactory\",\n    value: function findBackendFactory(backendName) {\n      if (!(backendName in this.registryFactory)) {\n        return null;\n      }\n      return this.registryFactory[backendName].factory;\n    }\n  }, {\n    key: \"registerBackend\",\n    value: function registerBackend(backendName, factory) {\n      var priority = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 1;\n      if (backendName in this.registryFactory) {\n        log.warn(\"\".concat(backendName, \" backend was already registered. \") + \"Reusing existing backend factory.\");\n        return false;\n      }\n      this.registryFactory[backendName] = {\n        factory: factory,\n        priority: priority\n      };\n      return true;\n    }\n  }, {\n    key: \"setBackend\",\n    value: function () {\n      var _setBackend = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee2(backendName) {\n        var _this$initializeBacke3, success, asyncInit, result;\n        return _regeneratorRuntime().wrap(function _callee2$(_context2) {\n          while (1) switch (_context2.prev = _context2.next) {\n            case 0:\n              if (!(this.registryFactory[backendName] == null)) {\n                _context2.next = 2;\n                break;\n              }\n              throw new Error(\"Backend name '\".concat(backendName, \"' not found in registry\"));\n            case 2:\n              this.backendName = backendName;\n              if (!(this.registry[backendName] == null)) {\n                _context2.next = 16;\n                break;\n              }\n              this.backendInstance = null;\n              _this$initializeBacke3 = this.initializeBackend(backendName), success = _this$initializeBacke3.success, asyncInit = _this$initializeBacke3.asyncInit;\n              if (!asyncInit) {\n                _context2.next = 12;\n                break;\n              }\n              _context2.next = 9;\n              return success;\n            case 9:\n              _context2.t0 = _context2.sent;\n              _context2.next = 13;\n              break;\n            case 12:\n              _context2.t0 = success;\n            case 13:\n              result = _context2.t0;\n              if (result) {\n                _context2.next = 16;\n                break;\n              }\n              return _context2.abrupt(\"return\", false);\n            case 16:\n              this.backendInstance = this.registry[backendName];\n              this.setupRegisteredKernels();\n              // Reset the profiler.\n              this.profiler = new Profiler(this.backendInstance);\n              return _context2.abrupt(\"return\", true);\n            case 20:\n            case \"end\":\n              return _context2.stop();\n          }\n        }, _callee2, this);\n      }));\n      function setBackend(_x) {\n        return _setBackend.apply(this, arguments);\n      }\n      return setBackend;\n    }()\n  }, {\n    key: \"setupRegisteredKernels\",\n    value: function setupRegisteredKernels() {\n      var _this = this;\n      var kernels = getKernelsForBackend(this.backendName);\n      kernels.forEach(function (kernel) {\n        if (kernel.setupFunc != null) {\n          kernel.setupFunc(_this.backendInstance);\n        }\n      });\n    }\n  }, {\n    key: \"disposeRegisteredKernels\",\n    value: function disposeRegisteredKernels(backendName) {\n      var _this2 = this;\n      var kernels = getKernelsForBackend(backendName);\n      kernels.forEach(function (kernel) {\n        if (kernel.disposeFunc != null) {\n          kernel.disposeFunc(_this2.registry[backendName]);\n        }\n      });\n    }\n    /**\n     * Initializes a backend by looking up the backend name in the factory\n     * registry and calling the factory method. Returns a boolean representing\n     * whether the initialization of the backend suceeded. Throws an error if\n     * there is no backend in the factory registry.\n     */\n  }, {\n    key: \"initializeBackend\",\n    value: function initializeBackend(backendName) {\n      var _this3 = this;\n      var registryFactoryEntry = this.registryFactory[backendName];\n      if (registryFactoryEntry == null) {\n        throw new Error(\"Cannot initialize backend \".concat(backendName, \", no registration found.\"));\n      }\n      try {\n        var backend = registryFactoryEntry.factory();\n        /* Test if the factory returns a promise.\n        Done in a more liberal way than\n        previous 'Promise.resolve(backend)===backend'\n        as we needed to account for custom Promise\n        implementations (e.g. Angular) */\n        if (backend && !(backend instanceof KernelBackend) && typeof backend.then === 'function') {\n          var promiseId = ++this.pendingBackendInitId;\n          var success = backend.then(function (backendInstance) {\n            // Outdated promise. Another backend was set in the meantime.\n            if (promiseId < _this3.pendingBackendInitId) {\n              return false;\n            }\n            _this3.registry[backendName] = backendInstance;\n            _this3.pendingBackendInit = null;\n            return true;\n          }).catch(function (err) {\n            // Outdated promise. Another backend was set in the meantime.\n            if (promiseId < _this3.pendingBackendInitId) {\n              return false;\n            }\n            _this3.pendingBackendInit = null;\n            log.warn(\"Initialization of backend \".concat(backendName, \" failed\"));\n            log.warn(err.stack || err.message);\n            return false;\n          });\n          this.pendingBackendInit = success;\n          return {\n            success: success,\n            asyncInit: true\n          };\n        } else {\n          this.registry[backendName] = backend;\n          return {\n            success: true,\n            asyncInit: false\n          };\n        }\n      } catch (err) {\n        log.warn(\"Initialization of backend \".concat(backendName, \" failed\"));\n        log.warn(err.stack || err.message);\n        return {\n          success: false,\n          asyncInit: false\n        };\n      }\n    }\n  }, {\n    key: \"removeBackend\",\n    value: function removeBackend(backendName) {\n      if (!(backendName in this.registryFactory)) {\n        throw new Error(\"\".concat(backendName, \" backend not found in registry\"));\n      }\n      if (this.backendName === backendName && this.pendingBackendInit != null) {\n        // There is a pending promise of the backend we want to remove. Make it\n        // obsolete.\n        this.pendingBackendInitId++;\n      }\n      if (backendName in this.registry) {\n        this.disposeRegisteredKernels(backendName);\n        this.registry[backendName].dispose();\n        delete this.registry[backendName];\n      }\n      delete this.registryFactory[backendName];\n      // Unset the backend if it is active.\n      if (this.backendName === backendName) {\n        this.pendingBackendInit = null;\n        this.backendName = null;\n        this.backendInstance = null;\n      }\n    }\n  }, {\n    key: \"getSortedBackends\",\n    value: function getSortedBackends() {\n      var _this4 = this;\n      if (Object.keys(this.registryFactory).length === 0) {\n        throw new Error('No backend found in registry.');\n      }\n      return Object.keys(this.registryFactory).sort(function (a, b) {\n        // Highest priority comes first.\n        return _this4.registryFactory[b].priority - _this4.registryFactory[a].priority;\n      });\n    }\n  }, {\n    key: \"initializeBackendsAndReturnBest\",\n    value: function initializeBackendsAndReturnBest() {\n      var sortedBackends = this.getSortedBackends();\n      for (var i = 0; i < sortedBackends.length; i++) {\n        var backendName = sortedBackends[i];\n        var _this$initializeBacke4 = this.initializeBackend(backendName),\n          success = _this$initializeBacke4.success,\n          asyncInit = _this$initializeBacke4.asyncInit;\n        if (asyncInit || success) {\n          return {\n            name: backendName,\n            asyncInit: asyncInit\n          };\n        }\n      }\n      throw new Error(\"Could not initialize any backends, all backend initializations \" + \"failed.\");\n    }\n  }, {\n    key: \"moveData\",\n    value: function moveData(backend, dataId) {\n      var info = this.state.tensorInfo.get(dataId);\n      var srcBackend = info.backend;\n      var values = this.readSync(dataId);\n      var refCount = srcBackend.refCount(dataId);\n      // Delete the tensor from the old backend and move it to the new\n      // backend.\n      srcBackend.disposeData(dataId, true);\n      info.backend = backend;\n      backend.move(dataId, values, info.shape, info.dtype, refCount);\n      if (this.shouldCheckForMemLeaks()) {\n        // Track the number of moves during a kernel execution to correctly\n        // detect memory leaks.\n        this.state.numDataMovesStack[this.state.numDataMovesStack.length - 1]++;\n      }\n    }\n  }, {\n    key: \"tidy\",\n    value: function tidy(nameOrFn, fn) {\n      var _this5 = this;\n      var name = null;\n      if (fn == null) {\n        // Called with only 1 argument.\n        if (typeof nameOrFn !== 'function') {\n          throw new Error('Please provide a function to tidy()');\n        }\n        fn = nameOrFn;\n      } else {\n        // Called with 2 arguments.\n        if (typeof nameOrFn !== 'string' && !(nameOrFn instanceof String)) {\n          throw new Error('When calling with two arguments, the first argument ' + 'to tidy() must be a string');\n        }\n        if (typeof fn !== 'function') {\n          throw new Error('When calling with two arguments, the 2nd argument ' + 'to tidy() must be a function');\n        }\n        name = nameOrFn;\n        // TODO(nsthorat,smilkov): Do operation logging and performance\n        // profiling.\n      }\n\n      var result;\n      return this.scopedRun(function () {\n        return _this5.startScope(name);\n      }, function () {\n        return _this5.endScope(result);\n      }, function () {\n        result = fn();\n        if (result instanceof Promise) {\n          console.error('Cannot return a Promise inside of tidy.');\n        }\n        return result;\n      });\n    }\n  }, {\n    key: \"scopedRun\",\n    value: function scopedRun(start, end, f) {\n      start();\n      try {\n        var res = f();\n        end();\n        return res;\n      } catch (ex) {\n        end();\n        throw ex;\n      }\n    }\n  }, {\n    key: \"nextTensorId\",\n    value: function nextTensorId() {\n      return Engine.nextTensorId++;\n    }\n  }, {\n    key: \"nextVariableId\",\n    value: function nextVariableId() {\n      return Engine.nextVariableId++;\n    }\n    /**\n     * This method is called instead of the public-facing tensor.clone() when\n     * saving a tensor for backwards pass. It makes sure to add the clone\n     * operation to the tape regardless of being called inside a kernel\n     * execution.\n     */\n  }, {\n    key: \"clone\",\n    value: function clone(x) {\n      var y = ENGINE.runKernel(Identity, {\n        x: x\n      });\n      var inputs = {\n        x: x\n      };\n      var grad = function grad(dy) {\n        return {\n          x: function x() {\n            var dtype = 'float32';\n            var gradInputs = {\n              x: dy\n            };\n            var attrs = {\n              dtype: dtype\n            };\n            return ENGINE.runKernel(Cast, gradInputs,\n            // tslint:disable-next-line: no-unnecessary-type-assertion\n            attrs);\n          }\n        };\n      };\n      var saved = [];\n      this.addTapeNode(this.state.activeScope.name, inputs, [y], grad, saved, {});\n      return y;\n    }\n    /**\n     * Execute a kernel with the given name and return the output tensor.\n     *\n     * @param kernelName The name of the kernel to execute.\n     * @param inputs A map of input names to tensors.\n     * @param attrs A map of attribute names to their values. An attribute is a\n     *     primitive (non-tensor) input to the kernel.\n     * @param inputsToSave A list of tensors, inputs to save for the backprop\n     *     computation.\n     * @param outputsToSave A list of booleans, specifying which output to save\n     *     for the backprop computation. These are booleans since the output\n     * tensors are not visible to the user.\n     */\n  }, {\n    key: \"runKernel\",\n    value: function runKernel(kernelName, inputs, attrs) {\n      if (this.backendName == null) {\n        // backend has not been initialized yet (backend initialization is lazy\n        // can be deferred until an op/ kernel is run).\n        // The below getter has side effects that will try to initialize the\n        // backend and set properties like this.backendName\n        // tslint:disable-next-line: no-unused-expression\n        this.backend;\n      }\n      var hasKernel = getKernel(kernelName, this.backendName) != null;\n      if (!hasKernel) {\n        throw new Error(\"Kernel '\".concat(kernelName, \"' not registered for backend '\").concat(this.backendName, \"'\"));\n      }\n      return this.runKernelFunc({\n        kernelName: kernelName,\n        inputs: inputs,\n        attrs: attrs\n      });\n    }\n  }, {\n    key: \"shouldCheckForMemLeaks\",\n    value: function shouldCheckForMemLeaks() {\n      return this.ENV.getBool('IS_TEST');\n    }\n  }, {\n    key: \"checkKernelForMemLeak\",\n    value: function checkKernelForMemLeak(kernelName, numDataIdsBefore, outInfos) {\n      var numDataIdsAfter = this.backend.numDataIds();\n      // Count the number of data ids associated with the result of the kernel.\n      var numOutputDataIds = 0;\n      outInfos.forEach(function (info) {\n        // Complex numbers allocate 3 data ids, one for 'real', one for\n        // 'imaginary', and one for the container that holds the former two.\n        numOutputDataIds += info.dtype === 'complex64' ? 3 : 1;\n      });\n      // Account for the number of moves during kernel execution. A \"data move\"\n      // can happen in the middle of a kernel execution, placing a new (key,value)\n      // pair in the data storage. Since data moves have net zero effect (we\n      // always remove the data from the old backend), we have to cancel them out\n      // when detecting memory leaks.\n      var numMoves = this.state.numDataMovesStack[this.state.numDataMovesStack.length - 1];\n      var dataIdsLeaked = numDataIdsAfter - numDataIdsBefore - numOutputDataIds - numMoves;\n      if (dataIdsLeaked > 0) {\n        throw new Error(\"Backend '\".concat(this.backendName, \"' has an internal memory leak \") + \"(\".concat(dataIdsLeaked, \" data ids) after running '\").concat(kernelName, \"'\"));\n      }\n    }\n    /**\n     * Internal helper method to execute a kernel Func\n     *\n     * Use `runKernel` to execute kernels from outside of engine.\n     */\n  }, {\n    key: \"runKernelFunc\",\n    value: function runKernelFunc(kernelParams) {\n      var _this6 = this;\n      var outputs;\n      var saved = [];\n      var isTapeOn = this.isTapeOn();\n      var startingBytecount = this.state.numBytes;\n      var startingNumTensors = this.state.numTensors;\n      if (this.shouldCheckForMemLeaks()) {\n        this.state.numDataMovesStack.push(0);\n      }\n      var kernelFunc;\n      if (this.backendName == null) {\n        // backend has not been initialized yet (backend initialization is lazy\n        // can be deferred until an op/ kernel is run).\n        // The below getter has side effects that will try to initialize the\n        // backend and set properties like this.backendName\n        // tslint:disable-next-line: no-unused-expression\n        this.backend;\n      }\n      var out;\n      var kernelOrScopeName = isRegisteredKernelInvocation(kernelParams) ? kernelParams.kernelName : this.state.activeScope != null ? this.state.activeScope.name : '';\n      // Create the kernelFunc from either a registered kernel OR passed in\n      // forward/backward functions (used by custom grad). In this context a\n      // kernelFunc wraps a kernel implementation with some bookkeeping.\n      if (isRegisteredKernelInvocation(kernelParams)) {\n        var kernelName = kernelParams.kernelName,\n          _inputs = kernelParams.inputs,\n          _attrs = kernelParams.attrs;\n        if (this.backendName == null) {\n          // backend has not been initialized yet (backend initialization is lazy\n          // can be deferred until an op/ kernel is run).\n          // The below getter has side effects that will try to initialize the\n          // backend and set properties like this.backendName\n          // tslint:disable-next-line: no-unused-expression\n          this.backend;\n        }\n        var kernel = getKernel(kernelName, this.backendName);\n        util.assert(kernel != null, function () {\n          return \"Cannot find registered kernel '\".concat(kernelName, \"' for backend '\").concat(_this6.backendName, \"'\");\n        });\n        kernelFunc = function kernelFunc() {\n          var numDataIdsBefore = _this6.backend.numDataIds();\n          out = kernel.kernelFunc({\n            inputs: _inputs,\n            attrs: _attrs,\n            backend: _this6.backend\n          });\n          var outInfos = Array.isArray(out) ? out : [out];\n          if (_this6.shouldCheckForMemLeaks()) {\n            _this6.checkKernelForMemLeak(kernelName, numDataIdsBefore, outInfos);\n          }\n          var outTensors = outInfos.map(function (outInfo) {\n            // todo (yassogba) remove this option (Tensor) when node backend\n            // methods have been modularized and they all return tensorInfo.\n            // TensorInfos do not have a rank attribute.\n            if (outInfo.rank != null) {\n              return outInfo;\n            }\n            return _this6.makeTensorFromTensorInfo(outInfo);\n          });\n          // Save any required inputs and outputs.\n          // Do not save unless we are recording to the tape. Otherwise it would\n          // cause a mem leak since there would be no backprop for these tensors\n          // (which would otherwise dispose them).\n          if (isTapeOn) {\n            var tensorsToSave = _this6.getTensorsForGradient(kernelName, _inputs, outTensors);\n            saved = _this6.saveTensorsForBackwardMode(tensorsToSave);\n          }\n          return outTensors;\n        };\n      } else {\n        var forwardFunc = kernelParams.forwardFunc;\n        // Running a customGrad op.\n        var saveFunc = function saveFunc(tensors) {\n          // Do not save unless we are recording to the tape. Otherwise it would\n          // cause a mem leak since we would never run backprop, which disposes\n          // the kept tensors.\n          if (!isTapeOn) {\n            return;\n          }\n          saved = tensors.map(function (tensor) {\n            return _this6.keep(_this6.clone(tensor));\n          });\n        };\n        kernelFunc = function kernelFunc() {\n          var numDataIdsBefore = _this6.backend.numDataIds();\n          out = _this6.tidy(function () {\n            return forwardFunc(_this6.backend, saveFunc);\n          });\n          var outs = Array.isArray(out) ? out : [out];\n          if (_this6.shouldCheckForMemLeaks()) {\n            // Scope name is used to print a more helpful error message if needed.\n            _this6.checkKernelForMemLeak(kernelOrScopeName, numDataIdsBefore, outs);\n          }\n          return outs;\n        };\n      }\n      //\n      // Run the kernelFunc. Optionally profiling it.\n      //\n      var inputs = kernelParams.inputs,\n        attrs = kernelParams.attrs;\n      var backwardsFunc = isRegisteredKernelInvocation(kernelParams) ? null : kernelParams.backwardsFunc;\n      var kernelProfile;\n      this.scopedRun(\n      // Stop recording to a tape when running a kernel.\n      function () {\n        return _this6.state.kernelDepth++;\n      }, function () {\n        return _this6.state.kernelDepth--;\n      }, function () {\n        if (!_this6.ENV.getBool('DEBUG') && !_this6.state.profiling) {\n          outputs = kernelFunc();\n        } else {\n          kernelProfile = _this6.profiler.profileKernel(kernelOrScopeName, inputs, function () {\n            return kernelFunc();\n          });\n          if (_this6.ENV.getBool('DEBUG')) {\n            _this6.profiler.logKernelProfile(kernelProfile);\n          }\n          outputs = kernelProfile.outputs;\n        }\n      });\n      if (isTapeOn) {\n        this.addTapeNode(kernelOrScopeName, inputs, outputs, backwardsFunc, saved, attrs);\n      }\n      if (this.state.profiling) {\n        this.state.activeProfile.kernels.push({\n          name: kernelOrScopeName,\n          bytesAdded: this.state.numBytes - startingBytecount,\n          totalBytesSnapshot: this.state.numBytes,\n          tensorsAdded: this.state.numTensors - startingNumTensors,\n          totalTensorsSnapshot: this.state.numTensors,\n          inputShapes: Object.keys(inputs).map(function (key) {\n            return inputs[key] != null ? inputs[key].shape : null;\n          }),\n          outputShapes: outputs.map(function (item) {\n            return item.shape;\n          }),\n          kernelTimeMs: kernelProfile.timeMs,\n          extraInfo: kernelProfile.extraInfo\n        });\n      }\n      return Array.isArray(out) ? outputs : outputs[0];\n    }\n    /**\n     * Saves tensors used in forward mode for use in backward mode.\n     *\n     * @param tensors the list of tensors to save.\n     */\n  }, {\n    key: \"saveTensorsForBackwardMode\",\n    value: function saveTensorsForBackwardMode(tensors) {\n      var _this7 = this;\n      var saved = tensors.map(function (tensor) {\n        return _this7.keep(_this7.clone(tensor));\n      });\n      return saved;\n    }\n    /**\n     * Returns a list of tensors to save for a given gradient calculation.\n     *\n     * @param kernelName name of kernel to look up gradient for.\n     * @param inputs a map of input tensors.\n     * @param outputs an array of output tensors from forward mode of kernel.\n     */\n  }, {\n    key: \"getTensorsForGradient\",\n    value: function getTensorsForGradient(kernelName, inputs, outputs) {\n      var gradConfig = getGradient(kernelName);\n      if (gradConfig != null) {\n        var inputsToSave = gradConfig.inputsToSave || [];\n        var outputsToSave = gradConfig.outputsToSave || [];\n        // If saveAllInputs is true, all inputs will be saved. Otherwise, inputs\n        // specified in inputsToSave will be saved.\n        var inputTensorsToSave;\n        if (gradConfig.saveAllInputs) {\n          util.assert(Array.isArray(inputs), function () {\n            return 'saveAllInputs is true, expected inputs to be an array.';\n          });\n          inputTensorsToSave = Object.keys(inputs).map(function (key) {\n            return inputs[key];\n          });\n        } else {\n          inputTensorsToSave = inputsToSave.map(function (inputName) {\n            return inputs[inputName];\n          });\n        }\n        var outputTensorsToSave = outputs.filter(function (_, i) {\n          return outputsToSave[i];\n        });\n        return inputTensorsToSave.concat(outputTensorsToSave);\n      }\n      // We return an empty list rather than throw an error because the kernel we\n      // are looking up may not actually be relevant to backproping through the\n      // overall function\n      //\n      // See 'does not error if irrelevant (pruned) ops are missing grads' test\n      // in gradients_test.ts for an example.\n      return [];\n    }\n    /**\n     * Internal method used by public APIs for tensor creation. Makes a new\n     * tensor with the provided shape, dtype and values. It always\n     * creates a new data id and writes the values to the underlying backend.\n     */\n  }, {\n    key: \"makeTensor\",\n    value: function makeTensor(values, shape, dtype, backend) {\n      if (values == null) {\n        throw new Error('Values passed to engine.makeTensor() are null');\n      }\n      dtype = dtype || 'float32';\n      backend = backend || this.backend;\n      var backendVals = values;\n      if (dtype === 'string' && util.isString(values[0])) {\n        backendVals = values.map(function (d) {\n          return util.encodeString(d);\n        });\n      }\n      var dataId = backend.write(backendVals, shape, dtype);\n      var t = new Tensor(shape, dtype, dataId, this.nextTensorId());\n      this.trackTensor(t, backend);\n      // Count bytes for string tensors.\n      if (dtype === 'string') {\n        var info = this.state.tensorInfo.get(dataId);\n        var newBytes = bytesFromStringArray(backendVals);\n        this.state.numBytes += newBytes - info.bytes;\n        info.bytes = newBytes;\n      }\n      return t;\n    }\n    /**\n     * Internal method used by backends. Makes a new tensor\n     * that is a wrapper around an existing data id. It doesn't create\n     * a new data id, only increments the ref count used in memory tracking.\n     * @deprecated\n     */\n  }, {\n    key: \"makeTensorFromDataId\",\n    value: function makeTensorFromDataId(dataId, shape, dtype, backend) {\n      dtype = dtype || 'float32';\n      var tensorInfo = {\n        dataId: dataId,\n        shape: shape,\n        dtype: dtype\n      };\n      return this.makeTensorFromTensorInfo(tensorInfo, backend);\n    }\n    /**\n     * Internal method used by backends. Makes a new tensor that is a wrapper\n     * around an existing data id in TensorInfo. It doesn't create a new data id,\n     * only increments the ref count used in memory tracking.\n     */\n  }, {\n    key: \"makeTensorFromTensorInfo\",\n    value: function makeTensorFromTensorInfo(tensorInfo, backend) {\n      var dataId = tensorInfo.dataId,\n        shape = tensorInfo.shape,\n        dtype = tensorInfo.dtype;\n      var t = new Tensor(shape, dtype, dataId, this.nextTensorId());\n      this.trackTensor(t, backend);\n      return t;\n    }\n  }, {\n    key: \"makeVariable\",\n    value: function makeVariable(initialValue) {\n      var trainable = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : true;\n      var name = arguments.length > 2 ? arguments[2] : undefined;\n      var dtype = arguments.length > 3 ? arguments[3] : undefined;\n      name = name || this.nextVariableId().toString();\n      if (dtype != null && dtype !== initialValue.dtype) {\n        initialValue = initialValue.cast(dtype);\n      }\n      var v = new Variable(initialValue, trainable, name, this.nextTensorId());\n      if (this.state.registeredVariables[v.name] != null) {\n        throw new Error(\"Variable with name \".concat(v.name, \" was already registered\"));\n      }\n      this.state.registeredVariables[v.name] = v;\n      this.incRef(v, this.backend);\n      return v;\n    }\n  }, {\n    key: \"trackTensor\",\n    value: function trackTensor(a, backend) {\n      this.state.numTensors++;\n      if (a.dtype === 'string') {\n        this.state.numStringTensors++;\n      }\n      // Bytes for complex numbers are counted by their components. Bytes for\n      // string tensors are counted when writing values.\n      var bytes = 0;\n      if (a.dtype !== 'complex64' && a.dtype !== 'string') {\n        bytes = a.size * util.bytesPerElement(a.dtype);\n      }\n      this.state.numBytes += bytes;\n      if (!this.state.tensorInfo.has(a.dataId)) {\n        this.state.numDataBuffers++;\n        this.state.tensorInfo.set(a.dataId, {\n          backend: backend || this.backend,\n          dtype: a.dtype,\n          shape: a.shape,\n          bytes: bytes\n        });\n      }\n      if (!(a instanceof Variable)) {\n        this.track(a);\n      }\n    }\n    // Track the tensor by dataId and increase the refCount for the dataId in the\n    // backend.\n    // TODO(pyu10055): This is currently used by makeVariable method, to increase\n    // refCount on the backend for the dataId. It can potentially be replaced with\n    // Identity op indead of calling backend directly.\n  }, {\n    key: \"incRef\",\n    value: function incRef(a, backend) {\n      this.trackTensor(a, backend);\n      this.backend.incRef(a.dataId);\n    }\n  }, {\n    key: \"removeDataId\",\n    value: function removeDataId(dataId, backend) {\n      if (this.state.tensorInfo.has(dataId) && this.state.tensorInfo.get(dataId).backend === backend) {\n        this.state.tensorInfo.delete(dataId);\n        this.state.numDataBuffers--;\n      }\n    }\n  }, {\n    key: \"disposeTensor\",\n    value: function disposeTensor(a) {\n      if (!this.state.tensorInfo.has(a.dataId)) {\n        return;\n      }\n      var info = this.state.tensorInfo.get(a.dataId);\n      this.state.numTensors--;\n      if (a.dtype === 'string') {\n        this.state.numStringTensors--;\n        this.state.numBytes -= info.bytes;\n      }\n      // Don't count bytes for complex numbers as they are counted by their\n      // components.\n      if (a.dtype !== 'complex64' && a.dtype !== 'string') {\n        var bytes = a.size * util.bytesPerElement(a.dtype);\n        this.state.numBytes -= bytes;\n      }\n      // Remove the reference to dataId if backend dispose the data successfully\n      if (info.backend.disposeData(a.dataId)) {\n        this.removeDataId(a.dataId, info.backend);\n      }\n      // TODO(nsthorat): Construct an error and save the stack trace for\n      // debugging when in debug mode. Creating a stack trace is too expensive\n      // to do unconditionally.\n    }\n  }, {\n    key: \"disposeVariables\",\n    value: function disposeVariables() {\n      for (var varName in this.state.registeredVariables) {\n        var v = this.state.registeredVariables[varName];\n        this.disposeVariable(v);\n      }\n    }\n  }, {\n    key: \"disposeVariable\",\n    value: function disposeVariable(v) {\n      this.disposeTensor(v);\n      if (this.state.registeredVariables[v.name] != null) {\n        delete this.state.registeredVariables[v.name];\n      }\n    }\n  }, {\n    key: \"memory\",\n    value: function memory() {\n      var info = this.backend.memory();\n      info.numTensors = this.state.numTensors;\n      info.numDataBuffers = this.state.numDataBuffers;\n      info.numBytes = this.state.numBytes;\n      if (this.state.numStringTensors > 0) {\n        info.unreliable = true;\n        if (info.reasons == null) {\n          info.reasons = [];\n        }\n        info.reasons.push('Memory usage by string tensors is approximate ' + '(2 bytes per character)');\n      }\n      return info;\n    }\n  }, {\n    key: \"profile\",\n    value: function () {\n      var _profile = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee3(query) {\n        var startBytes, startNumTensors, _iterator, _step, kernel;\n        return _regeneratorRuntime().wrap(function _callee3$(_context3) {\n          while (1) switch (_context3.prev = _context3.next) {\n            case 0:\n              this.state.profiling = true;\n              startBytes = this.state.numBytes;\n              startNumTensors = this.state.numTensors;\n              this.state.activeProfile.kernels = [];\n              _context3.next = 6;\n              return query();\n            case 6:\n              this.state.activeProfile.result = _context3.sent;\n              this.state.profiling = false;\n              this.state.activeProfile.peakBytes = Math.max.apply(Math, _toConsumableArray(this.state.activeProfile.kernels.map(function (d) {\n                return d.totalBytesSnapshot;\n              })));\n              this.state.activeProfile.newBytes = this.state.numBytes - startBytes;\n              this.state.activeProfile.newTensors = this.state.numTensors - startNumTensors;\n              _iterator = _createForOfIteratorHelper(this.state.activeProfile.kernels);\n              _context3.prev = 12;\n              _iterator.s();\n            case 14:\n              if ((_step = _iterator.n()).done) {\n                _context3.next = 24;\n                break;\n              }\n              kernel = _step.value;\n              _context3.next = 18;\n              return kernel.kernelTimeMs;\n            case 18:\n              kernel.kernelTimeMs = _context3.sent;\n              _context3.next = 21;\n              return kernel.extraInfo;\n            case 21:\n              kernel.extraInfo = _context3.sent;\n            case 22:\n              _context3.next = 14;\n              break;\n            case 24:\n              _context3.next = 29;\n              break;\n            case 26:\n              _context3.prev = 26;\n              _context3.t0 = _context3[\"catch\"](12);\n              _iterator.e(_context3.t0);\n            case 29:\n              _context3.prev = 29;\n              _iterator.f();\n              return _context3.finish(29);\n            case 32:\n              return _context3.abrupt(\"return\", this.state.activeProfile);\n            case 33:\n            case \"end\":\n              return _context3.stop();\n          }\n        }, _callee3, this, [[12, 26, 29, 32]]);\n      }));\n      function profile(_x2) {\n        return _profile.apply(this, arguments);\n      }\n      return profile;\n    }()\n  }, {\n    key: \"isTapeOn\",\n    value: function isTapeOn() {\n      return this.state.gradientDepth > 0 && this.state.kernelDepth === 0;\n    }\n  }, {\n    key: \"addTapeNode\",\n    value: function addTapeNode(kernelName, inputs, outputs, gradientsFunc, saved, attrs) {\n      var _this8 = this;\n      var tapeNode = {\n        id: this.state.nextTapeNodeId++,\n        kernelName: kernelName,\n        inputs: inputs,\n        outputs: outputs,\n        saved: saved\n      };\n      var gradConfig = getGradient(kernelName);\n      if (gradConfig != null) {\n        gradientsFunc = gradConfig.gradFunc;\n      }\n      if (gradientsFunc != null) {\n        tapeNode.gradient = function (dys) {\n          // TODO(smilkov): To optimize back-prop, pass dys that are not used in\n          // the backprop graph to the user as null instead of zeros\n          dys = dys.map(function (dy, i) {\n            if (dy == null) {\n              var output = outputs[i];\n              var vals = util.makeZerosTypedArray(output.size, output.dtype);\n              return _this8.makeTensor(vals, output.shape, output.dtype);\n            }\n            return dy;\n          });\n          // Grad functions of ops with single outputs expect a dy, while ops\n          // with multiple outputs expect dys (array of dy).\n          return gradientsFunc(dys.length > 1 ? dys : dys[0], saved, attrs);\n        };\n      }\n      this.state.activeTape.push(tapeNode);\n    }\n  }, {\n    key: \"keep\",\n    value: function keep(result) {\n      result.kept = true;\n      return result;\n    }\n  }, {\n    key: \"startTape\",\n    value: function startTape() {\n      if (this.state.gradientDepth === 0) {\n        this.state.activeTape = [];\n      }\n      this.state.gradientDepth++;\n    }\n  }, {\n    key: \"endTape\",\n    value: function endTape() {\n      this.state.gradientDepth--;\n    }\n    /**\n     * Start a scope. Use this with endScope() to achieve the same functionality\n     * as scope() without the need for a function closure.\n     */\n  }, {\n    key: \"startScope\",\n    value: function startScope(name) {\n      var scopeInfo = {\n        track: [],\n        name: 'unnamed scope',\n        id: this.state.nextScopeId++\n      };\n      if (name) {\n        scopeInfo.name = name;\n      }\n      this.state.scopeStack.push(scopeInfo);\n      this.state.activeScope = scopeInfo;\n    }\n    /**\n     * End a scope. Use this with startScope() to achieve the same functionality\n     * as scope() without the need for a function closure.\n     */\n  }, {\n    key: \"endScope\",\n    value: function endScope(result) {\n      var _this9 = this;\n      var tensorsToTrackInParent = getTensorsInContainer(result);\n      var tensorsToTrackInParentSet = new Set(tensorsToTrackInParent.map(function (t) {\n        return t.id;\n      }));\n      // Dispose the arrays tracked in this scope.\n      for (var i = 0; i < this.state.activeScope.track.length; i++) {\n        var tensor = this.state.activeScope.track[i];\n        if (!tensor.kept && !tensorsToTrackInParentSet.has(tensor.id)) {\n          tensor.dispose();\n        }\n      }\n      var oldScope = this.state.scopeStack.pop();\n      this.state.activeScope = this.state.scopeStack.length === 0 ? null : this.state.scopeStack[this.state.scopeStack.length - 1];\n      // Track the current result in the parent scope.\n      tensorsToTrackInParent.forEach(function (tensor) {\n        // Only track the tensor if was allocated in the inner scope and is not\n        // globally kept.\n        if (!tensor.kept && tensor.scopeId === oldScope.id) {\n          _this9.track(tensor);\n        }\n      });\n    }\n    /**\n     * Returns gradients of `f` with respect to each of the `xs`. The gradients\n     * returned are of the same length as `xs`, but some might be null if `f`\n     * was not a function of that `x`. It also takes optional dy to multiply the\n     * gradient, which defaults to `1`.\n     */\n  }, {\n    key: \"gradients\",\n    value: function gradients(f, xs, dy) {\n      var _this10 = this;\n      var allowNoGradients = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : false;\n      util.assert(xs.length > 0, function () {\n        return 'gradients() received an empty list of xs.';\n      });\n      if (dy != null && dy.dtype !== 'float32') {\n        throw new Error(\"dy must have 'float32' dtype, but has '\".concat(dy.dtype, \"'\"));\n      }\n      var y = this.scopedRun(function () {\n        return _this10.startTape();\n      }, function () {\n        return _this10.endTape();\n      }, function () {\n        return _this10.tidy('forward', f);\n      });\n      util.assert(y instanceof Tensor, function () {\n        return 'The result y returned by f() must be a tensor.';\n      });\n      // Filter out the nodes that don't connect x => y.\n      var filteredTape = getFilteredNodesXToY(this.state.activeTape, xs, y);\n      if (!allowNoGradients && filteredTape.length === 0 && xs.length > 0) {\n        throw new Error('Cannot compute gradient of y=f(x) with respect to x. Make sure ' + 'that the f you passed encloses all operations that lead from x ' + 'to y.');\n      }\n      return this.tidy('backward', function () {\n        var accumulatedGradientMap = {};\n        accumulatedGradientMap[y.id] = dy == null ? ones(y.shape) : dy;\n        // Backprop gradients through the filtered nodes.\n        backpropagateGradients(accumulatedGradientMap, filteredTape,\n        // Pass the tidy function to avoid circular dep with `tape.ts`.\n        function (f) {\n          return _this10.tidy(f);\n        },\n        // Pass an add function to avoide a circular dep with `tape.ts`.\n        add);\n        var grads = xs.map(function (x) {\n          return accumulatedGradientMap[x.id];\n        });\n        if (_this10.state.gradientDepth === 0) {\n          // This means that we are not computing higher-order gradients\n          // and can clean up the tape.\n          _this10.state.activeTape.forEach(function (node) {\n            var _iterator2 = _createForOfIteratorHelper(node.saved),\n              _step2;\n            try {\n              for (_iterator2.s(); !(_step2 = _iterator2.n()).done;) {\n                var tensor = _step2.value;\n                tensor.dispose();\n              }\n            } catch (err) {\n              _iterator2.e(err);\n            } finally {\n              _iterator2.f();\n            }\n          });\n          _this10.state.activeTape = null;\n        }\n        return {\n          value: y,\n          grads: grads\n        };\n      });\n    }\n  }, {\n    key: \"customGrad\",\n    value: function customGrad(f) {\n      var _this11 = this;\n      util.assert(util.isFunction(f), function () {\n        return 'The f passed in customGrad(f) must be a function.';\n      });\n      return function () {\n        for (var _len = arguments.length, inputs = new Array(_len), _key = 0; _key < _len; _key++) {\n          inputs[_key] = arguments[_key];\n        }\n        util.assert(inputs.every(function (t) {\n          return t instanceof Tensor;\n        }), function () {\n          return 'The args passed in customGrad(f)(x1, x2,...) must all be ' + 'tensors';\n        });\n        var res;\n        var inputMap = {};\n        inputs.forEach(function (input, i) {\n          inputMap[i] = input;\n        });\n        var forwardFunc = function forwardFunc(_, save) {\n          res = f.apply(void 0, [].concat(inputs, [save]));\n          util.assert(res.value instanceof Tensor, function () {\n            return 'The function f passed in customGrad(f) must return an ' + 'object where `obj.value` is a tensor';\n          });\n          util.assert(util.isFunction(res.gradFunc), function () {\n            return 'The function f passed in customGrad(f) must return an ' + 'object where `obj.gradFunc` is a function.';\n          });\n          return res.value;\n        };\n        var backwardsFunc = function backwardsFunc(dy, saved) {\n          var gradRes = res.gradFunc(dy, saved);\n          var grads = Array.isArray(gradRes) ? gradRes : [gradRes];\n          util.assert(grads.length === inputs.length, function () {\n            return 'The function f passed in customGrad(f) must return an ' + 'object where `obj.gradFunc` is a function that returns ' + 'the same number of tensors as inputs passed to f(...).';\n          });\n          util.assert(grads.every(function (t) {\n            return t instanceof Tensor;\n          }), function () {\n            return 'The function f passed in customGrad(f) must return an ' + 'object where `obj.gradFunc` is a function that returns ' + 'a list of only tensors.';\n          });\n          var gradMap = {};\n          grads.forEach(function (grad, i) {\n            gradMap[i] = function () {\n              return grad;\n            };\n          });\n          return gradMap;\n        };\n        return _this11.runKernelFunc({\n          forwardFunc: forwardFunc,\n          backwardsFunc: backwardsFunc,\n          inputs: inputMap\n        });\n      };\n    }\n  }, {\n    key: \"readSync\",\n    value: function readSync(dataId) {\n      // Route the read to the correct backend.\n      var info = this.state.tensorInfo.get(dataId);\n      return info.backend.readSync(dataId);\n    }\n  }, {\n    key: \"read\",\n    value: function read(dataId) {\n      // Route the read to the correct backend.\n      var info = this.state.tensorInfo.get(dataId);\n      return info.backend.read(dataId);\n    }\n  }, {\n    key: \"readToGPU\",\n    value: function readToGPU(dataId, options) {\n      // Route the read to the correct backend.\n      var info = this.state.tensorInfo.get(dataId);\n      return info.backend.readToGPU(dataId, options);\n    }\n  }, {\n    key: \"time\",\n    value: function () {\n      var _time = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee4(query) {\n        var start, timingInfo;\n        return _regeneratorRuntime().wrap(function _callee4$(_context4) {\n          while (1) switch (_context4.prev = _context4.next) {\n            case 0:\n              start = now();\n              _context4.next = 3;\n              return this.backend.time(query);\n            case 3:\n              timingInfo = _context4.sent;\n              timingInfo.wallMs = now() - start;\n              return _context4.abrupt(\"return\", timingInfo);\n            case 6:\n            case \"end\":\n              return _context4.stop();\n          }\n        }, _callee4, this);\n      }));\n      function time(_x3) {\n        return _time.apply(this, arguments);\n      }\n      return time;\n    }()\n    /**\n     * Tracks a Tensor in the current scope to be automatically cleaned up\n     * when the current scope ends, and returns the value.\n     *\n     * @param result The Tensor to track in the current scope.\n     */\n  }, {\n    key: \"track\",\n    value: function track(result) {\n      if (this.state.activeScope != null) {\n        result.scopeId = this.state.activeScope.id;\n        this.state.activeScope.track.push(result);\n      }\n      return result;\n    }\n  }, {\n    key: \"registeredVariables\",\n    get: function get() {\n      return this.state.registeredVariables;\n    }\n    /**\n     * Resets the engine state. Removes all backends but does not remove\n     * registered backend factories.\n     */\n  }, {\n    key: \"reset\",\n    value: function reset() {\n      // Make any pending promise obsolete.\n      this.pendingBackendInitId++;\n      this.state.dispose();\n      this.ENV.reset();\n      this.state = new EngineState();\n      for (var backendName in this.registry) {\n        this.disposeRegisteredKernels(backendName);\n        this.registry[backendName].dispose();\n        delete this.registry[backendName];\n      }\n      this.backendName = null;\n      this.backendInstance = null;\n      this.pendingBackendInit = null;\n    }\n  }]);\n  return Engine;\n}();\nEngine.nextTensorId = 0;\nEngine.nextVariableId = 0;\nfunction ones(shape) {\n  var values = makeOnesTypedArray(sizeFromShape(shape), 'float32');\n  return ENGINE.makeTensor(values, shape, 'float32');\n}\nexport function getOrMakeEngine() {\n  var ns = getGlobalNamespace();\n  if (ns._tfengine == null) {\n    var environment = new Environment(ns);\n    ns._tfengine = new Engine(environment);\n  }\n  setEnvironmentGlobal(ns._tfengine.ENV);\n  // Tell the current tensor interface that the global engine is responsible\n  // for tracking.\n  setTensorTracker(function () {\n    return ns._tfengine;\n  });\n  return ns._tfengine;\n}\nexport var ENGINE = getOrMakeEngine();\n/**\n * A implementation of the add op for use within engine and tape.\n *\n * This allows us to avoid a circular dependency between add.ts and engine.\n * It is exported to be available in tape tests.\n */\nexport function add(a, b) {\n  // We duplicate Add here to avoid a circular dependency with add.ts.\n  var inputs = {\n    a: a,\n    b: b\n  };\n  return ENGINE.runKernel(Add, inputs);\n}","map":{"version":3,"names":["KernelBackend","Environment","setEnvironmentGlobal","getGlobalNamespace","Add","Cast","Identity","getGradient","getKernel","getKernelsForBackend","log","Profiler","backpropagateGradients","getFilteredNodesXToY","setTensorTracker","Tensor","Variable","getTensorsInContainer","util","bytesFromStringArray","makeOnesTypedArray","now","sizeFromShape","isRegisteredKernelInvocation","kernelInvocation","kernelName","EngineState","_classCallCheck","registeredVariables","nextTapeNodeId","numBytes","numTensors","numStringTensors","numDataBuffers","gradientDepth","kernelDepth","scopeStack","numDataMovesStack","nextScopeId","tensorInfo","WeakMap","profiling","activeProfile","newBytes","newTensors","peakBytes","kernels","result","kernelNames","Array","from","Set","map","k","name","_createClass","key","value","dispose","variableName","Engine","ENV","registry","registryFactory","pendingBackendInitId","state","_ready","_asyncToGenerator","_regeneratorRuntime","mark","_callee","sortedBackends","i","backendName","success","wrap","_callee$","_context","prev","next","pendingBackendInit","abrupt","then","backendInstance","getSortedBackends","length","initializeBackend","sent","setBackend","Error","stop","ready","apply","arguments","get","concat","_this$initializeBacke","initializeBackendsAndReturnBest","asyncInit","backendNames","Object","keys","findBackend","_this$initializeBacke2","findBackendFactory","factory","registerBackend","priority","undefined","warn","_setBackend","_callee2","_this$initializeBacke3","_callee2$","_context2","t0","setupRegisteredKernels","profiler","_x","_this","forEach","kernel","setupFunc","disposeRegisteredKernels","_this2","disposeFunc","_this3","registryFactoryEntry","backend","promiseId","catch","err","stack","message","removeBackend","_this4","sort","a","b","_this$initializeBacke4","moveData","dataId","info","srcBackend","values","readSync","refCount","disposeData","move","shape","dtype","shouldCheckForMemLeaks","tidy","nameOrFn","fn","_this5","String","scopedRun","startScope","endScope","Promise","console","error","start","end","f","res","ex","nextTensorId","nextVariableId","clone","x","y","ENGINE","runKernel","inputs","grad","dy","gradInputs","attrs","saved","addTapeNode","activeScope","hasKernel","runKernelFunc","getBool","checkKernelForMemLeak","numDataIdsBefore","outInfos","numDataIdsAfter","numDataIds","numOutputDataIds","numMoves","dataIdsLeaked","kernelParams","_this6","outputs","isTapeOn","startingBytecount","startingNumTensors","push","kernelFunc","out","kernelOrScopeName","assert","isArray","outTensors","outInfo","rank","makeTensorFromTensorInfo","tensorsToSave","getTensorsForGradient","saveTensorsForBackwardMode","forwardFunc","saveFunc","tensors","tensor","keep","outs","backwardsFunc","kernelProfile","profileKernel","logKernelProfile","bytesAdded","totalBytesSnapshot","tensorsAdded","totalTensorsSnapshot","inputShapes","outputShapes","item","kernelTimeMs","timeMs","extraInfo","_this7","gradConfig","inputsToSave","outputsToSave","inputTensorsToSave","saveAllInputs","inputName","outputTensorsToSave","filter","_","makeTensor","backendVals","isString","d","encodeString","write","t","trackTensor","bytes","makeTensorFromDataId","makeVariable","initialValue","trainable","toString","cast","v","incRef","size","bytesPerElement","has","set","track","removeDataId","delete","disposeTensor","disposeVariables","varName","disposeVariable","memory","unreliable","reasons","_profile","_callee3","query","startBytes","startNumTensors","_iterator","_step","_callee3$","_context3","Math","max","_toConsumableArray","_createForOfIteratorHelper","s","n","done","e","finish","profile","_x2","gradientsFunc","_this8","tapeNode","id","gradFunc","gradient","dys","output","vals","makeZerosTypedArray","activeTape","kept","startTape","endTape","scopeInfo","_this9","tensorsToTrackInParent","tensorsToTrackInParentSet","oldScope","pop","scopeId","gradients","xs","_this10","allowNoGradients","filteredTape","accumulatedGradientMap","ones","add","grads","node","_iterator2","_step2","customGrad","_this11","isFunction","_len","_key","every","inputMap","input","save","gradRes","gradMap","read","readToGPU","options","_time","_callee4","timingInfo","_callee4$","_context4","time","wallMs","_x3","reset","getOrMakeEngine","ns","_tfengine","environment"],"sources":["C:\\Users\\vince\\OneDrive\\Documents\\GitHub\\tfjs-core\\src\\engine.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {BackendTimingInfo, DataMover, KernelBackend} from './backends/backend';\nimport {Environment, setEnvironmentGlobal} from './environment';\nimport {getGlobalNamespace} from './global_util';\nimport {Add, Cast, Identity} from './kernel_names';\nimport { getGradient, getKernel, getKernelsForBackend, GradFunc, NamedAttrMap } from './kernel_registry';\nimport { TensorInfo } from './tensor_info';\nimport * as log from './log';\nimport {KernelProfile, Profiler} from './profiler';\nimport {backpropagateGradients, getFilteredNodesXToY, TapeNode} from './tape';\nimport {DataToGPUOptions, GPUData, setTensorTracker, Tensor, TensorTracker, Variable} from './tensor';\nimport {DataId} from './tensor_info';\nimport {GradSaveFunc, NamedTensorMap, NamedVariableMap, TensorContainer} from './tensor_types';\nimport {getTensorsInContainer} from './tensor_util';\nimport {BackendValues, DataType, DataValues} from './types';\nimport * as util from './util';\nimport {bytesFromStringArray, makeOnesTypedArray, now, sizeFromShape} from './util';\n\n/**\n * A function that computes an output. The save function is for saving tensors\n * computed in the forward pass, that we need in the backward pass.\n */\nexport type ForwardFunc<T> = (backend: KernelBackend, save?: GradSaveFunc) => T;\n\n/**\n * @docalias (a: Tensor, b: Tensor,..., save?: Function) => {\n *   value: Tensor,\n *   gradFunc: (dy: Tensor, saved?: NamedTensorMap) => Tensor | Tensor[]\n * }\n */\nexport type CustomGradientFunc<T extends Tensor> =\n    (...inputs: Array<Tensor|GradSaveFunc>) => {\n      value: T;\n      gradFunc: (dy: T, saved: Tensor[]) => Tensor | Tensor[];\n    };\n\nexport type MemoryInfo = {\n  numTensors: number; numDataBuffers: number; numBytes: number;\n  unreliable?: boolean; reasons: string[];\n};\n\ntype KernelInfo = {\n  name: string; bytesAdded: number; totalBytesSnapshot: number;\n  tensorsAdded: number;\n  totalTensorsSnapshot: number;\n  inputShapes: number[][];\n  outputShapes: number[][];\n  kernelTimeMs: number | {error: string} | Promise<number|{error: string}>;\n  extraInfo: string | Promise<string>;\n};\n\nexport type ProfileInfo = {\n  newBytes: number; newTensors: number; peakBytes: number;\n  kernels: KernelInfo[];\n  result: TensorContainer;\n  kernelNames: string[];\n};\n\nexport interface TimingInfo extends BackendTimingInfo {\n  wallMs: number;\n}\n\n/** @docalias Function */\nexport type ScopeFn<T extends TensorContainer> = () => T;\n\ninterface ScopeState {\n  track: Tensor[];\n  name: string;\n  id: number;\n}\n\ninterface RegisteredKernelInvocation<I extends NamedTensorMap> {\n  kernelName: string;\n  inputs: I;\n  attrs?: NamedAttrMap;\n}\n\ninterface CustomGradKernelInvocation<T extends Tensor|Tensor[],\n                                               I extends NamedTensorMap> {\n  forwardFunc: ForwardFunc<T>;\n  backwardsFunc: (dy: T, saved: Tensor[]) => {\n    [P in keyof I]: () => I[P]\n  };\n  inputs: I;\n  attrs?: NamedAttrMap;\n}\n\nfunction isRegisteredKernelInvocation<T extends Tensor|Tensor[],\n                                                I extends NamedTensorMap>(\n    kernelInvocation: RegisteredKernelInvocation<I>|\n    CustomGradKernelInvocation<T, I>):\n    kernelInvocation is RegisteredKernelInvocation<I> {\n  return (kernelInvocation as RegisteredKernelInvocation<I>).kernelName != null;\n}\n\nclass EngineState {\n  // Public since optimizers will use it.\n  registeredVariables: NamedVariableMap = {};\n\n  nextTapeNodeId = 0;\n  numBytes = 0;\n  numTensors = 0;\n  numStringTensors = 0;\n  numDataBuffers = 0;\n\n  activeTape: TapeNode[];\n  // Number of nested tf.grad() statements when computing higher-order\n  // gradients. E.g. `1` for first-order gradients and `2` for second-order\n  // gradients. Used to track if the tape should be removed after a backprop.\n  gradientDepth = 0;\n  // Number of nested kernel calls. When kernel depth is greater than 1, we turn\n  // off the tape.\n  kernelDepth = 0;\n\n  // Keep Tensors that parallel the tapes.\n  activeScope: ScopeState;\n  scopeStack: ScopeState[] = [];\n  /**\n   * Keeps track of the number of data moves during a kernel execution. We\n   * maintain a stack since kernels can call other kernels, recursively.\n   */\n  numDataMovesStack: number[] = [];\n  nextScopeId = 0;\n\n  tensorInfo = new WeakMap<DataId, {\n    backend: KernelBackend,\n    bytes: number,\n    dtype: DataType,\n    shape: number[]\n  }>();\n\n  profiling = false;\n  activeProfile: ProfileInfo = {\n    newBytes: 0,\n    newTensors: 0,\n    peakBytes: 0,\n    kernels: [],\n    result: null,\n    get kernelNames():\n        string[] {\n          return Array.from(new Set(this.kernels.map(k => k.name)));\n        }\n  };\n\n  dispose() {\n    for (const variableName in this.registeredVariables) {\n      this.registeredVariables[variableName].dispose();\n    }\n  }\n}\n\nexport class Engine implements TensorTracker, DataMover {\n  state: EngineState;\n  backendName: string;\n  registry: {[id: string]: KernelBackend} = {};\n  registryFactory: {\n    [id: string]: {\n      factory: () => KernelBackend | Promise<KernelBackend>,\n      priority: number\n    }\n  } = {};\n\n  private profiler: Profiler;\n  private backendInstance: KernelBackend;\n  private pendingBackendInit: Promise<boolean>;\n  private pendingBackendInitId = 0;\n\n  constructor(public ENV: Environment) {\n    this.state = new EngineState();\n  }\n\n  async ready(): Promise<void> {\n    if (this.pendingBackendInit != null) {\n      return this.pendingBackendInit.then(() => {});\n    }\n    if (this.backendInstance != null) {\n      return;\n    }\n    const sortedBackends = this.getSortedBackends();\n\n    for (let i = 0; i < sortedBackends.length; i++) {\n      const backendName = sortedBackends[i];\n      const success = await this.initializeBackend(backendName).success;\n      if (success) {\n        await this.setBackend(backendName);\n        return;\n      }\n    }\n\n    throw new Error(\n        `Could not initialize any backends, all backend initializations ` +\n        `failed.`);\n  }\n\n  get backend(): KernelBackend {\n    if (this.pendingBackendInit != null) {\n      throw new Error(\n          `Backend '${this.backendName}' has not yet been initialized. Make ` +\n          `sure to await tf.ready() or await tf.setBackend() before calling ` +\n          `other methods`);\n    }\n    if (this.backendInstance == null) {\n      const {name, asyncInit} = this.initializeBackendsAndReturnBest();\n      if (asyncInit) {\n        throw new Error(\n            `The highest priority backend '${name}' has not yet been ` +\n            `initialized. Make sure to await tf.ready() or ` +\n            `await tf.setBackend() before calling other methods`);\n      }\n      this.setBackend(name);\n    }\n    return this.backendInstance;\n  }\n\n  backendNames(): string[] {\n    return Object.keys(this.registryFactory);\n  }\n\n  findBackend(backendName: string): KernelBackend {\n    if (!(backendName in this.registry)) {\n      // If the backend hasn't been initialized but we have a registry entry for\n      // it, initialize it and return it.\n      if (backendName in this.registryFactory) {\n        const {asyncInit} = this.initializeBackend(backendName);\n        if (asyncInit) {\n          // Backend is not ready yet.\n          return null;\n        }\n      } else {\n        return null;\n      }\n    }\n    return this.registry[backendName];\n  }\n\n  findBackendFactory(backendName: string):\n      () => KernelBackend | Promise<KernelBackend> {\n    if (!(backendName in this.registryFactory)) {\n      return null;\n    }\n    return this.registryFactory[backendName].factory;\n  }\n\n  registerBackend(\n      backendName: string,\n      factory: () => KernelBackend | Promise<KernelBackend>,\n      priority = 1): boolean {\n    if (backendName in this.registryFactory) {\n      log.warn(\n          `${backendName} backend was already registered. ` +\n          `Reusing existing backend factory.`);\n      return false;\n    }\n    this.registryFactory[backendName] = {factory, priority};\n    return true;\n  }\n\n  async setBackend(backendName: string): Promise<boolean> {\n    if (this.registryFactory[backendName] == null) {\n      throw new Error(`Backend name '${backendName}' not found in registry`);\n    }\n    this.backendName = backendName;\n    if (this.registry[backendName] == null) {\n      this.backendInstance = null;\n      const {success, asyncInit} = this.initializeBackend(backendName);\n      const result = asyncInit ? await success : success;\n      if (!result) {\n        return false;\n      }\n    }\n    this.backendInstance = this.registry[backendName];\n    this.setupRegisteredKernels();\n    // Reset the profiler.\n    this.profiler = new Profiler(this.backendInstance);\n\n    return true;\n  }\n\n  private setupRegisteredKernels(): void {\n    const kernels = getKernelsForBackend(this.backendName);\n    kernels.forEach(kernel => {\n      if (kernel.setupFunc != null) {\n        kernel.setupFunc(this.backendInstance);\n      }\n    });\n  }\n\n  private disposeRegisteredKernels(backendName: string): void {\n    const kernels = getKernelsForBackend(backendName);\n    kernels.forEach(kernel => {\n      if (kernel.disposeFunc != null) {\n        kernel.disposeFunc(this.registry[backendName]);\n      }\n    });\n  }\n\n  /**\n   * Initializes a backend by looking up the backend name in the factory\n   * registry and calling the factory method. Returns a boolean representing\n   * whether the initialization of the backend suceeded. Throws an error if\n   * there is no backend in the factory registry.\n   */\n  private initializeBackend(backendName: string):\n      {success: boolean|Promise<boolean>, asyncInit: boolean} {\n    const registryFactoryEntry = this.registryFactory[backendName];\n    if (registryFactoryEntry == null) {\n      throw new Error(\n          `Cannot initialize backend ${backendName}, no registration found.`);\n    }\n\n    try {\n      const backend = registryFactoryEntry.factory();\n      /* Test if the factory returns a promise.\n      Done in a more liberal way than\n      previous 'Promise.resolve(backend)===backend'\n      as we needed to account for custom Promise\n      implementations (e.g. Angular) */\n      if (backend && !(backend instanceof KernelBackend) &&\n          typeof backend.then === 'function') {\n        const promiseId = ++this.pendingBackendInitId;\n        const success =\n            backend\n                .then(backendInstance => {\n                  // Outdated promise. Another backend was set in the meantime.\n                  if (promiseId < this.pendingBackendInitId) {\n                    return false;\n                  }\n                  this.registry[backendName] = backendInstance;\n                  this.pendingBackendInit = null;\n                  return true;\n                })\n                .catch(err => {\n                  // Outdated promise. Another backend was set in the meantime.\n                  if (promiseId < this.pendingBackendInitId) {\n                    return false;\n                  }\n                  this.pendingBackendInit = null;\n                  log.warn(`Initialization of backend ${backendName} failed`);\n                  log.warn(err.stack || err.message);\n                  return false;\n                });\n        this.pendingBackendInit = success;\n        return {success, asyncInit: true};\n      } else {\n        this.registry[backendName] = backend as KernelBackend;\n        return {success: true, asyncInit: false};\n      }\n    } catch (err) {\n      log.warn(`Initialization of backend ${backendName} failed`);\n      log.warn(err.stack || err.message);\n      return {success: false, asyncInit: false};\n    }\n  }\n\n  removeBackend(backendName: string): void {\n    if (!(backendName in this.registryFactory)) {\n      throw new Error(`${backendName} backend not found in registry`);\n    }\n    if (this.backendName === backendName && this.pendingBackendInit != null) {\n      // There is a pending promise of the backend we want to remove. Make it\n      // obsolete.\n      this.pendingBackendInitId++;\n    }\n\n    if (backendName in this.registry) {\n      this.disposeRegisteredKernels(backendName);\n      this.registry[backendName].dispose();\n      delete this.registry[backendName];\n    }\n\n    delete this.registryFactory[backendName];\n\n    // Unset the backend if it is active.\n    if (this.backendName === backendName) {\n      this.pendingBackendInit = null;\n      this.backendName = null;\n      this.backendInstance = null;\n    }\n  }\n\n  private getSortedBackends(): string[] {\n    if (Object.keys(this.registryFactory).length === 0) {\n      throw new Error('No backend found in registry.');\n    }\n    return Object.keys(this.registryFactory).sort((a: string, b: string) => {\n      // Highest priority comes first.\n      return this.registryFactory[b].priority -\n          this.registryFactory[a].priority;\n    });\n  }\n\n  private initializeBackendsAndReturnBest():\n      {name: string, asyncInit: boolean} {\n    const sortedBackends = this.getSortedBackends();\n\n    for (let i = 0; i < sortedBackends.length; i++) {\n      const backendName = sortedBackends[i];\n      const {success, asyncInit} = this.initializeBackend(backendName);\n      if (asyncInit || success) {\n        return {name: backendName, asyncInit};\n      }\n    }\n    throw new Error(\n        `Could not initialize any backends, all backend initializations ` +\n        `failed.`);\n  }\n\n  moveData(backend: KernelBackend, dataId: DataId) {\n    const info = this.state.tensorInfo.get(dataId);\n    const srcBackend = info.backend;\n    const values = this.readSync(dataId);\n    const refCount = srcBackend.refCount(dataId);\n    // Delete the tensor from the old backend and move it to the new\n    // backend.\n    srcBackend.disposeData(dataId, true);\n    info.backend = backend;\n    backend.move(dataId, values, info.shape, info.dtype, refCount);\n    if (this.shouldCheckForMemLeaks()) {\n      // Track the number of moves during a kernel execution to correctly\n      // detect memory leaks.\n      this.state.numDataMovesStack[this.state.numDataMovesStack.length - 1]++;\n    }\n  }\n\n  tidy<T extends TensorContainer>(nameOrFn: string|ScopeFn<T>, fn?: ScopeFn<T>):\n      T {\n    let name: string = null;\n    if (fn == null) {\n      // Called with only 1 argument.\n      if (typeof nameOrFn !== 'function') {\n        throw new Error('Please provide a function to tidy()');\n      }\n      fn = nameOrFn;\n    } else {\n      // Called with 2 arguments.\n      if (typeof nameOrFn !== 'string' && !(nameOrFn instanceof String)) {\n        throw new Error(\n            'When calling with two arguments, the first argument ' +\n            'to tidy() must be a string');\n      }\n      if (typeof fn !== 'function') {\n        throw new Error(\n            'When calling with two arguments, the 2nd argument ' +\n            'to tidy() must be a function');\n      }\n      name = nameOrFn as string;\n      // TODO(nsthorat,smilkov): Do operation logging and performance\n      // profiling.\n    }\n    let result: T;\n    return this.scopedRun(\n        () => this.startScope(name), () => this.endScope(result), () => {\n          result = fn();\n          if (result instanceof Promise) {\n            console.error('Cannot return a Promise inside of tidy.');\n          }\n          return result;\n        });\n  }\n\n  private scopedRun<T>(start: () => void, end: () => void, f: () => T): T {\n    start();\n    try {\n      const res = f();\n      end();\n      return res;\n    } catch (ex) {\n      end();\n      throw ex;\n    }\n  }\n\n  private static nextTensorId = 0;\n  private nextTensorId(): number {\n    return Engine.nextTensorId++;\n  }\n\n  private static nextVariableId = 0;\n  private nextVariableId(): number {\n    return Engine.nextVariableId++;\n  }\n\n  /**\n   * This method is called instead of the public-facing tensor.clone() when\n   * saving a tensor for backwards pass. It makes sure to add the clone\n   * operation to the tape regardless of being called inside a kernel\n   * execution.\n   */\n  private clone(x: Tensor): Tensor {\n    const y: Tensor = ENGINE.runKernel(Identity,\n                                       {x} as unknown as NamedTensorMap);\n    const inputs = {x};\n    const grad = (dy: Tensor) => ({\n      x: () => {\n        const dtype = 'float32';\n        const gradInputs = {x: dy};\n        const attrs = {dtype};\n\n        return ENGINE.runKernel(\n                   Cast, gradInputs as unknown as NamedTensorMap,\n                   // tslint:disable-next-line: no-unnecessary-type-assertion\n                   attrs as unknown as NamedAttrMap) as Tensor;\n      }\n    });\n    const saved: Tensor[] = [];\n    this.addTapeNode(this.state.activeScope.name, inputs, [y], grad, saved, {});\n    return y;\n  }\n\n  /**\n   * Execute a kernel with the given name and return the output tensor.\n   *\n   * @param kernelName The name of the kernel to execute.\n   * @param inputs A map of input names to tensors.\n   * @param attrs A map of attribute names to their values. An attribute is a\n   *     primitive (non-tensor) input to the kernel.\n   * @param inputsToSave A list of tensors, inputs to save for the backprop\n   *     computation.\n   * @param outputsToSave A list of booleans, specifying which output to save\n   *     for the backprop computation. These are booleans since the output\n   * tensors are not visible to the user.\n   */\n  runKernel<T extends Tensor|Tensor[]>(\n      kernelName: string, inputs: NamedTensorMap, attrs?: NamedAttrMap): T {\n    if (this.backendName == null) {\n      // backend has not been initialized yet (backend initialization is lazy\n      // can be deferred until an op/ kernel is run).\n      // The below getter has side effects that will try to initialize the\n      // backend and set properties like this.backendName\n      // tslint:disable-next-line: no-unused-expression\n      this.backend;\n    }\n    const hasKernel = getKernel(kernelName, this.backendName) != null;\n    if (!hasKernel) {\n      throw new Error(`Kernel '${kernelName}' not registered for backend '${\n          this.backendName}'`);\n    }\n    return this.runKernelFunc({kernelName, inputs, attrs});\n  }\n\n  private shouldCheckForMemLeaks(): boolean {\n    return this.ENV.getBool('IS_TEST');\n  }\n\n  private checkKernelForMemLeak(\n      kernelName: string, numDataIdsBefore: number,\n      outInfos: TensorInfo[]): void {\n    const numDataIdsAfter = this.backend.numDataIds();\n\n    // Count the number of data ids associated with the result of the kernel.\n    let numOutputDataIds = 0;\n    outInfos.forEach(info => {\n      // Complex numbers allocate 3 data ids, one for 'real', one for\n      // 'imaginary', and one for the container that holds the former two.\n      numOutputDataIds += (info.dtype === 'complex64' ? 3 : 1);\n    });\n\n    // Account for the number of moves during kernel execution. A \"data move\"\n    // can happen in the middle of a kernel execution, placing a new (key,value)\n    // pair in the data storage. Since data moves have net zero effect (we\n    // always remove the data from the old backend), we have to cancel them out\n    // when detecting memory leaks.\n    const numMoves =\n        this.state.numDataMovesStack[this.state.numDataMovesStack.length - 1];\n    const dataIdsLeaked =\n        numDataIdsAfter - numDataIdsBefore - numOutputDataIds - numMoves;\n    if (dataIdsLeaked > 0) {\n      throw new Error(\n          `Backend '${this.backendName}' has an internal memory leak ` +\n          `(${dataIdsLeaked} data ids) after running '${kernelName}'`);\n    }\n  }\n\n  /**\n   * Internal helper method to execute a kernel Func\n   *\n   * Use `runKernel` to execute kernels from outside of engine.\n   */\n  private runKernelFunc<T extends Tensor|Tensor[], I extends NamedTensorMap>(\n      kernelParams: RegisteredKernelInvocation<I>|\n      CustomGradKernelInvocation<T, I>): T {\n    let outputs: Tensor[];\n    let saved: Tensor[] = [];\n    const isTapeOn = this.isTapeOn();\n\n    const startingBytecount = this.state.numBytes;\n    const startingNumTensors = this.state.numTensors;\n\n    if (this.shouldCheckForMemLeaks()) {\n      this.state.numDataMovesStack.push(0);\n    }\n\n    let kernelFunc: () => Tensor[];\n    if (this.backendName == null) {\n      // backend has not been initialized yet (backend initialization is lazy\n      // can be deferred until an op/ kernel is run).\n      // The below getter has side effects that will try to initialize the\n      // backend and set properties like this.backendName\n      // tslint:disable-next-line: no-unused-expression\n      this.backend;\n    }\n\n    let out: TensorInfo|TensorInfo[];\n\n    const kernelOrScopeName = isRegisteredKernelInvocation(kernelParams) ?\n        kernelParams.kernelName :\n        this.state.activeScope != null ? this.state.activeScope.name : '';\n\n    // Create the kernelFunc from either a registered kernel OR passed in\n    // forward/backward functions (used by custom grad). In this context a\n    // kernelFunc wraps a kernel implementation with some bookkeeping.\n\n    if (isRegisteredKernelInvocation(kernelParams)) {\n      const {kernelName, inputs, attrs} = kernelParams;\n      if (this.backendName == null) {\n        // backend has not been initialized yet (backend initialization is lazy\n        // can be deferred until an op/ kernel is run).\n        // The below getter has side effects that will try to initialize the\n        // backend and set properties like this.backendName\n        // tslint:disable-next-line: no-unused-expression\n        this.backend;\n      }\n      const kernel = getKernel(kernelName, this.backendName);\n      util.assert(\n          kernel != null,\n          () => `Cannot find registered kernel '${kernelName}' for backend '${\n              this.backendName}'`);\n\n      kernelFunc = () => {\n        const numDataIdsBefore = this.backend.numDataIds();\n        out = kernel.kernelFunc({inputs, attrs, backend: this.backend});\n        const outInfos = Array.isArray(out) ? out : [out];\n        if (this.shouldCheckForMemLeaks()) {\n          this.checkKernelForMemLeak(kernelName, numDataIdsBefore, outInfos);\n        }\n\n        const outTensors = outInfos.map((outInfo: TensorInfo|Tensor) => {\n          // todo (yassogba) remove this option (Tensor) when node backend\n          // methods have been modularized and they all return tensorInfo.\n          // TensorInfos do not have a rank attribute.\n          if ((outInfo as Tensor).rank != null) {\n            return outInfo as Tensor;\n          }\n          return this.makeTensorFromTensorInfo(outInfo);\n        });\n\n        // Save any required inputs and outputs.\n\n        // Do not save unless we are recording to the tape. Otherwise it would\n        // cause a mem leak since there would be no backprop for these tensors\n        // (which would otherwise dispose them).\n        if (isTapeOn) {\n          const tensorsToSave =\n              this.getTensorsForGradient(kernelName, inputs, outTensors);\n          saved = this.saveTensorsForBackwardMode(tensorsToSave);\n        }\n        return outTensors;\n      };\n    } else {\n      const {forwardFunc} = kernelParams;\n      // Running a customGrad op.\n      const saveFunc: GradSaveFunc = (tensors) => {\n        // Do not save unless we are recording to the tape. Otherwise it would\n        // cause a mem leak since we would never run backprop, which disposes\n        // the kept tensors.\n        if (!isTapeOn) {\n          return;\n        }\n        saved = tensors.map(tensor => this.keep(this.clone(tensor)));\n      };\n\n      kernelFunc = () => {\n        const numDataIdsBefore = this.backend.numDataIds();\n        out = this.tidy(() => forwardFunc(this.backend, saveFunc));\n        const outs = (Array.isArray(out) ? out : [out]) as Tensor[];\n        if (this.shouldCheckForMemLeaks()) {\n          // Scope name is used to print a more helpful error message if needed.\n          this.checkKernelForMemLeak(kernelOrScopeName, numDataIdsBefore, outs);\n        }\n        return outs;\n      };\n    }\n\n    //\n    // Run the kernelFunc. Optionally profiling it.\n    //\n    const {inputs, attrs} = kernelParams;\n    const backwardsFunc = isRegisteredKernelInvocation(kernelParams) ?\n        null :\n        kernelParams.backwardsFunc;\n\n    let kernelProfile: KernelProfile;\n    this.scopedRun(\n        // Stop recording to a tape when running a kernel.\n        () => this.state.kernelDepth++, () => this.state.kernelDepth--, () => {\n          if (!this.ENV.getBool('DEBUG') && !this.state.profiling) {\n            outputs = kernelFunc();\n          } else {\n            kernelProfile = this.profiler.profileKernel(\n                kernelOrScopeName, inputs, () => kernelFunc());\n            if (this.ENV.getBool('DEBUG')) {\n              this.profiler.logKernelProfile(kernelProfile);\n            }\n            outputs = kernelProfile.outputs;\n          }\n        });\n\n    if (isTapeOn) {\n      this.addTapeNode(\n          kernelOrScopeName, inputs, outputs, backwardsFunc, saved, attrs);\n    }\n\n    if (this.state.profiling) {\n      this.state.activeProfile.kernels.push({\n        name: kernelOrScopeName,\n        bytesAdded: this.state.numBytes - startingBytecount,\n        totalBytesSnapshot: this.state.numBytes,\n        tensorsAdded: this.state.numTensors - startingNumTensors,\n        totalTensorsSnapshot: this.state.numTensors,\n        inputShapes: Object.keys(inputs).map(\n            key => inputs[key] != null ? inputs[key].shape : null),\n        outputShapes: outputs.map(item => item.shape),\n        kernelTimeMs: kernelProfile.timeMs,\n        extraInfo: kernelProfile.extraInfo\n      });\n    }\n    return (Array.isArray(out) ? outputs : outputs[0]) as T;\n  }\n\n  /**\n   * Saves tensors used in forward mode for use in backward mode.\n   *\n   * @param tensors the list of tensors to save.\n   */\n  private saveTensorsForBackwardMode(tensors: Tensor[]): Tensor[] {\n    const saved = tensors.map(tensor => this.keep(this.clone(tensor)));\n    return saved;\n  }\n\n  /**\n   * Returns a list of tensors to save for a given gradient calculation.\n   *\n   * @param kernelName name of kernel to look up gradient for.\n   * @param inputs a map of input tensors.\n   * @param outputs an array of output tensors from forward mode of kernel.\n   */\n  private getTensorsForGradient(\n      kernelName: string, inputs: NamedTensorMap,\n      outputs: Tensor[]): Tensor[]|null {\n    const gradConfig = getGradient(kernelName);\n    if (gradConfig != null) {\n      const inputsToSave: string[] = gradConfig.inputsToSave || [];\n      const outputsToSave: boolean[] = gradConfig.outputsToSave || [];\n\n      // If saveAllInputs is true, all inputs will be saved. Otherwise, inputs\n      // specified in inputsToSave will be saved.\n      let inputTensorsToSave: Tensor[];\n      if (gradConfig.saveAllInputs) {\n        util.assert(\n            Array.isArray(inputs),\n            () => 'saveAllInputs is true, expected inputs to be an array.');\n\n        inputTensorsToSave = Object.keys(inputs).map((key) => inputs[key]);\n      } else {\n        inputTensorsToSave = inputsToSave.map((inputName) => inputs[inputName]);\n      }\n\n      const outputTensorsToSave: Tensor[] =\n          outputs.filter((_, i) => outputsToSave[i]);\n\n      return inputTensorsToSave.concat(outputTensorsToSave);\n    }\n    // We return an empty list rather than throw an error because the kernel we\n    // are looking up may not actually be relevant to backproping through the\n    // overall function\n    //\n    // See 'does not error if irrelevant (pruned) ops are missing grads' test\n    // in gradients_test.ts for an example.\n    return [];\n  }\n\n  /**\n   * Internal method used by public APIs for tensor creation. Makes a new\n   * tensor with the provided shape, dtype and values. It always\n   * creates a new data id and writes the values to the underlying backend.\n   */\n  makeTensor(\n      values: DataValues, shape: number[], dtype: DataType,\n      backend?: KernelBackend): Tensor {\n    if (values == null) {\n      throw new Error('Values passed to engine.makeTensor() are null');\n    }\n    dtype = dtype || 'float32';\n    backend = backend || this.backend;\n    let backendVals = values as BackendValues;\n    if (dtype === 'string' && util.isString(values[0])) {\n      backendVals = (values as string[]).map(d => util.encodeString(d));\n    }\n    const dataId = backend.write(backendVals, shape, dtype);\n    const t = new Tensor(shape, dtype, dataId, this.nextTensorId());\n    this.trackTensor(t, backend);\n\n    // Count bytes for string tensors.\n    if (dtype === 'string') {\n      const info = this.state.tensorInfo.get(dataId);\n      const newBytes = bytesFromStringArray(backendVals as Uint8Array[]);\n      this.state.numBytes += newBytes - info.bytes;\n      info.bytes = newBytes;\n    }\n    return t;\n  }\n\n  /**\n   * Internal method used by backends. Makes a new tensor\n   * that is a wrapper around an existing data id. It doesn't create\n   * a new data id, only increments the ref count used in memory tracking.\n   * @deprecated\n   */\n  makeTensorFromDataId(\n    dataId: DataId, shape: number[], dtype: DataType,\n    backend?: KernelBackend): Tensor {\n    dtype = dtype || 'float32';\n    const tensorInfo: TensorInfo = {dataId, shape, dtype};\n    return this.makeTensorFromTensorInfo(tensorInfo, backend);\n  }\n\n  /**\n   * Internal method used by backends. Makes a new tensor that is a wrapper\n   * around an existing data id in TensorInfo. It doesn't create a new data id,\n   * only increments the ref count used in memory tracking.\n   */\n  makeTensorFromTensorInfo(tensorInfo: TensorInfo, backend?: KernelBackend):\n      Tensor {\n    const {dataId, shape, dtype} = tensorInfo;\n    const t = new Tensor(shape, dtype, dataId, this.nextTensorId());\n    this.trackTensor(t, backend);\n    return t;\n  }\n\n  makeVariable(\n      initialValue: Tensor, trainable = true, name?: string,\n      dtype?: DataType): Variable {\n    name = name || this.nextVariableId().toString();\n    if (dtype != null && dtype !== initialValue.dtype) {\n      initialValue = initialValue.cast(dtype);\n    }\n    const v = new Variable(initialValue, trainable, name, this.nextTensorId());\n    if (this.state.registeredVariables[v.name] != null) {\n      throw new Error(`Variable with name ${v.name} was already registered`);\n    }\n    this.state.registeredVariables[v.name] = v;\n    this.incRef(v, this.backend);\n    return v;\n  }\n\n  trackTensor(a: Tensor, backend: KernelBackend): void {\n    this.state.numTensors++;\n    if (a.dtype === 'string') {\n      this.state.numStringTensors++;\n    }\n    // Bytes for complex numbers are counted by their components. Bytes for\n    // string tensors are counted when writing values.\n    let bytes = 0;\n    if (a.dtype !== 'complex64' && a.dtype !== 'string') {\n      bytes = a.size * util.bytesPerElement(a.dtype);\n    }\n    this.state.numBytes += bytes;\n\n    if (!this.state.tensorInfo.has(a.dataId)) {\n      this.state.numDataBuffers++;\n      this.state.tensorInfo.set(a.dataId, {\n        backend: backend || this.backend,\n        dtype: a.dtype,\n        shape: a.shape,\n        bytes\n      });\n    }\n\n    if (!(a instanceof Variable)) {\n      this.track(a);\n    }\n  }\n\n  // Track the tensor by dataId and increase the refCount for the dataId in the\n  // backend.\n  // TODO(pyu10055): This is currently used by makeVariable method, to increase\n  // refCount on the backend for the dataId. It can potentially be replaced with\n  // Identity op indead of calling backend directly.\n  incRef(a: Tensor, backend: KernelBackend): void {\n    this.trackTensor(a, backend);\n    this.backend.incRef(a.dataId);\n  }\n\n  removeDataId(dataId: DataId, backend: KernelBackend) {\n    if (this.state.tensorInfo.has(dataId) &&\n        this.state.tensorInfo.get(dataId).backend === backend) {\n      this.state.tensorInfo.delete(dataId);\n      this.state.numDataBuffers--;\n    }\n  }\n  disposeTensor(a: Tensor): void {\n    if (!this.state.tensorInfo.has(a.dataId)) {\n      return;\n    }\n    const info = this.state.tensorInfo.get(a.dataId);\n\n    this.state.numTensors--;\n    if (a.dtype === 'string') {\n      this.state.numStringTensors--;\n      this.state.numBytes -= info.bytes;\n    }\n    // Don't count bytes for complex numbers as they are counted by their\n    // components.\n    if (a.dtype !== 'complex64' && a.dtype !== 'string') {\n      const bytes = a.size * util.bytesPerElement(a.dtype);\n      this.state.numBytes -= bytes;\n    }\n\n    // Remove the reference to dataId if backend dispose the data successfully\n    if (info.backend.disposeData(a.dataId)) {\n      this.removeDataId(a.dataId, info.backend);\n    }\n\n    // TODO(nsthorat): Construct an error and save the stack trace for\n    // debugging when in debug mode. Creating a stack trace is too expensive\n    // to do unconditionally.\n  }\n\n  disposeVariables(): void {\n    for (const varName in this.state.registeredVariables) {\n      const v = this.state.registeredVariables[varName];\n      this.disposeVariable(v);\n    }\n  }\n\n  disposeVariable(v: Variable): void {\n    this.disposeTensor(v);\n    if (this.state.registeredVariables[v.name] != null) {\n      delete this.state.registeredVariables[v.name];\n    }\n  }\n\n  memory(): MemoryInfo {\n    const info = this.backend.memory() as MemoryInfo;\n    info.numTensors = this.state.numTensors;\n    info.numDataBuffers = this.state.numDataBuffers;\n    info.numBytes = this.state.numBytes;\n    if (this.state.numStringTensors > 0) {\n      info.unreliable = true;\n      if (info.reasons == null) {\n        info.reasons = [];\n      }\n      info.reasons.push(\n          'Memory usage by string tensors is approximate ' +\n          '(2 bytes per character)');\n    }\n    return info;\n  }\n\n  async profile(query: () => (TensorContainer | Promise<TensorContainer>)):\n      Promise<ProfileInfo> {\n    this.state.profiling = true;\n\n    const startBytes = this.state.numBytes;\n    const startNumTensors = this.state.numTensors;\n\n    this.state.activeProfile.kernels = [];\n    this.state.activeProfile.result = await query();\n\n    this.state.profiling = false;\n\n    this.state.activeProfile.peakBytes = Math.max(\n        ...this.state.activeProfile.kernels.map(d => d.totalBytesSnapshot));\n    this.state.activeProfile.newBytes = this.state.numBytes - startBytes;\n    this.state.activeProfile.newTensors =\n        this.state.numTensors - startNumTensors;\n    for (const kernel of this.state.activeProfile.kernels) {\n      kernel.kernelTimeMs = await kernel.kernelTimeMs;\n      kernel.extraInfo = await kernel.extraInfo;\n    }\n    return this.state.activeProfile;\n  }\n\n  isTapeOn(): boolean {\n    return this.state.gradientDepth > 0 && this.state.kernelDepth === 0;\n  }\n\n  private addTapeNode(\n      kernelName: string, inputs: NamedTensorMap, outputs: Tensor[],\n      gradientsFunc: GradFunc, saved: Tensor[], attrs: NamedAttrMap): void {\n    const tapeNode: TapeNode =\n        {id: this.state.nextTapeNodeId++, kernelName, inputs, outputs, saved};\n\n    const gradConfig = getGradient(kernelName);\n    if (gradConfig != null) {\n      gradientsFunc = gradConfig.gradFunc;\n    }\n    if (gradientsFunc != null) {\n      tapeNode.gradient = (dys: Tensor[]) => {\n        // TODO(smilkov): To optimize back-prop, pass dys that are not used in\n        // the backprop graph to the user as null instead of zeros\n        dys = dys.map((dy, i) => {\n          if (dy == null) {\n            const output = outputs[i];\n            const vals = util.makeZerosTypedArray(output.size, output.dtype);\n            return this.makeTensor(vals, output.shape, output.dtype);\n          }\n          return dy;\n        });\n        // Grad functions of ops with single outputs expect a dy, while ops\n        // with multiple outputs expect dys (array of dy).\n        return gradientsFunc(dys.length > 1 ? dys : dys[0], saved, attrs);\n      };\n    }\n    this.state.activeTape.push(tapeNode);\n  }\n\n  keep<T extends Tensor>(result: T): T {\n    result.kept = true;\n    return result;\n  }\n\n  private startTape() {\n    if (this.state.gradientDepth === 0) {\n      this.state.activeTape = [];\n    }\n    this.state.gradientDepth++;\n  }\n\n  private endTape() {\n    this.state.gradientDepth--;\n  }\n\n  /**\n   * Start a scope. Use this with endScope() to achieve the same functionality\n   * as scope() without the need for a function closure.\n   */\n  startScope(name?: string) {\n    const scopeInfo: ScopeState = {\n      track: [],\n      name: 'unnamed scope',\n      id: this.state.nextScopeId++\n    };\n    if (name) {\n      scopeInfo.name = name;\n    }\n    this.state.scopeStack.push(scopeInfo);\n    this.state.activeScope = scopeInfo;\n  }\n\n  /**\n   * End a scope. Use this with startScope() to achieve the same functionality\n   * as scope() without the need for a function closure.\n   */\n  endScope(result?: TensorContainer) {\n    const tensorsToTrackInParent = getTensorsInContainer(result);\n    const tensorsToTrackInParentSet =\n        new Set(tensorsToTrackInParent.map(t => t.id));\n\n    // Dispose the arrays tracked in this scope.\n    for (let i = 0; i < this.state.activeScope.track.length; i++) {\n      const tensor = this.state.activeScope.track[i];\n      if (!tensor.kept && !tensorsToTrackInParentSet.has(tensor.id)) {\n        tensor.dispose();\n      }\n    }\n\n    const oldScope = this.state.scopeStack.pop();\n    this.state.activeScope = this.state.scopeStack.length === 0 ?\n        null :\n        this.state.scopeStack[this.state.scopeStack.length - 1];\n\n    // Track the current result in the parent scope.\n    tensorsToTrackInParent.forEach(tensor => {\n      // Only track the tensor if was allocated in the inner scope and is not\n      // globally kept.\n      if (!tensor.kept && tensor.scopeId === oldScope.id) {\n        this.track(tensor);\n      }\n    });\n  }\n\n  /**\n   * Returns gradients of `f` with respect to each of the `xs`. The gradients\n   * returned are of the same length as `xs`, but some might be null if `f`\n   * was not a function of that `x`. It also takes optional dy to multiply the\n   * gradient, which defaults to `1`.\n   */\n  gradients<T extends Tensor>(\n      f: () => T, xs: Tensor[], dy?: T,\n      allowNoGradients = false): {value: T, grads: Tensor[]} {\n    util.assert(\n        xs.length > 0, () => 'gradients() received an empty list of xs.');\n    if (dy != null && dy.dtype !== 'float32') {\n      throw new Error(`dy must have 'float32' dtype, but has '${dy.dtype}'`);\n    }\n\n    const y = this.scopedRun(\n        () => this.startTape(), () => this.endTape(),\n        () => this.tidy('forward', f));\n\n    util.assert(\n        y instanceof Tensor,\n        () => 'The result y returned by f() must be a tensor.');\n    // Filter out the nodes that don't connect x => y.\n    const filteredTape = getFilteredNodesXToY(this.state.activeTape, xs, y);\n    if (!allowNoGradients && filteredTape.length === 0 && xs.length > 0) {\n      throw new Error(\n          'Cannot compute gradient of y=f(x) with respect to x. Make sure ' +\n          'that the f you passed encloses all operations that lead from x ' +\n          'to y.');\n    }\n\n    return this.tidy('backward', () => {\n      const accumulatedGradientMap: {[tensorId: number]: Tensor} = {};\n      accumulatedGradientMap[y.id] = (dy == null) ? ones(y.shape) : dy;\n\n      // Backprop gradients through the filtered nodes.\n      backpropagateGradients(\n          accumulatedGradientMap, filteredTape,\n          // Pass the tidy function to avoid circular dep with `tape.ts`.\n          f => this.tidy(f as ScopeFn<Tensor>),\n          // Pass an add function to avoide a circular dep with `tape.ts`.\n          add);\n      const grads = xs.map(x => accumulatedGradientMap[x.id]);\n\n      if (this.state.gradientDepth === 0) {\n        // This means that we are not computing higher-order gradients\n        // and can clean up the tape.\n        this.state.activeTape.forEach(node => {\n          for (const tensor of node.saved) {\n            tensor.dispose();\n          }\n        });\n        this.state.activeTape = null;\n      }\n      return {value: y, grads};\n    });\n  }\n\n  customGrad<T extends Tensor>(f: CustomGradientFunc<T>):\n      (...args: Array<Tensor|GradSaveFunc>) => T {\n    util.assert(\n        util.isFunction(f),\n        () => 'The f passed in customGrad(f) must be a function.');\n    return (...inputs: Tensor[]): T => {\n      util.assert(\n          inputs.every(t => t instanceof Tensor),\n          () => 'The args passed in customGrad(f)(x1, x2,...) must all be ' +\n              'tensors');\n\n      let res: {\n        value: T,\n        gradFunc: (dy: T, saved: Tensor[]) => Tensor | Tensor[],\n      };\n      const inputMap: NamedTensorMap = {};\n      inputs.forEach((input, i) => {\n        inputMap[i] = input;\n      });\n\n      const forwardFunc: ForwardFunc<T> = (_, save) => {\n        res = f(...[...inputs, save]);\n        util.assert(\n            res.value instanceof Tensor,\n            () => 'The function f passed in customGrad(f) must return an ' +\n                'object where `obj.value` is a tensor');\n        util.assert(\n            util.isFunction(res.gradFunc),\n            () => 'The function f passed in customGrad(f) must return an ' +\n                'object where `obj.gradFunc` is a function.');\n        return res.value;\n      };\n\n      const backwardsFunc = (dy: T, saved: Tensor[]) => {\n        const gradRes = res.gradFunc(dy, saved);\n        const grads: Tensor[] = Array.isArray(gradRes) ? gradRes : [gradRes];\n        util.assert(\n            grads.length === inputs.length,\n            () => 'The function f passed in customGrad(f) must return an ' +\n                'object where `obj.gradFunc` is a function that returns ' +\n                'the same number of tensors as inputs passed to f(...).');\n        util.assert(\n            grads.every(t => t instanceof Tensor),\n            () => 'The function f passed in customGrad(f) must return an ' +\n                'object where `obj.gradFunc` is a function that returns ' +\n                'a list of only tensors.');\n        const gradMap: {[key: string]: () => Tensor} = {};\n        grads.forEach((grad, i) => {\n          gradMap[i] = () => grad;\n        });\n        return gradMap;\n      };\n\n      return this.runKernelFunc({\n        forwardFunc,\n        backwardsFunc,\n        inputs: inputMap,\n      });\n    };\n  }\n\n  readSync(dataId: DataId): BackendValues {\n    // Route the read to the correct backend.\n    const info = this.state.tensorInfo.get(dataId);\n    return info.backend.readSync(dataId);\n  }\n  read(dataId: DataId): Promise<BackendValues> {\n    // Route the read to the correct backend.\n    const info = this.state.tensorInfo.get(dataId);\n    return info.backend.read(dataId);\n  }\n\n  readToGPU(dataId: DataId, options?: DataToGPUOptions): GPUData {\n    // Route the read to the correct backend.\n    const info = this.state.tensorInfo.get(dataId);\n    return info.backend.readToGPU(dataId, options);\n  }\n\n  async time(query: () => void): Promise<TimingInfo> {\n    const start = now();\n    const timingInfo = await this.backend.time(query) as TimingInfo;\n    timingInfo.wallMs = now() - start;\n    return timingInfo;\n  }\n\n  /**\n   * Tracks a Tensor in the current scope to be automatically cleaned up\n   * when the current scope ends, and returns the value.\n   *\n   * @param result The Tensor to track in the current scope.\n   */\n  private track<T extends Tensor>(result: T): T {\n    if (this.state.activeScope != null) {\n      result.scopeId = this.state.activeScope.id;\n      this.state.activeScope.track.push(result);\n    }\n\n    return result;\n  }\n\n  get registeredVariables(): NamedVariableMap {\n    return this.state.registeredVariables;\n  }\n\n  /**\n   * Resets the engine state. Removes all backends but does not remove\n   * registered backend factories.\n   */\n  reset(): void {\n    // Make any pending promise obsolete.\n    this.pendingBackendInitId++;\n\n    this.state.dispose();\n    this.ENV.reset();\n    this.state = new EngineState();\n\n    for (const backendName in this.registry) {\n      this.disposeRegisteredKernels(backendName);\n      this.registry[backendName].dispose();\n      delete this.registry[backendName];\n    }\n    this.backendName = null;\n    this.backendInstance = null;\n    this.pendingBackendInit = null;\n  }\n}\n\nfunction ones(shape: number[]): Tensor {\n  const values = makeOnesTypedArray(sizeFromShape(shape), 'float32');\n  return ENGINE.makeTensor(values, shape, 'float32');\n}\n\nexport function getOrMakeEngine(): Engine {\n  const ns = getGlobalNamespace() as unknown as {_tfengine: Engine};\n  if (ns._tfengine == null) {\n    const environment = new Environment(ns);\n    ns._tfengine = new Engine(environment);\n  }\n  setEnvironmentGlobal(ns._tfengine.ENV);\n\n  // Tell the current tensor interface that the global engine is responsible\n  // for tracking.\n  setTensorTracker(() => ns._tfengine);\n  return ns._tfengine;\n}\n\nexport const ENGINE = getOrMakeEngine();\n\n/**\n * A implementation of the add op for use within engine and tape.\n *\n * This allows us to avoid a circular dependency between add.ts and engine.\n * It is exported to be available in tape tests.\n */\nexport function add(a: Tensor, b: Tensor): Tensor {\n  // We duplicate Add here to avoid a circular dependency with add.ts.\n  const inputs = {a, b};\n  return ENGINE.runKernel(Add, inputs as unknown as NamedTensorMap);\n}\n"],"mappings":";;;;;;AAAA;;;;;;;;;;;;;;;;AAiBA,SAAsCA,aAAa,QAAO,oBAAoB;AAC9E,SAAQC,WAAW,EAAEC,oBAAoB,QAAO,eAAe;AAC/D,SAAQC,kBAAkB,QAAO,eAAe;AAChD,SAAQC,GAAG,EAAEC,IAAI,EAAEC,QAAQ,QAAO,gBAAgB;AAClD,SAASC,WAAW,EAAEC,SAAS,EAAEC,oBAAoB,QAAgC,mBAAmB;AAExG,OAAO,KAAKC,GAAG,MAAM,OAAO;AAC5B,SAAuBC,QAAQ,QAAO,YAAY;AAClD,SAAQC,sBAAsB,EAAEC,oBAAoB,QAAiB,QAAQ;AAC7E,SAAmCC,gBAAgB,EAAEC,MAAM,EAAiBC,QAAQ,QAAO,UAAU;AAGrG,SAAQC,qBAAqB,QAAO,eAAe;AAEnD,OAAO,KAAKC,IAAI,MAAM,QAAQ;AAC9B,SAAQC,oBAAoB,EAAEC,kBAAkB,EAAEC,GAAG,EAAEC,aAAa,QAAO,QAAQ;AAuEnF,SAASC,4BAA4BA,CAEjCC,gBACgC;EAElC,OAAQA,gBAAkD,CAACC,UAAU,IAAI,IAAI;AAC/E;AAAC,IAEKC,WAAW;EAAjB,SAAAA,YAAA;IAAAC,eAAA,OAAAD,WAAA;IACE;IACA,KAAAE,mBAAmB,GAAqB,EAAE;IAE1C,KAAAC,cAAc,GAAG,CAAC;IAClB,KAAAC,QAAQ,GAAG,CAAC;IACZ,KAAAC,UAAU,GAAG,CAAC;IACd,KAAAC,gBAAgB,GAAG,CAAC;IACpB,KAAAC,cAAc,GAAG,CAAC;IAGlB;IACA;IACA;IACA,KAAAC,aAAa,GAAG,CAAC;IACjB;IACA;IACA,KAAAC,WAAW,GAAG,CAAC;IAIf,KAAAC,UAAU,GAAiB,EAAE;IAC7B;;;;IAIA,KAAAC,iBAAiB,GAAa,EAAE;IAChC,KAAAC,WAAW,GAAG,CAAC;IAEf,KAAAC,UAAU,GAAG,IAAIC,OAAO,EAKpB;IAEJ,KAAAC,SAAS,GAAG,KAAK;IACjB,KAAAC,aAAa,GAAgB;MAC3BC,QAAQ,EAAE,CAAC;MACXC,UAAU,EAAE,CAAC;MACbC,SAAS,EAAE,CAAC;MACZC,OAAO,EAAE,EAAE;MACXC,MAAM,EAAE,IAAI;MACZ,IAAIC,WAAWA,CAAA;QAET,OAAOC,KAAK,CAACC,IAAI,CAAC,IAAIC,GAAG,CAAC,IAAI,CAACL,OAAO,CAACM,GAAG,CAAC,UAAAC,CAAC;UAAA,OAAIA,CAAC,CAACC,IAAI;QAAA,EAAC,CAAC,CAAC;MAC3D;KACL;EAOH;EAACC,YAAA,CAAA7B,WAAA;IAAA8B,GAAA;IAAAC,KAAA,EALC,SAAAC,QAAA,EAAO;MACL,KAAK,IAAMC,YAAY,IAAI,IAAI,CAAC/B,mBAAmB,EAAE;QACnD,IAAI,CAACA,mBAAmB,CAAC+B,YAAY,CAAC,CAACD,OAAO,EAAE;;IAEpD;EAAC;EAAA,OAAAhC,WAAA;AAAA;AAGH,WAAakC,MAAM;EAgBjB,SAAAA,OAAmBC,GAAgB;IAAAlC,eAAA,OAAAiC,MAAA;IAAhB,KAAAC,GAAG,GAAHA,GAAG;IAbtB,KAAAC,QAAQ,GAAkC,EAAE;IAC5C,KAAAC,eAAe,GAKX,EAAE;IAKE,KAAAC,oBAAoB,GAAG,CAAC;IAG9B,IAAI,CAACC,KAAK,GAAG,IAAIvC,WAAW,EAAE;EAChC;EAAC6B,YAAA,CAAAK,MAAA;IAAAJ,GAAA;IAAAC,KAAA;MAAA,IAAAS,MAAA,GAAAC,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CAED,SAAAC,QAAA;QAAA,IAAAC,cAAA,EAAAC,CAAA,EAAAC,WAAA,EAAAC,OAAA;QAAA,OAAAN,mBAAA,GAAAO,IAAA,UAAAC,SAAAC,QAAA;UAAA,kBAAAA,QAAA,CAAAC,IAAA,GAAAD,QAAA,CAAAE,IAAA;YAAA;cAAA,MACM,IAAI,CAACC,kBAAkB,IAAI,IAAI;gBAAAH,QAAA,CAAAE,IAAA;gBAAA;cAAA;cAAA,OAAAF,QAAA,CAAAI,MAAA,WAC1B,IAAI,CAACD,kBAAkB,CAACE,IAAI,CAAC,YAAK,CAAE,CAAC,CAAC;YAAA;cAAA,MAE3C,IAAI,CAACC,eAAe,IAAI,IAAI;gBAAAN,QAAA,CAAAE,IAAA;gBAAA;cAAA;cAAA,OAAAF,QAAA,CAAAI,MAAA;YAAA;cAG1BV,cAAc,GAAG,IAAI,CAACa,iBAAiB,EAAE;cAEtCZ,CAAC,GAAG,CAAC;YAAA;cAAA,MAAEA,CAAC,GAAGD,cAAc,CAACc,MAAM;gBAAAR,QAAA,CAAAE,IAAA;gBAAA;cAAA;cACjCN,WAAW,GAAGF,cAAc,CAACC,CAAC,CAAC;cAAAK,QAAA,CAAAE,IAAA;cAAA,OACf,IAAI,CAACO,iBAAiB,CAACb,WAAW,CAAC,CAACC,OAAO;YAAA;cAA3DA,OAAO,GAAAG,QAAA,CAAAU,IAAA;cAAA,KACTb,OAAO;gBAAAG,QAAA,CAAAE,IAAA;gBAAA;cAAA;cAAAF,QAAA,CAAAE,IAAA;cAAA,OACH,IAAI,CAACS,UAAU,CAACf,WAAW,CAAC;YAAA;cAAA,OAAAI,QAAA,CAAAI,MAAA;YAAA;cAJKT,CAAC,EAAE;cAAAK,QAAA,CAAAE,IAAA;cAAA;YAAA;cAAA,MASxC,IAAIU,KAAK,CACX,6EACS,CAAC;YAAA;YAAA;cAAA,OAAAZ,QAAA,CAAAa,IAAA;UAAA;QAAA,GAAApB,OAAA;MAAA,CACf;MAAA,SAAAqB,MAAA;QAAA,OAAAzB,MAAA,CAAA0B,KAAA,OAAAC,SAAA;MAAA;MAAA,OAAAF,KAAA;IAAA;EAAA;IAAAnC,GAAA;IAAAsC,GAAA,EAED,SAAAA,IAAA,EAAW;MACT,IAAI,IAAI,CAACd,kBAAkB,IAAI,IAAI,EAAE;QACnC,MAAM,IAAIS,KAAK,CACX,YAAAM,MAAA,CAAY,IAAI,CAACtB,WAAW,gHACuC,kBACpD,CAAC;;MAEtB,IAAI,IAAI,CAACU,eAAe,IAAI,IAAI,EAAE;QAChC,IAAAa,qBAAA,GAA0B,IAAI,CAACC,+BAA+B,EAAE;UAAzD3C,IAAI,GAAA0C,qBAAA,CAAJ1C,IAAI;UAAE4C,SAAS,GAAAF,qBAAA,CAATE,SAAS;QACtB,IAAIA,SAAS,EAAE;UACb,MAAM,IAAIT,KAAK,CACX,iCAAAM,MAAA,CAAiCzC,IAAI,2EACW,uDACI,CAAC;;QAE3D,IAAI,CAACkC,UAAU,CAAClC,IAAI,CAAC;;MAEvB,OAAO,IAAI,CAAC6B,eAAe;IAC7B;EAAC;IAAA3B,GAAA;IAAAC,KAAA,EAED,SAAA0C,aAAA,EAAY;MACV,OAAOC,MAAM,CAACC,IAAI,CAAC,IAAI,CAACtC,eAAe,CAAC;IAC1C;EAAC;IAAAP,GAAA;IAAAC,KAAA,EAED,SAAA6C,YAAY7B,WAAmB;MAC7B,IAAI,EAAEA,WAAW,IAAI,IAAI,CAACX,QAAQ,CAAC,EAAE;QACnC;QACA;QACA,IAAIW,WAAW,IAAI,IAAI,CAACV,eAAe,EAAE;UACvC,IAAAwC,sBAAA,GAAoB,IAAI,CAACjB,iBAAiB,CAACb,WAAW,CAAC;YAAhDyB,SAAS,GAAAK,sBAAA,CAATL,SAAS;UAChB,IAAIA,SAAS,EAAE;YACb;YACA,OAAO,IAAI;;SAEd,MAAM;UACL,OAAO,IAAI;;;MAGf,OAAO,IAAI,CAACpC,QAAQ,CAACW,WAAW,CAAC;IACnC;EAAC;IAAAjB,GAAA;IAAAC,KAAA,EAED,SAAA+C,mBAAmB/B,WAAmB;MAEpC,IAAI,EAAEA,WAAW,IAAI,IAAI,CAACV,eAAe,CAAC,EAAE;QAC1C,OAAO,IAAI;;MAEb,OAAO,IAAI,CAACA,eAAe,CAACU,WAAW,CAAC,CAACgC,OAAO;IAClD;EAAC;IAAAjD,GAAA;IAAAC,KAAA,EAED,SAAAiD,gBACIjC,WAAmB,EACnBgC,OAAqD,EACzC;MAAA,IAAZE,QAAQ,GAAAd,SAAA,CAAAR,MAAA,QAAAQ,SAAA,QAAAe,SAAA,GAAAf,SAAA,MAAG,CAAC;MACd,IAAIpB,WAAW,IAAI,IAAI,CAACV,eAAe,EAAE;QACvCrD,GAAG,CAACmG,IAAI,CACJ,GAAAd,MAAA,CAAGtB,WAAW,4EACqB,CAAC;QACxC,OAAO,KAAK;;MAEd,IAAI,CAACV,eAAe,CAACU,WAAW,CAAC,GAAG;QAACgC,OAAO,EAAPA,OAAO;QAAEE,QAAQ,EAARA;MAAQ,CAAC;MACvD,OAAO,IAAI;IACb;EAAC;IAAAnD,GAAA;IAAAC,KAAA;MAAA,IAAAqD,WAAA,GAAA3C,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CAED,SAAA0C,SAAiBtC,WAAmB;QAAA,IAAAuC,sBAAA,EAAAtC,OAAA,EAAAwB,SAAA,EAAAnD,MAAA;QAAA,OAAAqB,mBAAA,GAAAO,IAAA,UAAAsC,UAAAC,SAAA;UAAA,kBAAAA,SAAA,CAAApC,IAAA,GAAAoC,SAAA,CAAAnC,IAAA;YAAA;cAAA,MAC9B,IAAI,CAAChB,eAAe,CAACU,WAAW,CAAC,IAAI,IAAI;gBAAAyC,SAAA,CAAAnC,IAAA;gBAAA;cAAA;cAAA,MACrC,IAAIU,KAAK,kBAAAM,MAAA,CAAkBtB,WAAW,6BAA0B;YAAA;cAExE,IAAI,CAACA,WAAW,GAAGA,WAAW;cAAC,MAC3B,IAAI,CAACX,QAAQ,CAACW,WAAW,CAAC,IAAI,IAAI;gBAAAyC,SAAA,CAAAnC,IAAA;gBAAA;cAAA;cACpC,IAAI,CAACI,eAAe,GAAG,IAAI;cAAC6B,sBAAA,GACC,IAAI,CAAC1B,iBAAiB,CAACb,WAAW,CAAC,EAAzDC,OAAO,GAAAsC,sBAAA,CAAPtC,OAAO,EAAEwB,SAAS,GAAAc,sBAAA,CAATd,SAAS;cAAA,KACVA,SAAS;gBAAAgB,SAAA,CAAAnC,IAAA;gBAAA;cAAA;cAAAmC,SAAA,CAAAnC,IAAA;cAAA,OAASL,OAAO;YAAA;cAAAwC,SAAA,CAAAC,EAAA,GAAAD,SAAA,CAAA3B,IAAA;cAAA2B,SAAA,CAAAnC,IAAA;cAAA;YAAA;cAAAmC,SAAA,CAAAC,EAAA,GAAGzC,OAAO;YAAA;cAA5C3B,MAAM,GAAAmE,SAAA,CAAAC,EAAA;cAAA,IACPpE,MAAM;gBAAAmE,SAAA,CAAAnC,IAAA;gBAAA;cAAA;cAAA,OAAAmC,SAAA,CAAAjC,MAAA,WACF,KAAK;YAAA;cAGhB,IAAI,CAACE,eAAe,GAAG,IAAI,CAACrB,QAAQ,CAACW,WAAW,CAAC;cACjD,IAAI,CAAC2C,sBAAsB,EAAE;cAC7B;cACA,IAAI,CAACC,QAAQ,GAAG,IAAI1G,QAAQ,CAAC,IAAI,CAACwE,eAAe,CAAC;cAAC,OAAA+B,SAAA,CAAAjC,MAAA,WAE5C,IAAI;YAAA;YAAA;cAAA,OAAAiC,SAAA,CAAAxB,IAAA;UAAA;QAAA,GAAAqB,QAAA;MAAA,CACZ;MAAA,SAAAvB,WAAA8B,EAAA;QAAA,OAAAR,WAAA,CAAAlB,KAAA,OAAAC,SAAA;MAAA;MAAA,OAAAL,UAAA;IAAA;EAAA;IAAAhC,GAAA;IAAAC,KAAA,EAEO,SAAA2D,uBAAA,EAAsB;MAAA,IAAAG,KAAA;MAC5B,IAAMzE,OAAO,GAAGrC,oBAAoB,CAAC,IAAI,CAACgE,WAAW,CAAC;MACtD3B,OAAO,CAAC0E,OAAO,CAAC,UAAAC,MAAM,EAAG;QACvB,IAAIA,MAAM,CAACC,SAAS,IAAI,IAAI,EAAE;UAC5BD,MAAM,CAACC,SAAS,CAACH,KAAI,CAACpC,eAAe,CAAC;;MAE1C,CAAC,CAAC;IACJ;EAAC;IAAA3B,GAAA;IAAAC,KAAA,EAEO,SAAAkE,yBAAyBlD,WAAmB;MAAA,IAAAmD,MAAA;MAClD,IAAM9E,OAAO,GAAGrC,oBAAoB,CAACgE,WAAW,CAAC;MACjD3B,OAAO,CAAC0E,OAAO,CAAC,UAAAC,MAAM,EAAG;QACvB,IAAIA,MAAM,CAACI,WAAW,IAAI,IAAI,EAAE;UAC9BJ,MAAM,CAACI,WAAW,CAACD,MAAI,CAAC9D,QAAQ,CAACW,WAAW,CAAC,CAAC;;MAElD,CAAC,CAAC;IACJ;IAEA;;;;;;EAAA;IAAAjB,GAAA;IAAAC,KAAA,EAMQ,SAAA6B,kBAAkBb,WAAmB;MAAA,IAAAqD,MAAA;MAE3C,IAAMC,oBAAoB,GAAG,IAAI,CAAChE,eAAe,CAACU,WAAW,CAAC;MAC9D,IAAIsD,oBAAoB,IAAI,IAAI,EAAE;QAChC,MAAM,IAAItC,KAAK,8BAAAM,MAAA,CACkBtB,WAAW,8BAA2B;;MAGzE,IAAI;QACF,IAAMuD,OAAO,GAAGD,oBAAoB,CAACtB,OAAO,EAAE;QAC9C;;;;;QAKA,IAAIuB,OAAO,IAAI,EAAEA,OAAO,YAAYhI,aAAa,CAAC,IAC9C,OAAOgI,OAAO,CAAC9C,IAAI,KAAK,UAAU,EAAE;UACtC,IAAM+C,SAAS,GAAG,EAAE,IAAI,CAACjE,oBAAoB;UAC7C,IAAMU,OAAO,GACTsD,OAAO,CACF9C,IAAI,CAAC,UAAAC,eAAe,EAAG;YACtB;YACA,IAAI8C,SAAS,GAAGH,MAAI,CAAC9D,oBAAoB,EAAE;cACzC,OAAO,KAAK;;YAEd8D,MAAI,CAAChE,QAAQ,CAACW,WAAW,CAAC,GAAGU,eAAe;YAC5C2C,MAAI,CAAC9C,kBAAkB,GAAG,IAAI;YAC9B,OAAO,IAAI;UACb,CAAC,CAAC,CACDkD,KAAK,CAAC,UAAAC,GAAG,EAAG;YACX;YACA,IAAIF,SAAS,GAAGH,MAAI,CAAC9D,oBAAoB,EAAE;cACzC,OAAO,KAAK;;YAEd8D,MAAI,CAAC9C,kBAAkB,GAAG,IAAI;YAC9BtE,GAAG,CAACmG,IAAI,8BAAAd,MAAA,CAA8BtB,WAAW,aAAU;YAC3D/D,GAAG,CAACmG,IAAI,CAACsB,GAAG,CAACC,KAAK,IAAID,GAAG,CAACE,OAAO,CAAC;YAClC,OAAO,KAAK;UACd,CAAC,CAAC;UACV,IAAI,CAACrD,kBAAkB,GAAGN,OAAO;UACjC,OAAO;YAACA,OAAO,EAAPA,OAAO;YAAEwB,SAAS,EAAE;UAAI,CAAC;SAClC,MAAM;UACL,IAAI,CAACpC,QAAQ,CAACW,WAAW,CAAC,GAAGuD,OAAwB;UACrD,OAAO;YAACtD,OAAO,EAAE,IAAI;YAAEwB,SAAS,EAAE;UAAK,CAAC;;OAE3C,CAAC,OAAOiC,GAAG,EAAE;QACZzH,GAAG,CAACmG,IAAI,8BAAAd,MAAA,CAA8BtB,WAAW,aAAU;QAC3D/D,GAAG,CAACmG,IAAI,CAACsB,GAAG,CAACC,KAAK,IAAID,GAAG,CAACE,OAAO,CAAC;QAClC,OAAO;UAAC3D,OAAO,EAAE,KAAK;UAAEwB,SAAS,EAAE;QAAK,CAAC;;IAE7C;EAAC;IAAA1C,GAAA;IAAAC,KAAA,EAED,SAAA6E,cAAc7D,WAAmB;MAC/B,IAAI,EAAEA,WAAW,IAAI,IAAI,CAACV,eAAe,CAAC,EAAE;QAC1C,MAAM,IAAI0B,KAAK,IAAAM,MAAA,CAAItB,WAAW,oCAAiC;;MAEjE,IAAI,IAAI,CAACA,WAAW,KAAKA,WAAW,IAAI,IAAI,CAACO,kBAAkB,IAAI,IAAI,EAAE;QACvE;QACA;QACA,IAAI,CAAChB,oBAAoB,EAAE;;MAG7B,IAAIS,WAAW,IAAI,IAAI,CAACX,QAAQ,EAAE;QAChC,IAAI,CAAC6D,wBAAwB,CAAClD,WAAW,CAAC;QAC1C,IAAI,CAACX,QAAQ,CAACW,WAAW,CAAC,CAACf,OAAO,EAAE;QACpC,OAAO,IAAI,CAACI,QAAQ,CAACW,WAAW,CAAC;;MAGnC,OAAO,IAAI,CAACV,eAAe,CAACU,WAAW,CAAC;MAExC;MACA,IAAI,IAAI,CAACA,WAAW,KAAKA,WAAW,EAAE;QACpC,IAAI,CAACO,kBAAkB,GAAG,IAAI;QAC9B,IAAI,CAACP,WAAW,GAAG,IAAI;QACvB,IAAI,CAACU,eAAe,GAAG,IAAI;;IAE/B;EAAC;IAAA3B,GAAA;IAAAC,KAAA,EAEO,SAAA2B,kBAAA,EAAiB;MAAA,IAAAmD,MAAA;MACvB,IAAInC,MAAM,CAACC,IAAI,CAAC,IAAI,CAACtC,eAAe,CAAC,CAACsB,MAAM,KAAK,CAAC,EAAE;QAClD,MAAM,IAAII,KAAK,CAAC,+BAA+B,CAAC;;MAElD,OAAOW,MAAM,CAACC,IAAI,CAAC,IAAI,CAACtC,eAAe,CAAC,CAACyE,IAAI,CAAC,UAACC,CAAS,EAAEC,CAAS,EAAI;QACrE;QACA,OAAOH,MAAI,CAACxE,eAAe,CAAC2E,CAAC,CAAC,CAAC/B,QAAQ,GACnC4B,MAAI,CAACxE,eAAe,CAAC0E,CAAC,CAAC,CAAC9B,QAAQ;MACtC,CAAC,CAAC;IACJ;EAAC;IAAAnD,GAAA;IAAAC,KAAA,EAEO,SAAAwC,gCAAA,EAA+B;MAErC,IAAM1B,cAAc,GAAG,IAAI,CAACa,iBAAiB,EAAE;MAE/C,KAAK,IAAIZ,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGD,cAAc,CAACc,MAAM,EAAEb,CAAC,EAAE,EAAE;QAC9C,IAAMC,WAAW,GAAGF,cAAc,CAACC,CAAC,CAAC;QACrC,IAAAmE,sBAAA,GAA6B,IAAI,CAACrD,iBAAiB,CAACb,WAAW,CAAC;UAAzDC,OAAO,GAAAiE,sBAAA,CAAPjE,OAAO;UAAEwB,SAAS,GAAAyC,sBAAA,CAATzC,SAAS;QACzB,IAAIA,SAAS,IAAIxB,OAAO,EAAE;UACxB,OAAO;YAACpB,IAAI,EAAEmB,WAAW;YAAEyB,SAAS,EAATA;UAAS,CAAC;;;MAGzC,MAAM,IAAIT,KAAK,CACX,6EACS,CAAC;IAChB;EAAC;IAAAjC,GAAA;IAAAC,KAAA,EAED,SAAAmF,SAASZ,OAAsB,EAAEa,MAAc;MAC7C,IAAMC,IAAI,GAAG,IAAI,CAAC7E,KAAK,CAAC1B,UAAU,CAACuD,GAAG,CAAC+C,MAAM,CAAC;MAC9C,IAAME,UAAU,GAAGD,IAAI,CAACd,OAAO;MAC/B,IAAMgB,MAAM,GAAG,IAAI,CAACC,QAAQ,CAACJ,MAAM,CAAC;MACpC,IAAMK,QAAQ,GAAGH,UAAU,CAACG,QAAQ,CAACL,MAAM,CAAC;MAC5C;MACA;MACAE,UAAU,CAACI,WAAW,CAACN,MAAM,EAAE,IAAI,CAAC;MACpCC,IAAI,CAACd,OAAO,GAAGA,OAAO;MACtBA,OAAO,CAACoB,IAAI,CAACP,MAAM,EAAEG,MAAM,EAAEF,IAAI,CAACO,KAAK,EAAEP,IAAI,CAACQ,KAAK,EAAEJ,QAAQ,CAAC;MAC9D,IAAI,IAAI,CAACK,sBAAsB,EAAE,EAAE;QACjC;QACA;QACA,IAAI,CAACtF,KAAK,CAAC5B,iBAAiB,CAAC,IAAI,CAAC4B,KAAK,CAAC5B,iBAAiB,CAACgD,MAAM,GAAG,CAAC,CAAC,EAAE;;IAE3E;EAAC;IAAA7B,GAAA;IAAAC,KAAA,EAED,SAAA+F,KAAgCC,QAA2B,EAAEC,EAAe;MAAA,IAAAC,MAAA;MAE1E,IAAIrG,IAAI,GAAW,IAAI;MACvB,IAAIoG,EAAE,IAAI,IAAI,EAAE;QACd;QACA,IAAI,OAAOD,QAAQ,KAAK,UAAU,EAAE;UAClC,MAAM,IAAIhE,KAAK,CAAC,qCAAqC,CAAC;;QAExDiE,EAAE,GAAGD,QAAQ;OACd,MAAM;QACL;QACA,IAAI,OAAOA,QAAQ,KAAK,QAAQ,IAAI,EAAEA,QAAQ,YAAYG,MAAM,CAAC,EAAE;UACjE,MAAM,IAAInE,KAAK,CACX,sDAAsD,GACtD,4BAA4B,CAAC;;QAEnC,IAAI,OAAOiE,EAAE,KAAK,UAAU,EAAE;UAC5B,MAAM,IAAIjE,KAAK,CACX,oDAAoD,GACpD,8BAA8B,CAAC;;QAErCnC,IAAI,GAAGmG,QAAkB;QACzB;QACA;;;MAEF,IAAI1G,MAAS;MACb,OAAO,IAAI,CAAC8G,SAAS,CACjB;QAAA,OAAMF,MAAI,CAACG,UAAU,CAACxG,IAAI,CAAC;MAAA,GAAE;QAAA,OAAMqG,MAAI,CAACI,QAAQ,CAAChH,MAAM,CAAC;MAAA,GAAE,YAAK;QAC7DA,MAAM,GAAG2G,EAAE,EAAE;QACb,IAAI3G,MAAM,YAAYiH,OAAO,EAAE;UAC7BC,OAAO,CAACC,KAAK,CAAC,yCAAyC,CAAC;;QAE1D,OAAOnH,MAAM;MACf,CAAC,CAAC;IACR;EAAC;IAAAS,GAAA;IAAAC,KAAA,EAEO,SAAAoG,UAAaM,KAAiB,EAAEC,GAAe,EAAEC,CAAU;MACjEF,KAAK,EAAE;MACP,IAAI;QACF,IAAMG,GAAG,GAAGD,CAAC,EAAE;QACfD,GAAG,EAAE;QACL,OAAOE,GAAG;OACX,CAAC,OAAOC,EAAE,EAAE;QACXH,GAAG,EAAE;QACL,MAAMG,EAAE;;IAEZ;EAAC;IAAA/G,GAAA;IAAAC,KAAA,EAGO,SAAA+G,aAAA,EAAY;MAClB,OAAO5G,MAAM,CAAC4G,YAAY,EAAE;IAC9B;EAAC;IAAAhH,GAAA;IAAAC,KAAA,EAGO,SAAAgH,eAAA,EAAc;MACpB,OAAO7G,MAAM,CAAC6G,cAAc,EAAE;IAChC;IAEA;;;;;;EAAA;IAAAjH,GAAA;IAAAC,KAAA,EAMQ,SAAAiH,MAAMC,CAAS;MACrB,IAAMC,CAAC,GAAWC,MAAM,CAACC,SAAS,CAACxK,QAAQ,EACR;QAACqK,CAAC,EAADA;MAAC,CAA8B,CAAC;MACpE,IAAMI,MAAM,GAAG;QAACJ,CAAC,EAADA;MAAC,CAAC;MAClB,IAAMK,IAAI,GAAG,SAAPA,IAAIA,CAAIC,EAAU;QAAA,OAAM;UAC5BN,CAAC,EAAE,SAAAA,EAAA,EAAK;YACN,IAAMrB,KAAK,GAAG,SAAS;YACvB,IAAM4B,UAAU,GAAG;cAACP,CAAC,EAAEM;YAAE,CAAC;YAC1B,IAAME,KAAK,GAAG;cAAC7B,KAAK,EAALA;YAAK,CAAC;YAErB,OAAOuB,MAAM,CAACC,SAAS,CACZzK,IAAI,EAAE6K,UAAuC;YAC7C;YACAC,KAAgC,CAAW;UACxD;SACD;MAAA,CAAC;MACF,IAAMC,KAAK,GAAa,EAAE;MAC1B,IAAI,CAACC,WAAW,CAAC,IAAI,CAACpH,KAAK,CAACqH,WAAW,CAAChI,IAAI,EAAEyH,MAAM,EAAE,CAACH,CAAC,CAAC,EAAEI,IAAI,EAAEI,KAAK,EAAE,EAAE,CAAC;MAC3E,OAAOR,CAAC;IACV;IAEA;;;;;;;;;;;;;EAAA;IAAApH,GAAA;IAAAC,KAAA,EAaA,SAAAqH,UACIrJ,UAAkB,EAAEsJ,MAAsB,EAAEI,KAAoB;MAClE,IAAI,IAAI,CAAC1G,WAAW,IAAI,IAAI,EAAE;QAC5B;QACA;QACA;QACA;QACA;QACA,IAAI,CAACuD,OAAO;;MAEd,IAAMuD,SAAS,GAAG/K,SAAS,CAACiB,UAAU,EAAE,IAAI,CAACgD,WAAW,CAAC,IAAI,IAAI;MACjE,IAAI,CAAC8G,SAAS,EAAE;QACd,MAAM,IAAI9F,KAAK,YAAAM,MAAA,CAAYtE,UAAU,oCAAAsE,MAAA,CACjC,IAAI,CAACtB,WAAW,OAAI;;MAE1B,OAAO,IAAI,CAAC+G,aAAa,CAAC;QAAC/J,UAAU,EAAVA,UAAU;QAAEsJ,MAAM,EAANA,MAAM;QAAEI,KAAK,EAALA;MAAK,CAAC,CAAC;IACxD;EAAC;IAAA3H,GAAA;IAAAC,KAAA,EAEO,SAAA8F,uBAAA,EAAsB;MAC5B,OAAO,IAAI,CAAC1F,GAAG,CAAC4H,OAAO,CAAC,SAAS,CAAC;IACpC;EAAC;IAAAjI,GAAA;IAAAC,KAAA,EAEO,SAAAiI,sBACJjK,UAAkB,EAAEkK,gBAAwB,EAC5CC,QAAsB;MACxB,IAAMC,eAAe,GAAG,IAAI,CAAC7D,OAAO,CAAC8D,UAAU,EAAE;MAEjD;MACA,IAAIC,gBAAgB,GAAG,CAAC;MACxBH,QAAQ,CAACpE,OAAO,CAAC,UAAAsB,IAAI,EAAG;QACtB;QACA;QACAiD,gBAAgB,IAAKjD,IAAI,CAACQ,KAAK,KAAK,WAAW,GAAG,CAAC,GAAG,CAAE;MAC1D,CAAC,CAAC;MAEF;MACA;MACA;MACA;MACA;MACA,IAAM0C,QAAQ,GACV,IAAI,CAAC/H,KAAK,CAAC5B,iBAAiB,CAAC,IAAI,CAAC4B,KAAK,CAAC5B,iBAAiB,CAACgD,MAAM,GAAG,CAAC,CAAC;MACzE,IAAM4G,aAAa,GACfJ,eAAe,GAAGF,gBAAgB,GAAGI,gBAAgB,GAAGC,QAAQ;MACpE,IAAIC,aAAa,GAAG,CAAC,EAAE;QACrB,MAAM,IAAIxG,KAAK,CACX,YAAAM,MAAA,CAAY,IAAI,CAACtB,WAAW,0CAAAsB,MAAA,CACxBkG,aAAa,gCAAAlG,MAAA,CAA6BtE,UAAU,MAAG,CAAC;;IAEpE;IAEA;;;;;EAAA;IAAA+B,GAAA;IAAAC,KAAA,EAKQ,SAAA+H,cACJU,YACgC;MAAA,IAAAC,MAAA;MAClC,IAAIC,OAAiB;MACrB,IAAIhB,KAAK,GAAa,EAAE;MACxB,IAAMiB,QAAQ,GAAG,IAAI,CAACA,QAAQ,EAAE;MAEhC,IAAMC,iBAAiB,GAAG,IAAI,CAACrI,KAAK,CAACnC,QAAQ;MAC7C,IAAMyK,kBAAkB,GAAG,IAAI,CAACtI,KAAK,CAAClC,UAAU;MAEhD,IAAI,IAAI,CAACwH,sBAAsB,EAAE,EAAE;QACjC,IAAI,CAACtF,KAAK,CAAC5B,iBAAiB,CAACmK,IAAI,CAAC,CAAC,CAAC;;MAGtC,IAAIC,UAA0B;MAC9B,IAAI,IAAI,CAAChI,WAAW,IAAI,IAAI,EAAE;QAC5B;QACA;QACA;QACA;QACA;QACA,IAAI,CAACuD,OAAO;;MAGd,IAAI0E,GAA4B;MAEhC,IAAMC,iBAAiB,GAAGpL,4BAA4B,CAAC2K,YAAY,CAAC,GAChEA,YAAY,CAACzK,UAAU,GACvB,IAAI,CAACwC,KAAK,CAACqH,WAAW,IAAI,IAAI,GAAG,IAAI,CAACrH,KAAK,CAACqH,WAAW,CAAChI,IAAI,GAAG,EAAE;MAErE;MACA;MACA;MAEA,IAAI/B,4BAA4B,CAAC2K,YAAY,CAAC,EAAE;QAC9C,IAAOzK,UAAU,GAAmByK,YAAY,CAAzCzK,UAAU;UAAEsJ,OAAM,GAAWmB,YAAY,CAA7BnB,MAAM;UAAEI,MAAK,GAAIe,YAAY,CAArBf,KAAK;QAChC,IAAI,IAAI,CAAC1G,WAAW,IAAI,IAAI,EAAE;UAC5B;UACA;UACA;UACA;UACA;UACA,IAAI,CAACuD,OAAO;;QAEd,IAAMP,MAAM,GAAGjH,SAAS,CAACiB,UAAU,EAAE,IAAI,CAACgD,WAAW,CAAC;QACtDvD,IAAI,CAAC0L,MAAM,CACPnF,MAAM,IAAI,IAAI,EACd;UAAA,yCAAA1B,MAAA,CAAwCtE,UAAU,qBAAAsE,MAAA,CAC9CoG,MAAI,CAAC1H,WAAW;QAAA,CAAG,CAAC;QAE5BgI,UAAU,GAAG,SAAAA,WAAA,EAAK;UAChB,IAAMd,gBAAgB,GAAGQ,MAAI,CAACnE,OAAO,CAAC8D,UAAU,EAAE;UAClDY,GAAG,GAAGjF,MAAM,CAACgF,UAAU,CAAC;YAAC1B,MAAM,EAANA,OAAM;YAAEI,KAAK,EAALA,MAAK;YAAEnD,OAAO,EAAEmE,MAAI,CAACnE;UAAO,CAAC,CAAC;UAC/D,IAAM4D,QAAQ,GAAG3I,KAAK,CAAC4J,OAAO,CAACH,GAAG,CAAC,GAAGA,GAAG,GAAG,CAACA,GAAG,CAAC;UACjD,IAAIP,MAAI,CAAC5C,sBAAsB,EAAE,EAAE;YACjC4C,MAAI,CAACT,qBAAqB,CAACjK,UAAU,EAAEkK,gBAAgB,EAAEC,QAAQ,CAAC;;UAGpE,IAAMkB,UAAU,GAAGlB,QAAQ,CAACxI,GAAG,CAAC,UAAC2J,OAA0B,EAAI;YAC7D;YACA;YACA;YACA,IAAKA,OAAkB,CAACC,IAAI,IAAI,IAAI,EAAE;cACpC,OAAOD,OAAiB;;YAE1B,OAAOZ,MAAI,CAACc,wBAAwB,CAACF,OAAO,CAAC;UAC/C,CAAC,CAAC;UAEF;UAEA;UACA;UACA;UACA,IAAIV,QAAQ,EAAE;YACZ,IAAMa,aAAa,GACff,MAAI,CAACgB,qBAAqB,CAAC1L,UAAU,EAAEsJ,OAAM,EAAE+B,UAAU,CAAC;YAC9D1B,KAAK,GAAGe,MAAI,CAACiB,0BAA0B,CAACF,aAAa,CAAC;;UAExD,OAAOJ,UAAU;QACnB,CAAC;OACF,MAAM;QACL,IAAOO,WAAW,GAAInB,YAAY,CAA3BmB,WAAW;QAClB;QACA,IAAMC,QAAQ,GAAiB,SAAzBA,QAAQA,CAAkBC,OAAO,EAAI;UACzC;UACA;UACA;UACA,IAAI,CAAClB,QAAQ,EAAE;YACb;;UAEFjB,KAAK,GAAGmC,OAAO,CAACnK,GAAG,CAAC,UAAAoK,MAAM;YAAA,OAAIrB,MAAI,CAACsB,IAAI,CAACtB,MAAI,CAACzB,KAAK,CAAC8C,MAAM,CAAC,CAAC;UAAA,EAAC;QAC9D,CAAC;QAEDf,UAAU,GAAG,SAAAA,WAAA,EAAK;UAChB,IAAMd,gBAAgB,GAAGQ,MAAI,CAACnE,OAAO,CAAC8D,UAAU,EAAE;UAClDY,GAAG,GAAGP,MAAI,CAAC3C,IAAI,CAAC;YAAA,OAAM6D,WAAW,CAAClB,MAAI,CAACnE,OAAO,EAAEsF,QAAQ,CAAC;UAAA,EAAC;UAC1D,IAAMI,IAAI,GAAIzK,KAAK,CAAC4J,OAAO,CAACH,GAAG,CAAC,GAAGA,GAAG,GAAG,CAACA,GAAG,CAAc;UAC3D,IAAIP,MAAI,CAAC5C,sBAAsB,EAAE,EAAE;YACjC;YACA4C,MAAI,CAACT,qBAAqB,CAACiB,iBAAiB,EAAEhB,gBAAgB,EAAE+B,IAAI,CAAC;;UAEvE,OAAOA,IAAI;QACb,CAAC;;MAGH;MACA;MACA;MACA,IAAO3C,MAAM,GAAWmB,YAAY,CAA7BnB,MAAM;QAAEI,KAAK,GAAIe,YAAY,CAArBf,KAAK;MACpB,IAAMwC,aAAa,GAAGpM,4BAA4B,CAAC2K,YAAY,CAAC,GAC5D,IAAI,GACJA,YAAY,CAACyB,aAAa;MAE9B,IAAIC,aAA4B;MAChC,IAAI,CAAC/D,SAAS;MACV;MACA;QAAA,OAAMsC,MAAI,CAAClI,KAAK,CAAC9B,WAAW,EAAE;MAAA,GAAE;QAAA,OAAMgK,MAAI,CAAClI,KAAK,CAAC9B,WAAW,EAAE;MAAA,GAAE,YAAK;QACnE,IAAI,CAACgK,MAAI,CAACtI,GAAG,CAAC4H,OAAO,CAAC,OAAO,CAAC,IAAI,CAACU,MAAI,CAAClI,KAAK,CAACxB,SAAS,EAAE;UACvD2J,OAAO,GAAGK,UAAU,EAAE;SACvB,MAAM;UACLmB,aAAa,GAAGzB,MAAI,CAAC9E,QAAQ,CAACwG,aAAa,CACvClB,iBAAiB,EAAE5B,MAAM,EAAE;YAAA,OAAM0B,UAAU,EAAE;UAAA,EAAC;UAClD,IAAIN,MAAI,CAACtI,GAAG,CAAC4H,OAAO,CAAC,OAAO,CAAC,EAAE;YAC7BU,MAAI,CAAC9E,QAAQ,CAACyG,gBAAgB,CAACF,aAAa,CAAC;;UAE/CxB,OAAO,GAAGwB,aAAa,CAACxB,OAAO;;MAEnC,CAAC,CAAC;MAEN,IAAIC,QAAQ,EAAE;QACZ,IAAI,CAAChB,WAAW,CACZsB,iBAAiB,EAAE5B,MAAM,EAAEqB,OAAO,EAAEuB,aAAa,EAAEvC,KAAK,EAAED,KAAK,CAAC;;MAGtE,IAAI,IAAI,CAAClH,KAAK,CAACxB,SAAS,EAAE;QACxB,IAAI,CAACwB,KAAK,CAACvB,aAAa,CAACI,OAAO,CAAC0J,IAAI,CAAC;UACpClJ,IAAI,EAAEqJ,iBAAiB;UACvBoB,UAAU,EAAE,IAAI,CAAC9J,KAAK,CAACnC,QAAQ,GAAGwK,iBAAiB;UACnD0B,kBAAkB,EAAE,IAAI,CAAC/J,KAAK,CAACnC,QAAQ;UACvCmM,YAAY,EAAE,IAAI,CAAChK,KAAK,CAAClC,UAAU,GAAGwK,kBAAkB;UACxD2B,oBAAoB,EAAE,IAAI,CAACjK,KAAK,CAAClC,UAAU;UAC3CoM,WAAW,EAAE/H,MAAM,CAACC,IAAI,CAAC0E,MAAM,CAAC,CAAC3H,GAAG,CAChC,UAAAI,GAAG;YAAA,OAAIuH,MAAM,CAACvH,GAAG,CAAC,IAAI,IAAI,GAAGuH,MAAM,CAACvH,GAAG,CAAC,CAAC6F,KAAK,GAAG,IAAI;UAAA,EAAC;UAC1D+E,YAAY,EAAEhC,OAAO,CAAChJ,GAAG,CAAC,UAAAiL,IAAI;YAAA,OAAIA,IAAI,CAAChF,KAAK;UAAA,EAAC;UAC7CiF,YAAY,EAAEV,aAAa,CAACW,MAAM;UAClCC,SAAS,EAAEZ,aAAa,CAACY;SAC1B,CAAC;;MAEJ,OAAQvL,KAAK,CAAC4J,OAAO,CAACH,GAAG,CAAC,GAAGN,OAAO,GAAGA,OAAO,CAAC,CAAC,CAAC;IACnD;IAEA;;;;;EAAA;IAAA5I,GAAA;IAAAC,KAAA,EAKQ,SAAA2J,2BAA2BG,OAAiB;MAAA,IAAAkB,MAAA;MAClD,IAAMrD,KAAK,GAAGmC,OAAO,CAACnK,GAAG,CAAC,UAAAoK,MAAM;QAAA,OAAIiB,MAAI,CAAChB,IAAI,CAACgB,MAAI,CAAC/D,KAAK,CAAC8C,MAAM,CAAC,CAAC;MAAA,EAAC;MAClE,OAAOpC,KAAK;IACd;IAEA;;;;;;;EAAA;IAAA5H,GAAA;IAAAC,KAAA,EAOQ,SAAA0J,sBACJ1L,UAAkB,EAAEsJ,MAAsB,EAC1CqB,OAAiB;MACnB,IAAMsC,UAAU,GAAGnO,WAAW,CAACkB,UAAU,CAAC;MAC1C,IAAIiN,UAAU,IAAI,IAAI,EAAE;QACtB,IAAMC,YAAY,GAAaD,UAAU,CAACC,YAAY,IAAI,EAAE;QAC5D,IAAMC,aAAa,GAAcF,UAAU,CAACE,aAAa,IAAI,EAAE;QAE/D;QACA;QACA,IAAIC,kBAA4B;QAChC,IAAIH,UAAU,CAACI,aAAa,EAAE;UAC5B5N,IAAI,CAAC0L,MAAM,CACP3J,KAAK,CAAC4J,OAAO,CAAC9B,MAAM,CAAC,EACrB;YAAA,OAAM,wDAAwD;UAAA,EAAC;UAEnE8D,kBAAkB,GAAGzI,MAAM,CAACC,IAAI,CAAC0E,MAAM,CAAC,CAAC3H,GAAG,CAAC,UAACI,GAAG;YAAA,OAAKuH,MAAM,CAACvH,GAAG,CAAC;UAAA,EAAC;SACnE,MAAM;UACLqL,kBAAkB,GAAGF,YAAY,CAACvL,GAAG,CAAC,UAAC2L,SAAS;YAAA,OAAKhE,MAAM,CAACgE,SAAS,CAAC;UAAA,EAAC;;QAGzE,IAAMC,mBAAmB,GACrB5C,OAAO,CAAC6C,MAAM,CAAC,UAACC,CAAC,EAAE1K,CAAC;UAAA,OAAKoK,aAAa,CAACpK,CAAC,CAAC;QAAA,EAAC;QAE9C,OAAOqK,kBAAkB,CAAC9I,MAAM,CAACiJ,mBAAmB,CAAC;;MAEvD;MACA;MACA;MACA;MACA;MACA;MACA,OAAO,EAAE;IACX;IAEA;;;;;EAAA;IAAAxL,GAAA;IAAAC,KAAA,EAKA,SAAA0L,WACInG,MAAkB,EAAEK,KAAe,EAAEC,KAAe,EACpDtB,OAAuB;MACzB,IAAIgB,MAAM,IAAI,IAAI,EAAE;QAClB,MAAM,IAAIvD,KAAK,CAAC,+CAA+C,CAAC;;MAElE6D,KAAK,GAAGA,KAAK,IAAI,SAAS;MAC1BtB,OAAO,GAAGA,OAAO,IAAI,IAAI,CAACA,OAAO;MACjC,IAAIoH,WAAW,GAAGpG,MAAuB;MACzC,IAAIM,KAAK,KAAK,QAAQ,IAAIpI,IAAI,CAACmO,QAAQ,CAACrG,MAAM,CAAC,CAAC,CAAC,CAAC,EAAE;QAClDoG,WAAW,GAAIpG,MAAmB,CAAC5F,GAAG,CAAC,UAAAkM,CAAC;UAAA,OAAIpO,IAAI,CAACqO,YAAY,CAACD,CAAC,CAAC;QAAA,EAAC;;MAEnE,IAAMzG,MAAM,GAAGb,OAAO,CAACwH,KAAK,CAACJ,WAAW,EAAE/F,KAAK,EAAEC,KAAK,CAAC;MACvD,IAAMmG,CAAC,GAAG,IAAI1O,MAAM,CAACsI,KAAK,EAAEC,KAAK,EAAET,MAAM,EAAE,IAAI,CAAC2B,YAAY,EAAE,CAAC;MAC/D,IAAI,CAACkF,WAAW,CAACD,CAAC,EAAEzH,OAAO,CAAC;MAE5B;MACA,IAAIsB,KAAK,KAAK,QAAQ,EAAE;QACtB,IAAMR,IAAI,GAAG,IAAI,CAAC7E,KAAK,CAAC1B,UAAU,CAACuD,GAAG,CAAC+C,MAAM,CAAC;QAC9C,IAAMlG,QAAQ,GAAGxB,oBAAoB,CAACiO,WAA2B,CAAC;QAClE,IAAI,CAACnL,KAAK,CAACnC,QAAQ,IAAIa,QAAQ,GAAGmG,IAAI,CAAC6G,KAAK;QAC5C7G,IAAI,CAAC6G,KAAK,GAAGhN,QAAQ;;MAEvB,OAAO8M,CAAC;IACV;IAEA;;;;;;EAAA;IAAAjM,GAAA;IAAAC,KAAA,EAMA,SAAAmM,qBACE/G,MAAc,EAAEQ,KAAe,EAAEC,KAAe,EAChDtB,OAAuB;MACvBsB,KAAK,GAAGA,KAAK,IAAI,SAAS;MAC1B,IAAM/G,UAAU,GAAe;QAACsG,MAAM,EAANA,MAAM;QAAEQ,KAAK,EAALA,KAAK;QAAEC,KAAK,EAALA;MAAK,CAAC;MACrD,OAAO,IAAI,CAAC2D,wBAAwB,CAAC1K,UAAU,EAAEyF,OAAO,CAAC;IAC3D;IAEA;;;;;EAAA;IAAAxE,GAAA;IAAAC,KAAA,EAKA,SAAAwJ,yBAAyB1K,UAAsB,EAAEyF,OAAuB;MAEtE,IAAOa,MAAM,GAAkBtG,UAAU,CAAlCsG,MAAM;QAAEQ,KAAK,GAAW9G,UAAU,CAA1B8G,KAAK;QAAEC,KAAK,GAAI/G,UAAU,CAAnB+G,KAAK;MAC3B,IAAMmG,CAAC,GAAG,IAAI1O,MAAM,CAACsI,KAAK,EAAEC,KAAK,EAAET,MAAM,EAAE,IAAI,CAAC2B,YAAY,EAAE,CAAC;MAC/D,IAAI,CAACkF,WAAW,CAACD,CAAC,EAAEzH,OAAO,CAAC;MAC5B,OAAOyH,CAAC;IACV;EAAC;IAAAjM,GAAA;IAAAC,KAAA,EAED,SAAAoM,aACIC,YAAoB,EACJ;MAAA,IADMC,SAAS,GAAAlK,SAAA,CAAAR,MAAA,QAAAQ,SAAA,QAAAe,SAAA,GAAAf,SAAA,MAAG,IAAI;MAAA,IAAEvC,IAAa,GAAAuC,SAAA,CAAAR,MAAA,OAAAQ,SAAA,MAAAe,SAAA;MAAA,IACrD0C,KAAgB,GAAAzD,SAAA,CAAAR,MAAA,OAAAQ,SAAA,MAAAe,SAAA;MAClBtD,IAAI,GAAGA,IAAI,IAAI,IAAI,CAACmH,cAAc,EAAE,CAACuF,QAAQ,EAAE;MAC/C,IAAI1G,KAAK,IAAI,IAAI,IAAIA,KAAK,KAAKwG,YAAY,CAACxG,KAAK,EAAE;QACjDwG,YAAY,GAAGA,YAAY,CAACG,IAAI,CAAC3G,KAAK,CAAC;;MAEzC,IAAM4G,CAAC,GAAG,IAAIlP,QAAQ,CAAC8O,YAAY,EAAEC,SAAS,EAAEzM,IAAI,EAAE,IAAI,CAACkH,YAAY,EAAE,CAAC;MAC1E,IAAI,IAAI,CAACvG,KAAK,CAACrC,mBAAmB,CAACsO,CAAC,CAAC5M,IAAI,CAAC,IAAI,IAAI,EAAE;QAClD,MAAM,IAAImC,KAAK,uBAAAM,MAAA,CAAuBmK,CAAC,CAAC5M,IAAI,6BAA0B;;MAExE,IAAI,CAACW,KAAK,CAACrC,mBAAmB,CAACsO,CAAC,CAAC5M,IAAI,CAAC,GAAG4M,CAAC;MAC1C,IAAI,CAACC,MAAM,CAACD,CAAC,EAAE,IAAI,CAAClI,OAAO,CAAC;MAC5B,OAAOkI,CAAC;IACV;EAAC;IAAA1M,GAAA;IAAAC,KAAA,EAED,SAAAiM,YAAYjH,CAAS,EAAET,OAAsB;MAC3C,IAAI,CAAC/D,KAAK,CAAClC,UAAU,EAAE;MACvB,IAAI0G,CAAC,CAACa,KAAK,KAAK,QAAQ,EAAE;QACxB,IAAI,CAACrF,KAAK,CAACjC,gBAAgB,EAAE;;MAE/B;MACA;MACA,IAAI2N,KAAK,GAAG,CAAC;MACb,IAAIlH,CAAC,CAACa,KAAK,KAAK,WAAW,IAAIb,CAAC,CAACa,KAAK,KAAK,QAAQ,EAAE;QACnDqG,KAAK,GAAGlH,CAAC,CAAC2H,IAAI,GAAGlP,IAAI,CAACmP,eAAe,CAAC5H,CAAC,CAACa,KAAK,CAAC;;MAEhD,IAAI,CAACrF,KAAK,CAACnC,QAAQ,IAAI6N,KAAK;MAE5B,IAAI,CAAC,IAAI,CAAC1L,KAAK,CAAC1B,UAAU,CAAC+N,GAAG,CAAC7H,CAAC,CAACI,MAAM,CAAC,EAAE;QACxC,IAAI,CAAC5E,KAAK,CAAChC,cAAc,EAAE;QAC3B,IAAI,CAACgC,KAAK,CAAC1B,UAAU,CAACgO,GAAG,CAAC9H,CAAC,CAACI,MAAM,EAAE;UAClCb,OAAO,EAAEA,OAAO,IAAI,IAAI,CAACA,OAAO;UAChCsB,KAAK,EAAEb,CAAC,CAACa,KAAK;UACdD,KAAK,EAAEZ,CAAC,CAACY,KAAK;UACdsG,KAAK,EAALA;SACD,CAAC;;MAGJ,IAAI,EAAElH,CAAC,YAAYzH,QAAQ,CAAC,EAAE;QAC5B,IAAI,CAACwP,KAAK,CAAC/H,CAAC,CAAC;;IAEjB;IAEA;IACA;IACA;IACA;IACA;EAAA;IAAAjF,GAAA;IAAAC,KAAA,EACA,SAAA0M,OAAO1H,CAAS,EAAET,OAAsB;MACtC,IAAI,CAAC0H,WAAW,CAACjH,CAAC,EAAET,OAAO,CAAC;MAC5B,IAAI,CAACA,OAAO,CAACmI,MAAM,CAAC1H,CAAC,CAACI,MAAM,CAAC;IAC/B;EAAC;IAAArF,GAAA;IAAAC,KAAA,EAED,SAAAgN,aAAa5H,MAAc,EAAEb,OAAsB;MACjD,IAAI,IAAI,CAAC/D,KAAK,CAAC1B,UAAU,CAAC+N,GAAG,CAACzH,MAAM,CAAC,IACjC,IAAI,CAAC5E,KAAK,CAAC1B,UAAU,CAACuD,GAAG,CAAC+C,MAAM,CAAC,CAACb,OAAO,KAAKA,OAAO,EAAE;QACzD,IAAI,CAAC/D,KAAK,CAAC1B,UAAU,CAACmO,MAAM,CAAC7H,MAAM,CAAC;QACpC,IAAI,CAAC5E,KAAK,CAAChC,cAAc,EAAE;;IAE/B;EAAC;IAAAuB,GAAA;IAAAC,KAAA,EACD,SAAAkN,cAAclI,CAAS;MACrB,IAAI,CAAC,IAAI,CAACxE,KAAK,CAAC1B,UAAU,CAAC+N,GAAG,CAAC7H,CAAC,CAACI,MAAM,CAAC,EAAE;QACxC;;MAEF,IAAMC,IAAI,GAAG,IAAI,CAAC7E,KAAK,CAAC1B,UAAU,CAACuD,GAAG,CAAC2C,CAAC,CAACI,MAAM,CAAC;MAEhD,IAAI,CAAC5E,KAAK,CAAClC,UAAU,EAAE;MACvB,IAAI0G,CAAC,CAACa,KAAK,KAAK,QAAQ,EAAE;QACxB,IAAI,CAACrF,KAAK,CAACjC,gBAAgB,EAAE;QAC7B,IAAI,CAACiC,KAAK,CAACnC,QAAQ,IAAIgH,IAAI,CAAC6G,KAAK;;MAEnC;MACA;MACA,IAAIlH,CAAC,CAACa,KAAK,KAAK,WAAW,IAAIb,CAAC,CAACa,KAAK,KAAK,QAAQ,EAAE;QACnD,IAAMqG,KAAK,GAAGlH,CAAC,CAAC2H,IAAI,GAAGlP,IAAI,CAACmP,eAAe,CAAC5H,CAAC,CAACa,KAAK,CAAC;QACpD,IAAI,CAACrF,KAAK,CAACnC,QAAQ,IAAI6N,KAAK;;MAG9B;MACA,IAAI7G,IAAI,CAACd,OAAO,CAACmB,WAAW,CAACV,CAAC,CAACI,MAAM,CAAC,EAAE;QACtC,IAAI,CAAC4H,YAAY,CAAChI,CAAC,CAACI,MAAM,EAAEC,IAAI,CAACd,OAAO,CAAC;;MAG3C;MACA;MACA;IACF;EAAC;IAAAxE,GAAA;IAAAC,KAAA,EAED,SAAAmN,iBAAA,EAAgB;MACd,KAAK,IAAMC,OAAO,IAAI,IAAI,CAAC5M,KAAK,CAACrC,mBAAmB,EAAE;QACpD,IAAMsO,CAAC,GAAG,IAAI,CAACjM,KAAK,CAACrC,mBAAmB,CAACiP,OAAO,CAAC;QACjD,IAAI,CAACC,eAAe,CAACZ,CAAC,CAAC;;IAE3B;EAAC;IAAA1M,GAAA;IAAAC,KAAA,EAED,SAAAqN,gBAAgBZ,CAAW;MACzB,IAAI,CAACS,aAAa,CAACT,CAAC,CAAC;MACrB,IAAI,IAAI,CAACjM,KAAK,CAACrC,mBAAmB,CAACsO,CAAC,CAAC5M,IAAI,CAAC,IAAI,IAAI,EAAE;QAClD,OAAO,IAAI,CAACW,KAAK,CAACrC,mBAAmB,CAACsO,CAAC,CAAC5M,IAAI,CAAC;;IAEjD;EAAC;IAAAE,GAAA;IAAAC,KAAA,EAED,SAAAsN,OAAA,EAAM;MACJ,IAAMjI,IAAI,GAAG,IAAI,CAACd,OAAO,CAAC+I,MAAM,EAAgB;MAChDjI,IAAI,CAAC/G,UAAU,GAAG,IAAI,CAACkC,KAAK,CAAClC,UAAU;MACvC+G,IAAI,CAAC7G,cAAc,GAAG,IAAI,CAACgC,KAAK,CAAChC,cAAc;MAC/C6G,IAAI,CAAChH,QAAQ,GAAG,IAAI,CAACmC,KAAK,CAACnC,QAAQ;MACnC,IAAI,IAAI,CAACmC,KAAK,CAACjC,gBAAgB,GAAG,CAAC,EAAE;QACnC8G,IAAI,CAACkI,UAAU,GAAG,IAAI;QACtB,IAAIlI,IAAI,CAACmI,OAAO,IAAI,IAAI,EAAE;UACxBnI,IAAI,CAACmI,OAAO,GAAG,EAAE;;QAEnBnI,IAAI,CAACmI,OAAO,CAACzE,IAAI,CACb,gDAAgD,GAChD,yBAAyB,CAAC;;MAEhC,OAAO1D,IAAI;IACb;EAAC;IAAAtF,GAAA;IAAAC,KAAA;MAAA,IAAAyN,QAAA,GAAA/M,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CAED,SAAA8M,SAAcC,KAAyD;QAAA,IAAAC,UAAA,EAAAC,eAAA,EAAAC,SAAA,EAAAC,KAAA,EAAA/J,MAAA;QAAA,OAAArD,mBAAA,GAAAO,IAAA,UAAA8M,UAAAC,SAAA;UAAA,kBAAAA,SAAA,CAAA5M,IAAA,GAAA4M,SAAA,CAAA3M,IAAA;YAAA;cAErE,IAAI,CAACd,KAAK,CAACxB,SAAS,GAAG,IAAI;cAErB4O,UAAU,GAAG,IAAI,CAACpN,KAAK,CAACnC,QAAQ;cAChCwP,eAAe,GAAG,IAAI,CAACrN,KAAK,CAAClC,UAAU;cAE7C,IAAI,CAACkC,KAAK,CAACvB,aAAa,CAACI,OAAO,GAAG,EAAE;cAAC4O,SAAA,CAAA3M,IAAA;cAAA,OACEqM,KAAK,EAAE;YAAA;cAA/C,IAAI,CAACnN,KAAK,CAACvB,aAAa,CAACK,MAAM,GAAA2O,SAAA,CAAAnM,IAAA;cAE/B,IAAI,CAACtB,KAAK,CAACxB,SAAS,GAAG,KAAK;cAE5B,IAAI,CAACwB,KAAK,CAACvB,aAAa,CAACG,SAAS,GAAG8O,IAAI,CAACC,GAAG,CAAAhM,KAAA,CAAR+L,IAAI,EAAAE,kBAAA,CAClC,IAAI,CAAC5N,KAAK,CAACvB,aAAa,CAACI,OAAO,CAACM,GAAG,CAAC,UAAAkM,CAAC;gBAAA,OAAIA,CAAC,CAACtB,kBAAkB;cAAA,EAAC,EAAC;cACvE,IAAI,CAAC/J,KAAK,CAACvB,aAAa,CAACC,QAAQ,GAAG,IAAI,CAACsB,KAAK,CAACnC,QAAQ,GAAGuP,UAAU;cACpE,IAAI,CAACpN,KAAK,CAACvB,aAAa,CAACE,UAAU,GAC/B,IAAI,CAACqB,KAAK,CAAClC,UAAU,GAAGuP,eAAe;cAACC,SAAA,GAAAO,0BAAA,CACvB,IAAI,CAAC7N,KAAK,CAACvB,aAAa,CAACI,OAAO;cAAA4O,SAAA,CAAA5M,IAAA;cAAAyM,SAAA,CAAAQ,CAAA;YAAA;cAAA,KAAAP,KAAA,GAAAD,SAAA,CAAAS,CAAA,IAAAC,IAAA;gBAAAP,SAAA,CAAA3M,IAAA;gBAAA;cAAA;cAA1C0C,MAAM,GAAA+J,KAAA,CAAA/N,KAAA;cAAAiO,SAAA,CAAA3M,IAAA;cAAA,OACa0C,MAAM,CAAC6G,YAAY;YAAA;cAA/C7G,MAAM,CAAC6G,YAAY,GAAAoD,SAAA,CAAAnM,IAAA;cAAAmM,SAAA,CAAA3M,IAAA;cAAA,OACM0C,MAAM,CAAC+G,SAAS;YAAA;cAAzC/G,MAAM,CAAC+G,SAAS,GAAAkD,SAAA,CAAAnM,IAAA;YAAA;cAAAmM,SAAA,CAAA3M,IAAA;cAAA;YAAA;cAAA2M,SAAA,CAAA3M,IAAA;cAAA;YAAA;cAAA2M,SAAA,CAAA5M,IAAA;cAAA4M,SAAA,CAAAvK,EAAA,GAAAuK,SAAA;cAAAH,SAAA,CAAAW,CAAA,CAAAR,SAAA,CAAAvK,EAAA;YAAA;cAAAuK,SAAA,CAAA5M,IAAA;cAAAyM,SAAA,CAAAlH,CAAA;cAAA,OAAAqH,SAAA,CAAAS,MAAA;YAAA;cAAA,OAAAT,SAAA,CAAAzM,MAAA,WAEX,IAAI,CAAChB,KAAK,CAACvB,aAAa;YAAA;YAAA;cAAA,OAAAgP,SAAA,CAAAhM,IAAA;UAAA;QAAA,GAAAyL,QAAA;MAAA,CAChC;MAAA,SAAAiB,QAAAC,GAAA;QAAA,OAAAnB,QAAA,CAAAtL,KAAA,OAAAC,SAAA;MAAA;MAAA,OAAAuM,OAAA;IAAA;EAAA;IAAA5O,GAAA;IAAAC,KAAA,EAED,SAAA4I,SAAA,EAAQ;MACN,OAAO,IAAI,CAACpI,KAAK,CAAC/B,aAAa,GAAG,CAAC,IAAI,IAAI,CAAC+B,KAAK,CAAC9B,WAAW,KAAK,CAAC;IACrE;EAAC;IAAAqB,GAAA;IAAAC,KAAA,EAEO,SAAA4H,YACJ5J,UAAkB,EAAEsJ,MAAsB,EAAEqB,OAAiB,EAC7DkG,aAAuB,EAAElH,KAAe,EAAED,KAAmB;MAAA,IAAAoH,MAAA;MAC/D,IAAMC,QAAQ,GACV;QAACC,EAAE,EAAE,IAAI,CAACxO,KAAK,CAACpC,cAAc,EAAE;QAAEJ,UAAU,EAAVA,UAAU;QAAEsJ,MAAM,EAANA,MAAM;QAAEqB,OAAO,EAAPA,OAAO;QAAEhB,KAAK,EAALA;MAAK,CAAC;MAEzE,IAAMsD,UAAU,GAAGnO,WAAW,CAACkB,UAAU,CAAC;MAC1C,IAAIiN,UAAU,IAAI,IAAI,EAAE;QACtB4D,aAAa,GAAG5D,UAAU,CAACgE,QAAQ;;MAErC,IAAIJ,aAAa,IAAI,IAAI,EAAE;QACzBE,QAAQ,CAACG,QAAQ,GAAG,UAACC,GAAa,EAAI;UACpC;UACA;UACAA,GAAG,GAAGA,GAAG,CAACxP,GAAG,CAAC,UAAC6H,EAAE,EAAEzG,CAAC,EAAI;YACtB,IAAIyG,EAAE,IAAI,IAAI,EAAE;cACd,IAAM4H,MAAM,GAAGzG,OAAO,CAAC5H,CAAC,CAAC;cACzB,IAAMsO,IAAI,GAAG5R,IAAI,CAAC6R,mBAAmB,CAACF,MAAM,CAACzC,IAAI,EAAEyC,MAAM,CAACvJ,KAAK,CAAC;cAChE,OAAOiJ,MAAI,CAACpD,UAAU,CAAC2D,IAAI,EAAED,MAAM,CAACxJ,KAAK,EAAEwJ,MAAM,CAACvJ,KAAK,CAAC;;YAE1D,OAAO2B,EAAE;UACX,CAAC,CAAC;UACF;UACA;UACA,OAAOqH,aAAa,CAACM,GAAG,CAACvN,MAAM,GAAG,CAAC,GAAGuN,GAAG,GAAGA,GAAG,CAAC,CAAC,CAAC,EAAExH,KAAK,EAAED,KAAK,CAAC;QACnE,CAAC;;MAEH,IAAI,CAAClH,KAAK,CAAC+O,UAAU,CAACxG,IAAI,CAACgG,QAAQ,CAAC;IACtC;EAAC;IAAAhP,GAAA;IAAAC,KAAA,EAED,SAAAgK,KAAuB1K,MAAS;MAC9BA,MAAM,CAACkQ,IAAI,GAAG,IAAI;MAClB,OAAOlQ,MAAM;IACf;EAAC;IAAAS,GAAA;IAAAC,KAAA,EAEO,SAAAyP,UAAA,EAAS;MACf,IAAI,IAAI,CAACjP,KAAK,CAAC/B,aAAa,KAAK,CAAC,EAAE;QAClC,IAAI,CAAC+B,KAAK,CAAC+O,UAAU,GAAG,EAAE;;MAE5B,IAAI,CAAC/O,KAAK,CAAC/B,aAAa,EAAE;IAC5B;EAAC;IAAAsB,GAAA;IAAAC,KAAA,EAEO,SAAA0P,QAAA,EAAO;MACb,IAAI,CAAClP,KAAK,CAAC/B,aAAa,EAAE;IAC5B;IAEA;;;;EAAA;IAAAsB,GAAA;IAAAC,KAAA,EAIA,SAAAqG,WAAWxG,IAAa;MACtB,IAAM8P,SAAS,GAAe;QAC5B5C,KAAK,EAAE,EAAE;QACTlN,IAAI,EAAE,eAAe;QACrBmP,EAAE,EAAE,IAAI,CAACxO,KAAK,CAAC3B,WAAW;OAC3B;MACD,IAAIgB,IAAI,EAAE;QACR8P,SAAS,CAAC9P,IAAI,GAAGA,IAAI;;MAEvB,IAAI,CAACW,KAAK,CAAC7B,UAAU,CAACoK,IAAI,CAAC4G,SAAS,CAAC;MACrC,IAAI,CAACnP,KAAK,CAACqH,WAAW,GAAG8H,SAAS;IACpC;IAEA;;;;EAAA;IAAA5P,GAAA;IAAAC,KAAA,EAIA,SAAAsG,SAAShH,MAAwB;MAAA,IAAAsQ,MAAA;MAC/B,IAAMC,sBAAsB,GAAGrS,qBAAqB,CAAC8B,MAAM,CAAC;MAC5D,IAAMwQ,yBAAyB,GAC3B,IAAIpQ,GAAG,CAACmQ,sBAAsB,CAAClQ,GAAG,CAAC,UAAAqM,CAAC;QAAA,OAAIA,CAAC,CAACgD,EAAE;MAAA,EAAC,CAAC;MAElD;MACA,KAAK,IAAIjO,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG,IAAI,CAACP,KAAK,CAACqH,WAAW,CAACkF,KAAK,CAACnL,MAAM,EAAEb,CAAC,EAAE,EAAE;QAC5D,IAAMgJ,MAAM,GAAG,IAAI,CAACvJ,KAAK,CAACqH,WAAW,CAACkF,KAAK,CAAChM,CAAC,CAAC;QAC9C,IAAI,CAACgJ,MAAM,CAACyF,IAAI,IAAI,CAACM,yBAAyB,CAACjD,GAAG,CAAC9C,MAAM,CAACiF,EAAE,CAAC,EAAE;UAC7DjF,MAAM,CAAC9J,OAAO,EAAE;;;MAIpB,IAAM8P,QAAQ,GAAG,IAAI,CAACvP,KAAK,CAAC7B,UAAU,CAACqR,GAAG,EAAE;MAC5C,IAAI,CAACxP,KAAK,CAACqH,WAAW,GAAG,IAAI,CAACrH,KAAK,CAAC7B,UAAU,CAACiD,MAAM,KAAK,CAAC,GACvD,IAAI,GACJ,IAAI,CAACpB,KAAK,CAAC7B,UAAU,CAAC,IAAI,CAAC6B,KAAK,CAAC7B,UAAU,CAACiD,MAAM,GAAG,CAAC,CAAC;MAE3D;MACAiO,sBAAsB,CAAC9L,OAAO,CAAC,UAAAgG,MAAM,EAAG;QACtC;QACA;QACA,IAAI,CAACA,MAAM,CAACyF,IAAI,IAAIzF,MAAM,CAACkG,OAAO,KAAKF,QAAQ,CAACf,EAAE,EAAE;UAClDY,MAAI,CAAC7C,KAAK,CAAChD,MAAM,CAAC;;MAEtB,CAAC,CAAC;IACJ;IAEA;;;;;;EAAA;IAAAhK,GAAA;IAAAC,KAAA,EAMA,SAAAkQ,UACItJ,CAAU,EAAEuJ,EAAY,EAAE3I,EAAM,EACR;MAAA,IAAA4I,OAAA;MAAA,IAAxBC,gBAAgB,GAAAjO,SAAA,CAAAR,MAAA,QAAAQ,SAAA,QAAAe,SAAA,GAAAf,SAAA,MAAG,KAAK;MAC1B3E,IAAI,CAAC0L,MAAM,CACPgH,EAAE,CAACvO,MAAM,GAAG,CAAC,EAAE;QAAA,OAAM,2CAA2C;MAAA,EAAC;MACrE,IAAI4F,EAAE,IAAI,IAAI,IAAIA,EAAE,CAAC3B,KAAK,KAAK,SAAS,EAAE;QACxC,MAAM,IAAI7D,KAAK,2CAAAM,MAAA,CAA2CkF,EAAE,CAAC3B,KAAK,OAAI;;MAGxE,IAAMsB,CAAC,GAAG,IAAI,CAACf,SAAS,CACpB;QAAA,OAAMgK,OAAI,CAACX,SAAS,EAAE;MAAA,GAAE;QAAA,OAAMW,OAAI,CAACV,OAAO,EAAE;MAAA,GAC5C;QAAA,OAAMU,OAAI,CAACrK,IAAI,CAAC,SAAS,EAAEa,CAAC,CAAC;MAAA,EAAC;MAElCnJ,IAAI,CAAC0L,MAAM,CACPhC,CAAC,YAAY7J,MAAM,EACnB;QAAA,OAAM,gDAAgD;MAAA,EAAC;MAC3D;MACA,IAAMgT,YAAY,GAAGlT,oBAAoB,CAAC,IAAI,CAACoD,KAAK,CAAC+O,UAAU,EAAEY,EAAE,EAAEhJ,CAAC,CAAC;MACvE,IAAI,CAACkJ,gBAAgB,IAAIC,YAAY,CAAC1O,MAAM,KAAK,CAAC,IAAIuO,EAAE,CAACvO,MAAM,GAAG,CAAC,EAAE;QACnE,MAAM,IAAII,KAAK,CACX,iEAAiE,GACjE,iEAAiE,GACjE,OAAO,CAAC;;MAGd,OAAO,IAAI,CAAC+D,IAAI,CAAC,UAAU,EAAE,YAAK;QAChC,IAAMwK,sBAAsB,GAAiC,EAAE;QAC/DA,sBAAsB,CAACpJ,CAAC,CAAC6H,EAAE,CAAC,GAAIxH,EAAE,IAAI,IAAI,GAAIgJ,IAAI,CAACrJ,CAAC,CAACvB,KAAK,CAAC,GAAG4B,EAAE;QAEhE;QACArK,sBAAsB,CAClBoT,sBAAsB,EAAED,YAAY;QACpC;QACA,UAAA1J,CAAC;UAAA,OAAIwJ,OAAI,CAACrK,IAAI,CAACa,CAAoB,CAAC;QAAA;QACpC;QACA6J,GAAG,CAAC;QACR,IAAMC,KAAK,GAAGP,EAAE,CAACxQ,GAAG,CAAC,UAAAuH,CAAC;UAAA,OAAIqJ,sBAAsB,CAACrJ,CAAC,CAAC8H,EAAE,CAAC;QAAA,EAAC;QAEvD,IAAIoB,OAAI,CAAC5P,KAAK,CAAC/B,aAAa,KAAK,CAAC,EAAE;UAClC;UACA;UACA2R,OAAI,CAAC5P,KAAK,CAAC+O,UAAU,CAACxL,OAAO,CAAC,UAAA4M,IAAI,EAAG;YAAA,IAAAC,UAAA,GAAAvC,0BAAA,CACdsC,IAAI,CAAChJ,KAAK;cAAAkJ,MAAA;YAAA;cAA/B,KAAAD,UAAA,CAAAtC,CAAA,MAAAuC,MAAA,GAAAD,UAAA,CAAArC,CAAA,IAAAC,IAAA,GAAiC;gBAAA,IAAtBzE,MAAM,GAAA8G,MAAA,CAAA7Q,KAAA;gBACf+J,MAAM,CAAC9J,OAAO,EAAE;;YACjB,SAAAyE,GAAA;cAAAkM,UAAA,CAAAnC,CAAA,CAAA/J,GAAA;YAAA;cAAAkM,UAAA,CAAAhK,CAAA;YAAA;UACH,CAAC,CAAC;UACFwJ,OAAI,CAAC5P,KAAK,CAAC+O,UAAU,GAAG,IAAI;;QAE9B,OAAO;UAACvP,KAAK,EAAEmH,CAAC;UAAEuJ,KAAK,EAALA;QAAK,CAAC;MAC1B,CAAC,CAAC;IACJ;EAAC;IAAA3Q,GAAA;IAAAC,KAAA,EAED,SAAA8Q,WAA6BlK,CAAwB;MAAA,IAAAmK,OAAA;MAEnDtT,IAAI,CAAC0L,MAAM,CACP1L,IAAI,CAACuT,UAAU,CAACpK,CAAC,CAAC,EAClB;QAAA,OAAM,mDAAmD;MAAA,EAAC;MAC9D,OAAO,YAA2B;QAAA,SAAAqK,IAAA,GAAA7O,SAAA,CAAAR,MAAA,EAAvB0F,MAAgB,OAAA9H,KAAA,CAAAyR,IAAA,GAAAC,IAAA,MAAAA,IAAA,GAAAD,IAAA,EAAAC,IAAA;UAAhB5J,MAAgB,CAAA4J,IAAA,IAAA9O,SAAA,CAAA8O,IAAA;QAAA;QACzBzT,IAAI,CAAC0L,MAAM,CACP7B,MAAM,CAAC6J,KAAK,CAAC,UAAAnF,CAAC;UAAA,OAAIA,CAAC,YAAY1O,MAAM;QAAA,EAAC,EACtC;UAAA,OAAM,2DAA2D,GAC7D,SAAS;QAAA,EAAC;QAElB,IAAIuJ,GAGH;QACD,IAAMuK,QAAQ,GAAmB,EAAE;QACnC9J,MAAM,CAACvD,OAAO,CAAC,UAACsN,KAAK,EAAEtQ,CAAC,EAAI;UAC1BqQ,QAAQ,CAACrQ,CAAC,CAAC,GAAGsQ,KAAK;QACrB,CAAC,CAAC;QAEF,IAAMzH,WAAW,GAAmB,SAA9BA,WAAWA,CAAoB6B,CAAC,EAAE6F,IAAI,EAAI;UAC9CzK,GAAG,GAAGD,CAAC,CAAAzE,KAAA,YAAAG,MAAA,CAAQgF,MAAM,GAAEgK,IAAI,GAAE;UAC7B7T,IAAI,CAAC0L,MAAM,CACPtC,GAAG,CAAC7G,KAAK,YAAY1C,MAAM,EAC3B;YAAA,OAAM,wDAAwD,GAC1D,sCAAsC;UAAA,EAAC;UAC/CG,IAAI,CAAC0L,MAAM,CACP1L,IAAI,CAACuT,UAAU,CAACnK,GAAG,CAACoI,QAAQ,CAAC,EAC7B;YAAA,OAAM,wDAAwD,GAC1D,4CAA4C;UAAA,EAAC;UACrD,OAAOpI,GAAG,CAAC7G,KAAK;QAClB,CAAC;QAED,IAAMkK,aAAa,GAAG,SAAhBA,aAAaA,CAAI1C,EAAK,EAAEG,KAAe,EAAI;UAC/C,IAAM4J,OAAO,GAAG1K,GAAG,CAACoI,QAAQ,CAACzH,EAAE,EAAEG,KAAK,CAAC;UACvC,IAAM+I,KAAK,GAAalR,KAAK,CAAC4J,OAAO,CAACmI,OAAO,CAAC,GAAGA,OAAO,GAAG,CAACA,OAAO,CAAC;UACpE9T,IAAI,CAAC0L,MAAM,CACPuH,KAAK,CAAC9O,MAAM,KAAK0F,MAAM,CAAC1F,MAAM,EAC9B;YAAA,OAAM,wDAAwD,GAC1D,yDAAyD,GACzD,wDAAwD;UAAA,EAAC;UACjEnE,IAAI,CAAC0L,MAAM,CACPuH,KAAK,CAACS,KAAK,CAAC,UAAAnF,CAAC;YAAA,OAAIA,CAAC,YAAY1O,MAAM;UAAA,EAAC,EACrC;YAAA,OAAM,wDAAwD,GAC1D,yDAAyD,GACzD,yBAAyB;UAAA,EAAC;UAClC,IAAMkU,OAAO,GAAkC,EAAE;UACjDd,KAAK,CAAC3M,OAAO,CAAC,UAACwD,IAAI,EAAExG,CAAC,EAAI;YACxByQ,OAAO,CAACzQ,CAAC,CAAC,GAAG;cAAA,OAAMwG,IAAI;YAAA;UACzB,CAAC,CAAC;UACF,OAAOiK,OAAO;QAChB,CAAC;QAED,OAAOT,OAAI,CAAChJ,aAAa,CAAC;UACxB6B,WAAW,EAAXA,WAAW;UACXM,aAAa,EAAbA,aAAa;UACb5C,MAAM,EAAE8J;SACT,CAAC;MACJ,CAAC;IACH;EAAC;IAAArR,GAAA;IAAAC,KAAA,EAED,SAAAwF,SAASJ,MAAc;MACrB;MACA,IAAMC,IAAI,GAAG,IAAI,CAAC7E,KAAK,CAAC1B,UAAU,CAACuD,GAAG,CAAC+C,MAAM,CAAC;MAC9C,OAAOC,IAAI,CAACd,OAAO,CAACiB,QAAQ,CAACJ,MAAM,CAAC;IACtC;EAAC;IAAArF,GAAA;IAAAC,KAAA,EACD,SAAAyR,KAAKrM,MAAc;MACjB;MACA,IAAMC,IAAI,GAAG,IAAI,CAAC7E,KAAK,CAAC1B,UAAU,CAACuD,GAAG,CAAC+C,MAAM,CAAC;MAC9C,OAAOC,IAAI,CAACd,OAAO,CAACkN,IAAI,CAACrM,MAAM,CAAC;IAClC;EAAC;IAAArF,GAAA;IAAAC,KAAA,EAED,SAAA0R,UAAUtM,MAAc,EAAEuM,OAA0B;MAClD;MACA,IAAMtM,IAAI,GAAG,IAAI,CAAC7E,KAAK,CAAC1B,UAAU,CAACuD,GAAG,CAAC+C,MAAM,CAAC;MAC9C,OAAOC,IAAI,CAACd,OAAO,CAACmN,SAAS,CAACtM,MAAM,EAAEuM,OAAO,CAAC;IAChD;EAAC;IAAA5R,GAAA;IAAAC,KAAA;MAAA,IAAA4R,KAAA,GAAAlR,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CAED,SAAAiR,SAAWlE,KAAiB;QAAA,IAAAjH,KAAA,EAAAoL,UAAA;QAAA,OAAAnR,mBAAA,GAAAO,IAAA,UAAA6Q,UAAAC,SAAA;UAAA,kBAAAA,SAAA,CAAA3Q,IAAA,GAAA2Q,SAAA,CAAA1Q,IAAA;YAAA;cACpBoF,KAAK,GAAG9I,GAAG,EAAE;cAAAoU,SAAA,CAAA1Q,IAAA;cAAA,OACM,IAAI,CAACiD,OAAO,CAAC0N,IAAI,CAACtE,KAAK,CAAe;YAAA;cAAzDmE,UAAU,GAAAE,SAAA,CAAAlQ,IAAA;cAChBgQ,UAAU,CAACI,MAAM,GAAGtU,GAAG,EAAE,GAAG8I,KAAK;cAAC,OAAAsL,SAAA,CAAAxQ,MAAA,WAC3BsQ,UAAU;YAAA;YAAA;cAAA,OAAAE,SAAA,CAAA/P,IAAA;UAAA;QAAA,GAAA4P,QAAA;MAAA,CAClB;MAAA,SAAAI,KAAAE,GAAA;QAAA,OAAAP,KAAA,CAAAzP,KAAA,OAAAC,SAAA;MAAA;MAAA,OAAA6P,IAAA;IAAA;IAED;;;;;;EAAA;IAAAlS,GAAA;IAAAC,KAAA,EAMQ,SAAA+M,MAAwBzN,MAAS;MACvC,IAAI,IAAI,CAACkB,KAAK,CAACqH,WAAW,IAAI,IAAI,EAAE;QAClCvI,MAAM,CAAC2Q,OAAO,GAAG,IAAI,CAACzP,KAAK,CAACqH,WAAW,CAACmH,EAAE;QAC1C,IAAI,CAACxO,KAAK,CAACqH,WAAW,CAACkF,KAAK,CAAChE,IAAI,CAACzJ,MAAM,CAAC;;MAG3C,OAAOA,MAAM;IACf;EAAC;IAAAS,GAAA;IAAAsC,GAAA,EAED,SAAAA,IAAA,EAAuB;MACrB,OAAO,IAAI,CAAC7B,KAAK,CAACrC,mBAAmB;IACvC;IAEA;;;;EAAA;IAAA4B,GAAA;IAAAC,KAAA,EAIA,SAAAoS,MAAA,EAAK;MACH;MACA,IAAI,CAAC7R,oBAAoB,EAAE;MAE3B,IAAI,CAACC,KAAK,CAACP,OAAO,EAAE;MACpB,IAAI,CAACG,GAAG,CAACgS,KAAK,EAAE;MAChB,IAAI,CAAC5R,KAAK,GAAG,IAAIvC,WAAW,EAAE;MAE9B,KAAK,IAAM+C,WAAW,IAAI,IAAI,CAACX,QAAQ,EAAE;QACvC,IAAI,CAAC6D,wBAAwB,CAAClD,WAAW,CAAC;QAC1C,IAAI,CAACX,QAAQ,CAACW,WAAW,CAAC,CAACf,OAAO,EAAE;QACpC,OAAO,IAAI,CAACI,QAAQ,CAACW,WAAW,CAAC;;MAEnC,IAAI,CAACA,WAAW,GAAG,IAAI;MACvB,IAAI,CAACU,eAAe,GAAG,IAAI;MAC3B,IAAI,CAACH,kBAAkB,GAAG,IAAI;IAChC;EAAC;EAAA,OAAApB,MAAA;AAAA;AAxxBcA,MAAA,CAAA4G,YAAY,GAAG,CAAC;AAKhB5G,MAAA,CAAA6G,cAAc,GAAG,CAAC;AAsxBnC,SAASwJ,IAAIA,CAAC5K,KAAe;EAC3B,IAAML,MAAM,GAAG5H,kBAAkB,CAACE,aAAa,CAAC+H,KAAK,CAAC,EAAE,SAAS,CAAC;EAClE,OAAOwB,MAAM,CAACsE,UAAU,CAACnG,MAAM,EAAEK,KAAK,EAAE,SAAS,CAAC;AACpD;AAEA,OAAM,SAAUyM,eAAeA,CAAA;EAC7B,IAAMC,EAAE,GAAG5V,kBAAkB,EAAoC;EACjE,IAAI4V,EAAE,CAACC,SAAS,IAAI,IAAI,EAAE;IACxB,IAAMC,WAAW,GAAG,IAAIhW,WAAW,CAAC8V,EAAE,CAAC;IACvCA,EAAE,CAACC,SAAS,GAAG,IAAIpS,MAAM,CAACqS,WAAW,CAAC;;EAExC/V,oBAAoB,CAAC6V,EAAE,CAACC,SAAS,CAACnS,GAAG,CAAC;EAEtC;EACA;EACA/C,gBAAgB,CAAC;IAAA,OAAMiV,EAAE,CAACC,SAAS;EAAA,EAAC;EACpC,OAAOD,EAAE,CAACC,SAAS;AACrB;AAEA,OAAO,IAAMnL,MAAM,GAAGiL,eAAe,EAAE;AAEvC;;;;;;AAMA,OAAM,SAAU5B,GAAGA,CAACzL,CAAS,EAAEC,CAAS;EACtC;EACA,IAAMqC,MAAM,GAAG;IAACtC,CAAC,EAADA,CAAC;IAAEC,CAAC,EAADA;EAAC,CAAC;EACrB,OAAOmC,MAAM,CAACC,SAAS,CAAC1K,GAAG,EAAE2K,MAAmC,CAAC;AACnE"},"metadata":{},"sourceType":"module","externalDependencies":[]}