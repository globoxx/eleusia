{"ast":null,"code":"import _slicedToArray from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/slicedToArray.js\";\n/**\r\n * @license\r\n * Copyright 2019 Google LLC. All Rights Reserved.\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n * http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n * =============================================================================\r\n */\nimport { ENGINE } from '../../engine';\nimport { customGrad } from '../../gradients';\nimport { FusedDepthwiseConv2D } from '../../kernel_names';\nimport { makeTypesMatch } from '../../tensor_util';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { add } from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport * as conv_util from '../conv_util';\nimport { depthwiseConv2d as unfusedDepthwiseConv2d } from '../depthwise_conv2d';\nimport { depthwiseConv2dNativeBackpropFilter } from '../depthwise_conv2d_native_backprop_filter';\nimport { depthwiseConv2dNativeBackpropInput } from '../depthwise_conv2d_native_backprop_input';\nimport { applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse } from '../fused_util';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\n/**\r\n * Computes depthwise 2D convolution, optionally fused with adding a\r\n * bias and applying an activation.\r\n *\r\n * Given a 4D `input` array and a `filter` array of shape\r\n * `[filterHeight, filterWidth, inChannels, channelMultiplier]` containing\r\n * `inChannels` convolutional filters of depth 1, this op applies a\r\n * different filter to each input channel (expanding from 1 channel to\r\n * `channelMultiplier` channels for each), then concatenates the results\r\n * together. The output has `inChannels * channelMultiplier` channels.\r\n *\r\n * See\r\n * [https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d](\r\n *     https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d)\r\n * for more details.\r\n *\r\n * @param obj An object with the following properties:\r\n * @param x The input tensor, of rank 4 or rank 3, of shape\r\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is\r\n * assumed.\r\n * @param filter The filter tensor, rank 4, of shape\r\n *     `[filterHeight, filterWidth, inChannels, channelMultiplier]`.\r\n * @param strides The strides of the convolution: `[strideHeight,\r\n * strideWidth]`. If strides is a single number, then `strideHeight ==\r\n * strideWidth`.\r\n * @param pad The type of padding algorithm.\r\n *   - `same` and stride 1: output will be of same size as input,\r\n *       regardless of filter size.\r\n *   - `valid`: output will be smaller than input if filter is larger\r\n *       than 1x1.\r\n *   - For more info, see this guide:\r\n *     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](\r\n *          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)\r\n * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`\r\n *     in which we sample input values across the height and width dimensions\r\n *     in atrous convolution. Defaults to `[1, 1]`. If `rate` is a single\r\n *     number, then `dilationHeight == dilationWidth`. If it is greater than\r\n *     1, then all values of `strides` must be 1.\r\n * @param dataFormat: An optional string from: \"NHWC\", \"NCHW\". Defaults to\r\n *     \"NHWC\". Specify the data format of the input and output data. With the\r\n *     default format \"NHWC\", the data is stored in the order of: [batch,\r\n *     height, width, channels]. Only \"NHWC\" is currently supported.\r\n * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is\r\n *     provided, it will default to truncate.\r\n * @param bias Tensor to be added to the result.\r\n * @param activation Name of activation kernel (defaults to `linear`).\r\n * @param preluActivationWeights Tensor of prelu weights to be applied as part\r\n *     of a `prelu` activation, typically the same shape as `x`.\r\n * @param leakyreluAlpha Optional. Alpha to be applied as part of a `leakyrelu`\r\n *     activation.\r\n */\nfunction fusedDepthwiseConv2d_(_ref) {\n  var x = _ref.x,\n    filter = _ref.filter,\n    strides = _ref.strides,\n    pad = _ref.pad,\n    _ref$dataFormat = _ref.dataFormat,\n    dataFormat = _ref$dataFormat === void 0 ? 'NHWC' : _ref$dataFormat,\n    _ref$dilations = _ref.dilations,\n    dilations = _ref$dilations === void 0 ? [1, 1] : _ref$dilations,\n    dimRoundingMode = _ref.dimRoundingMode,\n    bias = _ref.bias,\n    _ref$activation = _ref.activation,\n    activation = _ref$activation === void 0 ? 'linear' : _ref$activation,\n    preluActivationWeights = _ref.preluActivationWeights,\n    leakyreluAlpha = _ref.leakyreluAlpha;\n  if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n    var result = unfusedDepthwiseConv2d(x, filter, strides, pad, dataFormat, dilations, dimRoundingMode);\n    if (bias != null) {\n      result = add(result, bias);\n    }\n    return applyActivation(result, activation, preluActivationWeights, leakyreluAlpha);\n  }\n  var $x = convertToTensor(x, 'x', 'depthwiseConv2d', 'float32');\n  var $filter = convertToTensor(filter, 'filter', 'depthwiseConv2d', 'float32');\n  var x4D = $x;\n  var reshapedTo4D = false;\n  if ($x.rank === 3) {\n    reshapedTo4D = true;\n    x4D = reshape($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);\n  }\n  util.assert(x4D.rank === 4, function () {\n    return \"Error in fused depthwiseConv2d: input must be rank 4, but got \" + \"rank \".concat(x4D.rank, \".\");\n  });\n  util.assert($filter.rank === 4, function () {\n    return \"Error in fused depthwiseConv2d: filter must be rank 4, \" + \"but got rank \".concat($filter.rank, \".\");\n  });\n  util.assert(x4D.shape[3] === $filter.shape[2], function () {\n    return \"Error in fused depthwiseConv2d: number of input channels \" + \"(\".concat(x4D.shape[3], \") must match the inChannels dimension in \") + \"filter \".concat($filter.shape[2], \".\");\n  });\n  if (dilations == null) {\n    dilations = [1, 1];\n  }\n  util.assert(conv_util.eitherStridesOrDilationsAreOne(strides, dilations), function () {\n    return 'Error in fused depthwiseConv2d: Either strides or dilations must ' + \"be 1. Got strides \".concat(strides, \" and dilations '\").concat(dilations, \"'\");\n  });\n  conv_util.checkPadOnDimRoundingMode('fused depthwiseConv2d', pad, dimRoundingMode);\n  var convInfo = conv_util.computeConv2DInfo(x4D.shape, $filter.shape, strides, dilations, pad, dimRoundingMode, true /* depthwise */);\n  var $bias;\n  if (bias != null) {\n    $bias = convertToTensor(bias, 'bias', 'fused conv2d');\n    var _makeTypesMatch = makeTypesMatch($bias, $x);\n    var _makeTypesMatch2 = _slicedToArray(_makeTypesMatch, 1);\n    $bias = _makeTypesMatch2[0];\n    broadcast_util.assertAndGetBroadcastShape(convInfo.outShape, $bias.shape);\n  }\n  var $preluActivationWeights;\n  if (preluActivationWeights != null) {\n    $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused depthwiseConv2d');\n  }\n  var grad = function grad(dy, saved) {\n    util.assert(conv_util.tupleValuesAreOne(dilations), function () {\n      return 'Error in gradient of fused depthwiseConv2d: dilation rates ' + \"greater than 1 are not yet supported. Got dilations \" + \"'\".concat(dilations, \"'\");\n    });\n    var _saved = _slicedToArray(saved, 4),\n      $filter = _saved[0],\n      x4D = _saved[1],\n      y = _saved[2],\n      bias = _saved[3];\n    var dyActivation = getFusedDyActivation(dy, y, activation);\n    var xDer = depthwiseConv2dNativeBackpropInput(x4D.shape, dyActivation, $filter, strides, pad, dilations, dimRoundingMode);\n    var filterDer = depthwiseConv2dNativeBackpropFilter(x4D, dyActivation, $filter.shape, strides, pad, dilations, dimRoundingMode);\n    if (bias != null) {\n      var biasDer = getFusedBiasGradient($bias, dyActivation);\n      return [xDer, filterDer, biasDer];\n    }\n    return [xDer, filterDer];\n  };\n  var inputs = {\n    x: x4D,\n    filter: $filter,\n    bias: $bias,\n    preluActivationWeights: $preluActivationWeights\n  };\n  var attrs = {\n    strides: strides,\n    pad: pad,\n    dataFormat: dataFormat,\n    dilations: dilations,\n    dimRoundingMode: dimRoundingMode,\n    activation: activation,\n    leakyreluAlpha: leakyreluAlpha\n  };\n  // Depending on the the params passed in we will have different number of\n  // inputs and thus a a different number of elements in the gradient.\n  if (bias == null) {\n    var customOp = customGrad(function (x4D, filter, save) {\n      // tslint:disable-next-line: no-unnecessary-type-assertion\n      var res = ENGINE.runKernel(FusedDepthwiseConv2D, inputs, attrs);\n      save([filter, x4D, res]);\n      if (reshapedTo4D) {\n        // tslint:disable-next-line: no-unnecessary-type-assertion\n        res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n      }\n      return {\n        value: res,\n        gradFunc: grad\n      };\n    });\n    return customOp(x4D, $filter);\n  } else {\n    var customOpWithBias = customGrad(function (x4D, filter, bias, save) {\n      // tslint:disable-next-line: no-unnecessary-type-assertion\n      var res = ENGINE.runKernel(FusedDepthwiseConv2D, inputs, attrs);\n      save([filter, x4D, res, bias]);\n      if (reshapedTo4D) {\n        // tslint:disable-next-line: no-unnecessary-type-assertion\n        res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n      }\n      return {\n        value: res,\n        gradFunc: grad\n      };\n    });\n    return customOpWithBias(x4D, $filter, $bias);\n  }\n}\nexport var depthwiseConv2d = /* @__PURE__ */op({\n  fusedDepthwiseConv2d_: fusedDepthwiseConv2d_\n});","map":{"version":3,"names":["ENGINE","customGrad","FusedDepthwiseConv2D","makeTypesMatch","convertToTensor","util","add","broadcast_util","conv_util","depthwiseConv2d","unfusedDepthwiseConv2d","depthwiseConv2dNativeBackpropFilter","depthwiseConv2dNativeBackpropInput","applyActivation","getFusedBiasGradient","getFusedDyActivation","shouldFuse","op","reshape","fusedDepthwiseConv2d_","_ref","x","filter","strides","pad","_ref$dataFormat","dataFormat","_ref$dilations","dilations","dimRoundingMode","bias","_ref$activation","activation","preluActivationWeights","leakyreluAlpha","state","gradientDepth","result","$x","$filter","x4D","reshapedTo4D","rank","shape","assert","concat","eitherStridesOrDilationsAreOne","checkPadOnDimRoundingMode","convInfo","computeConv2DInfo","$bias","_makeTypesMatch","_makeTypesMatch2","_slicedToArray","assertAndGetBroadcastShape","outShape","$preluActivationWeights","grad","dy","saved","tupleValuesAreOne","_saved","y","dyActivation","xDer","filterDer","biasDer","inputs","attrs","customOp","save","res","runKernel","value","gradFunc","customOpWithBias"],"sources":["C:\\Users\\vince\\OneDrive\\Documents\\GitHub\\tfjs-core\\src\\ops\\fused\\depthwise_conv2d.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from '../../engine';\nimport {customGrad} from '../../gradients';\nimport {FusedDepthwiseConv2D, FusedDepthwiseConv2DAttrs, FusedDepthwiseConv2DInputs} from '../../kernel_names';\nimport {NamedAttrMap} from '../../kernel_registry';\nimport {Tensor, Tensor3D, Tensor4D} from '../../tensor';\nimport {GradSaveFunc, NamedTensorMap} from '../../tensor_types';\nimport {makeTypesMatch} from '../../tensor_util';\nimport {convertToTensor} from '../../tensor_util_env';\nimport {TensorLike} from '../../types';\nimport * as util from '../../util';\nimport {add} from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport * as conv_util from '../conv_util';\nimport {depthwiseConv2d as unfusedDepthwiseConv2d} from '../depthwise_conv2d';\nimport {depthwiseConv2dNativeBackpropFilter} from '../depthwise_conv2d_native_backprop_filter';\nimport {depthwiseConv2dNativeBackpropInput} from '../depthwise_conv2d_native_backprop_input';\nimport {Activation} from '../fused_types';\nimport {applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse} from '../fused_util';\nimport {op} from '../operation';\nimport {reshape} from '../reshape';\n\n/**\n * Computes depthwise 2D convolution, optionally fused with adding a\n * bias and applying an activation.\n *\n * Given a 4D `input` array and a `filter` array of shape\n * `[filterHeight, filterWidth, inChannels, channelMultiplier]` containing\n * `inChannels` convolutional filters of depth 1, this op applies a\n * different filter to each input channel (expanding from 1 channel to\n * `channelMultiplier` channels for each), then concatenates the results\n * together. The output has `inChannels * channelMultiplier` channels.\n *\n * See\n * [https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d](\n *     https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d)\n * for more details.\n *\n * @param obj An object with the following properties:\n * @param x The input tensor, of rank 4 or rank 3, of shape\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is\n * assumed.\n * @param filter The filter tensor, rank 4, of shape\n *     `[filterHeight, filterWidth, inChannels, channelMultiplier]`.\n * @param strides The strides of the convolution: `[strideHeight,\n * strideWidth]`. If strides is a single number, then `strideHeight ==\n * strideWidth`.\n * @param pad The type of padding algorithm.\n *   - `same` and stride 1: output will be of same size as input,\n *       regardless of filter size.\n *   - `valid`: output will be smaller than input if filter is larger\n *       than 1x1.\n *   - For more info, see this guide:\n *     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](\n *          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)\n * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`\n *     in which we sample input values across the height and width dimensions\n *     in atrous convolution. Defaults to `[1, 1]`. If `rate` is a single\n *     number, then `dilationHeight == dilationWidth`. If it is greater than\n *     1, then all values of `strides` must be 1.\n * @param dataFormat: An optional string from: \"NHWC\", \"NCHW\". Defaults to\n *     \"NHWC\". Specify the data format of the input and output data. With the\n *     default format \"NHWC\", the data is stored in the order of: [batch,\n *     height, width, channels]. Only \"NHWC\" is currently supported.\n * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is\n *     provided, it will default to truncate.\n * @param bias Tensor to be added to the result.\n * @param activation Name of activation kernel (defaults to `linear`).\n * @param preluActivationWeights Tensor of prelu weights to be applied as part\n *     of a `prelu` activation, typically the same shape as `x`.\n * @param leakyreluAlpha Optional. Alpha to be applied as part of a `leakyrelu`\n *     activation.\n */\nfunction fusedDepthwiseConv2d_<T extends Tensor3D|Tensor4D>({\n  x,\n  filter,\n  strides,\n  pad,\n  dataFormat = 'NHWC',\n  dilations = [1, 1],\n  dimRoundingMode,\n  bias,\n  activation = 'linear',\n  preluActivationWeights,\n  leakyreluAlpha\n}: {\n  x: T|TensorLike,\n  filter: Tensor4D|TensorLike,\n  strides: [number, number]|number,\n  pad: 'valid'|'same'|number,\n  dataFormat?: 'NHWC'|'NCHW',\n  dilations?: [number, number]|number,\n  dimRoundingMode?: 'floor'|'round'|'ceil',\n  bias?: Tensor|TensorLike,\n  activation?: Activation,\n  preluActivationWeights?: Tensor,\n  leakyreluAlpha?: number\n}): T {\n  if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n    let result = unfusedDepthwiseConv2d(\n        x, filter, strides, pad, dataFormat, dilations, dimRoundingMode);\n    if (bias != null) {\n      result = add(result, bias);\n    }\n\n    return applyActivation(\n               result, activation, preluActivationWeights, leakyreluAlpha) as T;\n  }\n\n  const $x = convertToTensor(x, 'x', 'depthwiseConv2d', 'float32');\n  const $filter =\n      convertToTensor(filter, 'filter', 'depthwiseConv2d', 'float32');\n\n  let x4D = $x as Tensor4D;\n  let reshapedTo4D = false;\n  if ($x.rank === 3) {\n    reshapedTo4D = true;\n    x4D = reshape($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);\n  }\n  util.assert(\n      x4D.rank === 4,\n      () => `Error in fused depthwiseConv2d: input must be rank 4, but got ` +\n          `rank ${x4D.rank}.`);\n  util.assert(\n      $filter.rank === 4,\n      () => `Error in fused depthwiseConv2d: filter must be rank 4, ` +\n          `but got rank ${$filter.rank}.`);\n  util.assert(\n      x4D.shape[3] === $filter.shape[2],\n      () => `Error in fused depthwiseConv2d: number of input channels ` +\n          `(${x4D.shape[3]}) must match the inChannels dimension in ` +\n          `filter ${$filter.shape[2]}.`);\n  if (dilations == null) {\n    dilations = [1, 1];\n  }\n  util.assert(\n      conv_util.eitherStridesOrDilationsAreOne(strides, dilations),\n      () =>\n          'Error in fused depthwiseConv2d: Either strides or dilations must ' +\n          `be 1. Got strides ${strides} and dilations '${dilations}'`);\n  conv_util.checkPadOnDimRoundingMode(\n      'fused depthwiseConv2d', pad, dimRoundingMode);\n  const convInfo = conv_util.computeConv2DInfo(\n      x4D.shape, $filter.shape, strides, dilations, pad, dimRoundingMode,\n      true /* depthwise */);\n\n  let $bias: Tensor;\n  if (bias != null) {\n    $bias = convertToTensor(bias, 'bias', 'fused conv2d');\n    [$bias] = makeTypesMatch($bias, $x);\n\n    broadcast_util.assertAndGetBroadcastShape(convInfo.outShape, $bias.shape);\n  }\n\n  let $preluActivationWeights: Tensor;\n  if (preluActivationWeights != null) {\n    $preluActivationWeights = convertToTensor(\n        preluActivationWeights, 'prelu weights', 'fused depthwiseConv2d');\n  }\n\n  const grad = (dy: Tensor4D, saved: Tensor[]) => {\n    util.assert(\n        conv_util.tupleValuesAreOne(dilations),\n        () => 'Error in gradient of fused depthwiseConv2d: dilation rates ' +\n            `greater than 1 are not yet supported. Got dilations ` +\n            `'${dilations}'`);\n    const [$filter, x4D, y, bias] = saved;\n\n    const dyActivation = getFusedDyActivation(dy, y, activation) as Tensor4D;\n\n    const xDer = depthwiseConv2dNativeBackpropInput(\n        (x4D as Tensor4D).shape, dyActivation, $filter as Tensor4D, strides,\n        pad, dilations, dimRoundingMode);\n    const filterDer = depthwiseConv2dNativeBackpropFilter(\n        x4D as Tensor4D, dyActivation, ($filter as Tensor4D).shape, strides,\n        pad, dilations, dimRoundingMode);\n\n    if (bias != null) {\n      const biasDer = getFusedBiasGradient($bias, dyActivation);\n      return [xDer, filterDer, biasDer];\n    }\n    return [xDer, filterDer];\n  };\n\n  const inputs: FusedDepthwiseConv2DInputs = {\n    x: x4D,\n    filter: $filter,\n    bias: $bias,\n    preluActivationWeights: $preluActivationWeights\n  };\n  const attrs: FusedDepthwiseConv2DAttrs = {\n    strides,\n    pad,\n    dataFormat,\n    dilations,\n    dimRoundingMode,\n    activation,\n    leakyreluAlpha\n  };\n\n  // Depending on the the params passed in we will have different number of\n  // inputs and thus a a different number of elements in the gradient.\n  if (bias == null) {\n    const customOp =\n        customGrad((x4D: Tensor4D, filter: Tensor4D, save: GradSaveFunc) => {\n          // tslint:disable-next-line: no-unnecessary-type-assertion\n          let res: Tensor4D|Tensor3D = ENGINE.runKernel(\n              FusedDepthwiseConv2D, inputs as unknown as NamedTensorMap,\n              attrs as unknown as NamedAttrMap);\n\n          save([filter, x4D, res]);\n\n          if (reshapedTo4D) {\n            // tslint:disable-next-line: no-unnecessary-type-assertion\n            res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]) as\n                Tensor3D;\n          }\n\n          return {value: res, gradFunc: grad};\n        });\n    return customOp(x4D, $filter) as T;\n  } else {\n    const customOpWithBias = customGrad(\n        (x4D: Tensor4D, filter: Tensor4D, bias: Tensor, save: GradSaveFunc) => {\n          // tslint:disable-next-line: no-unnecessary-type-assertion\n          let res: Tensor4D|Tensor3D = ENGINE.runKernel(\n              FusedDepthwiseConv2D, inputs as unknown as NamedTensorMap,\n              attrs as unknown as NamedAttrMap);\n\n          save([filter, x4D, res, bias]);\n\n          if (reshapedTo4D) {\n            // tslint:disable-next-line: no-unnecessary-type-assertion\n            res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]) as\n                Tensor3D;\n          }\n\n          return {value: res, gradFunc: grad};\n        });\n\n    return customOpWithBias(x4D, $filter, $bias) as T;\n  }\n}\nexport const depthwiseConv2d = /* @__PURE__ */ op({fusedDepthwiseConv2d_});\n"],"mappings":";AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQA,MAAM,QAAO,cAAc;AACnC,SAAQC,UAAU,QAAO,iBAAiB;AAC1C,SAAQC,oBAAoB,QAA8D,oBAAoB;AAI9G,SAAQC,cAAc,QAAO,mBAAmB;AAChD,SAAQC,eAAe,QAAO,uBAAuB;AAErD,OAAO,KAAKC,IAAI,MAAM,YAAY;AAClC,SAAQC,GAAG,QAAO,QAAQ;AAC1B,OAAO,KAAKC,cAAc,MAAM,mBAAmB;AACnD,OAAO,KAAKC,SAAS,MAAM,cAAc;AACzC,SAAQC,eAAe,IAAIC,sBAAsB,QAAO,qBAAqB;AAC7E,SAAQC,mCAAmC,QAAO,4CAA4C;AAC9F,SAAQC,kCAAkC,QAAO,2CAA2C;AAE5F,SAAQC,eAAe,EAAEC,oBAAoB,EAAEC,oBAAoB,EAAEC,UAAU,QAAO,eAAe;AACrG,SAAQC,EAAE,QAAO,cAAc;AAC/B,SAAQC,OAAO,QAAO,YAAY;AAElC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAmDA,SAASC,qBAAqBA,CAAAC,IAAA,EAwB7B;EAAA,IAvBCC,CAAC,GAAAD,IAAA,CAADC,CAAC;IACDC,MAAM,GAAAF,IAAA,CAANE,MAAM;IACNC,OAAO,GAAAH,IAAA,CAAPG,OAAO;IACPC,GAAG,GAAAJ,IAAA,CAAHI,GAAG;IAAAC,eAAA,GAAAL,IAAA,CACHM,UAAU;IAAVA,UAAU,GAAAD,eAAA,cAAG,MAAM,GAAAA,eAAA;IAAAE,cAAA,GAAAP,IAAA,CACnBQ,SAAS;IAATA,SAAS,GAAAD,cAAA,cAAG,CAAC,CAAC,EAAE,CAAC,CAAC,GAAAA,cAAA;IAClBE,eAAe,GAAAT,IAAA,CAAfS,eAAe;IACfC,IAAI,GAAAV,IAAA,CAAJU,IAAI;IAAAC,eAAA,GAAAX,IAAA,CACJY,UAAU;IAAVA,UAAU,GAAAD,eAAA,cAAG,QAAQ,GAAAA,eAAA;IACrBE,sBAAsB,GAAAb,IAAA,CAAtBa,sBAAsB;IACtBC,cAAc,GAAAd,IAAA,CAAdc,cAAc;EAcd,IAAIlB,UAAU,CAAChB,MAAM,CAACmC,KAAK,CAACC,aAAa,EAAEJ,UAAU,CAAC,KAAK,KAAK,EAAE;IAChE,IAAIK,MAAM,GAAG3B,sBAAsB,CAC/BW,CAAC,EAAEC,MAAM,EAAEC,OAAO,EAAEC,GAAG,EAAEE,UAAU,EAAEE,SAAS,EAAEC,eAAe,CAAC;IACpE,IAAIC,IAAI,IAAI,IAAI,EAAE;MAChBO,MAAM,GAAG/B,GAAG,CAAC+B,MAAM,EAAEP,IAAI,CAAC;;IAG5B,OAAOjB,eAAe,CACXwB,MAAM,EAAEL,UAAU,EAAEC,sBAAsB,EAAEC,cAAc,CAAM;;EAG7E,IAAMI,EAAE,GAAGlC,eAAe,CAACiB,CAAC,EAAE,GAAG,EAAE,iBAAiB,EAAE,SAAS,CAAC;EAChE,IAAMkB,OAAO,GACTnC,eAAe,CAACkB,MAAM,EAAE,QAAQ,EAAE,iBAAiB,EAAE,SAAS,CAAC;EAEnE,IAAIkB,GAAG,GAAGF,EAAc;EACxB,IAAIG,YAAY,GAAG,KAAK;EACxB,IAAIH,EAAE,CAACI,IAAI,KAAK,CAAC,EAAE;IACjBD,YAAY,GAAG,IAAI;IACnBD,GAAG,GAAGtB,OAAO,CAACoB,EAAE,EAAE,CAAC,CAAC,EAAEA,EAAE,CAACK,KAAK,CAAC,CAAC,CAAC,EAAEL,EAAE,CAACK,KAAK,CAAC,CAAC,CAAC,EAAEL,EAAE,CAACK,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC;;EAE/DtC,IAAI,CAACuC,MAAM,CACPJ,GAAG,CAACE,IAAI,KAAK,CAAC,EACd;IAAA,OAAM,2EAAAG,MAAA,CACML,GAAG,CAACE,IAAI,MAAG;EAAA,EAAC;EAC5BrC,IAAI,CAACuC,MAAM,CACPL,OAAO,CAACG,IAAI,KAAK,CAAC,EAClB;IAAA,OAAM,4EAAAG,MAAA,CACcN,OAAO,CAACG,IAAI,MAAG;EAAA,EAAC;EACxCrC,IAAI,CAACuC,MAAM,CACPJ,GAAG,CAACG,KAAK,CAAC,CAAC,CAAC,KAAKJ,OAAO,CAACI,KAAK,CAAC,CAAC,CAAC,EACjC;IAAA,OAAM,kEAAAE,MAAA,CACEL,GAAG,CAACG,KAAK,CAAC,CAAC,CAAC,8CAA2C,aAAAE,MAAA,CACjDN,OAAO,CAACI,KAAK,CAAC,CAAC,CAAC,MAAG;EAAA,EAAC;EACtC,IAAIf,SAAS,IAAI,IAAI,EAAE;IACrBA,SAAS,GAAG,CAAC,CAAC,EAAE,CAAC,CAAC;;EAEpBvB,IAAI,CAACuC,MAAM,CACPpC,SAAS,CAACsC,8BAA8B,CAACvB,OAAO,EAAEK,SAAS,CAAC,EAC5D;IAAA,OACI,mEAAmE,wBAAAiB,MAAA,CAC9CtB,OAAO,sBAAAsB,MAAA,CAAmBjB,SAAS,MAAG;EAAA,EAAC;EACpEpB,SAAS,CAACuC,yBAAyB,CAC/B,uBAAuB,EAAEvB,GAAG,EAAEK,eAAe,CAAC;EAClD,IAAMmB,QAAQ,GAAGxC,SAAS,CAACyC,iBAAiB,CACxCT,GAAG,CAACG,KAAK,EAAEJ,OAAO,CAACI,KAAK,EAAEpB,OAAO,EAAEK,SAAS,EAAEJ,GAAG,EAAEK,eAAe,EAClE,IAAI,CAAC,gBAAgB;EAEzB,IAAIqB,KAAa;EACjB,IAAIpB,IAAI,IAAI,IAAI,EAAE;IAChBoB,KAAK,GAAG9C,eAAe,CAAC0B,IAAI,EAAE,MAAM,EAAE,cAAc,CAAC;IAAC,IAAAqB,eAAA,GAC5ChD,cAAc,CAAC+C,KAAK,EAAEZ,EAAE,CAAC;IAAA,IAAAc,gBAAA,GAAAC,cAAA,CAAAF,eAAA;IAAlCD,KAAK,GAAAE,gBAAA;IAEN7C,cAAc,CAAC+C,0BAA0B,CAACN,QAAQ,CAACO,QAAQ,EAAEL,KAAK,CAACP,KAAK,CAAC;;EAG3E,IAAIa,uBAA+B;EACnC,IAAIvB,sBAAsB,IAAI,IAAI,EAAE;IAClCuB,uBAAuB,GAAGpD,eAAe,CACrC6B,sBAAsB,EAAE,eAAe,EAAE,uBAAuB,CAAC;;EAGvE,IAAMwB,IAAI,GAAG,SAAPA,IAAIA,CAAIC,EAAY,EAAEC,KAAe,EAAI;IAC7CtD,IAAI,CAACuC,MAAM,CACPpC,SAAS,CAACoD,iBAAiB,CAAChC,SAAS,CAAC,EACtC;MAAA,OAAM,6DAA6D,yDACT,OAAAiB,MAAA,CAClDjB,SAAS,MAAG;IAAA,EAAC;IACzB,IAAAiC,MAAA,GAAAR,cAAA,CAAgCM,KAAK;MAA9BpB,OAAO,GAAAsB,MAAA;MAAErB,GAAG,GAAAqB,MAAA;MAAEC,CAAC,GAAAD,MAAA;MAAE/B,IAAI,GAAA+B,MAAA;IAE5B,IAAME,YAAY,GAAGhD,oBAAoB,CAAC2C,EAAE,EAAEI,CAAC,EAAE9B,UAAU,CAAa;IAExE,IAAMgC,IAAI,GAAGpD,kCAAkC,CAC1C4B,GAAgB,CAACG,KAAK,EAAEoB,YAAY,EAAExB,OAAmB,EAAEhB,OAAO,EACnEC,GAAG,EAAEI,SAAS,EAAEC,eAAe,CAAC;IACpC,IAAMoC,SAAS,GAAGtD,mCAAmC,CACjD6B,GAAe,EAAEuB,YAAY,EAAGxB,OAAoB,CAACI,KAAK,EAAEpB,OAAO,EACnEC,GAAG,EAAEI,SAAS,EAAEC,eAAe,CAAC;IAEpC,IAAIC,IAAI,IAAI,IAAI,EAAE;MAChB,IAAMoC,OAAO,GAAGpD,oBAAoB,CAACoC,KAAK,EAAEa,YAAY,CAAC;MACzD,OAAO,CAACC,IAAI,EAAEC,SAAS,EAAEC,OAAO,CAAC;;IAEnC,OAAO,CAACF,IAAI,EAAEC,SAAS,CAAC;EAC1B,CAAC;EAED,IAAME,MAAM,GAA+B;IACzC9C,CAAC,EAAEmB,GAAG;IACNlB,MAAM,EAAEiB,OAAO;IACfT,IAAI,EAAEoB,KAAK;IACXjB,sBAAsB,EAAEuB;GACzB;EACD,IAAMY,KAAK,GAA8B;IACvC7C,OAAO,EAAPA,OAAO;IACPC,GAAG,EAAHA,GAAG;IACHE,UAAU,EAAVA,UAAU;IACVE,SAAS,EAATA,SAAS;IACTC,eAAe,EAAfA,eAAe;IACfG,UAAU,EAAVA,UAAU;IACVE,cAAc,EAAdA;GACD;EAED;EACA;EACA,IAAIJ,IAAI,IAAI,IAAI,EAAE;IAChB,IAAMuC,QAAQ,GACVpE,UAAU,CAAC,UAACuC,GAAa,EAAElB,MAAgB,EAAEgD,IAAkB,EAAI;MACjE;MACA,IAAIC,GAAG,GAAsBvE,MAAM,CAACwE,SAAS,CACzCtE,oBAAoB,EAAEiE,MAAmC,EACzDC,KAAgC,CAAC;MAErCE,IAAI,CAAC,CAAChD,MAAM,EAAEkB,GAAG,EAAE+B,GAAG,CAAC,CAAC;MAExB,IAAI9B,YAAY,EAAE;QAChB;QACA8B,GAAG,GAAGrD,OAAO,CAACqD,GAAG,EAAE,CAACA,GAAG,CAAC5B,KAAK,CAAC,CAAC,CAAC,EAAE4B,GAAG,CAAC5B,KAAK,CAAC,CAAC,CAAC,EAAE4B,GAAG,CAAC5B,KAAK,CAAC,CAAC,CAAC,CAAC,CACjD;;MAGd,OAAO;QAAC8B,KAAK,EAAEF,GAAG;QAAEG,QAAQ,EAAEjB;MAAI,CAAC;IACrC,CAAC,CAAC;IACN,OAAOY,QAAQ,CAAC7B,GAAG,EAAED,OAAO,CAAM;GACnC,MAAM;IACL,IAAMoC,gBAAgB,GAAG1E,UAAU,CAC/B,UAACuC,GAAa,EAAElB,MAAgB,EAAEQ,IAAY,EAAEwC,IAAkB,EAAI;MACpE;MACA,IAAIC,GAAG,GAAsBvE,MAAM,CAACwE,SAAS,CACzCtE,oBAAoB,EAAEiE,MAAmC,EACzDC,KAAgC,CAAC;MAErCE,IAAI,CAAC,CAAChD,MAAM,EAAEkB,GAAG,EAAE+B,GAAG,EAAEzC,IAAI,CAAC,CAAC;MAE9B,IAAIW,YAAY,EAAE;QAChB;QACA8B,GAAG,GAAGrD,OAAO,CAACqD,GAAG,EAAE,CAACA,GAAG,CAAC5B,KAAK,CAAC,CAAC,CAAC,EAAE4B,GAAG,CAAC5B,KAAK,CAAC,CAAC,CAAC,EAAE4B,GAAG,CAAC5B,KAAK,CAAC,CAAC,CAAC,CAAC,CACjD;;MAGd,OAAO;QAAC8B,KAAK,EAAEF,GAAG;QAAEG,QAAQ,EAAEjB;MAAI,CAAC;IACrC,CAAC,CAAC;IAEN,OAAOkB,gBAAgB,CAACnC,GAAG,EAAED,OAAO,EAAEW,KAAK,CAAM;;AAErD;AACA,OAAO,IAAMzC,eAAe,GAAG,eAAgBQ,EAAE,CAAC;EAACE,qBAAqB,EAArBA;AAAqB,CAAC,CAAC"},"metadata":{},"sourceType":"module","externalDependencies":[]}