{"ast":null,"code":"import _get from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/get.js\";\nimport _getPrototypeOf from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/getPrototypeOf.js\";\nimport _createForOfIteratorHelper from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/createForOfIteratorHelper.js\";\nimport _classCallCheck from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/classCallCheck.js\";\nimport _createClass from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/createClass.js\";\nimport _inherits from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/inherits.js\";\nimport _createSuper from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/createSuper.js\";\n/**\r\n * @license\r\n * Copyright 2018 Google LLC\r\n *\r\n * Use of this source code is governed by an MIT-style\r\n * license that can be found in the LICENSE file or at\r\n * https://opensource.org/licenses/MIT.\r\n * =============================================================================\r\n */\n/**\r\n * TensorFlow.js Layers: Merge Layers.\r\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy, util } from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport { Layer } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { l2Normalize } from '../losses';\nimport * as generic_utils from '../utils/generic_utils';\nimport * as mathUtils from '../utils/math_utils';\nimport { getExactlyOneShape } from '../utils/types_utils';\n/**\r\n * Generic Merge layer for element-wise merge functions.\r\n *\r\n * Used to implement `Sum`, `Average`, `Concatenate`, etc.\r\n */\nexport var Merge = /*#__PURE__*/function (_Layer) {\n  _inherits(Merge, _Layer);\n  var _super = _createSuper(Merge);\n  function Merge(args) {\n    var _this;\n    _classCallCheck(this, Merge);\n    _this = _super.call(this, args || {});\n    _this.supportsMasking = true;\n    return _this;\n  }\n  /**\r\n   * Logic for merging multiple tensors, to be overridden by subclasses.\r\n   * @param inputs\r\n   */\n  _createClass(Merge, [{\n    key: \"mergeFunction\",\n    value: function mergeFunction(inputs) {\n      throw new NotImplementedError();\n    }\n    /**\r\n     * Computes the shape of the result of an elementwise operation.\r\n     *\r\n     * @param shape1: Shape of the first tensor.\r\n     * @param shape2: Shape of the second tensor.\r\n     * @returns Expected output shape when an elementwise operation is carried\r\n     *   out on 2 tensors with shapes `shape1` and `shape2`.\r\n     * @throws ValueError: If `shape1` and `shape2` are not compatible for\r\n     *   element-wise operations.\r\n     */\n  }, {\n    key: \"computeElementwiseOpOutputShape\",\n    value: function computeElementwiseOpOutputShape(shape1, shape2) {\n      if (shape1 == null || shape2 == null) {\n        return null;\n      } else if (shape1.length < shape2.length) {\n        return this.computeElementwiseOpOutputShape(shape2, shape1);\n      } else if (shape2.length === 0) {\n        return shape1;\n      }\n      var outputShape = shape1.slice(0, shape1.length - shape2.length);\n      for (var k = 0; k < shape2.length; ++k) {\n        var i = shape1[shape1.length - shape2.length + k];\n        var j = shape2[k];\n        if (i == null || j == null || i < 0 || j < 0) {\n          outputShape.push(null);\n        } else if (i === 1) {\n          outputShape.push(j);\n        } else if (j === 1) {\n          outputShape.push(i);\n        } else {\n          if (i !== j) {\n            throw new ValueError('Operands could not be broadcast together with shapes ' + JSON.stringify(shape1) + ' ' + JSON.stringify(shape2));\n          }\n          outputShape.push(i);\n        }\n      }\n      return outputShape;\n    }\n  }, {\n    key: \"build\",\n    value: function build(inputShape) {\n      // Used purely for shape validation.\n      if (Array.isArray(inputShape) && !Array.isArray(inputShape[0])) {\n        // Make sure that inputShape is an Array of shape.\n        inputShape = [getExactlyOneShape(inputShape)];\n      }\n      inputShape = inputShape;\n      if (inputShape.length < 2) {\n        throw new ValueError('A merge layer should be called on an Array of at least 2 inputs.' + \" Got \".concat(inputShape.length, \" input(s).\"));\n      }\n      // Make sure that there is at most one unique batch size among the input\n      // shapes.\n      var batchSizes = [];\n      var _iterator = _createForOfIteratorHelper(inputShape),\n        _step;\n      try {\n        for (_iterator.s(); !(_step = _iterator.n()).done;) {\n          var _shape = _step.value;\n          if (_shape != null && _shape[0] !== null) {\n            batchSizes.push(_shape[0]);\n          }\n        }\n      } catch (err) {\n        _iterator.e(err);\n      } finally {\n        _iterator.f();\n      }\n      batchSizes = generic_utils.unique(batchSizes);\n      if (batchSizes.length > 1) {\n        throw new ValueError(\"Can not merge tensors with different batch sizes. \" + \"Got tensors with shapes: \".concat(JSON.stringify(inputShape), \".\"));\n      }\n      var outputShape = inputShape[0] == null ? null : inputShape[0].slice(1);\n      for (var i = 1; i < inputShape.length; ++i) {\n        var shape = inputShape[i] == null ? null : inputShape[i].slice(1);\n        outputShape = this.computeElementwiseOpOutputShape(outputShape, shape);\n      }\n      // If the inputs have different ranks, we have to reshape them to make them\n      // broadcastable.\n      var allRanks = inputShape.map(function (shape) {\n        return shape.length;\n      });\n      if (inputShape.indexOf(null) === -1 && generic_utils.unique(allRanks).length === 1) {\n        this.reshapeRequired = false;\n      } else {\n        this.reshapeRequired = true;\n      }\n    }\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this2 = this;\n      return tidy(function () {\n        inputs = inputs;\n        if (_this2.reshapeRequired) {\n          var reshapedInputs = [];\n          var inputDims = inputs.map(function (input) {\n            return input.rank;\n          });\n          if (inputDims.indexOf(null) === -1) {\n            // If ranks of all inputs are available, we simply expand each of them\n            // at axis=1 until all of them have the same rank.\n            var maxNDim = mathUtils.max(inputDims);\n            var _iterator2 = _createForOfIteratorHelper(inputs),\n              _step2;\n            try {\n              for (_iterator2.s(); !(_step2 = _iterator2.n()).done;) {\n                var x = _step2.value;\n                var xNDim = x.rank;\n                for (var k = 0; k < maxNDim - xNDim; ++k) {\n                  x = K.expandDims(x, 1);\n                }\n                reshapedInputs.push(x);\n              }\n            } catch (err) {\n              _iterator2.e(err);\n            } finally {\n              _iterator2.f();\n            }\n            return _this2.mergeFunction(reshapedInputs);\n          } else {\n            // Transpose all inputs so that batch size is the last dimension.\n            // [batchSize, dim1, dim2, ...] -> [dim1, dim2, ..., batchSize]\n            var transposed = false;\n            var _iterator3 = _createForOfIteratorHelper(inputs),\n              _step3;\n            try {\n              for (_iterator3.s(); !(_step3 = _iterator3.n()).done;) {\n                var _x = _step3.value;\n                var _xNDim = _x.rank;\n                if (_xNDim == null) {\n                  var xShape = _x.shape;\n                  var _batchSize = xShape[0];\n                  var _newShape = xShape.slice(1).concat([_batchSize]);\n                  var xTransposed = tfc.reshape(_x, [_batchSize].concat(mathUtils.arrayProd(xShape.slice(1))));\n                  xTransposed = tfc.transpose(xTransposed, [1, 0]);\n                  xTransposed = tfc.reshape(xTransposed, _newShape);\n                  reshapedInputs.push(xTransposed);\n                  transposed = true;\n                } else if (_xNDim > 1) {\n                  var _dims = mathUtils.range(1, _xNDim).concat([0]);\n                  reshapedInputs.push(tfc.transpose(_x, _dims));\n                  transposed = true;\n                } else {\n                  // We don't transpose inputs if they are 1D vectors or scalars.\n                  reshapedInputs.push(_x);\n                }\n              }\n            } catch (err) {\n              _iterator3.e(err);\n            } finally {\n              _iterator3.f();\n            }\n            var y = _this2.mergeFunction(reshapedInputs);\n            var yNDim = y.rank;\n            if (transposed) {\n              // If inputs have been transposed, we have to transpose the output\n              // too.\n              if (yNDim == null) {\n                var yShape = y.shape;\n                var _yNDim = yShape.length;\n                var batchSize = yShape[_yNDim - 1];\n                var newShape = [batchSize].concat(yShape.slice(0, yShape.length - 1));\n                y = tfc.reshape(tfc.transpose(tfc.reshape(y, [-1, batchSize]), [1, 0]), newShape);\n              } else if (yNDim > 1) {\n                var dims = [yNDim - 1].concat(mathUtils.range(0, yNDim - 1));\n                y = tfc.transpose(y, dims);\n              }\n            }\n            return y;\n          }\n        } else {\n          return _this2.mergeFunction(inputs);\n        }\n      });\n    }\n  }, {\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      inputShape = inputShape;\n      var outputShape;\n      if (inputShape[0] == null) {\n        outputShape = null;\n      } else {\n        outputShape = inputShape[0].slice(1);\n      }\n      for (var i = 1; i < inputShape.length; ++i) {\n        var shape = inputShape[i] == null ? null : inputShape[i].slice(1);\n        outputShape = this.computeElementwiseOpOutputShape(outputShape, shape);\n      }\n      var batchSizes = [];\n      var _iterator4 = _createForOfIteratorHelper(inputShape),\n        _step4;\n      try {\n        for (_iterator4.s(); !(_step4 = _iterator4.n()).done;) {\n          var _shape2 = _step4.value;\n          if (_shape2 != null && _shape2[0] !== null) {\n            batchSizes.push(_shape2[0]);\n          }\n        }\n      } catch (err) {\n        _iterator4.e(err);\n      } finally {\n        _iterator4.f();\n      }\n      batchSizes = generic_utils.unique(batchSizes);\n      if (batchSizes.length === 1) {\n        outputShape = batchSizes.concat(outputShape);\n      } else {\n        outputShape = [null].concat(outputShape);\n      }\n      return outputShape;\n    }\n  }, {\n    key: \"computeMask\",\n    value: function computeMask(inputs, mask) {\n      return tfc.tidy(function () {\n        if (mask == null) {\n          return null;\n        }\n        if (!Array.isArray(mask)) {\n          throw new ValueError('`mask` should be an Array');\n        }\n        if (!Array.isArray(inputs)) {\n          throw new ValueError('`inputs` should be an Array');\n        }\n        if (mask.length !== inputs.length) {\n          throw new ValueError(\"The Array 'inputs' and 'mask' are expected to have the same \" + \"length, but have different lengths \" + \"(\".concat(inputs.length, \" vs \").concat(mask.length, \")\"));\n        }\n        if (mask.every(function (m) {\n          return m == null;\n        })) {\n          return null;\n        }\n        mask = mask.map(function (m) {\n          return m == null ? m : tfc.expandDims(m, 0);\n        });\n        var output = mask[0];\n        for (var i = 1; i < mask.length - 1; ++i) {\n          output = tfc.logicalAnd(output, mask[i]);\n        }\n        return output;\n      });\n    }\n  }]);\n  return Merge;\n}(Layer);\nexport var Add = /*#__PURE__*/function (_Merge) {\n  _inherits(Add, _Merge);\n  var _super2 = _createSuper(Add);\n  function Add(args) {\n    _classCallCheck(this, Add);\n    return _super2.call(this, args);\n  }\n  _createClass(Add, [{\n    key: \"mergeFunction\",\n    value: function mergeFunction(inputs) {\n      return tidy(function () {\n        var output = inputs[0].clone();\n        for (var i = 1; i < inputs.length; ++i) {\n          output = tfc.add(output, inputs[i]);\n        }\n        return output;\n      });\n    }\n  }]);\n  return Add;\n}(Merge);\n/** @nocollapse */\nAdd.className = 'Add';\nserialization.registerClass(Add);\n/**\r\n * Calculate the element-wise sum of inputs, which all have the same shape.\r\n *\r\n * This function can be invoked in three ways.\r\n *\r\n * 1. Construct an instance of `Add` layer, by using no input argument\r\n *    or a single configuration argument. The resultant `Add` layer can then\r\n *    be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\r\n *\r\n * ```js\r\n * const addLayer = tf.layers.add();\r\n *\r\n * // The layer can be applied to inputs.\r\n * const input1 = tf.input({shape: [2, 2]});\r\n * const input2 = tf.input({shape: [2, 2]});\r\n * const output = addLayer.apply([input1, input2]);\r\n * console.log(output.shape);\r\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\r\n * // dimension.\r\n * ```\r\n *\r\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\r\n *    an `Layer` object internally and calls its `apply` method on the inputs,\r\n *    generating a new `tf.SymbolicTensor`. For example:\r\n *\r\n * ```js\r\n * const input1 = tf.input({shape: [2, 2]});\r\n * const input2 = tf.input({shape: [2, 2]});\r\n * const output = tf.layers.add([input1, input2]);\r\n * console.log(output.shape);\r\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\r\n * // dimension.\r\n * ```\r\n *\r\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\r\n *    an `Layer` object internally and calls its `apply` method on the inputs,\r\n *    generating a new `tf.Tensor` as the result of the computation. For\r\n * example:\r\n *\r\n * ```js\r\n * const input1 = tf.tensor2d([1, 2, 3, 4], [2, 2]);\r\n * const input2 = tf.tensor2d([10, 20, 30, 40], [2, 2]);\r\n * tf.layers.add([input1, input2]).print();\r\n * // Gives [[11, 22], [33, 44]].\r\n *\r\n */\nexport function add(config) {\n  if (Array.isArray(config)) {\n    var layer = new Add({});\n    return layer.apply(config);\n  } else {\n    return new Add(config);\n  }\n}\nexport var Multiply = /*#__PURE__*/function (_Merge2) {\n  _inherits(Multiply, _Merge2);\n  var _super3 = _createSuper(Multiply);\n  function Multiply(args) {\n    _classCallCheck(this, Multiply);\n    return _super3.call(this, args);\n  }\n  _createClass(Multiply, [{\n    key: \"mergeFunction\",\n    value: function mergeFunction(inputs) {\n      return tidy(function () {\n        var output = inputs[0].clone();\n        for (var i = 1; i < inputs.length; ++i) {\n          output = tfc.mul(output, inputs[i]);\n        }\n        return output;\n      });\n    }\n  }]);\n  return Multiply;\n}(Merge);\n/** @nocollapse */\nMultiply.className = 'Multiply';\nserialization.registerClass(Multiply);\n/**\r\n * Calculate the element-wise product of inputs, which all have the same shape.\r\n *\r\n * This function can be invoked in three ways.\r\n *\r\n * 1. Construct an instance of `Multiply` layer, by using no input argument\r\n *    or a single configuration argument. The resultant `Multiply` layer can\r\n *    then be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\r\n *\r\n * ```js\r\n * const multiplyLayer = tf.layers.multiply();\r\n *\r\n * // The layer can be applied to inputs.\r\n * const input1 = tf.input({shape: [2, 2]});\r\n * const input2 = tf.input({shape: [2, 2]});\r\n * const output = multiplyLayer.apply([input1, input2]);\r\n * console.log(output.shape);\r\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\r\n * // dimension.\r\n * ```\r\n *\r\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\r\n *    an `Layer` object internally and calls its `apply` method on the inputs,\r\n *    generating a new `tf.SymbolicTensor`. For example:\r\n *\r\n * ```js\r\n * const input1 = tf.input({shape: [2, 2]});\r\n * const input2 = tf.input({shape: [2, 2]});\r\n * const output = tf.layers.multiply([input1, input2]);\r\n * console.log(output.shape);\r\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\r\n * // dimension.\r\n * ```\r\n *\r\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\r\n *    an `Layer` object internally and calls its `apply` method on the inputs,\r\n *    generating a new `tf.Tensor` as the result of the computation. For\r\n * example:\r\n *\r\n * ```js\r\n * const input1 = tf.tensor2d([1, 2, 3, 4], [2, 2]);\r\n * const input2 = tf.tensor2d([10, 20, 30, 40], [2, 2]);\r\n * tf.layers.multiply([input1, input2]).print();\r\n * // Gives [[10, 40], [90, 160]].\r\n *\r\n */\nexport function multiply(config) {\n  if (Array.isArray(config)) {\n    var layer = new Multiply({});\n    return layer.apply(config);\n  } else {\n    return new Multiply(config);\n  }\n}\nexport var Average = /*#__PURE__*/function (_Merge3) {\n  _inherits(Average, _Merge3);\n  var _super4 = _createSuper(Average);\n  function Average(args) {\n    _classCallCheck(this, Average);\n    return _super4.call(this, args);\n  }\n  _createClass(Average, [{\n    key: \"mergeFunction\",\n    value: function mergeFunction(inputs) {\n      return tidy(function () {\n        var output = inputs[0].clone();\n        for (var i = 1; i < inputs.length; ++i) {\n          output = tfc.add(output, inputs[i]);\n        }\n        return tfc.mul(1 / inputs.length, output);\n      });\n    }\n  }]);\n  return Average;\n}(Merge);\n/** @nocollapse */\nAverage.className = 'Average';\nserialization.registerClass(Average);\n/**\r\n * Calculate the element-wise arithmetic mean of inputs, which all have the same\r\n * shape.\r\n *\r\n * This function can be invoked in three ways.\r\n *\r\n * 1. Construct an instance of `Average` layer, by using no input argument\r\n *    or a single configuration argument. The resultant `Average` layer can then\r\n *    be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\r\n *\r\n * ```js\r\n * const averageLayer = tf.layers.average();\r\n *\r\n * // The layer can be applied to inputs.\r\n * const input1 = tf.input({shape: [2, 2]});\r\n * const input2 = tf.input({shape: [2, 2]});\r\n * const output = averageLayer.apply([input1, input2]);\r\n * console.log(output.shape);\r\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\r\n * // dimension.\r\n * ```\r\n *\r\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\r\n *    an `Layer` object internally and calls its `apply` method on the inputs,\r\n *    generating a new `tf.SymbolicTensor`. For example:\r\n *\r\n * ```js\r\n * const input1 = tf.input({shape: [2, 2]});\r\n * const input2 = tf.input({shape: [2, 2]});\r\n * const output = tf.layers.average([input1, input2]);\r\n * console.log(output.shape);\r\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\r\n * // dimension.\r\n * ```\r\n *\r\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\r\n *    an `Layer` object internally and calls its `apply` method on the inputs,\r\n *    generating a new `tf.Tensor` as the result of the computation. For\r\n * example:\r\n *\r\n * ```js\r\n * const input1 = tf.tensor2d([1, 2, 3, 4], [2, 2]);\r\n * const input2 = tf.tensor2d([10, 20, 30, 40], [2, 2]);\r\n * tf.layers.average([input1, input2]).print();\r\n * // Gives [[5.5, 11], [16.5, 22]].\r\n *\r\n */\nexport function average(config) {\n  if (Array.isArray(config)) {\n    var layer = new Average({});\n    return layer.apply(config);\n  } else {\n    return new Average(config);\n  }\n}\nexport var Maximum = /*#__PURE__*/function (_Merge4) {\n  _inherits(Maximum, _Merge4);\n  var _super5 = _createSuper(Maximum);\n  function Maximum(args) {\n    _classCallCheck(this, Maximum);\n    return _super5.call(this, args);\n  }\n  _createClass(Maximum, [{\n    key: \"mergeFunction\",\n    value: function mergeFunction(inputs) {\n      return tidy(function () {\n        var output = inputs[0];\n        for (var i = 1; i < inputs.length; ++i) {\n          output = tfc.maximum(output, inputs[i]);\n        }\n        return output;\n      });\n    }\n  }]);\n  return Maximum;\n}(Merge);\n/** @nocollapse */\nMaximum.className = 'Maximum';\nserialization.registerClass(Maximum);\n/**\r\n * Calculate the element-wise maximum of inputs, which all have the same shape.\r\n *\r\n * This function can be invoked in three ways.\r\n *\r\n * 1. Construct an instance of `Maximum` layer, by using no input argument\r\n *    or a single configuration argument. The resultant `Maximum` layer can then\r\n *    be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\r\n *\r\n * ```js\r\n * const maximumLayer = tf.layers.maximum();\r\n *\r\n * // The layer can be applied to inputs.\r\n * const input1 = tf.input({shape: [2, 2]});\r\n * const input2 = tf.input({shape: [2, 2]});\r\n * const output = maximumLayer.apply([input1, input2]);\r\n * console.log(output.shape);\r\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\r\n * // dimension.\r\n * ```\r\n *\r\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\r\n *    an `Layer` object internally and calls its `apply` method on the inputs,\r\n *    generating a new `tf.SymbolicTensor`. For example:\r\n *\r\n * ```js\r\n * const input1 = tf.input({shape: [2, 2]});\r\n * const input2 = tf.input({shape: [2, 2]});\r\n * const output = tf.layers.maximum([input1, input2]);\r\n * console.log(output.shape);\r\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\r\n * // dimension.\r\n * ```\r\n *\r\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\r\n *    an `Layer` object internally and calls its `apply` method on the inputs,\r\n *    generating a new `tf.Tensor` as the result of the computation. For\r\n * example:\r\n *\r\n * ```js\r\n * const input1 = tf.tensor2d([1, 20, 3, 40], [2, 2]);\r\n * const input2 = tf.tensor2d([10, 2, 30, 4], [2, 2]);\r\n * tf.layers.maximum([input1, input2]).print();\r\n * // Gives [[10, 20], [30, 40]].\r\n *\r\n */\nexport function maximum(config) {\n  if (Array.isArray(config)) {\n    var layer = new Maximum({});\n    return layer.apply(config);\n  } else {\n    return new Maximum(config);\n  }\n}\nexport var Minimum = /*#__PURE__*/function (_Merge5) {\n  _inherits(Minimum, _Merge5);\n  var _super6 = _createSuper(Minimum);\n  function Minimum(args) {\n    _classCallCheck(this, Minimum);\n    return _super6.call(this, args);\n  }\n  _createClass(Minimum, [{\n    key: \"mergeFunction\",\n    value: function mergeFunction(inputs) {\n      return tidy(function () {\n        var output = inputs[0];\n        for (var i = 1; i < inputs.length; ++i) {\n          output = tfc.minimum(output, inputs[i]);\n        }\n        return output;\n      });\n    }\n  }]);\n  return Minimum;\n}(Merge);\n/** @nocollapse */\nMinimum.className = 'Minimum';\nserialization.registerClass(Minimum);\n/**\r\n * Calculate the element-wise minimum of inputs, which all have the same shape.\r\n *\r\n * This function can be invoked in three ways.\r\n *\r\n * 1. Construct an instance of `Minimum` layer, by using no input argument\r\n *    or a single configuration argument. The resultant `Minimum` layer can then\r\n *    be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\r\n *\r\n * ```js\r\n * const minimumLayer = tf.layers.minimum();\r\n *\r\n * // The layer can be applied to inputs.\r\n * const input1 = tf.input({shape: [2, 2]});\r\n * const input2 = tf.input({shape: [2, 2]});\r\n * const output = minimumLayer.apply([input1, input2]);\r\n * console.log(output.shape);\r\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\r\n * // dimension.\r\n * ```\r\n *\r\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\r\n *    an `Layer` object internally and calls its `apply` method on the inputs,\r\n *    generating a new `tf.SymbolicTensor`. For example:\r\n *\r\n * ```js\r\n * const input1 = tf.input({shape: [2, 2]});\r\n * const input2 = tf.input({shape: [2, 2]});\r\n * const output = tf.layers.minimum([input1, input2]);\r\n * console.log(output.shape);\r\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\r\n * // dimension.\r\n * ```\r\n *\r\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\r\n *    an `Layer` object internally and calls its `apply` method on the inputs,\r\n *    generating a new `tf.Tensor` as the result of the computation. For\r\n * example:\r\n *\r\n * ```js\r\n * const input1 = tf.tensor2d([1, 20, 3, 40], [2, 2]);\r\n * const input2 = tf.tensor2d([10, 2, 30, 4], [2, 2]);\r\n * tf.layers.minimum([input1, input2]).print();\r\n * // Gives [[1, 2], [3, 4]].\r\n *\r\n */\nexport function minimum(config) {\n  if (Array.isArray(config)) {\n    var layer = new Minimum({});\n    return layer.apply(config);\n  } else {\n    return new Minimum(config);\n  }\n}\nexport var Concatenate = /*#__PURE__*/function (_Merge6) {\n  _inherits(Concatenate, _Merge6);\n  var _super7 = _createSuper(Concatenate);\n  function Concatenate(args) {\n    var _this3;\n    _classCallCheck(this, Concatenate);\n    _this3 = _super7.call(this, args);\n    _this3.DEFAULT_AXIS = -1;\n    if (args == null) {\n      args = {};\n    }\n    _this3.axis = args.axis == null ? _this3.DEFAULT_AXIS : args.axis;\n    _this3.supportsMasking = true;\n    _this3.reshapeRequired = false;\n    return _this3;\n  }\n  _createClass(Concatenate, [{\n    key: \"build\",\n    value: function build(inputShape) {\n      // Used purely for shape validation.]\n      if (!(Array.isArray(inputShape) && Array.isArray(inputShape[0])) || inputShape.length === 1) {\n        throw new ValueError('A `Concatenate` layer should be called on a list of at least 2 ' + 'inputs');\n      }\n      inputShape = inputShape;\n      var allNoneShape = true;\n      var _iterator5 = _createForOfIteratorHelper(inputShape),\n        _step5;\n      try {\n        for (_iterator5.s(); !(_step5 = _iterator5.n()).done;) {\n          var _shape3 = _step5.value;\n          if (_shape3 != null) {\n            allNoneShape = false;\n            break;\n          }\n        }\n      } catch (err) {\n        _iterator5.e(err);\n      } finally {\n        _iterator5.f();\n      }\n      if (allNoneShape) {\n        return;\n      }\n      var shapeSet = [];\n      for (var i = 0; i < inputShape.length; ++i) {\n        var shapeWithoutConcatAxis = inputShape[i].slice();\n        shapeWithoutConcatAxis.splice(this.axis, 1);\n        var exists = false;\n        var _iterator6 = _createForOfIteratorHelper(shapeSet),\n          _step6;\n        try {\n          for (_iterator6.s(); !(_step6 = _iterator6.n()).done;) {\n            var shape = _step6.value;\n            if (util.arraysEqual(shape, shapeWithoutConcatAxis)) {\n              exists = true;\n              break;\n            }\n          }\n        } catch (err) {\n          _iterator6.e(err);\n        } finally {\n          _iterator6.f();\n        }\n        if (!exists) {\n          shapeSet.push(shapeWithoutConcatAxis);\n        }\n      }\n      if (shapeSet.length > 1) {\n        throw new ValueError('A `Concatenate` layer requires inputs with matching shapes ' + 'except for the concat axis. Got input shapes: ' + JSON.stringify(inputShape));\n      }\n    }\n  }, {\n    key: \"mergeFunction\",\n    value: function mergeFunction(inputs) {\n      var _this4 = this;\n      return tidy(function () {\n        return K.concatenate(inputs, _this4.axis);\n      });\n    }\n  }, {\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      if (!(Array.isArray(inputShape) && Array.isArray(inputShape[0]))) {\n        throw new ValueError('A `Concatenate` layer should be called on a list of inputs.');\n      }\n      var inputShapes = inputShape;\n      var outputShape = inputShapes[0].slice();\n      var axis = this.axis < 0 ? outputShape.length + this.axis : this.axis;\n      // Porting Note: the line above is because TypeScript doesn't support\n      //   negative indices.\n      var _iterator7 = _createForOfIteratorHelper(inputShapes.slice(1)),\n        _step7;\n      try {\n        for (_iterator7.s(); !(_step7 = _iterator7.n()).done;) {\n          var shape = _step7.value;\n          if (outputShape[axis] == null || shape[axis] == null) {\n            outputShape[axis] = null;\n            break;\n          }\n          outputShape[axis] += shape[axis];\n        }\n      } catch (err) {\n        _iterator7.e(err);\n      } finally {\n        _iterator7.f();\n      }\n      return outputShape;\n    }\n  }, {\n    key: \"computeMask\",\n    value: function computeMask(inputs, mask) {\n      var _this5 = this;\n      if (mask == null) {\n        return null;\n      }\n      if (!Array.isArray(mask)) {\n        throw new ValueError('`mask` should be an array for Concatenate');\n      }\n      if (!Array.isArray(inputs)) {\n        throw new ValueError('`inputs` should be an array for Concatenate');\n      }\n      if (mask.length !== inputs.length) {\n        throw new ValueError(\"Mismatch in the length of mask (\".concat(mask.length, \") \") + \"and the legnth of inputs (\".concat(inputs.length, \")\"));\n      }\n      return tfc.tidy(function () {\n        var allNullMasks = true;\n        mask.forEach(function (m) {\n          if (m != null) {\n            allNullMasks = false;\n            return;\n          }\n        });\n        if (allNullMasks) {\n          return null;\n        }\n        var outputMasks = [];\n        for (var i = 0; i < inputs.length; ++i) {\n          if (mask[i] == null) {\n            // Input is unmasked. Append all 1's to masks.\n            outputMasks.push(tfc.cast(tfc.onesLike(inputs[i]), 'bool'));\n          } else if (mask[i].rank < inputs[i].rank) {\n            // Mask is smaller than the input, expand it.\n            outputMasks.push(tfc.expandDims(mask[i], -1));\n          } else {\n            outputMasks.push(mask[i]);\n          }\n        }\n        var concatenatedMasks = tfc.concat(outputMasks, _this5.axis);\n        return tfc.all(concatenatedMasks, -1, false);\n      });\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        'axis': this.axis\n      };\n      var baseConfig = _get(_getPrototypeOf(Concatenate.prototype), \"getConfig\", this).call(this);\n      Object.assign(config, baseConfig);\n      return config;\n    }\n  }]);\n  return Concatenate;\n}(Merge);\n/** @nocollapse */\nConcatenate.className = 'Concatenate';\nserialization.registerClass(Concatenate);\n/**\r\n * Concatenate an `Array` of inputs.\r\n *\r\n * This function can be invoked in three ways.\r\n *\r\n * 1. Construct an instance of `Concatenate` layer, by using no input argument\r\n *    or a single configuration argument. The resultant `Concatenate` layer can\r\n *    then be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\r\n *\r\n * ```js\r\n * const concatLayer = tf.layers.concatenate();\r\n *\r\n * // The layer can be applied to inputs.\r\n * const input1 = tf.input({shape: [2, 3]});\r\n * const input2 = tf.input({shape: [2, 4]});\r\n * const output = concatLayer.apply([input1, input2]);\r\n * console.log(output.shape);\r\n * // You get [null, 2, 7], with the first dimension as the undetermined batch\r\n * // dimension and the last dimension as the result of concatenating the\r\n * // last dimensions of the two inputs.\r\n * ```\r\n *\r\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\r\n *    an `Layer` object internally and calls its `apply` method on the inputs,\r\n *    generating a new `tf.SymbolicTensor`. For example:\r\n *\r\n * ```js\r\n * const input1 = tf.input({shape: [2, 3]});\r\n * const input2 = tf.input({shape: [2, 4]});\r\n * const output = tf.layers.concatenate([input1, input2]);\r\n * console.log(output.shape);\r\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\r\n * // dimension and the last dimension as the result of concatenating the\r\n * // last dimensions of the two inputs.\r\n * ```\r\n *\r\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\r\n *    an `Layer` object internally and calls its `apply` method on the inputs,\r\n *    generating a new `tf.Tensor` as the result of the computation. For\r\n * example:\r\n *\r\n * ```js\r\n * const input1 = tf.tensor2d([[1, 2], [3, 4]], [2, 2]);\r\n * const input2 = tf.tensor2d([[10, 20], [30, 40]], [2, 2]);\r\n * tf.layers.concatenate([input1, input2]).print();\r\n * // Gives [[1, 2, 10, 20], [3, 4, 30, 40]].\r\n *\r\n */\nexport function concatenate(config) {\n  if (Array.isArray(config)) {\n    var layer = new Concatenate({});\n    return layer.apply(config);\n  } else {\n    return new Concatenate(config);\n  }\n}\n/**\r\n * Interpretable potentially negative axis index.\r\n *\r\n * For example, given axis = -1, and dim = 3, this function will return 2.\r\n *\r\n * @param axis The axis index, may be a positive, zero or negative integer.\r\n * @param dim Total number of dimensions, a positive integer.\r\n * @returns A non-negative axis index equivalent to the input `axis`.\r\n */\nfunction interpretAxis(axis, dim) {\n  while (axis < 0) {\n    axis += dim;\n  }\n  return axis;\n}\nfunction batchDot(x, y, axes) {\n  if (x.shape.length > 3 || y.shape.length > 3) {\n    throw new NotImplementedError('batchDot is not implemented for tensors of 4D or higher rank yet');\n  }\n  tfc.util.assert(x.shape.length >= 2, function () {\n    return \"batchDot requires the rank of x to be >= 2, \" + \"but got \".concat(x.shape.length);\n  });\n  tfc.util.assert(x.shape.length >= 2, function () {\n    return \"batchDot requires the rank of y to be >= 2, \" + \"but got \".concat(y.shape.length);\n  });\n  if (typeof axes === 'number') {\n    axes = [axes, axes];\n  }\n  if (x.dtype === 'complex64' || y.dtype === 'complex64') {\n    throw new NotImplementedError('batchDot is not implemented for complex64-type Tensors yet.');\n  }\n  var xNDim = x.shape.length;\n  var yNDim = y.shape.length;\n  if (axes == null) {\n    // Behave like batchMatmul by default.\n    axes = [xNDim - 1, yNDim - 2];\n  }\n  var axesArray = axes;\n  return tfc.tidy(function () {\n    var diff;\n    if (xNDim > yNDim) {\n      diff = xNDim - yNDim;\n      var diffShape = [];\n      for (var i = 0; i < diff; ++i) {\n        diffShape.push(1);\n      }\n      y = tfc.reshape(y, y.shape.concat(diffShape));\n    } else if (yNDim > xNDim) {\n      diff = yNDim - xNDim;\n      var _diffShape = [];\n      for (var _i = 0; _i < diff; ++_i) {\n        _diffShape.push(1);\n      }\n      x = tfc.reshape(x, x.shape.concat(_diffShape));\n    } else {\n      diff = 0;\n    }\n    var out;\n    if (x.shape.length === 2 && y.shape.length === 2) {\n      if (axesArray[0] === axesArray[1]) {\n        out = tfc.sum(tfc.mul(x, y), axesArray[0]);\n      } else {\n        out = tfc.sum(tfc.mul(tfc.transpose(x, [1, 0]), y), axesArray[1]);\n      }\n    } else {\n      var adjX = axesArray[0] !== x.shape.length - 1;\n      var adjY = axesArray[1] === y.shape.length - 1;\n      out = tfc.matMul(x, y, adjX, adjY);\n    }\n    if (diff > 0) {\n      var idx;\n      if (xNDim > yNDim) {\n        idx = xNDim + yNDim - 3;\n      } else {\n        idx = xNDim - 1;\n      }\n      var squeezeAxes = [];\n      for (var _i2 = idx; _i2 < idx + diff; ++_i2) {\n        squeezeAxes.push(_i2);\n      }\n      out = tfc.squeeze(out, squeezeAxes);\n    }\n    if (out.shape.length === 1) {\n      out = tfc.expandDims(out, 1);\n    }\n    return out;\n  });\n}\nexport var Dot = /*#__PURE__*/function (_Merge7) {\n  _inherits(Dot, _Merge7);\n  var _super8 = _createSuper(Dot);\n  function Dot(args) {\n    var _this6;\n    _classCallCheck(this, Dot);\n    _this6 = _super8.call(this, args);\n    _this6.axes = args.axes;\n    _this6.normalize = args.normalize == null ? false : args.normalize;\n    _this6.supportsMasking = true;\n    _this6.reshapeRequired = false;\n    return _this6;\n  }\n  _createClass(Dot, [{\n    key: \"build\",\n    value: function build(inputShape) {\n      tfc.util.assert(Array.isArray(inputShape) && inputShape.length === 2 && Array.isArray(inputShape[0]) && Array.isArray(inputShape[1]), function () {\n        return 'A `Dot` layer should be called on a list of exactly 2 inputs.';\n      });\n      var shape1 = inputShape[0];\n      var shape2 = inputShape[1];\n      if (shape1.length > 3 || shape2.length > 3) {\n        throw new NotImplementedError('Dot layer does not support tensors of 4D or higher rank yet.');\n      }\n      var axes = this.interpretAxes(shape1, shape2);\n      if (shape1[axes[0]] !== shape2[axes[1]]) {\n        throw new ValueError(\"Dimension incompatibility: \" + \"\".concat(shape1[axes[0]], \" !== \").concat(shape2[axes[1]]));\n      }\n    }\n  }, {\n    key: \"mergeFunction\",\n    value: function mergeFunction(inputs) {\n      if (inputs.length !== 2) {\n        throw new ValueError('A `Dot` layer must be called on exactly 2 inputs, ' + \"but received \".concat(inputs.length, \" input(s).\"));\n      }\n      var x1 = inputs[0];\n      var x2 = inputs[1];\n      var axes;\n      if (!Array.isArray(this.axes)) {\n        axes = [interpretAxis(this.axes, x1.shape.length), interpretAxis(this.axes, x2.shape.length)];\n      } else {\n        axes = this.axes.map(function (axis, i) {\n          return interpretAxis(axis, inputs[i].shape.length);\n        });\n      }\n      if (this.normalize) {\n        x1 = l2Normalize(x1, axes[0]);\n        x2 = l2Normalize(x2, axes[1]);\n      }\n      return batchDot(x1, x2, axes);\n    }\n  }, {\n    key: \"interpretAxes\",\n    value: function interpretAxes(shape1, shape2) {\n      var axes;\n      if (!Array.isArray(this.axes)) {\n        // `this.axes` is a single integer.\n        axes = [interpretAxis(this.axes, shape1.length), interpretAxis(this.axes, shape2.length)];\n      } else {\n        // `this.axes` is an Array of integers.\n        axes = this.axes;\n      }\n      return axes;\n    }\n  }, {\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      tfc.util.assert(Array.isArray(inputShape) && inputShape.length === 2 && Array.isArray(inputShape[0]) && Array.isArray(inputShape[1]), function () {\n        return 'A `Dot` layer should be called on a list of exactly 2 inputs.';\n      });\n      var shape1 = inputShape[0].slice();\n      var shape2 = inputShape[1].slice();\n      if (shape1.length > 3 || shape2.length > 3) {\n        throw new NotImplementedError('Dot layer does not support tensors of 4D or higher rank yet.');\n      }\n      var axes = this.interpretAxes(shape1, shape2);\n      shape1.splice(axes[0], 1);\n      shape2.splice(axes[1], 1);\n      shape2.splice(0, 1);\n      var outputShape = shape1.concat(shape2);\n      if (outputShape.length === 1) {\n        outputShape.push(1);\n      }\n      return outputShape;\n    }\n  }, {\n    key: \"computeMask\",\n    value: function computeMask(inputs, mask) {\n      return null;\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        'axes': this.axes,\n        'normalize': this.normalize\n      };\n      var baseConfig = _get(_getPrototypeOf(Dot.prototype), \"getConfig\", this).call(this);\n      Object.assign(config, baseConfig);\n      return config;\n    }\n  }]);\n  return Dot;\n}(Merge);\n/** @nocollapse */\nDot.className = 'Dot';\nserialization.registerClass(Dot);\n// TODO(cais): Add functional interfaces for the merge layers.","map":{"version":3,"names":["tfc","serialization","tidy","util","K","Layer","NotImplementedError","ValueError","l2Normalize","generic_utils","mathUtils","getExactlyOneShape","Merge","_Layer","_inherits","_super","_createSuper","args","_this","_classCallCheck","call","supportsMasking","_createClass","key","value","mergeFunction","inputs","computeElementwiseOpOutputShape","shape1","shape2","length","outputShape","slice","k","i","j","push","JSON","stringify","build","inputShape","Array","isArray","concat","batchSizes","_iterator","_createForOfIteratorHelper","_step","s","n","done","shape","err","e","f","unique","allRanks","map","indexOf","reshapeRequired","kwargs","_this2","reshapedInputs","inputDims","input","rank","maxNDim","max","_iterator2","_step2","x","xNDim","expandDims","transposed","_iterator3","_step3","xShape","batchSize","newShape","xTransposed","reshape","arrayProd","transpose","dims","range","y","yNDim","yShape","computeOutputShape","_iterator4","_step4","computeMask","mask","every","m","output","logicalAnd","Add","_Merge","_super2","clone","add","className","registerClass","config","layer","apply","Multiply","_Merge2","_super3","mul","multiply","Average","_Merge3","_super4","average","Maximum","_Merge4","_super5","maximum","Minimum","_Merge5","_super6","minimum","Concatenate","_Merge6","_super7","_this3","DEFAULT_AXIS","axis","allNoneShape","_iterator5","_step5","shapeSet","shapeWithoutConcatAxis","splice","exists","_iterator6","_step6","arraysEqual","_this4","concatenate","inputShapes","_iterator7","_step7","_this5","allNullMasks","forEach","outputMasks","cast","onesLike","concatenatedMasks","all","getConfig","baseConfig","_get","_getPrototypeOf","prototype","Object","assign","interpretAxis","dim","batchDot","axes","assert","dtype","axesArray","diff","diffShape","out","sum","adjX","adjY","matMul","idx","squeezeAxes","squeeze","Dot","_Merge7","_super8","_this6","normalize","interpretAxes","x1","x2"],"sources":["C:\\Users\\vince\\OneDrive\\Documents\\GitHub\\tfjs-layers\\src\\layers\\merge.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * TensorFlow.js Layers: Merge Layers.\n */\n\nimport * as tfc from '@tensorflow/tfjs-core';\nimport {serialization, Tensor, tidy, util} from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport {Layer, LayerArgs, SymbolicTensor} from '../engine/topology';\nimport {NotImplementedError, ValueError} from '../errors';\nimport {Shape} from '../keras_format/common';\nimport {l2Normalize} from '../losses';\nimport {Kwargs} from '../types';\nimport * as generic_utils from '../utils/generic_utils';\nimport * as mathUtils from '../utils/math_utils';\nimport {getExactlyOneShape} from '../utils/types_utils';\n\n/**\n * Generic Merge layer for element-wise merge functions.\n *\n * Used to implement `Sum`, `Average`, `Concatenate`, etc.\n */\nexport abstract class Merge extends Layer {\n  protected reshapeRequired: boolean;\n\n  constructor(args?: LayerArgs) {\n    super(args || {});\n    this.supportsMasking = true;\n  }\n\n  /**\n   * Logic for merging multiple tensors, to be overridden by subclasses.\n   * @param inputs\n   */\n  protected mergeFunction(inputs: Tensor[]): Tensor {\n    throw new NotImplementedError();\n  }\n\n  /**\n   * Computes the shape of the result of an elementwise operation.\n   *\n   * @param shape1: Shape of the first tensor.\n   * @param shape2: Shape of the second tensor.\n   * @returns Expected output shape when an elementwise operation is carried\n   *   out on 2 tensors with shapes `shape1` and `shape2`.\n   * @throws ValueError: If `shape1` and `shape2` are not compatible for\n   *   element-wise operations.\n   */\n  private computeElementwiseOpOutputShape(shape1: Shape, shape2: Shape): Shape {\n    if (shape1 == null || shape2 == null) {\n      return null;\n    } else if (shape1.length < shape2.length) {\n      return this.computeElementwiseOpOutputShape(shape2, shape1);\n    } else if (shape2.length === 0) {\n      return shape1;\n    }\n    const outputShape: Shape = shape1.slice(0, shape1.length - shape2.length);\n    for (let k = 0; k < shape2.length; ++k) {\n      const i = shape1[shape1.length - shape2.length + k];\n      const j = shape2[k];\n      if (i == null || j == null || i < 0 || j < 0) {\n        outputShape.push(null);\n      } else if (i === 1) {\n        outputShape.push(j);\n      } else if (j === 1) {\n        outputShape.push(i);\n      } else {\n        if (i !== j) {\n          throw new ValueError(\n              'Operands could not be broadcast together with shapes ' +\n              JSON.stringify(shape1) + ' ' + JSON.stringify(shape2));\n        }\n        outputShape.push(i);\n      }\n    }\n    return outputShape;\n  }\n\n  override build(inputShape: Shape|Shape[]): void {\n    // Used purely for shape validation.\n    if (Array.isArray(inputShape) && !Array.isArray(inputShape[0])) {\n      // Make sure that inputShape is an Array of shape.\n      inputShape = [getExactlyOneShape(inputShape)];\n    }\n    inputShape = inputShape as Shape[];\n    if (inputShape.length < 2) {\n      throw new ValueError(\n          'A merge layer should be called on an Array of at least 2 inputs.' +\n          ` Got ${inputShape.length} input(s).`);\n    }\n\n    // Make sure that there is at most one unique batch size among the input\n    // shapes.\n    let batchSizes: number[] = [];\n    for (const shape of inputShape) {\n      if (shape != null && shape[0] !== null) {\n        batchSizes.push(shape[0]);\n      }\n    }\n    batchSizes = generic_utils.unique(batchSizes);\n    if (batchSizes.length > 1) {\n      throw new ValueError(\n          `Can not merge tensors with different batch sizes. ` +\n          `Got tensors with shapes: ${JSON.stringify(inputShape)}.`);\n    }\n\n    let outputShape: Shape =\n        inputShape[0] == null ? null : inputShape[0].slice(1);\n    for (let i = 1; i < inputShape.length; ++i) {\n      const shape = inputShape[i] == null ? null : inputShape[i].slice(1);\n      outputShape = this.computeElementwiseOpOutputShape(outputShape, shape);\n    }\n    // If the inputs have different ranks, we have to reshape them to make them\n    // broadcastable.\n    const allRanks = inputShape.map(shape => shape.length);\n    if (inputShape.indexOf(null) === -1 &&\n        generic_utils.unique(allRanks).length === 1) {\n      this.reshapeRequired = false;\n    } else {\n      this.reshapeRequired = true;\n    }\n  }\n\n  override call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      inputs = inputs as Tensor[];\n      if (this.reshapeRequired) {\n        const reshapedInputs: Tensor[] = [];\n        const inputDims = inputs.map(input => input.rank);\n        if (inputDims.indexOf(null) === -1) {\n          // If ranks of all inputs are available, we simply expand each of them\n          // at axis=1 until all of them have the same rank.\n          const maxNDim = mathUtils.max(inputDims);\n          for (let x of inputs) {\n            const xNDim = x.rank;\n            for (let k = 0; k < maxNDim - xNDim; ++k) {\n              x = K.expandDims(x, 1);\n            }\n            reshapedInputs.push(x);\n          }\n          return this.mergeFunction(reshapedInputs);\n        } else {\n          // Transpose all inputs so that batch size is the last dimension.\n          // [batchSize, dim1, dim2, ...] -> [dim1, dim2, ..., batchSize]\n          let transposed = false;\n          for (const x of inputs) {\n            const xNDim = x.rank;\n            if (xNDim == null) {\n              const xShape = x.shape;\n              const batchSize = xShape[0];\n              const newShape = xShape.slice(1).concat([batchSize]);\n              let xTransposed = tfc.reshape(\n                  x, [batchSize].concat(mathUtils.arrayProd(xShape.slice(1))));\n              xTransposed = tfc.transpose(xTransposed, [1, 0]);\n              xTransposed = tfc.reshape(xTransposed, newShape);\n              reshapedInputs.push(xTransposed);\n              transposed = true;\n            } else if (xNDim > 1) {\n              const dims = mathUtils.range(1, xNDim).concat([0]);\n              reshapedInputs.push(tfc.transpose(x, dims));\n              transposed = true;\n            } else {\n              // We don't transpose inputs if they are 1D vectors or scalars.\n              reshapedInputs.push(x);\n            }\n          }\n          let y = this.mergeFunction(reshapedInputs);\n          const yNDim = y.rank;\n          if (transposed) {\n            // If inputs have been transposed, we have to transpose the output\n            // too.\n            if (yNDim == null) {\n              const yShape = y.shape;\n              const yNDim = yShape.length;\n              const batchSize = yShape[yNDim - 1];\n              const newShape =\n                  [batchSize].concat(yShape.slice(0, yShape.length - 1));\n              y = tfc.reshape(\n                  tfc.transpose(tfc.reshape(y, [-1, batchSize]), [1, 0]),\n                  newShape);\n            } else if (yNDim > 1) {\n              const dims = [yNDim - 1].concat(mathUtils.range(0, yNDim - 1));\n              y = tfc.transpose(y, dims);\n            }\n          }\n          return y;\n        }\n      } else {\n        return this.mergeFunction(inputs);\n      }\n    });\n  }\n\n  override computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    inputShape = inputShape as Shape[];\n    let outputShape: Shape;\n    if (inputShape[0] == null) {\n      outputShape = null;\n    } else {\n      outputShape = inputShape[0].slice(1);\n    }\n    for (let i = 1; i < inputShape.length; ++i) {\n      const shape = inputShape[i] == null ? null : inputShape[i].slice(1);\n      outputShape = this.computeElementwiseOpOutputShape(outputShape, shape);\n    }\n\n    let batchSizes: number[] = [];\n    for (const shape of inputShape) {\n      if (shape != null && shape[0] !== null) {\n        batchSizes.push(shape[0]);\n      }\n    }\n    batchSizes = generic_utils.unique(batchSizes);\n    if (batchSizes.length === 1) {\n      outputShape = batchSizes.concat(outputShape);\n    } else {\n      outputShape = [null].concat(outputShape);\n    }\n    return outputShape;\n  }\n\n  override computeMask(inputs: Tensor|Tensor[], mask?: Tensor|Tensor[]):\n      Tensor {\n    return tfc.tidy(() => {\n      if (mask == null) {\n        return null;\n      }\n      if (!Array.isArray(mask)) {\n        throw new ValueError('`mask` should be an Array');\n      }\n      if (!Array.isArray(inputs)) {\n        throw new ValueError('`inputs` should be an Array');\n      }\n      if (mask.length !== inputs.length) {\n        throw new ValueError(\n            `The Array 'inputs' and 'mask' are expected to have the same ` +\n            `length, but have different lengths ` +\n            `(${inputs.length} vs ${mask.length})`);\n      }\n      if (mask.every(m => m == null)) {\n        return null;\n      }\n      mask = mask.map(m => m == null ? m : tfc.expandDims(m, 0));\n      let output = mask[0];\n      for (let i = 1; i < mask.length - 1; ++i) {\n        output = tfc.logicalAnd(output, mask[i]);\n      }\n      return output;\n    });\n  }\n}\n\nexport class Add extends Merge {\n  /** @nocollapse */\n  static className = 'Add';\n  constructor(args?: LayerArgs) {\n    super(args);\n  }\n\n  protected override mergeFunction(inputs: Tensor[]): Tensor {\n    return tidy(() => {\n      let output = inputs[0].clone();\n      for (let i = 1; i < inputs.length; ++i) {\n        output = tfc.add(output, inputs[i]);\n      }\n      return output;\n    });\n  }\n}\nserialization.registerClass(Add);\n\n/**\n * Calculate the element-wise sum of inputs, which all have the same shape.\n *\n * This function can be invoked in three ways.\n *\n * 1. Construct an instance of `Add` layer, by using no input argument\n *    or a single configuration argument. The resultant `Add` layer can then\n *    be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\n *\n * ```js\n * const addLayer = tf.layers.add();\n *\n * // The layer can be applied to inputs.\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = addLayer.apply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.SymbolicTensor`. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = tf.layers.add([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.Tensor` as the result of the computation. For\n * example:\n *\n * ```js\n * const input1 = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n * const input2 = tf.tensor2d([10, 20, 30, 40], [2, 2]);\n * tf.layers.add([input1, input2]).print();\n * // Gives [[11, 22], [33, 44]].\n *\n */\nexport function add(config?: SymbolicTensor[]|Tensor[]|LayerArgs): Layer|\n    SymbolicTensor|Tensor {\n  if (Array.isArray(config)) {\n    const layer = new Add({});\n    return layer.apply(config) as SymbolicTensor | Tensor;\n  } else {\n    return new Add(config);\n  }\n}\n\nexport class Multiply extends Merge {\n  /** @nocollapse */\n  static className = 'Multiply';\n  constructor(args?: LayerArgs) {\n    super(args);\n  }\n\n  protected override mergeFunction(inputs: Tensor[]): Tensor {\n    return tidy(() => {\n      let output = inputs[0].clone();\n      for (let i = 1; i < inputs.length; ++i) {\n        output = tfc.mul(output, inputs[i]);\n      }\n      return output;\n    });\n  }\n}\nserialization.registerClass(Multiply);\n\n/**\n * Calculate the element-wise product of inputs, which all have the same shape.\n *\n * This function can be invoked in three ways.\n *\n * 1. Construct an instance of `Multiply` layer, by using no input argument\n *    or a single configuration argument. The resultant `Multiply` layer can\n *    then be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\n *\n * ```js\n * const multiplyLayer = tf.layers.multiply();\n *\n * // The layer can be applied to inputs.\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = multiplyLayer.apply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.SymbolicTensor`. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = tf.layers.multiply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.Tensor` as the result of the computation. For\n * example:\n *\n * ```js\n * const input1 = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n * const input2 = tf.tensor2d([10, 20, 30, 40], [2, 2]);\n * tf.layers.multiply([input1, input2]).print();\n * // Gives [[10, 40], [90, 160]].\n *\n */\nexport function multiply(config?: SymbolicTensor[]|Tensor[]|LayerArgs): Layer|\n    SymbolicTensor|Tensor {\n  if (Array.isArray(config)) {\n    const layer = new Multiply({});\n    return layer.apply(config) as SymbolicTensor | Tensor;\n  } else {\n    return new Multiply(config);\n  }\n}\n\nexport class Average extends Merge {\n  /** @nocollapse */\n  static className = 'Average';\n  constructor(args?: LayerArgs) {\n    super(args);\n  }\n\n  protected override mergeFunction(inputs: Tensor[]): Tensor {\n    return tidy(() => {\n      let output = inputs[0].clone();\n      for (let i = 1; i < inputs.length; ++i) {\n        output = tfc.add(output, inputs[i]);\n      }\n      return tfc.mul(1 / inputs.length, output);\n    });\n  }\n}\nserialization.registerClass(Average);\n\n/**\n * Calculate the element-wise arithmetic mean of inputs, which all have the same\n * shape.\n *\n * This function can be invoked in three ways.\n *\n * 1. Construct an instance of `Average` layer, by using no input argument\n *    or a single configuration argument. The resultant `Average` layer can then\n *    be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\n *\n * ```js\n * const averageLayer = tf.layers.average();\n *\n * // The layer can be applied to inputs.\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = averageLayer.apply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.SymbolicTensor`. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = tf.layers.average([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.Tensor` as the result of the computation. For\n * example:\n *\n * ```js\n * const input1 = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n * const input2 = tf.tensor2d([10, 20, 30, 40], [2, 2]);\n * tf.layers.average([input1, input2]).print();\n * // Gives [[5.5, 11], [16.5, 22]].\n *\n */\nexport function average(config?: SymbolicTensor[]|Tensor[]|LayerArgs): Layer|\n    SymbolicTensor|Tensor {\n  if (Array.isArray(config)) {\n    const layer = new Average({});\n    return layer.apply(config) as SymbolicTensor | Tensor;\n  } else {\n    return new Average(config);\n  }\n}\n\nexport class Maximum extends Merge {\n  /** @nocollapse */\n  static className = 'Maximum';\n  constructor(args?: LayerArgs) {\n    super(args);\n  }\n\n  protected override mergeFunction(inputs: Tensor[]): Tensor {\n    return tidy(() => {\n      let output = inputs[0];\n      for (let i = 1; i < inputs.length; ++i) {\n        output = tfc.maximum(output, inputs[i]);\n      }\n      return output;\n    });\n  }\n}\nserialization.registerClass(Maximum);\n\n/**\n * Calculate the element-wise maximum of inputs, which all have the same shape.\n *\n * This function can be invoked in three ways.\n *\n * 1. Construct an instance of `Maximum` layer, by using no input argument\n *    or a single configuration argument. The resultant `Maximum` layer can then\n *    be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\n *\n * ```js\n * const maximumLayer = tf.layers.maximum();\n *\n * // The layer can be applied to inputs.\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = maximumLayer.apply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.SymbolicTensor`. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = tf.layers.maximum([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.Tensor` as the result of the computation. For\n * example:\n *\n * ```js\n * const input1 = tf.tensor2d([1, 20, 3, 40], [2, 2]);\n * const input2 = tf.tensor2d([10, 2, 30, 4], [2, 2]);\n * tf.layers.maximum([input1, input2]).print();\n * // Gives [[10, 20], [30, 40]].\n *\n */\nexport function maximum(config?: SymbolicTensor[]|Tensor[]|LayerArgs): Layer|\n    SymbolicTensor|Tensor {\n  if (Array.isArray(config)) {\n    const layer = new Maximum({});\n    return layer.apply(config) as SymbolicTensor | Tensor;\n  } else {\n    return new Maximum(config);\n  }\n}\n\nexport class Minimum extends Merge {\n  /** @nocollapse */\n  static className = 'Minimum';\n  constructor(args?: LayerArgs) {\n    super(args);\n  }\n\n  protected override mergeFunction(inputs: Tensor[]): Tensor {\n    return tidy(() => {\n      let output = inputs[0];\n      for (let i = 1; i < inputs.length; ++i) {\n        output = tfc.minimum(output, inputs[i]);\n      }\n      return output;\n    });\n  }\n}\nserialization.registerClass(Minimum);\n\n/**\n * Calculate the element-wise minimum of inputs, which all have the same shape.\n *\n * This function can be invoked in three ways.\n *\n * 1. Construct an instance of `Minimum` layer, by using no input argument\n *    or a single configuration argument. The resultant `Minimum` layer can then\n *    be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\n *\n * ```js\n * const minimumLayer = tf.layers.minimum();\n *\n * // The layer can be applied to inputs.\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = minimumLayer.apply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.SymbolicTensor`. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = tf.layers.minimum([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.Tensor` as the result of the computation. For\n * example:\n *\n * ```js\n * const input1 = tf.tensor2d([1, 20, 3, 40], [2, 2]);\n * const input2 = tf.tensor2d([10, 2, 30, 4], [2, 2]);\n * tf.layers.minimum([input1, input2]).print();\n * // Gives [[1, 2], [3, 4]].\n *\n */\nexport function minimum(config?: SymbolicTensor[]|Tensor[]|LayerArgs): Layer|\n    SymbolicTensor|Tensor {\n  if (Array.isArray(config)) {\n    const layer = new Minimum({});\n    return layer.apply(config) as SymbolicTensor | Tensor;\n  } else {\n    return new Minimum(config);\n  }\n}\n\nexport declare interface ConcatenateLayerArgs extends LayerArgs {\n  /**\n   * Axis along which to concatenate.\n   */\n  axis?: number;\n}\n\nexport class Concatenate extends Merge {\n  /** @nocollapse */\n  static className = 'Concatenate';\n  readonly DEFAULT_AXIS = -1;\n  private readonly axis: number;\n\n  constructor(args?: ConcatenateLayerArgs) {\n    super(args);\n    if (args == null) {\n      args = {};\n    }\n    this.axis = args.axis == null ? this.DEFAULT_AXIS : args.axis;\n    this.supportsMasking = true;\n    this.reshapeRequired = false;\n  }\n\n  override build(inputShape: Shape|Shape[]): void {\n    // Used purely for shape validation.]\n    if (!(Array.isArray(inputShape) && Array.isArray(inputShape[0])) ||\n        inputShape.length === 1) {\n      throw new ValueError(\n          'A `Concatenate` layer should be called on a list of at least 2 ' +\n          'inputs');\n    }\n    inputShape = inputShape as Shape[];\n\n    let allNoneShape = true;\n    for (const shape of inputShape) {\n      if (shape != null) {\n        allNoneShape = false;\n        break;\n      }\n    }\n    if (allNoneShape) {\n      return;\n    }\n\n    const shapeSet: Shape[] = [];\n    for (let i = 0; i < inputShape.length; ++i) {\n      const shapeWithoutConcatAxis = inputShape[i].slice();\n      shapeWithoutConcatAxis.splice(this.axis, 1);\n      let exists = false;\n      for (const shape of shapeSet) {\n        if (util.arraysEqual(shape, shapeWithoutConcatAxis)) {\n          exists = true;\n          break;\n        }\n      }\n      if (!exists) {\n        shapeSet.push(shapeWithoutConcatAxis);\n      }\n    }\n    if (shapeSet.length > 1) {\n      throw new ValueError(\n          'A `Concatenate` layer requires inputs with matching shapes ' +\n          'except for the concat axis. Got input shapes: ' +\n          JSON.stringify(inputShape));\n    }\n  }\n\n  protected override mergeFunction(inputs: Tensor[]): Tensor {\n    return tidy(() => {\n      return K.concatenate(inputs, this.axis);\n    });\n  }\n\n  override computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    if (!(Array.isArray(inputShape) && Array.isArray(inputShape[0]))) {\n      throw new ValueError(\n          'A `Concatenate` layer should be called on a list of inputs.');\n    }\n    const inputShapes = inputShape as Shape[];\n    const outputShape = inputShapes[0].slice();\n    const axis = this.axis < 0 ? outputShape.length + this.axis : this.axis;\n    // Porting Note: the line above is because TypeScript doesn't support\n    //   negative indices.\n    for (const shape of inputShapes.slice(1)) {\n      if (outputShape[axis] == null || shape[axis] == null) {\n        outputShape[axis] = null;\n        break;\n      }\n      outputShape[axis] += shape[axis];\n    }\n    return outputShape;\n  }\n\n  override computeMask(inputs: Tensor|Tensor[], mask?: Tensor|Tensor[]):\n      Tensor {\n    if (mask == null) {\n      return null;\n    }\n    if (!Array.isArray(mask)) {\n      throw new ValueError('`mask` should be an array for Concatenate');\n    }\n    if (!Array.isArray(inputs)) {\n      throw new ValueError('`inputs` should be an array for Concatenate');\n    }\n    if (mask.length !== inputs.length) {\n      throw new ValueError(\n          `Mismatch in the length of mask (${mask.length}) ` +\n          `and the legnth of inputs (${inputs.length})`);\n    }\n    return tfc.tidy(() => {\n      let allNullMasks = true;\n      mask.forEach(m => {\n        if (m != null) {\n          allNullMasks = false;\n          return;\n        }\n      });\n      if (allNullMasks) {\n        return null;\n      }\n      const outputMasks: Tensor[] = [];\n      for (let i = 0; i < inputs.length; ++i) {\n        if (mask[i] == null) {\n          // Input is unmasked. Append all 1's to masks.\n          outputMasks.push(tfc.cast(tfc.onesLike(inputs[i]), 'bool'));\n        } else if (mask[i].rank < inputs[i].rank) {\n          // Mask is smaller than the input, expand it.\n          outputMasks.push(tfc.expandDims(mask[i], -1));\n        } else {\n          outputMasks.push(mask[i]);\n        }\n      }\n      const concatenatedMasks = tfc.concat(outputMasks, this.axis);\n      return tfc.all(concatenatedMasks, -1, false);\n    });\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {\n      'axis': this.axis,\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(Concatenate);\n\n/**\n * Concatenate an `Array` of inputs.\n *\n * This function can be invoked in three ways.\n *\n * 1. Construct an instance of `Concatenate` layer, by using no input argument\n *    or a single configuration argument. The resultant `Concatenate` layer can\n *    then be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\n *\n * ```js\n * const concatLayer = tf.layers.concatenate();\n *\n * // The layer can be applied to inputs.\n * const input1 = tf.input({shape: [2, 3]});\n * const input2 = tf.input({shape: [2, 4]});\n * const output = concatLayer.apply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 7], with the first dimension as the undetermined batch\n * // dimension and the last dimension as the result of concatenating the\n * // last dimensions of the two inputs.\n * ```\n *\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.SymbolicTensor`. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 3]});\n * const input2 = tf.input({shape: [2, 4]});\n * const output = tf.layers.concatenate([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension and the last dimension as the result of concatenating the\n * // last dimensions of the two inputs.\n * ```\n *\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.Tensor` as the result of the computation. For\n * example:\n *\n * ```js\n * const input1 = tf.tensor2d([[1, 2], [3, 4]], [2, 2]);\n * const input2 = tf.tensor2d([[10, 20], [30, 40]], [2, 2]);\n * tf.layers.concatenate([input1, input2]).print();\n * // Gives [[1, 2, 10, 20], [3, 4, 30, 40]].\n *\n */\nexport function concatenate(config?: SymbolicTensor[]|Tensor[]|\n                            ConcatenateLayerArgs): Layer|SymbolicTensor|Tensor {\n  if (Array.isArray(config)) {\n    const layer = new Concatenate({});\n    return layer.apply(config) as SymbolicTensor | Tensor;\n  } else {\n    return new Concatenate(config);\n  }\n}\n\nexport declare interface DotLayerArgs extends LayerArgs {\n  /**\n   * Axis or axes along which the dot product will be taken.\n   *\n   * Integer or an Array of integers.\n   */\n  axes: number|[number, number];\n\n  /**\n   * Whether to L2-normalize samples along the dot product axis\n   * before taking the dot product.\n   *\n   * If set to `true`, the output of the dot product is the cosine\n   * proximity between the two samples.\n   */\n  normalize?: boolean;\n}\n\n/**\n * Interpretable potentially negative axis index.\n *\n * For example, given axis = -1, and dim = 3, this function will return 2.\n *\n * @param axis The axis index, may be a positive, zero or negative integer.\n * @param dim Total number of dimensions, a positive integer.\n * @returns A non-negative axis index equivalent to the input `axis`.\n */\nfunction interpretAxis(axis: number, dim: number): number {\n  while (axis < 0) {\n    axis += dim;\n  }\n  return axis;\n}\n\nfunction batchDot(x: Tensor, y: Tensor, axes: number|[number, number]): Tensor {\n  if (x.shape.length > 3 || y.shape.length > 3) {\n    throw new NotImplementedError(\n        'batchDot is not implemented for tensors of 4D or higher rank yet');\n  }\n  tfc.util.assert(\n      x.shape.length >= 2,\n      () => `batchDot requires the rank of x to be >= 2, ` +\n          `but got ${x.shape.length}`);\n  tfc.util.assert(\n      x.shape.length >= 2,\n      () => `batchDot requires the rank of y to be >= 2, ` +\n          `but got ${y.shape.length}`);\n\n  if (typeof axes === 'number') {\n    axes = [axes, axes];\n  }\n\n  if (x.dtype === 'complex64' || y.dtype === 'complex64') {\n    throw new NotImplementedError(\n        'batchDot is not implemented for complex64-type Tensors yet.');\n  }\n\n  const xNDim = x.shape.length;\n  const yNDim = y.shape.length;\n  if (axes == null) {\n    // Behave like batchMatmul by default.\n    axes = [xNDim - 1, yNDim - 2];\n  }\n  const axesArray = axes as [number, number];\n\n  return tfc.tidy(() => {\n    let diff: number;\n    if (xNDim > yNDim) {\n      diff = xNDim - yNDim;\n      const diffShape: Shape = [];\n      for (let i = 0; i < diff; ++i) {\n        diffShape.push(1);\n      }\n      y = tfc.reshape(y, y.shape.concat(diffShape));\n    } else if (yNDim > xNDim) {\n      diff = yNDim - xNDim;\n      const diffShape: Shape = [];\n      for (let i = 0; i < diff; ++i) {\n        diffShape.push(1);\n      }\n      x = tfc.reshape(x, x.shape.concat(diffShape));\n    } else {\n      diff = 0;\n    }\n\n    let out: Tensor;\n    if (x.shape.length === 2 && y.shape.length === 2) {\n      if (axesArray[0] === axesArray[1]) {\n        out = tfc.sum(tfc.mul(x, y), axesArray[0]);\n      } else {\n        out = tfc.sum(tfc.mul(tfc.transpose(x, [1, 0]), y), axesArray[1]);\n      }\n    } else {\n      const adjX = axesArray[0] !== x.shape.length - 1;\n      const adjY = axesArray[1] === y.shape.length - 1;\n      out = tfc.matMul(x, y, adjX, adjY);\n    }\n\n    if (diff > 0) {\n      let idx: number;\n      if (xNDim > yNDim) {\n        idx = xNDim + yNDim - 3;\n      } else {\n        idx = xNDim - 1;\n      }\n      const squeezeAxes: number[] = [];\n      for (let i = idx; i < idx + diff; ++i) {\n        squeezeAxes.push(i);\n      }\n      out = tfc.squeeze(out, squeezeAxes);\n    }\n    if (out.shape.length === 1) {\n      out = tfc.expandDims(out, 1);\n    }\n    return out;\n  });\n}\n\nexport class Dot extends Merge {\n  /** @nocollapse */\n  static className = 'Dot';\n\n  private axes: number|[number, number];\n  private normalize: boolean;\n\n  constructor(args: DotLayerArgs) {\n    super(args);\n    this.axes = args.axes;\n    this.normalize = args.normalize == null ? false : args.normalize;\n    this.supportsMasking = true;\n    this.reshapeRequired = false;\n  }\n\n  override build(inputShape: Shape|Shape[]): void {\n    tfc.util.assert(\n        Array.isArray(inputShape) && inputShape.length === 2 &&\n            Array.isArray(inputShape[0]) && Array.isArray(inputShape[1]),\n        () => 'A `Dot` layer should be called on a list of exactly 2 inputs.');\n    const shape1 = inputShape[0] as Shape;\n    const shape2 = inputShape[1] as Shape;\n    if (shape1.length > 3 || shape2.length > 3) {\n      throw new NotImplementedError(\n          'Dot layer does not support tensors of 4D or higher rank yet.');\n    }\n\n    const axes = this.interpretAxes(shape1, shape2);\n    if (shape1[axes[0]] !== shape2[axes[1]]) {\n      throw new ValueError(\n          `Dimension incompatibility: ` +\n          `${shape1[axes[0]]} !== ${shape2[axes[1]]}`);\n    }\n  }\n\n  protected override mergeFunction(inputs: Tensor[]): Tensor {\n    if (inputs.length !== 2) {\n      throw new ValueError(\n          'A `Dot` layer must be called on exactly 2 inputs, ' +\n          `but received ${inputs.length} input(s).`);\n    }\n\n    let x1 = inputs[0];\n    let x2 = inputs[1];\n    let axes: [number, number];\n    if (!Array.isArray(this.axes)) {\n      axes = [\n        interpretAxis(this.axes, x1.shape.length),\n        interpretAxis(this.axes, x2.shape.length)\n      ];\n    } else {\n      axes = this.axes.map(\n                 (axis, i) => interpretAxis(\n                     axis, inputs[i].shape.length)) as [number, number];\n    }\n    if (this.normalize) {\n      x1 = l2Normalize(x1, axes[0]);\n      x2 = l2Normalize(x2, axes[1]);\n    }\n    return batchDot(x1, x2, axes);\n  }\n\n  private interpretAxes(shape1: Shape, shape2: Shape): number[] {\n    let axes: number[];\n    if (!Array.isArray(this.axes)) {\n      // `this.axes` is a single integer.\n      axes = [\n        interpretAxis(this.axes, shape1.length),\n        interpretAxis(this.axes, shape2.length)\n      ];\n    } else {\n      // `this.axes` is an Array of integers.\n      axes = this.axes;\n    }\n    return axes;\n  }\n\n  override computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    tfc.util.assert(\n        Array.isArray(inputShape) && inputShape.length === 2 &&\n            Array.isArray(inputShape[0]) && Array.isArray(inputShape[1]),\n        () => 'A `Dot` layer should be called on a list of exactly 2 inputs.');\n    const shape1 = (inputShape[0] as Shape).slice();\n    const shape2 = (inputShape[1] as Shape).slice();\n    if (shape1.length > 3 || shape2.length > 3) {\n      throw new NotImplementedError(\n          'Dot layer does not support tensors of 4D or higher rank yet.');\n    }\n\n    const axes = this.interpretAxes(shape1, shape2);\n    shape1.splice(axes[0], 1);\n    shape2.splice(axes[1], 1);\n    shape2.splice(0, 1);\n    const outputShape = shape1.concat(shape2);\n    if (outputShape.length === 1) {\n      outputShape.push(1);\n    }\n    return outputShape;\n  }\n\n  override computeMask(inputs: Tensor|Tensor[], mask?: Tensor|Tensor[]):\n      Tensor {\n    return null;\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {\n      'axes': this.axes,\n      'normalize': this.normalize\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(Dot);\n\n// TODO(cais): Add functional interfaces for the merge layers.\n"],"mappings":";;;;;;;AAAA;;;;;;;;;AAUA;;;AAIA,OAAO,KAAKA,GAAG,MAAM,uBAAuB;AAC5C,SAAQC,aAAa,EAAUC,IAAI,EAAEC,IAAI,QAAO,uBAAuB;AACvE,OAAO,KAAKC,CAAC,MAAM,yBAAyB;AAC5C,SAAQC,KAAK,QAAkC,oBAAoB;AACnE,SAAQC,mBAAmB,EAAEC,UAAU,QAAO,WAAW;AAEzD,SAAQC,WAAW,QAAO,WAAW;AAErC,OAAO,KAAKC,aAAa,MAAM,wBAAwB;AACvD,OAAO,KAAKC,SAAS,MAAM,qBAAqB;AAChD,SAAQC,kBAAkB,QAAO,sBAAsB;AAEvD;;;;;AAKA,WAAsBC,KAAM,0BAAAC,MAAA;EAAAC,SAAA,CAAAF,KAAA,EAAAC,MAAA;EAAA,IAAAE,MAAA,GAAAC,YAAA,CAAAJ,KAAA;EAG1B,SAAAA,MAAYK,IAAgB;IAAA,IAAAC,KAAA;IAAAC,eAAA,OAAAP,KAAA;IAC1BM,KAAA,GAAAH,MAAA,CAAAK,IAAA,OAAMH,IAAI,IAAI,EAAE;IAChBC,KAAA,CAAKG,eAAe,GAAG,IAAI;IAAC,OAAAH,KAAA;EAC9B;EAEA;;;;EAAAI,YAAA,CAAAV,KAAA;IAAAW,GAAA;IAAAC,KAAA,EAIU,SAAAC,cAAcC,MAAgB;MACtC,MAAM,IAAIpB,mBAAmB,EAAE;IACjC;IAEA;;;;;;;;;;EAAA;IAAAiB,GAAA;IAAAC,KAAA,EAUQ,SAAAG,gCAAgCC,MAAa,EAAEC,MAAa;MAClE,IAAID,MAAM,IAAI,IAAI,IAAIC,MAAM,IAAI,IAAI,EAAE;QACpC,OAAO,IAAI;OACZ,MAAM,IAAID,MAAM,CAACE,MAAM,GAAGD,MAAM,CAACC,MAAM,EAAE;QACxC,OAAO,IAAI,CAACH,+BAA+B,CAACE,MAAM,EAAED,MAAM,CAAC;OAC5D,MAAM,IAAIC,MAAM,CAACC,MAAM,KAAK,CAAC,EAAE;QAC9B,OAAOF,MAAM;;MAEf,IAAMG,WAAW,GAAUH,MAAM,CAACI,KAAK,CAAC,CAAC,EAAEJ,MAAM,CAACE,MAAM,GAAGD,MAAM,CAACC,MAAM,CAAC;MACzE,KAAK,IAAIG,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGJ,MAAM,CAACC,MAAM,EAAE,EAAEG,CAAC,EAAE;QACtC,IAAMC,CAAC,GAAGN,MAAM,CAACA,MAAM,CAACE,MAAM,GAAGD,MAAM,CAACC,MAAM,GAAGG,CAAC,CAAC;QACnD,IAAME,CAAC,GAAGN,MAAM,CAACI,CAAC,CAAC;QACnB,IAAIC,CAAC,IAAI,IAAI,IAAIC,CAAC,IAAI,IAAI,IAAID,CAAC,GAAG,CAAC,IAAIC,CAAC,GAAG,CAAC,EAAE;UAC5CJ,WAAW,CAACK,IAAI,CAAC,IAAI,CAAC;SACvB,MAAM,IAAIF,CAAC,KAAK,CAAC,EAAE;UAClBH,WAAW,CAACK,IAAI,CAACD,CAAC,CAAC;SACpB,MAAM,IAAIA,CAAC,KAAK,CAAC,EAAE;UAClBJ,WAAW,CAACK,IAAI,CAACF,CAAC,CAAC;SACpB,MAAM;UACL,IAAIA,CAAC,KAAKC,CAAC,EAAE;YACX,MAAM,IAAI5B,UAAU,CAChB,uDAAuD,GACvD8B,IAAI,CAACC,SAAS,CAACV,MAAM,CAAC,GAAG,GAAG,GAAGS,IAAI,CAACC,SAAS,CAACT,MAAM,CAAC,CAAC;;UAE5DE,WAAW,CAACK,IAAI,CAACF,CAAC,CAAC;;;MAGvB,OAAOH,WAAW;IACpB;EAAC;IAAAR,GAAA;IAAAC,KAAA,EAEQ,SAAAe,MAAMC,UAAyB;MACtC;MACA,IAAIC,KAAK,CAACC,OAAO,CAACF,UAAU,CAAC,IAAI,CAACC,KAAK,CAACC,OAAO,CAACF,UAAU,CAAC,CAAC,CAAC,CAAC,EAAE;QAC9D;QACAA,UAAU,GAAG,CAAC7B,kBAAkB,CAAC6B,UAAU,CAAC,CAAC;;MAE/CA,UAAU,GAAGA,UAAqB;MAClC,IAAIA,UAAU,CAACV,MAAM,GAAG,CAAC,EAAE;QACzB,MAAM,IAAIvB,UAAU,CAChB,kEAAkE,WAAAoC,MAAA,CAC1DH,UAAU,CAACV,MAAM,eAAY,CAAC;;MAG5C;MACA;MACA,IAAIc,UAAU,GAAa,EAAE;MAAC,IAAAC,SAAA,GAAAC,0BAAA,CACVN,UAAU;QAAAO,KAAA;MAAA;QAA9B,KAAAF,SAAA,CAAAG,CAAA,MAAAD,KAAA,GAAAF,SAAA,CAAAI,CAAA,IAAAC,IAAA,GAAgC;UAAA,IAArBC,MAAK,GAAAJ,KAAA,CAAAvB,KAAA;UACd,IAAI2B,MAAK,IAAI,IAAI,IAAIA,MAAK,CAAC,CAAC,CAAC,KAAK,IAAI,EAAE;YACtCP,UAAU,CAACR,IAAI,CAACe,MAAK,CAAC,CAAC,CAAC,CAAC;;;MAE5B,SAAAC,GAAA;QAAAP,SAAA,CAAAQ,CAAA,CAAAD,GAAA;MAAA;QAAAP,SAAA,CAAAS,CAAA;MAAA;MACDV,UAAU,GAAGnC,aAAa,CAAC8C,MAAM,CAACX,UAAU,CAAC;MAC7C,IAAIA,UAAU,CAACd,MAAM,GAAG,CAAC,EAAE;QACzB,MAAM,IAAIvB,UAAU,CAChB,mFAAAoC,MAAA,CAC4BN,IAAI,CAACC,SAAS,CAACE,UAAU,CAAC,MAAG,CAAC;;MAGhE,IAAIT,WAAW,GACXS,UAAU,CAAC,CAAC,CAAC,IAAI,IAAI,GAAG,IAAI,GAAGA,UAAU,CAAC,CAAC,CAAC,CAACR,KAAK,CAAC,CAAC,CAAC;MACzD,KAAK,IAAIE,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGM,UAAU,CAACV,MAAM,EAAE,EAAEI,CAAC,EAAE;QAC1C,IAAMiB,KAAK,GAAGX,UAAU,CAACN,CAAC,CAAC,IAAI,IAAI,GAAG,IAAI,GAAGM,UAAU,CAACN,CAAC,CAAC,CAACF,KAAK,CAAC,CAAC,CAAC;QACnED,WAAW,GAAG,IAAI,CAACJ,+BAA+B,CAACI,WAAW,EAAEoB,KAAK,CAAC;;MAExE;MACA;MACA,IAAMK,QAAQ,GAAGhB,UAAU,CAACiB,GAAG,CAAC,UAAAN,KAAK;QAAA,OAAIA,KAAK,CAACrB,MAAM;MAAA,EAAC;MACtD,IAAIU,UAAU,CAACkB,OAAO,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC,IAC/BjD,aAAa,CAAC8C,MAAM,CAACC,QAAQ,CAAC,CAAC1B,MAAM,KAAK,CAAC,EAAE;QAC/C,IAAI,CAAC6B,eAAe,GAAG,KAAK;OAC7B,MAAM;QACL,IAAI,CAACA,eAAe,GAAG,IAAI;;IAE/B;EAAC;IAAApC,GAAA;IAAAC,KAAA,EAEQ,SAAAJ,KAAKM,MAAuB,EAAEkC,MAAc;MAAA,IAAAC,MAAA;MACnD,OAAO3D,IAAI,CAAC,YAAK;QACfwB,MAAM,GAAGA,MAAkB;QAC3B,IAAImC,MAAI,CAACF,eAAe,EAAE;UACxB,IAAMG,cAAc,GAAa,EAAE;UACnC,IAAMC,SAAS,GAAGrC,MAAM,CAAC+B,GAAG,CAAC,UAAAO,KAAK;YAAA,OAAIA,KAAK,CAACC,IAAI;UAAA,EAAC;UACjD,IAAIF,SAAS,CAACL,OAAO,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC,EAAE;YAClC;YACA;YACA,IAAMQ,OAAO,GAAGxD,SAAS,CAACyD,GAAG,CAACJ,SAAS,CAAC;YAAC,IAAAK,UAAA,GAAAtB,0BAAA,CAC3BpB,MAAM;cAAA2C,MAAA;YAAA;cAApB,KAAAD,UAAA,CAAApB,CAAA,MAAAqB,MAAA,GAAAD,UAAA,CAAAnB,CAAA,IAAAC,IAAA,GAAsB;gBAAA,IAAboB,CAAC,GAAAD,MAAA,CAAA7C,KAAA;gBACR,IAAM+C,KAAK,GAAGD,CAAC,CAACL,IAAI;gBACpB,KAAK,IAAIhC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGiC,OAAO,GAAGK,KAAK,EAAE,EAAEtC,CAAC,EAAE;kBACxCqC,CAAC,GAAGlE,CAAC,CAACoE,UAAU,CAACF,CAAC,EAAE,CAAC,CAAC;;gBAExBR,cAAc,CAAC1B,IAAI,CAACkC,CAAC,CAAC;;YACvB,SAAAlB,GAAA;cAAAgB,UAAA,CAAAf,CAAA,CAAAD,GAAA;YAAA;cAAAgB,UAAA,CAAAd,CAAA;YAAA;YACD,OAAOO,MAAI,CAACpC,aAAa,CAACqC,cAAc,CAAC;WAC1C,MAAM;YACL;YACA;YACA,IAAIW,UAAU,GAAG,KAAK;YAAC,IAAAC,UAAA,GAAA5B,0BAAA,CACPpB,MAAM;cAAAiD,MAAA;YAAA;cAAtB,KAAAD,UAAA,CAAA1B,CAAA,MAAA2B,MAAA,GAAAD,UAAA,CAAAzB,CAAA,IAAAC,IAAA,GAAwB;gBAAA,IAAboB,EAAC,GAAAK,MAAA,CAAAnD,KAAA;gBACV,IAAM+C,MAAK,GAAGD,EAAC,CAACL,IAAI;gBACpB,IAAIM,MAAK,IAAI,IAAI,EAAE;kBACjB,IAAMK,MAAM,GAAGN,EAAC,CAACnB,KAAK;kBACtB,IAAM0B,UAAS,GAAGD,MAAM,CAAC,CAAC,CAAC;kBAC3B,IAAME,SAAQ,GAAGF,MAAM,CAAC5C,KAAK,CAAC,CAAC,CAAC,CAACW,MAAM,CAAC,CAACkC,UAAS,CAAC,CAAC;kBACpD,IAAIE,WAAW,GAAG/E,GAAG,CAACgF,OAAO,CACzBV,EAAC,EAAE,CAACO,UAAS,CAAC,CAAClC,MAAM,CAACjC,SAAS,CAACuE,SAAS,CAACL,MAAM,CAAC5C,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;kBAChE+C,WAAW,GAAG/E,GAAG,CAACkF,SAAS,CAACH,WAAW,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;kBAChDA,WAAW,GAAG/E,GAAG,CAACgF,OAAO,CAACD,WAAW,EAAED,SAAQ,CAAC;kBAChDhB,cAAc,CAAC1B,IAAI,CAAC2C,WAAW,CAAC;kBAChCN,UAAU,GAAG,IAAI;iBAClB,MAAM,IAAIF,MAAK,GAAG,CAAC,EAAE;kBACpB,IAAMY,KAAI,GAAGzE,SAAS,CAAC0E,KAAK,CAAC,CAAC,EAAEb,MAAK,CAAC,CAAC5B,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC;kBAClDmB,cAAc,CAAC1B,IAAI,CAACpC,GAAG,CAACkF,SAAS,CAACZ,EAAC,EAAEa,KAAI,CAAC,CAAC;kBAC3CV,UAAU,GAAG,IAAI;iBAClB,MAAM;kBACL;kBACAX,cAAc,CAAC1B,IAAI,CAACkC,EAAC,CAAC;;;YAEzB,SAAAlB,GAAA;cAAAsB,UAAA,CAAArB,CAAA,CAAAD,GAAA;YAAA;cAAAsB,UAAA,CAAApB,CAAA;YAAA;YACD,IAAI+B,CAAC,GAAGxB,MAAI,CAACpC,aAAa,CAACqC,cAAc,CAAC;YAC1C,IAAMwB,KAAK,GAAGD,CAAC,CAACpB,IAAI;YACpB,IAAIQ,UAAU,EAAE;cACd;cACA;cACA,IAAIa,KAAK,IAAI,IAAI,EAAE;gBACjB,IAAMC,MAAM,GAAGF,CAAC,CAAClC,KAAK;gBACtB,IAAMmC,MAAK,GAAGC,MAAM,CAACzD,MAAM;gBAC3B,IAAM+C,SAAS,GAAGU,MAAM,CAACD,MAAK,GAAG,CAAC,CAAC;gBACnC,IAAMR,QAAQ,GACV,CAACD,SAAS,CAAC,CAAClC,MAAM,CAAC4C,MAAM,CAACvD,KAAK,CAAC,CAAC,EAAEuD,MAAM,CAACzD,MAAM,GAAG,CAAC,CAAC,CAAC;gBAC1DuD,CAAC,GAAGrF,GAAG,CAACgF,OAAO,CACXhF,GAAG,CAACkF,SAAS,CAAClF,GAAG,CAACgF,OAAO,CAACK,CAAC,EAAE,CAAC,CAAC,CAAC,EAAER,SAAS,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EACtDC,QAAQ,CAAC;eACd,MAAM,IAAIQ,KAAK,GAAG,CAAC,EAAE;gBACpB,IAAMH,IAAI,GAAG,CAACG,KAAK,GAAG,CAAC,CAAC,CAAC3C,MAAM,CAACjC,SAAS,CAAC0E,KAAK,CAAC,CAAC,EAAEE,KAAK,GAAG,CAAC,CAAC,CAAC;gBAC9DD,CAAC,GAAGrF,GAAG,CAACkF,SAAS,CAACG,CAAC,EAAEF,IAAI,CAAC;;;YAG9B,OAAOE,CAAC;;SAEX,MAAM;UACL,OAAOxB,MAAI,CAACpC,aAAa,CAACC,MAAM,CAAC;;MAErC,CAAC,CAAC;IACJ;EAAC;IAAAH,GAAA;IAAAC,KAAA,EAEQ,SAAAgE,mBAAmBhD,UAAyB;MACnDA,UAAU,GAAGA,UAAqB;MAClC,IAAIT,WAAkB;MACtB,IAAIS,UAAU,CAAC,CAAC,CAAC,IAAI,IAAI,EAAE;QACzBT,WAAW,GAAG,IAAI;OACnB,MAAM;QACLA,WAAW,GAAGS,UAAU,CAAC,CAAC,CAAC,CAACR,KAAK,CAAC,CAAC,CAAC;;MAEtC,KAAK,IAAIE,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGM,UAAU,CAACV,MAAM,EAAE,EAAEI,CAAC,EAAE;QAC1C,IAAMiB,KAAK,GAAGX,UAAU,CAACN,CAAC,CAAC,IAAI,IAAI,GAAG,IAAI,GAAGM,UAAU,CAACN,CAAC,CAAC,CAACF,KAAK,CAAC,CAAC,CAAC;QACnED,WAAW,GAAG,IAAI,CAACJ,+BAA+B,CAACI,WAAW,EAAEoB,KAAK,CAAC;;MAGxE,IAAIP,UAAU,GAAa,EAAE;MAAC,IAAA6C,UAAA,GAAA3C,0BAAA,CACVN,UAAU;QAAAkD,MAAA;MAAA;QAA9B,KAAAD,UAAA,CAAAzC,CAAA,MAAA0C,MAAA,GAAAD,UAAA,CAAAxC,CAAA,IAAAC,IAAA,GAAgC;UAAA,IAArBC,OAAK,GAAAuC,MAAA,CAAAlE,KAAA;UACd,IAAI2B,OAAK,IAAI,IAAI,IAAIA,OAAK,CAAC,CAAC,CAAC,KAAK,IAAI,EAAE;YACtCP,UAAU,CAACR,IAAI,CAACe,OAAK,CAAC,CAAC,CAAC,CAAC;;;MAE5B,SAAAC,GAAA;QAAAqC,UAAA,CAAApC,CAAA,CAAAD,GAAA;MAAA;QAAAqC,UAAA,CAAAnC,CAAA;MAAA;MACDV,UAAU,GAAGnC,aAAa,CAAC8C,MAAM,CAACX,UAAU,CAAC;MAC7C,IAAIA,UAAU,CAACd,MAAM,KAAK,CAAC,EAAE;QAC3BC,WAAW,GAAGa,UAAU,CAACD,MAAM,CAACZ,WAAW,CAAC;OAC7C,MAAM;QACLA,WAAW,GAAG,CAAC,IAAI,CAAC,CAACY,MAAM,CAACZ,WAAW,CAAC;;MAE1C,OAAOA,WAAW;IACpB;EAAC;IAAAR,GAAA;IAAAC,KAAA,EAEQ,SAAAmE,YAAYjE,MAAuB,EAAEkE,IAAsB;MAElE,OAAO5F,GAAG,CAACE,IAAI,CAAC,YAAK;QACnB,IAAI0F,IAAI,IAAI,IAAI,EAAE;UAChB,OAAO,IAAI;;QAEb,IAAI,CAACnD,KAAK,CAACC,OAAO,CAACkD,IAAI,CAAC,EAAE;UACxB,MAAM,IAAIrF,UAAU,CAAC,2BAA2B,CAAC;;QAEnD,IAAI,CAACkC,KAAK,CAACC,OAAO,CAAChB,MAAM,CAAC,EAAE;UAC1B,MAAM,IAAInB,UAAU,CAAC,6BAA6B,CAAC;;QAErD,IAAIqF,IAAI,CAAC9D,MAAM,KAAKJ,MAAM,CAACI,MAAM,EAAE;UACjC,MAAM,IAAIvB,UAAU,CAChB,sGACqC,OAAAoC,MAAA,CACjCjB,MAAM,CAACI,MAAM,UAAAa,MAAA,CAAOiD,IAAI,CAAC9D,MAAM,MAAG,CAAC;;QAE7C,IAAI8D,IAAI,CAACC,KAAK,CAAC,UAAAC,CAAC;UAAA,OAAIA,CAAC,IAAI,IAAI;QAAA,EAAC,EAAE;UAC9B,OAAO,IAAI;;QAEbF,IAAI,GAAGA,IAAI,CAACnC,GAAG,CAAC,UAAAqC,CAAC;UAAA,OAAIA,CAAC,IAAI,IAAI,GAAGA,CAAC,GAAG9F,GAAG,CAACwE,UAAU,CAACsB,CAAC,EAAE,CAAC,CAAC;QAAA,EAAC;QAC1D,IAAIC,MAAM,GAAGH,IAAI,CAAC,CAAC,CAAC;QACpB,KAAK,IAAI1D,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG0D,IAAI,CAAC9D,MAAM,GAAG,CAAC,EAAE,EAAEI,CAAC,EAAE;UACxC6D,MAAM,GAAG/F,GAAG,CAACgG,UAAU,CAACD,MAAM,EAAEH,IAAI,CAAC1D,CAAC,CAAC,CAAC;;QAE1C,OAAO6D,MAAM;MACf,CAAC,CAAC;IACJ;EAAC;EAAA,OAAAnF,KAAA;AAAA,EAnOiCP,KAAK;AAsOzC,WAAa4F,GAAI,0BAAAC,MAAA;EAAApF,SAAA,CAAAmF,GAAA,EAAAC,MAAA;EAAA,IAAAC,OAAA,GAAAnF,YAAA,CAAAiF,GAAA;EAGf,SAAAA,IAAYhF,IAAgB;IAAAE,eAAA,OAAA8E,GAAA;IAAA,OAAAE,OAAA,CAAA/E,IAAA,OACpBH,IAAI;EACZ;EAACK,YAAA,CAAA2E,GAAA;IAAA1E,GAAA;IAAAC,KAAA,EAEkB,SAAAC,cAAcC,MAAgB;MAC/C,OAAOxB,IAAI,CAAC,YAAK;QACf,IAAI6F,MAAM,GAAGrE,MAAM,CAAC,CAAC,CAAC,CAAC0E,KAAK,EAAE;QAC9B,KAAK,IAAIlE,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGR,MAAM,CAACI,MAAM,EAAE,EAAEI,CAAC,EAAE;UACtC6D,MAAM,GAAG/F,GAAG,CAACqG,GAAG,CAACN,MAAM,EAAErE,MAAM,CAACQ,CAAC,CAAC,CAAC;;QAErC,OAAO6D,MAAM;MACf,CAAC,CAAC;IACJ;EAAC;EAAA,OAAAE,GAAA;AAAA,EAfsBrF,KAAK;AAC5B;AACOqF,GAAA,CAAAK,SAAS,GAAG,KAAK;AAe1BrG,aAAa,CAACsG,aAAa,CAACN,GAAG,CAAC;AAEhC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA8CA,OAAM,SAAUI,GAAGA,CAACG,MAA4C;EAE9D,IAAI/D,KAAK,CAACC,OAAO,CAAC8D,MAAM,CAAC,EAAE;IACzB,IAAMC,KAAK,GAAG,IAAIR,GAAG,CAAC,EAAE,CAAC;IACzB,OAAOQ,KAAK,CAACC,KAAK,CAACF,MAAM,CAA4B;GACtD,MAAM;IACL,OAAO,IAAIP,GAAG,CAACO,MAAM,CAAC;;AAE1B;AAEA,WAAaG,QAAS,0BAAAC,OAAA;EAAA9F,SAAA,CAAA6F,QAAA,EAAAC,OAAA;EAAA,IAAAC,OAAA,GAAA7F,YAAA,CAAA2F,QAAA;EAGpB,SAAAA,SAAY1F,IAAgB;IAAAE,eAAA,OAAAwF,QAAA;IAAA,OAAAE,OAAA,CAAAzF,IAAA,OACpBH,IAAI;EACZ;EAACK,YAAA,CAAAqF,QAAA;IAAApF,GAAA;IAAAC,KAAA,EAEkB,SAAAC,cAAcC,MAAgB;MAC/C,OAAOxB,IAAI,CAAC,YAAK;QACf,IAAI6F,MAAM,GAAGrE,MAAM,CAAC,CAAC,CAAC,CAAC0E,KAAK,EAAE;QAC9B,KAAK,IAAIlE,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGR,MAAM,CAACI,MAAM,EAAE,EAAEI,CAAC,EAAE;UACtC6D,MAAM,GAAG/F,GAAG,CAAC8G,GAAG,CAACf,MAAM,EAAErE,MAAM,CAACQ,CAAC,CAAC,CAAC;;QAErC,OAAO6D,MAAM;MACf,CAAC,CAAC;IACJ;EAAC;EAAA,OAAAY,QAAA;AAAA,EAf2B/F,KAAK;AACjC;AACO+F,QAAA,CAAAL,SAAS,GAAG,UAAU;AAe/BrG,aAAa,CAACsG,aAAa,CAACI,QAAQ,CAAC;AAErC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA8CA,OAAM,SAAUI,QAAQA,CAACP,MAA4C;EAEnE,IAAI/D,KAAK,CAACC,OAAO,CAAC8D,MAAM,CAAC,EAAE;IACzB,IAAMC,KAAK,GAAG,IAAIE,QAAQ,CAAC,EAAE,CAAC;IAC9B,OAAOF,KAAK,CAACC,KAAK,CAACF,MAAM,CAA4B;GACtD,MAAM;IACL,OAAO,IAAIG,QAAQ,CAACH,MAAM,CAAC;;AAE/B;AAEA,WAAaQ,OAAQ,0BAAAC,OAAA;EAAAnG,SAAA,CAAAkG,OAAA,EAAAC,OAAA;EAAA,IAAAC,OAAA,GAAAlG,YAAA,CAAAgG,OAAA;EAGnB,SAAAA,QAAY/F,IAAgB;IAAAE,eAAA,OAAA6F,OAAA;IAAA,OAAAE,OAAA,CAAA9F,IAAA,OACpBH,IAAI;EACZ;EAACK,YAAA,CAAA0F,OAAA;IAAAzF,GAAA;IAAAC,KAAA,EAEkB,SAAAC,cAAcC,MAAgB;MAC/C,OAAOxB,IAAI,CAAC,YAAK;QACf,IAAI6F,MAAM,GAAGrE,MAAM,CAAC,CAAC,CAAC,CAAC0E,KAAK,EAAE;QAC9B,KAAK,IAAIlE,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGR,MAAM,CAACI,MAAM,EAAE,EAAEI,CAAC,EAAE;UACtC6D,MAAM,GAAG/F,GAAG,CAACqG,GAAG,CAACN,MAAM,EAAErE,MAAM,CAACQ,CAAC,CAAC,CAAC;;QAErC,OAAOlC,GAAG,CAAC8G,GAAG,CAAC,CAAC,GAAGpF,MAAM,CAACI,MAAM,EAAEiE,MAAM,CAAC;MAC3C,CAAC,CAAC;IACJ;EAAC;EAAA,OAAAiB,OAAA;AAAA,EAf0BpG,KAAK;AAChC;AACOoG,OAAA,CAAAV,SAAS,GAAG,SAAS;AAe9BrG,aAAa,CAACsG,aAAa,CAACS,OAAO,CAAC;AAEpC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA+CA,OAAM,SAAUG,OAAOA,CAACX,MAA4C;EAElE,IAAI/D,KAAK,CAACC,OAAO,CAAC8D,MAAM,CAAC,EAAE;IACzB,IAAMC,KAAK,GAAG,IAAIO,OAAO,CAAC,EAAE,CAAC;IAC7B,OAAOP,KAAK,CAACC,KAAK,CAACF,MAAM,CAA4B;GACtD,MAAM;IACL,OAAO,IAAIQ,OAAO,CAACR,MAAM,CAAC;;AAE9B;AAEA,WAAaY,OAAQ,0BAAAC,OAAA;EAAAvG,SAAA,CAAAsG,OAAA,EAAAC,OAAA;EAAA,IAAAC,OAAA,GAAAtG,YAAA,CAAAoG,OAAA;EAGnB,SAAAA,QAAYnG,IAAgB;IAAAE,eAAA,OAAAiG,OAAA;IAAA,OAAAE,OAAA,CAAAlG,IAAA,OACpBH,IAAI;EACZ;EAACK,YAAA,CAAA8F,OAAA;IAAA7F,GAAA;IAAAC,KAAA,EAEkB,SAAAC,cAAcC,MAAgB;MAC/C,OAAOxB,IAAI,CAAC,YAAK;QACf,IAAI6F,MAAM,GAAGrE,MAAM,CAAC,CAAC,CAAC;QACtB,KAAK,IAAIQ,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGR,MAAM,CAACI,MAAM,EAAE,EAAEI,CAAC,EAAE;UACtC6D,MAAM,GAAG/F,GAAG,CAACuH,OAAO,CAACxB,MAAM,EAAErE,MAAM,CAACQ,CAAC,CAAC,CAAC;;QAEzC,OAAO6D,MAAM;MACf,CAAC,CAAC;IACJ;EAAC;EAAA,OAAAqB,OAAA;AAAA,EAf0BxG,KAAK;AAChC;AACOwG,OAAA,CAAAd,SAAS,GAAG,SAAS;AAe9BrG,aAAa,CAACsG,aAAa,CAACa,OAAO,CAAC;AAEpC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA8CA,OAAM,SAAUG,OAAOA,CAACf,MAA4C;EAElE,IAAI/D,KAAK,CAACC,OAAO,CAAC8D,MAAM,CAAC,EAAE;IACzB,IAAMC,KAAK,GAAG,IAAIW,OAAO,CAAC,EAAE,CAAC;IAC7B,OAAOX,KAAK,CAACC,KAAK,CAACF,MAAM,CAA4B;GACtD,MAAM;IACL,OAAO,IAAIY,OAAO,CAACZ,MAAM,CAAC;;AAE9B;AAEA,WAAagB,OAAQ,0BAAAC,OAAA;EAAA3G,SAAA,CAAA0G,OAAA,EAAAC,OAAA;EAAA,IAAAC,OAAA,GAAA1G,YAAA,CAAAwG,OAAA;EAGnB,SAAAA,QAAYvG,IAAgB;IAAAE,eAAA,OAAAqG,OAAA;IAAA,OAAAE,OAAA,CAAAtG,IAAA,OACpBH,IAAI;EACZ;EAACK,YAAA,CAAAkG,OAAA;IAAAjG,GAAA;IAAAC,KAAA,EAEkB,SAAAC,cAAcC,MAAgB;MAC/C,OAAOxB,IAAI,CAAC,YAAK;QACf,IAAI6F,MAAM,GAAGrE,MAAM,CAAC,CAAC,CAAC;QACtB,KAAK,IAAIQ,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGR,MAAM,CAACI,MAAM,EAAE,EAAEI,CAAC,EAAE;UACtC6D,MAAM,GAAG/F,GAAG,CAAC2H,OAAO,CAAC5B,MAAM,EAAErE,MAAM,CAACQ,CAAC,CAAC,CAAC;;QAEzC,OAAO6D,MAAM;MACf,CAAC,CAAC;IACJ;EAAC;EAAA,OAAAyB,OAAA;AAAA,EAf0B5G,KAAK;AAChC;AACO4G,OAAA,CAAAlB,SAAS,GAAG,SAAS;AAe9BrG,aAAa,CAACsG,aAAa,CAACiB,OAAO,CAAC;AAEpC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA8CA,OAAM,SAAUG,OAAOA,CAACnB,MAA4C;EAElE,IAAI/D,KAAK,CAACC,OAAO,CAAC8D,MAAM,CAAC,EAAE;IACzB,IAAMC,KAAK,GAAG,IAAIe,OAAO,CAAC,EAAE,CAAC;IAC7B,OAAOf,KAAK,CAACC,KAAK,CAACF,MAAM,CAA4B;GACtD,MAAM;IACL,OAAO,IAAIgB,OAAO,CAAChB,MAAM,CAAC;;AAE9B;AASA,WAAaoB,WAAY,0BAAAC,OAAA;EAAA/G,SAAA,CAAA8G,WAAA,EAAAC,OAAA;EAAA,IAAAC,OAAA,GAAA9G,YAAA,CAAA4G,WAAA;EAMvB,SAAAA,YAAY3G,IAA2B;IAAA,IAAA8G,MAAA;IAAA5G,eAAA,OAAAyG,WAAA;IACrCG,MAAA,GAAAD,OAAA,CAAA1G,IAAA,OAAMH,IAAI;IAJH8G,MAAA,CAAAC,YAAY,GAAG,CAAC,CAAC;IAKxB,IAAI/G,IAAI,IAAI,IAAI,EAAE;MAChBA,IAAI,GAAG,EAAE;;IAEX8G,MAAA,CAAKE,IAAI,GAAGhH,IAAI,CAACgH,IAAI,IAAI,IAAI,GAAGF,MAAA,CAAKC,YAAY,GAAG/G,IAAI,CAACgH,IAAI;IAC7DF,MAAA,CAAK1G,eAAe,GAAG,IAAI;IAC3B0G,MAAA,CAAKpE,eAAe,GAAG,KAAK;IAAC,OAAAoE,MAAA;EAC/B;EAACzG,YAAA,CAAAsG,WAAA;IAAArG,GAAA;IAAAC,KAAA,EAEQ,SAAAe,MAAMC,UAAyB;MACtC;MACA,IAAI,EAAEC,KAAK,CAACC,OAAO,CAACF,UAAU,CAAC,IAAIC,KAAK,CAACC,OAAO,CAACF,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC,IAC5DA,UAAU,CAACV,MAAM,KAAK,CAAC,EAAE;QAC3B,MAAM,IAAIvB,UAAU,CAChB,iEAAiE,GACjE,QAAQ,CAAC;;MAEfiC,UAAU,GAAGA,UAAqB;MAElC,IAAI0F,YAAY,GAAG,IAAI;MAAC,IAAAC,UAAA,GAAArF,0BAAA,CACJN,UAAU;QAAA4F,MAAA;MAAA;QAA9B,KAAAD,UAAA,CAAAnF,CAAA,MAAAoF,MAAA,GAAAD,UAAA,CAAAlF,CAAA,IAAAC,IAAA,GAAgC;UAAA,IAArBC,OAAK,GAAAiF,MAAA,CAAA5G,KAAA;UACd,IAAI2B,OAAK,IAAI,IAAI,EAAE;YACjB+E,YAAY,GAAG,KAAK;YACpB;;;MAEH,SAAA9E,GAAA;QAAA+E,UAAA,CAAA9E,CAAA,CAAAD,GAAA;MAAA;QAAA+E,UAAA,CAAA7E,CAAA;MAAA;MACD,IAAI4E,YAAY,EAAE;QAChB;;MAGF,IAAMG,QAAQ,GAAY,EAAE;MAC5B,KAAK,IAAInG,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGM,UAAU,CAACV,MAAM,EAAE,EAAEI,CAAC,EAAE;QAC1C,IAAMoG,sBAAsB,GAAG9F,UAAU,CAACN,CAAC,CAAC,CAACF,KAAK,EAAE;QACpDsG,sBAAsB,CAACC,MAAM,CAAC,IAAI,CAACN,IAAI,EAAE,CAAC,CAAC;QAC3C,IAAIO,MAAM,GAAG,KAAK;QAAC,IAAAC,UAAA,GAAA3F,0BAAA,CACCuF,QAAQ;UAAAK,MAAA;QAAA;UAA5B,KAAAD,UAAA,CAAAzF,CAAA,MAAA0F,MAAA,GAAAD,UAAA,CAAAxF,CAAA,IAAAC,IAAA,GAA8B;YAAA,IAAnBC,KAAK,GAAAuF,MAAA,CAAAlH,KAAA;YACd,IAAIrB,IAAI,CAACwI,WAAW,CAACxF,KAAK,EAAEmF,sBAAsB,CAAC,EAAE;cACnDE,MAAM,GAAG,IAAI;cACb;;;QAEH,SAAApF,GAAA;UAAAqF,UAAA,CAAApF,CAAA,CAAAD,GAAA;QAAA;UAAAqF,UAAA,CAAAnF,CAAA;QAAA;QACD,IAAI,CAACkF,MAAM,EAAE;UACXH,QAAQ,CAACjG,IAAI,CAACkG,sBAAsB,CAAC;;;MAGzC,IAAID,QAAQ,CAACvG,MAAM,GAAG,CAAC,EAAE;QACvB,MAAM,IAAIvB,UAAU,CAChB,6DAA6D,GAC7D,gDAAgD,GAChD8B,IAAI,CAACC,SAAS,CAACE,UAAU,CAAC,CAAC;;IAEnC;EAAC;IAAAjB,GAAA;IAAAC,KAAA,EAEkB,SAAAC,cAAcC,MAAgB;MAAA,IAAAkH,MAAA;MAC/C,OAAO1I,IAAI,CAAC,YAAK;QACf,OAAOE,CAAC,CAACyI,WAAW,CAACnH,MAAM,EAAEkH,MAAI,CAACX,IAAI,CAAC;MACzC,CAAC,CAAC;IACJ;EAAC;IAAA1G,GAAA;IAAAC,KAAA,EAEQ,SAAAgE,mBAAmBhD,UAAyB;MACnD,IAAI,EAAEC,KAAK,CAACC,OAAO,CAACF,UAAU,CAAC,IAAIC,KAAK,CAACC,OAAO,CAACF,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE;QAChE,MAAM,IAAIjC,UAAU,CAChB,6DAA6D,CAAC;;MAEpE,IAAMuI,WAAW,GAAGtG,UAAqB;MACzC,IAAMT,WAAW,GAAG+G,WAAW,CAAC,CAAC,CAAC,CAAC9G,KAAK,EAAE;MAC1C,IAAMiG,IAAI,GAAG,IAAI,CAACA,IAAI,GAAG,CAAC,GAAGlG,WAAW,CAACD,MAAM,GAAG,IAAI,CAACmG,IAAI,GAAG,IAAI,CAACA,IAAI;MACvE;MACA;MAAA,IAAAc,UAAA,GAAAjG,0BAAA,CACoBgG,WAAW,CAAC9G,KAAK,CAAC,CAAC,CAAC;QAAAgH,MAAA;MAAA;QAAxC,KAAAD,UAAA,CAAA/F,CAAA,MAAAgG,MAAA,GAAAD,UAAA,CAAA9F,CAAA,IAAAC,IAAA,GAA0C;UAAA,IAA/BC,KAAK,GAAA6F,MAAA,CAAAxH,KAAA;UACd,IAAIO,WAAW,CAACkG,IAAI,CAAC,IAAI,IAAI,IAAI9E,KAAK,CAAC8E,IAAI,CAAC,IAAI,IAAI,EAAE;YACpDlG,WAAW,CAACkG,IAAI,CAAC,GAAG,IAAI;YACxB;;UAEFlG,WAAW,CAACkG,IAAI,CAAC,IAAI9E,KAAK,CAAC8E,IAAI,CAAC;;MACjC,SAAA7E,GAAA;QAAA2F,UAAA,CAAA1F,CAAA,CAAAD,GAAA;MAAA;QAAA2F,UAAA,CAAAzF,CAAA;MAAA;MACD,OAAOvB,WAAW;IACpB;EAAC;IAAAR,GAAA;IAAAC,KAAA,EAEQ,SAAAmE,YAAYjE,MAAuB,EAAEkE,IAAsB;MAAA,IAAAqD,MAAA;MAElE,IAAIrD,IAAI,IAAI,IAAI,EAAE;QAChB,OAAO,IAAI;;MAEb,IAAI,CAACnD,KAAK,CAACC,OAAO,CAACkD,IAAI,CAAC,EAAE;QACxB,MAAM,IAAIrF,UAAU,CAAC,2CAA2C,CAAC;;MAEnE,IAAI,CAACkC,KAAK,CAACC,OAAO,CAAChB,MAAM,CAAC,EAAE;QAC1B,MAAM,IAAInB,UAAU,CAAC,6CAA6C,CAAC;;MAErE,IAAIqF,IAAI,CAAC9D,MAAM,KAAKJ,MAAM,CAACI,MAAM,EAAE;QACjC,MAAM,IAAIvB,UAAU,CAChB,mCAAAoC,MAAA,CAAmCiD,IAAI,CAAC9D,MAAM,uCAAAa,MAAA,CACjBjB,MAAM,CAACI,MAAM,MAAG,CAAC;;MAEpD,OAAO9B,GAAG,CAACE,IAAI,CAAC,YAAK;QACnB,IAAIgJ,YAAY,GAAG,IAAI;QACvBtD,IAAI,CAACuD,OAAO,CAAC,UAAArD,CAAC,EAAG;UACf,IAAIA,CAAC,IAAI,IAAI,EAAE;YACboD,YAAY,GAAG,KAAK;YACpB;;QAEJ,CAAC,CAAC;QACF,IAAIA,YAAY,EAAE;UAChB,OAAO,IAAI;;QAEb,IAAME,WAAW,GAAa,EAAE;QAChC,KAAK,IAAIlH,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGR,MAAM,CAACI,MAAM,EAAE,EAAEI,CAAC,EAAE;UACtC,IAAI0D,IAAI,CAAC1D,CAAC,CAAC,IAAI,IAAI,EAAE;YACnB;YACAkH,WAAW,CAAChH,IAAI,CAACpC,GAAG,CAACqJ,IAAI,CAACrJ,GAAG,CAACsJ,QAAQ,CAAC5H,MAAM,CAACQ,CAAC,CAAC,CAAC,EAAE,MAAM,CAAC,CAAC;WAC5D,MAAM,IAAI0D,IAAI,CAAC1D,CAAC,CAAC,CAAC+B,IAAI,GAAGvC,MAAM,CAACQ,CAAC,CAAC,CAAC+B,IAAI,EAAE;YACxC;YACAmF,WAAW,CAAChH,IAAI,CAACpC,GAAG,CAACwE,UAAU,CAACoB,IAAI,CAAC1D,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;WAC9C,MAAM;YACLkH,WAAW,CAAChH,IAAI,CAACwD,IAAI,CAAC1D,CAAC,CAAC,CAAC;;;QAG7B,IAAMqH,iBAAiB,GAAGvJ,GAAG,CAAC2C,MAAM,CAACyG,WAAW,EAAEH,MAAI,CAAChB,IAAI,CAAC;QAC5D,OAAOjI,GAAG,CAACwJ,GAAG,CAACD,iBAAiB,EAAE,CAAC,CAAC,EAAE,KAAK,CAAC;MAC9C,CAAC,CAAC;IACJ;EAAC;IAAAhI,GAAA;IAAAC,KAAA,EAEQ,SAAAiI,UAAA,EAAS;MAChB,IAAMjD,MAAM,GAA6B;QACvC,MAAM,EAAE,IAAI,CAACyB;OACd;MACD,IAAMyB,UAAU,GAAAC,IAAA,CAAAC,eAAA,CAAAhC,WAAA,CAAAiC,SAAA,sBAAAzI,IAAA,MAAoB;MACpC0I,MAAM,CAACC,MAAM,CAACvD,MAAM,EAAEkD,UAAU,CAAC;MACjC,OAAOlD,MAAM;IACf;EAAC;EAAA,OAAAoB,WAAA;AAAA,EAzI8BhH,KAAK;AACpC;AACOgH,WAAA,CAAAtB,SAAS,GAAG,aAAa;AAyIlCrG,aAAa,CAACsG,aAAa,CAACqB,WAAW,CAAC;AAExC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAgDA,OAAM,SAAUiB,WAAWA,CAACrC,MACoB;EAC9C,IAAI/D,KAAK,CAACC,OAAO,CAAC8D,MAAM,CAAC,EAAE;IACzB,IAAMC,KAAK,GAAG,IAAImB,WAAW,CAAC,EAAE,CAAC;IACjC,OAAOnB,KAAK,CAACC,KAAK,CAACF,MAAM,CAA4B;GACtD,MAAM;IACL,OAAO,IAAIoB,WAAW,CAACpB,MAAM,CAAC;;AAElC;AAoBA;;;;;;;;;AASA,SAASwD,aAAaA,CAAC/B,IAAY,EAAEgC,GAAW;EAC9C,OAAOhC,IAAI,GAAG,CAAC,EAAE;IACfA,IAAI,IAAIgC,GAAG;;EAEb,OAAOhC,IAAI;AACb;AAEA,SAASiC,QAAQA,CAAC5F,CAAS,EAAEe,CAAS,EAAE8E,IAA6B;EACnE,IAAI7F,CAAC,CAACnB,KAAK,CAACrB,MAAM,GAAG,CAAC,IAAIuD,CAAC,CAAClC,KAAK,CAACrB,MAAM,GAAG,CAAC,EAAE;IAC5C,MAAM,IAAIxB,mBAAmB,CACzB,kEAAkE,CAAC;;EAEzEN,GAAG,CAACG,IAAI,CAACiK,MAAM,CACX9F,CAAC,CAACnB,KAAK,CAACrB,MAAM,IAAI,CAAC,EACnB;IAAA,OAAM,4DAAAa,MAAA,CACS2B,CAAC,CAACnB,KAAK,CAACrB,MAAM,CAAE;EAAA,EAAC;EACpC9B,GAAG,CAACG,IAAI,CAACiK,MAAM,CACX9F,CAAC,CAACnB,KAAK,CAACrB,MAAM,IAAI,CAAC,EACnB;IAAA,OAAM,4DAAAa,MAAA,CACS0C,CAAC,CAAClC,KAAK,CAACrB,MAAM,CAAE;EAAA,EAAC;EAEpC,IAAI,OAAOqI,IAAI,KAAK,QAAQ,EAAE;IAC5BA,IAAI,GAAG,CAACA,IAAI,EAAEA,IAAI,CAAC;;EAGrB,IAAI7F,CAAC,CAAC+F,KAAK,KAAK,WAAW,IAAIhF,CAAC,CAACgF,KAAK,KAAK,WAAW,EAAE;IACtD,MAAM,IAAI/J,mBAAmB,CACzB,6DAA6D,CAAC;;EAGpE,IAAMiE,KAAK,GAAGD,CAAC,CAACnB,KAAK,CAACrB,MAAM;EAC5B,IAAMwD,KAAK,GAAGD,CAAC,CAAClC,KAAK,CAACrB,MAAM;EAC5B,IAAIqI,IAAI,IAAI,IAAI,EAAE;IAChB;IACAA,IAAI,GAAG,CAAC5F,KAAK,GAAG,CAAC,EAAEe,KAAK,GAAG,CAAC,CAAC;;EAE/B,IAAMgF,SAAS,GAAGH,IAAwB;EAE1C,OAAOnK,GAAG,CAACE,IAAI,CAAC,YAAK;IACnB,IAAIqK,IAAY;IAChB,IAAIhG,KAAK,GAAGe,KAAK,EAAE;MACjBiF,IAAI,GAAGhG,KAAK,GAAGe,KAAK;MACpB,IAAMkF,SAAS,GAAU,EAAE;MAC3B,KAAK,IAAItI,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGqI,IAAI,EAAE,EAAErI,CAAC,EAAE;QAC7BsI,SAAS,CAACpI,IAAI,CAAC,CAAC,CAAC;;MAEnBiD,CAAC,GAAGrF,GAAG,CAACgF,OAAO,CAACK,CAAC,EAAEA,CAAC,CAAClC,KAAK,CAACR,MAAM,CAAC6H,SAAS,CAAC,CAAC;KAC9C,MAAM,IAAIlF,KAAK,GAAGf,KAAK,EAAE;MACxBgG,IAAI,GAAGjF,KAAK,GAAGf,KAAK;MACpB,IAAMiG,UAAS,GAAU,EAAE;MAC3B,KAAK,IAAItI,EAAC,GAAG,CAAC,EAAEA,EAAC,GAAGqI,IAAI,EAAE,EAAErI,EAAC,EAAE;QAC7BsI,UAAS,CAACpI,IAAI,CAAC,CAAC,CAAC;;MAEnBkC,CAAC,GAAGtE,GAAG,CAACgF,OAAO,CAACV,CAAC,EAAEA,CAAC,CAACnB,KAAK,CAACR,MAAM,CAAC6H,UAAS,CAAC,CAAC;KAC9C,MAAM;MACLD,IAAI,GAAG,CAAC;;IAGV,IAAIE,GAAW;IACf,IAAInG,CAAC,CAACnB,KAAK,CAACrB,MAAM,KAAK,CAAC,IAAIuD,CAAC,CAAClC,KAAK,CAACrB,MAAM,KAAK,CAAC,EAAE;MAChD,IAAIwI,SAAS,CAAC,CAAC,CAAC,KAAKA,SAAS,CAAC,CAAC,CAAC,EAAE;QACjCG,GAAG,GAAGzK,GAAG,CAAC0K,GAAG,CAAC1K,GAAG,CAAC8G,GAAG,CAACxC,CAAC,EAAEe,CAAC,CAAC,EAAEiF,SAAS,CAAC,CAAC,CAAC,CAAC;OAC3C,MAAM;QACLG,GAAG,GAAGzK,GAAG,CAAC0K,GAAG,CAAC1K,GAAG,CAAC8G,GAAG,CAAC9G,GAAG,CAACkF,SAAS,CAACZ,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAEe,CAAC,CAAC,EAAEiF,SAAS,CAAC,CAAC,CAAC,CAAC;;KAEpE,MAAM;MACL,IAAMK,IAAI,GAAGL,SAAS,CAAC,CAAC,CAAC,KAAKhG,CAAC,CAACnB,KAAK,CAACrB,MAAM,GAAG,CAAC;MAChD,IAAM8I,IAAI,GAAGN,SAAS,CAAC,CAAC,CAAC,KAAKjF,CAAC,CAAClC,KAAK,CAACrB,MAAM,GAAG,CAAC;MAChD2I,GAAG,GAAGzK,GAAG,CAAC6K,MAAM,CAACvG,CAAC,EAAEe,CAAC,EAAEsF,IAAI,EAAEC,IAAI,CAAC;;IAGpC,IAAIL,IAAI,GAAG,CAAC,EAAE;MACZ,IAAIO,GAAW;MACf,IAAIvG,KAAK,GAAGe,KAAK,EAAE;QACjBwF,GAAG,GAAGvG,KAAK,GAAGe,KAAK,GAAG,CAAC;OACxB,MAAM;QACLwF,GAAG,GAAGvG,KAAK,GAAG,CAAC;;MAEjB,IAAMwG,WAAW,GAAa,EAAE;MAChC,KAAK,IAAI7I,GAAC,GAAG4I,GAAG,EAAE5I,GAAC,GAAG4I,GAAG,GAAGP,IAAI,EAAE,EAAErI,GAAC,EAAE;QACrC6I,WAAW,CAAC3I,IAAI,CAACF,GAAC,CAAC;;MAErBuI,GAAG,GAAGzK,GAAG,CAACgL,OAAO,CAACP,GAAG,EAAEM,WAAW,CAAC;;IAErC,IAAIN,GAAG,CAACtH,KAAK,CAACrB,MAAM,KAAK,CAAC,EAAE;MAC1B2I,GAAG,GAAGzK,GAAG,CAACwE,UAAU,CAACiG,GAAG,EAAE,CAAC,CAAC;;IAE9B,OAAOA,GAAG;EACZ,CAAC,CAAC;AACJ;AAEA,WAAaQ,GAAI,0BAAAC,OAAA;EAAApK,SAAA,CAAAmK,GAAA,EAAAC,OAAA;EAAA,IAAAC,OAAA,GAAAnK,YAAA,CAAAiK,GAAA;EAOf,SAAAA,IAAYhK,IAAkB;IAAA,IAAAmK,MAAA;IAAAjK,eAAA,OAAA8J,GAAA;IAC5BG,MAAA,GAAAD,OAAA,CAAA/J,IAAA,OAAMH,IAAI;IACVmK,MAAA,CAAKjB,IAAI,GAAGlJ,IAAI,CAACkJ,IAAI;IACrBiB,MAAA,CAAKC,SAAS,GAAGpK,IAAI,CAACoK,SAAS,IAAI,IAAI,GAAG,KAAK,GAAGpK,IAAI,CAACoK,SAAS;IAChED,MAAA,CAAK/J,eAAe,GAAG,IAAI;IAC3B+J,MAAA,CAAKzH,eAAe,GAAG,KAAK;IAAC,OAAAyH,MAAA;EAC/B;EAAC9J,YAAA,CAAA2J,GAAA;IAAA1J,GAAA;IAAAC,KAAA,EAEQ,SAAAe,MAAMC,UAAyB;MACtCxC,GAAG,CAACG,IAAI,CAACiK,MAAM,CACX3H,KAAK,CAACC,OAAO,CAACF,UAAU,CAAC,IAAIA,UAAU,CAACV,MAAM,KAAK,CAAC,IAChDW,KAAK,CAACC,OAAO,CAACF,UAAU,CAAC,CAAC,CAAC,CAAC,IAAIC,KAAK,CAACC,OAAO,CAACF,UAAU,CAAC,CAAC,CAAC,CAAC,EAChE;QAAA,OAAM,+DAA+D;MAAA,EAAC;MAC1E,IAAMZ,MAAM,GAAGY,UAAU,CAAC,CAAC,CAAU;MACrC,IAAMX,MAAM,GAAGW,UAAU,CAAC,CAAC,CAAU;MACrC,IAAIZ,MAAM,CAACE,MAAM,GAAG,CAAC,IAAID,MAAM,CAACC,MAAM,GAAG,CAAC,EAAE;QAC1C,MAAM,IAAIxB,mBAAmB,CACzB,8DAA8D,CAAC;;MAGrE,IAAM6J,IAAI,GAAG,IAAI,CAACmB,aAAa,CAAC1J,MAAM,EAAEC,MAAM,CAAC;MAC/C,IAAID,MAAM,CAACuI,IAAI,CAAC,CAAC,CAAC,CAAC,KAAKtI,MAAM,CAACsI,IAAI,CAAC,CAAC,CAAC,CAAC,EAAE;QACvC,MAAM,IAAI5J,UAAU,CAChB,mCAAAoC,MAAA,CACGf,MAAM,CAACuI,IAAI,CAAC,CAAC,CAAC,CAAC,WAAAxH,MAAA,CAAQd,MAAM,CAACsI,IAAI,CAAC,CAAC,CAAC,CAAC,CAAE,CAAC;;IAEpD;EAAC;IAAA5I,GAAA;IAAAC,KAAA,EAEkB,SAAAC,cAAcC,MAAgB;MAC/C,IAAIA,MAAM,CAACI,MAAM,KAAK,CAAC,EAAE;QACvB,MAAM,IAAIvB,UAAU,CAChB,oDAAoD,mBAAAoC,MAAA,CACpCjB,MAAM,CAACI,MAAM,eAAY,CAAC;;MAGhD,IAAIyJ,EAAE,GAAG7J,MAAM,CAAC,CAAC,CAAC;MAClB,IAAI8J,EAAE,GAAG9J,MAAM,CAAC,CAAC,CAAC;MAClB,IAAIyI,IAAsB;MAC1B,IAAI,CAAC1H,KAAK,CAACC,OAAO,CAAC,IAAI,CAACyH,IAAI,CAAC,EAAE;QAC7BA,IAAI,GAAG,CACLH,aAAa,CAAC,IAAI,CAACG,IAAI,EAAEoB,EAAE,CAACpI,KAAK,CAACrB,MAAM,CAAC,EACzCkI,aAAa,CAAC,IAAI,CAACG,IAAI,EAAEqB,EAAE,CAACrI,KAAK,CAACrB,MAAM,CAAC,CAC1C;OACF,MAAM;QACLqI,IAAI,GAAG,IAAI,CAACA,IAAI,CAAC1G,GAAG,CACT,UAACwE,IAAI,EAAE/F,CAAC;UAAA,OAAK8H,aAAa,CACtB/B,IAAI,EAAEvG,MAAM,CAACQ,CAAC,CAAC,CAACiB,KAAK,CAACrB,MAAM,CAAC;QAAA,EAAqB;;MAEnE,IAAI,IAAI,CAACuJ,SAAS,EAAE;QAClBE,EAAE,GAAG/K,WAAW,CAAC+K,EAAE,EAAEpB,IAAI,CAAC,CAAC,CAAC,CAAC;QAC7BqB,EAAE,GAAGhL,WAAW,CAACgL,EAAE,EAAErB,IAAI,CAAC,CAAC,CAAC,CAAC;;MAE/B,OAAOD,QAAQ,CAACqB,EAAE,EAAEC,EAAE,EAAErB,IAAI,CAAC;IAC/B;EAAC;IAAA5I,GAAA;IAAAC,KAAA,EAEO,SAAA8J,cAAc1J,MAAa,EAAEC,MAAa;MAChD,IAAIsI,IAAc;MAClB,IAAI,CAAC1H,KAAK,CAACC,OAAO,CAAC,IAAI,CAACyH,IAAI,CAAC,EAAE;QAC7B;QACAA,IAAI,GAAG,CACLH,aAAa,CAAC,IAAI,CAACG,IAAI,EAAEvI,MAAM,CAACE,MAAM,CAAC,EACvCkI,aAAa,CAAC,IAAI,CAACG,IAAI,EAAEtI,MAAM,CAACC,MAAM,CAAC,CACxC;OACF,MAAM;QACL;QACAqI,IAAI,GAAG,IAAI,CAACA,IAAI;;MAElB,OAAOA,IAAI;IACb;EAAC;IAAA5I,GAAA;IAAAC,KAAA,EAEQ,SAAAgE,mBAAmBhD,UAAyB;MACnDxC,GAAG,CAACG,IAAI,CAACiK,MAAM,CACX3H,KAAK,CAACC,OAAO,CAACF,UAAU,CAAC,IAAIA,UAAU,CAACV,MAAM,KAAK,CAAC,IAChDW,KAAK,CAACC,OAAO,CAACF,UAAU,CAAC,CAAC,CAAC,CAAC,IAAIC,KAAK,CAACC,OAAO,CAACF,UAAU,CAAC,CAAC,CAAC,CAAC,EAChE;QAAA,OAAM,+DAA+D;MAAA,EAAC;MAC1E,IAAMZ,MAAM,GAAIY,UAAU,CAAC,CAAC,CAAW,CAACR,KAAK,EAAE;MAC/C,IAAMH,MAAM,GAAIW,UAAU,CAAC,CAAC,CAAW,CAACR,KAAK,EAAE;MAC/C,IAAIJ,MAAM,CAACE,MAAM,GAAG,CAAC,IAAID,MAAM,CAACC,MAAM,GAAG,CAAC,EAAE;QAC1C,MAAM,IAAIxB,mBAAmB,CACzB,8DAA8D,CAAC;;MAGrE,IAAM6J,IAAI,GAAG,IAAI,CAACmB,aAAa,CAAC1J,MAAM,EAAEC,MAAM,CAAC;MAC/CD,MAAM,CAAC2G,MAAM,CAAC4B,IAAI,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC;MACzBtI,MAAM,CAAC0G,MAAM,CAAC4B,IAAI,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC;MACzBtI,MAAM,CAAC0G,MAAM,CAAC,CAAC,EAAE,CAAC,CAAC;MACnB,IAAMxG,WAAW,GAAGH,MAAM,CAACe,MAAM,CAACd,MAAM,CAAC;MACzC,IAAIE,WAAW,CAACD,MAAM,KAAK,CAAC,EAAE;QAC5BC,WAAW,CAACK,IAAI,CAAC,CAAC,CAAC;;MAErB,OAAOL,WAAW;IACpB;EAAC;IAAAR,GAAA;IAAAC,KAAA,EAEQ,SAAAmE,YAAYjE,MAAuB,EAAEkE,IAAsB;MAElE,OAAO,IAAI;IACb;EAAC;IAAArE,GAAA;IAAAC,KAAA,EAEQ,SAAAiI,UAAA,EAAS;MAChB,IAAMjD,MAAM,GAA6B;QACvC,MAAM,EAAE,IAAI,CAAC2D,IAAI;QACjB,WAAW,EAAE,IAAI,CAACkB;OACnB;MACD,IAAM3B,UAAU,GAAAC,IAAA,CAAAC,eAAA,CAAAqB,GAAA,CAAApB,SAAA,sBAAAzI,IAAA,MAAoB;MACpC0I,MAAM,CAACC,MAAM,CAACvD,MAAM,EAAEkD,UAAU,CAAC;MACjC,OAAOlD,MAAM;IACf;EAAC;EAAA,OAAAyE,GAAA;AAAA,EAjHsBrK,KAAK;AAC5B;AACOqK,GAAA,CAAA3E,SAAS,GAAG,KAAK;AAiH1BrG,aAAa,CAACsG,aAAa,CAAC0E,GAAG,CAAC;AAEhC"},"metadata":{},"sourceType":"module","externalDependencies":[]}