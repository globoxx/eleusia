{"ast":null,"code":"/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { broadcast_util, upcastType, util } from '@tensorflow/tfjs-core';\nimport { mapActivationToShaderProgram } from '../kernel_utils/kernel_funcs_utils';\nimport { MatMulPackedProgram } from '../mulmat_packed_gpu';\nimport { multiply } from './Multiply';\nimport { reshape } from './Reshape';\nimport { sum } from './Sum';\nimport { transpose } from './Transpose';\n// Empirically determined minimal shared dimension in matmul before we forward\n// to a.mul(b).sum() in order to take advantage of GPU parallelism. See\n// https://github.com/tensorflow/tfjs-core/pull/1379 for benchmarks.\nexport var MATMUL_SHARED_DIM_THRESHOLD = 1000;\nexport function batchMatMulImpl(_ref) {\n  var a = _ref.a,\n    b = _ref.b,\n    transposeA = _ref.transposeA,\n    transposeB = _ref.transposeB,\n    backend = _ref.backend,\n    _ref$bias = _ref.bias,\n    bias = _ref$bias === void 0 ? null : _ref$bias,\n    _ref$preluActivationW = _ref.preluActivationWeights,\n    preluActivationWeights = _ref$preluActivationW === void 0 ? null : _ref$preluActivationW,\n    _ref$leakyreluAlpha = _ref.leakyreluAlpha,\n    leakyreluAlpha = _ref$leakyreluAlpha === void 0 ? 0 : _ref$leakyreluAlpha,\n    _ref$activation = _ref.activation,\n    activation = _ref$activation === void 0 ? null : _ref$activation;\n  var aRank = a.shape.length;\n  var bRank = b.shape.length;\n  var innerShapeA = transposeA ? a.shape[aRank - 2] : a.shape[aRank - 1];\n  var innerShapeB = transposeB ? b.shape[bRank - 1] : b.shape[bRank - 2];\n  var outerShapeA = transposeA ? a.shape[aRank - 1] : a.shape[aRank - 2];\n  var outerShapeB = transposeB ? b.shape[bRank - 2] : b.shape[bRank - 1];\n  var outerDimsA = a.shape.slice(0, -2);\n  var outerDimsB = b.shape.slice(0, -2);\n  var batchDimA = util.sizeFromShape(outerDimsA);\n  var batchDimB = util.sizeFromShape(outerDimsB);\n  var outShapeOuterDims = broadcast_util.assertAndGetBroadcastShape(a.shape.slice(0, -2), b.shape.slice(0, -2));\n  var outShape = outShapeOuterDims.concat([outerShapeA, outerShapeB]);\n  util.assert(innerShapeA === innerShapeB, function () {\n    return \"Error in matMul: inner shapes (\".concat(innerShapeA, \") and (\") + \"\".concat(innerShapeB, \") of Tensors with shapes \").concat(a.shape, \" and \") + \"\".concat(b.shape, \" and transposeA=\").concat(transposeA) + \" and transposeB=\".concat(transposeB, \" must match.\");\n  });\n  var a3dShape = transposeA ? [batchDimA, innerShapeA, outerShapeA] : [batchDimA, outerShapeA, innerShapeA];\n  var b3dShape = transposeB ? [batchDimB, outerShapeB, innerShapeB] : [batchDimB, innerShapeB, outerShapeB];\n  // The rest of the implementation is designed to operate on rank-3 tensors\n  var a3d = reshape({\n    inputs: {\n      x: a\n    },\n    backend: backend,\n    attrs: {\n      shape: a3dShape\n    }\n  });\n  var b3d = reshape({\n    inputs: {\n      x: b\n    },\n    backend: backend,\n    attrs: {\n      shape: b3dShape\n    }\n  });\n  var intermediates = [a3d, b3d];\n  var batchDim = Math.max(batchDimA, batchDimB);\n  var sharedDim = transposeA ? a3d.shape[1] : a3d.shape[2];\n  var hasBias = bias != null;\n  var hasPreluActivationWeights = preluActivationWeights != null;\n  var hasLeakyreluAlpha = activation === 'leakyrelu';\n  var fusedActivation = activation != null ? mapActivationToShaderProgram(activation, true) : null;\n  var containsFusedOps = hasBias || hasPreluActivationWeights || hasLeakyreluAlpha || fusedActivation != null;\n  var out;\n  // Since the matrices are vectors, it is faster to call mul().sum()\n  // because sum() is O(sqrt(N)) due to divide-and-conquer.\n  if ((outerShapeA === 1 || outerShapeB === 1) && sharedDim > MATMUL_SHARED_DIM_THRESHOLD && containsFusedOps === false) {\n    var aVec = a3d;\n    var bVec = b3d;\n    if (transposeA) {\n      aVec = transpose({\n        inputs: {\n          x: a3d\n        },\n        backend: backend,\n        attrs: {\n          perm: [0, 2, 1]\n        }\n      });\n      intermediates.push(aVec);\n    }\n    if (transposeB) {\n      bVec = transpose({\n        inputs: {\n          x: b3d\n        },\n        backend: backend,\n        attrs: {\n          perm: [0, 2, 1]\n        }\n      });\n      intermediates.push(bVec);\n    }\n    var shouldReshapeA = outerShapeB !== 1;\n    var shouldReshapeB = outerShapeB === 1;\n    var aVec3d = aVec;\n    if (shouldReshapeA) {\n      aVec3d = reshape({\n        inputs: {\n          x: aVec\n        },\n        backend: backend,\n        attrs: {\n          shape: [batchDim, sharedDim, 1]\n        }\n      });\n      intermediates.push(aVec3d);\n    }\n    var axis = outerShapeB === 1 ? 2 : 1;\n    var bVec3d = bVec;\n    if (shouldReshapeB) {\n      bVec3d = reshape({\n        inputs: {\n          x: bVec\n        },\n        backend: backend,\n        attrs: {\n          shape: [batchDim, 1, sharedDim]\n        }\n      });\n      intermediates.push(bVec3d);\n    }\n    var product = multiply({\n      inputs: {\n        a: aVec3d,\n        b: bVec3d\n      },\n      backend: backend\n    });\n    out = sum({\n      inputs: {\n        x: product\n      },\n      backend: backend,\n      attrs: {\n        axis: axis,\n        keepDims: true\n      }\n    });\n    intermediates.push(product);\n  } else {\n    var dtype = upcastType(a.dtype, b.dtype);\n    var program = new MatMulPackedProgram(a3dShape, b3dShape, [batchDim, outerShapeA, outerShapeB], transposeA, transposeB, hasBias, fusedActivation, hasPreluActivationWeights, hasLeakyreluAlpha);\n    var inputs = [a3d, b3d];\n    if (bias != null) {\n      inputs.push(bias);\n    }\n    if (hasPreluActivationWeights) {\n      inputs.push(preluActivationWeights);\n    }\n    if (hasLeakyreluAlpha) {\n      var $leakyreluAlpha = backend.makeTensorInfo([], 'float32', util.createScalarValue(leakyreluAlpha, 'float32'));\n      inputs.push($leakyreluAlpha);\n      intermediates.push($leakyreluAlpha);\n    }\n    out = backend.runWebGLProgram(program, inputs, dtype);\n  }\n  var outReshaped = reshape({\n    inputs: {\n      x: out\n    },\n    backend: backend,\n    attrs: {\n      shape: outShape\n    }\n  });\n  intermediates.push(out);\n  for (var _i = 0, _intermediates = intermediates; _i < _intermediates.length; _i++) {\n    var i = _intermediates[_i];\n    backend.disposeIntermediateTensorInfo(i);\n  }\n  return outReshaped;\n}","map":{"version":3,"names":["broadcast_util","upcastType","util","mapActivationToShaderProgram","MatMulPackedProgram","multiply","reshape","sum","transpose","MATMUL_SHARED_DIM_THRESHOLD","batchMatMulImpl","_ref","a","b","transposeA","transposeB","backend","_ref$bias","bias","_ref$preluActivationW","preluActivationWeights","_ref$leakyreluAlpha","leakyreluAlpha","_ref$activation","activation","aRank","shape","length","bRank","innerShapeA","innerShapeB","outerShapeA","outerShapeB","outerDimsA","slice","outerDimsB","batchDimA","sizeFromShape","batchDimB","outShapeOuterDims","assertAndGetBroadcastShape","outShape","concat","assert","a3dShape","b3dShape","a3d","inputs","x","attrs","b3d","intermediates","batchDim","Math","max","sharedDim","hasBias","hasPreluActivationWeights","hasLeakyreluAlpha","fusedActivation","containsFusedOps","out","aVec","bVec","perm","push","shouldReshapeA","shouldReshapeB","aVec3d","axis","bVec3d","product","keepDims","dtype","program","$leakyreluAlpha","makeTensorInfo","createScalarValue","runWebGLProgram","outReshaped","_i","_intermediates","i","disposeIntermediateTensorInfo"],"sources":["C:\\Users\\vince\\OneDrive\\Documents\\GitHub\\tfjs-backend-webgl\\src\\kernels\\BatchMatMul_impl.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, broadcast_util, TensorInfo, upcastType, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendWebGL} from '../backend_webgl';\nimport {mapActivationToShaderProgram} from '../kernel_utils/kernel_funcs_utils';\nimport {MatMulPackedProgram} from '../mulmat_packed_gpu';\n\nimport {multiply} from './Multiply';\nimport {reshape} from './Reshape';\nimport {sum} from './Sum';\nimport {transpose} from './Transpose';\n\n// Empirically determined minimal shared dimension in matmul before we forward\n// to a.mul(b).sum() in order to take advantage of GPU parallelism. See\n// https://github.com/tensorflow/tfjs-core/pull/1379 for benchmarks.\nexport const MATMUL_SHARED_DIM_THRESHOLD = 1000;\n\ntype BatchMatMulConfig = {\n  a: TensorInfo,\n  b: TensorInfo,\n  transposeA: boolean,\n  transposeB: boolean,\n  backend: MathBackendWebGL,\n  bias?: TensorInfo,\n  preluActivationWeights?: TensorInfo,\n  leakyreluAlpha?: number,\n  activation?: backend_util.Activation\n};\n\nexport function batchMatMulImpl({\n  a,\n  b,\n  transposeA,\n  transposeB,\n  backend,\n  bias = null,\n  preluActivationWeights = null,\n  leakyreluAlpha = 0,\n  activation = null\n}: BatchMatMulConfig): TensorInfo {\n  const aRank = a.shape.length;\n  const bRank = b.shape.length;\n\n  const innerShapeA = transposeA ? a.shape[aRank - 2] : a.shape[aRank - 1];\n  const innerShapeB = transposeB ? b.shape[bRank - 1] : b.shape[bRank - 2];\n\n  const outerShapeA = transposeA ? a.shape[aRank - 1] : a.shape[aRank - 2];\n  const outerShapeB = transposeB ? b.shape[bRank - 2] : b.shape[bRank - 1];\n\n  const outerDimsA = a.shape.slice(0, -2);\n  const outerDimsB = b.shape.slice(0, -2);\n\n  const batchDimA = util.sizeFromShape(outerDimsA);\n  const batchDimB = util.sizeFromShape(outerDimsB);\n\n  const outShapeOuterDims = broadcast_util.assertAndGetBroadcastShape(\n      a.shape.slice(0, -2), b.shape.slice(0, -2));\n  const outShape = outShapeOuterDims.concat([outerShapeA, outerShapeB]);\n\n  util.assert(\n      innerShapeA === innerShapeB,\n      () => `Error in matMul: inner shapes (${innerShapeA}) and (` +\n          `${innerShapeB}) of Tensors with shapes ${a.shape} and ` +\n          `${b.shape} and transposeA=${transposeA}` +\n          ` and transposeB=${transposeB} must match.`);\n\n  const a3dShape: [number, number, number] = transposeA ?\n      [batchDimA, innerShapeA, outerShapeA] :\n      [batchDimA, outerShapeA, innerShapeA];\n  const b3dShape: [number, number, number] = transposeB ?\n      [batchDimB, outerShapeB, innerShapeB] :\n      [batchDimB, innerShapeB, outerShapeB];\n\n  // The rest of the implementation is designed to operate on rank-3 tensors\n  const a3d = reshape({inputs: {x: a}, backend, attrs: {shape: a3dShape}});\n  const b3d = reshape({inputs: {x: b}, backend, attrs: {shape: b3dShape}});\n\n  const intermediates: TensorInfo[] = [a3d, b3d];\n\n  const batchDim = Math.max(batchDimA, batchDimB);\n  const sharedDim = transposeA ? a3d.shape[1] : a3d.shape[2];\n\n  const hasBias = bias != null;\n  const hasPreluActivationWeights = preluActivationWeights != null;\n  const hasLeakyreluAlpha = activation === 'leakyrelu';\n  const fusedActivation = activation != null ?\n      mapActivationToShaderProgram(activation, true) :\n      null;\n  const containsFusedOps = hasBias || hasPreluActivationWeights ||\n      hasLeakyreluAlpha || fusedActivation != null;\n  let out: TensorInfo;\n\n  // Since the matrices are vectors, it is faster to call mul().sum()\n  // because sum() is O(sqrt(N)) due to divide-and-conquer.\n  if ((outerShapeA === 1 || outerShapeB === 1) &&\n      sharedDim > MATMUL_SHARED_DIM_THRESHOLD && containsFusedOps === false) {\n    let aVec = a3d;\n    let bVec = b3d;\n    if (transposeA) {\n      aVec = transpose({inputs: {x: a3d}, backend, attrs: {perm: [0, 2, 1]}});\n      intermediates.push(aVec);\n    }\n    if (transposeB) {\n      bVec = transpose({inputs: {x: b3d}, backend, attrs: {perm: [0, 2, 1]}});\n      intermediates.push(bVec);\n    }\n\n    const shouldReshapeA = outerShapeB !== 1;\n    const shouldReshapeB = outerShapeB === 1;\n\n    let aVec3d = aVec;\n    if (shouldReshapeA) {\n      aVec3d = reshape({\n        inputs: {x: aVec},\n        backend,\n        attrs: {shape: [batchDim, sharedDim, 1]}\n      });\n\n      intermediates.push(aVec3d);\n    }\n\n    const axis = outerShapeB === 1 ? 2 : 1;\n\n    let bVec3d = bVec;\n    if (shouldReshapeB) {\n      bVec3d = reshape({\n        inputs: {x: bVec},\n        backend,\n        attrs: {shape: [batchDim, 1, sharedDim]}\n      });\n\n      intermediates.push(bVec3d);\n    }\n\n    const product = multiply({inputs: {a: aVec3d, b: bVec3d}, backend});\n    out = sum({inputs: {x: product}, backend, attrs: {axis, keepDims: true}});\n    intermediates.push(product);\n  } else {\n    const dtype = upcastType(a.dtype, b.dtype);\n\n    const program = new MatMulPackedProgram(\n        a3dShape, b3dShape, [batchDim, outerShapeA, outerShapeB], transposeA,\n        transposeB, hasBias, fusedActivation, hasPreluActivationWeights,\n        hasLeakyreluAlpha);\n\n    const inputs: TensorInfo[] = [a3d, b3d];\n    if (bias != null) {\n      inputs.push(bias);\n    }\n    if (hasPreluActivationWeights) {\n      inputs.push(preluActivationWeights);\n    }\n    if (hasLeakyreluAlpha) {\n      const $leakyreluAlpha = backend.makeTensorInfo(\n          [], 'float32',\n          util.createScalarValue(leakyreluAlpha as unknown as 'float32', 'float32'));\n      inputs.push($leakyreluAlpha);\n      intermediates.push($leakyreluAlpha);\n    }\n\n    out = backend.runWebGLProgram(program, inputs, dtype);\n  }\n\n  const outReshaped =\n      reshape({inputs: {x: out}, backend, attrs: {shape: outShape}});\n  intermediates.push(out);\n  for (const i of intermediates) {\n    backend.disposeIntermediateTensorInfo(i);\n  }\n  return outReshaped;\n}\n"],"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA,SAAsBA,cAAc,EAAcC,UAAU,EAAEC,IAAI,QAAO,uBAAuB;AAGhG,SAAQC,4BAA4B,QAAO,oCAAoC;AAC/E,SAAQC,mBAAmB,QAAO,sBAAsB;AAExD,SAAQC,QAAQ,QAAO,YAAY;AACnC,SAAQC,OAAO,QAAO,WAAW;AACjC,SAAQC,GAAG,QAAO,OAAO;AACzB,SAAQC,SAAS,QAAO,aAAa;AAErC;AACA;AACA;AACA,OAAO,IAAMC,2BAA2B,GAAG,IAAI;AAc/C,OAAM,SAAUC,eAAeA,CAAAC,IAAA,EAUX;EAAA,IATlBC,CAAC,GAAAD,IAAA,CAADC,CAAC;IACDC,CAAC,GAAAF,IAAA,CAADE,CAAC;IACDC,UAAU,GAAAH,IAAA,CAAVG,UAAU;IACVC,UAAU,GAAAJ,IAAA,CAAVI,UAAU;IACVC,OAAO,GAAAL,IAAA,CAAPK,OAAO;IAAAC,SAAA,GAAAN,IAAA,CACPO,IAAI;IAAJA,IAAI,GAAAD,SAAA,cAAG,IAAI,GAAAA,SAAA;IAAAE,qBAAA,GAAAR,IAAA,CACXS,sBAAsB;IAAtBA,sBAAsB,GAAAD,qBAAA,cAAG,IAAI,GAAAA,qBAAA;IAAAE,mBAAA,GAAAV,IAAA,CAC7BW,cAAc;IAAdA,cAAc,GAAAD,mBAAA,cAAG,CAAC,GAAAA,mBAAA;IAAAE,eAAA,GAAAZ,IAAA,CAClBa,UAAU;IAAVA,UAAU,GAAAD,eAAA,cAAG,IAAI,GAAAA,eAAA;EAEjB,IAAME,KAAK,GAAGb,CAAC,CAACc,KAAK,CAACC,MAAM;EAC5B,IAAMC,KAAK,GAAGf,CAAC,CAACa,KAAK,CAACC,MAAM;EAE5B,IAAME,WAAW,GAAGf,UAAU,GAAGF,CAAC,CAACc,KAAK,CAACD,KAAK,GAAG,CAAC,CAAC,GAAGb,CAAC,CAACc,KAAK,CAACD,KAAK,GAAG,CAAC,CAAC;EACxE,IAAMK,WAAW,GAAGf,UAAU,GAAGF,CAAC,CAACa,KAAK,CAACE,KAAK,GAAG,CAAC,CAAC,GAAGf,CAAC,CAACa,KAAK,CAACE,KAAK,GAAG,CAAC,CAAC;EAExE,IAAMG,WAAW,GAAGjB,UAAU,GAAGF,CAAC,CAACc,KAAK,CAACD,KAAK,GAAG,CAAC,CAAC,GAAGb,CAAC,CAACc,KAAK,CAACD,KAAK,GAAG,CAAC,CAAC;EACxE,IAAMO,WAAW,GAAGjB,UAAU,GAAGF,CAAC,CAACa,KAAK,CAACE,KAAK,GAAG,CAAC,CAAC,GAAGf,CAAC,CAACa,KAAK,CAACE,KAAK,GAAG,CAAC,CAAC;EAExE,IAAMK,UAAU,GAAGrB,CAAC,CAACc,KAAK,CAACQ,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;EACvC,IAAMC,UAAU,GAAGtB,CAAC,CAACa,KAAK,CAACQ,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;EAEvC,IAAME,SAAS,GAAGlC,IAAI,CAACmC,aAAa,CAACJ,UAAU,CAAC;EAChD,IAAMK,SAAS,GAAGpC,IAAI,CAACmC,aAAa,CAACF,UAAU,CAAC;EAEhD,IAAMI,iBAAiB,GAAGvC,cAAc,CAACwC,0BAA0B,CAC/D5B,CAAC,CAACc,KAAK,CAACQ,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAErB,CAAC,CAACa,KAAK,CAACQ,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;EAC/C,IAAMO,QAAQ,GAAGF,iBAAiB,CAACG,MAAM,CAAC,CAACX,WAAW,EAAEC,WAAW,CAAC,CAAC;EAErE9B,IAAI,CAACyC,MAAM,CACPd,WAAW,KAAKC,WAAW,EAC3B;IAAA,OAAM,kCAAAY,MAAA,CAAkCb,WAAW,kBAAAa,MAAA,CAC5CZ,WAAW,+BAAAY,MAAA,CAA4B9B,CAAC,CAACc,KAAK,UAAO,MAAAgB,MAAA,CACrD7B,CAAC,CAACa,KAAK,sBAAAgB,MAAA,CAAmB5B,UAAU,CAAE,sBAAA4B,MAAA,CACtB3B,UAAU,iBAAc;EAAA,EAAC;EAEpD,IAAM6B,QAAQ,GAA6B9B,UAAU,GACjD,CAACsB,SAAS,EAAEP,WAAW,EAAEE,WAAW,CAAC,GACrC,CAACK,SAAS,EAAEL,WAAW,EAAEF,WAAW,CAAC;EACzC,IAAMgB,QAAQ,GAA6B9B,UAAU,GACjD,CAACuB,SAAS,EAAEN,WAAW,EAAEF,WAAW,CAAC,GACrC,CAACQ,SAAS,EAAER,WAAW,EAAEE,WAAW,CAAC;EAEzC;EACA,IAAMc,GAAG,GAAGxC,OAAO,CAAC;IAACyC,MAAM,EAAE;MAACC,CAAC,EAAEpC;IAAC,CAAC;IAAEI,OAAO,EAAPA,OAAO;IAAEiC,KAAK,EAAE;MAACvB,KAAK,EAAEkB;IAAQ;EAAC,CAAC,CAAC;EACxE,IAAMM,GAAG,GAAG5C,OAAO,CAAC;IAACyC,MAAM,EAAE;MAACC,CAAC,EAAEnC;IAAC,CAAC;IAAEG,OAAO,EAAPA,OAAO;IAAEiC,KAAK,EAAE;MAACvB,KAAK,EAAEmB;IAAQ;EAAC,CAAC,CAAC;EAExE,IAAMM,aAAa,GAAiB,CAACL,GAAG,EAAEI,GAAG,CAAC;EAE9C,IAAME,QAAQ,GAAGC,IAAI,CAACC,GAAG,CAAClB,SAAS,EAAEE,SAAS,CAAC;EAC/C,IAAMiB,SAAS,GAAGzC,UAAU,GAAGgC,GAAG,CAACpB,KAAK,CAAC,CAAC,CAAC,GAAGoB,GAAG,CAACpB,KAAK,CAAC,CAAC,CAAC;EAE1D,IAAM8B,OAAO,GAAGtC,IAAI,IAAI,IAAI;EAC5B,IAAMuC,yBAAyB,GAAGrC,sBAAsB,IAAI,IAAI;EAChE,IAAMsC,iBAAiB,GAAGlC,UAAU,KAAK,WAAW;EACpD,IAAMmC,eAAe,GAAGnC,UAAU,IAAI,IAAI,GACtCrB,4BAA4B,CAACqB,UAAU,EAAE,IAAI,CAAC,GAC9C,IAAI;EACR,IAAMoC,gBAAgB,GAAGJ,OAAO,IAAIC,yBAAyB,IACzDC,iBAAiB,IAAIC,eAAe,IAAI,IAAI;EAChD,IAAIE,GAAe;EAEnB;EACA;EACA,IAAI,CAAC9B,WAAW,KAAK,CAAC,IAAIC,WAAW,KAAK,CAAC,KACvCuB,SAAS,GAAG9C,2BAA2B,IAAImD,gBAAgB,KAAK,KAAK,EAAE;IACzE,IAAIE,IAAI,GAAGhB,GAAG;IACd,IAAIiB,IAAI,GAAGb,GAAG;IACd,IAAIpC,UAAU,EAAE;MACdgD,IAAI,GAAGtD,SAAS,CAAC;QAACuC,MAAM,EAAE;UAACC,CAAC,EAAEF;QAAG,CAAC;QAAE9B,OAAO,EAAPA,OAAO;QAAEiC,KAAK,EAAE;UAACe,IAAI,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC;QAAC;MAAC,CAAC,CAAC;MACvEb,aAAa,CAACc,IAAI,CAACH,IAAI,CAAC;;IAE1B,IAAI/C,UAAU,EAAE;MACdgD,IAAI,GAAGvD,SAAS,CAAC;QAACuC,MAAM,EAAE;UAACC,CAAC,EAAEE;QAAG,CAAC;QAAElC,OAAO,EAAPA,OAAO;QAAEiC,KAAK,EAAE;UAACe,IAAI,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC;QAAC;MAAC,CAAC,CAAC;MACvEb,aAAa,CAACc,IAAI,CAACF,IAAI,CAAC;;IAG1B,IAAMG,cAAc,GAAGlC,WAAW,KAAK,CAAC;IACxC,IAAMmC,cAAc,GAAGnC,WAAW,KAAK,CAAC;IAExC,IAAIoC,MAAM,GAAGN,IAAI;IACjB,IAAII,cAAc,EAAE;MAClBE,MAAM,GAAG9D,OAAO,CAAC;QACfyC,MAAM,EAAE;UAACC,CAAC,EAAEc;QAAI,CAAC;QACjB9C,OAAO,EAAPA,OAAO;QACPiC,KAAK,EAAE;UAACvB,KAAK,EAAE,CAAC0B,QAAQ,EAAEG,SAAS,EAAE,CAAC;QAAC;OACxC,CAAC;MAEFJ,aAAa,CAACc,IAAI,CAACG,MAAM,CAAC;;IAG5B,IAAMC,IAAI,GAAGrC,WAAW,KAAK,CAAC,GAAG,CAAC,GAAG,CAAC;IAEtC,IAAIsC,MAAM,GAAGP,IAAI;IACjB,IAAII,cAAc,EAAE;MAClBG,MAAM,GAAGhE,OAAO,CAAC;QACfyC,MAAM,EAAE;UAACC,CAAC,EAAEe;QAAI,CAAC;QACjB/C,OAAO,EAAPA,OAAO;QACPiC,KAAK,EAAE;UAACvB,KAAK,EAAE,CAAC0B,QAAQ,EAAE,CAAC,EAAEG,SAAS;QAAC;OACxC,CAAC;MAEFJ,aAAa,CAACc,IAAI,CAACK,MAAM,CAAC;;IAG5B,IAAMC,OAAO,GAAGlE,QAAQ,CAAC;MAAC0C,MAAM,EAAE;QAACnC,CAAC,EAAEwD,MAAM;QAAEvD,CAAC,EAAEyD;MAAM,CAAC;MAAEtD,OAAO,EAAPA;IAAO,CAAC,CAAC;IACnE6C,GAAG,GAAGtD,GAAG,CAAC;MAACwC,MAAM,EAAE;QAACC,CAAC,EAAEuB;MAAO,CAAC;MAAEvD,OAAO,EAAPA,OAAO;MAAEiC,KAAK,EAAE;QAACoB,IAAI,EAAJA,IAAI;QAAEG,QAAQ,EAAE;MAAI;IAAC,CAAC,CAAC;IACzErB,aAAa,CAACc,IAAI,CAACM,OAAO,CAAC;GAC5B,MAAM;IACL,IAAME,KAAK,GAAGxE,UAAU,CAACW,CAAC,CAAC6D,KAAK,EAAE5D,CAAC,CAAC4D,KAAK,CAAC;IAE1C,IAAMC,OAAO,GAAG,IAAItE,mBAAmB,CACnCwC,QAAQ,EAAEC,QAAQ,EAAE,CAACO,QAAQ,EAAErB,WAAW,EAAEC,WAAW,CAAC,EAAElB,UAAU,EACpEC,UAAU,EAAEyC,OAAO,EAAEG,eAAe,EAAEF,yBAAyB,EAC/DC,iBAAiB,CAAC;IAEtB,IAAMX,MAAM,GAAiB,CAACD,GAAG,EAAEI,GAAG,CAAC;IACvC,IAAIhC,IAAI,IAAI,IAAI,EAAE;MAChB6B,MAAM,CAACkB,IAAI,CAAC/C,IAAI,CAAC;;IAEnB,IAAIuC,yBAAyB,EAAE;MAC7BV,MAAM,CAACkB,IAAI,CAAC7C,sBAAsB,CAAC;;IAErC,IAAIsC,iBAAiB,EAAE;MACrB,IAAMiB,eAAe,GAAG3D,OAAO,CAAC4D,cAAc,CAC1C,EAAE,EAAE,SAAS,EACb1E,IAAI,CAAC2E,iBAAiB,CAACvD,cAAsC,EAAE,SAAS,CAAC,CAAC;MAC9EyB,MAAM,CAACkB,IAAI,CAACU,eAAe,CAAC;MAC5BxB,aAAa,CAACc,IAAI,CAACU,eAAe,CAAC;;IAGrCd,GAAG,GAAG7C,OAAO,CAAC8D,eAAe,CAACJ,OAAO,EAAE3B,MAAM,EAAE0B,KAAK,CAAC;;EAGvD,IAAMM,WAAW,GACbzE,OAAO,CAAC;IAACyC,MAAM,EAAE;MAACC,CAAC,EAAEa;IAAG,CAAC;IAAE7C,OAAO,EAAPA,OAAO;IAAEiC,KAAK,EAAE;MAACvB,KAAK,EAAEe;IAAQ;EAAC,CAAC,CAAC;EAClEU,aAAa,CAACc,IAAI,CAACJ,GAAG,CAAC;EACvB,SAAAmB,EAAA,MAAAC,cAAA,GAAgB9B,aAAa,EAAA6B,EAAA,GAAAC,cAAA,CAAAtD,MAAA,EAAAqD,EAAA,IAAE;IAA1B,IAAME,CAAC,GAAAD,cAAA,CAAAD,EAAA;IACVhE,OAAO,CAACmE,6BAA6B,CAACD,CAAC,CAAC;;EAE1C,OAAOH,WAAW;AACpB"},"metadata":{},"sourceType":"module","externalDependencies":[]}