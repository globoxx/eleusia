{"ast":null,"code":"import _classCallCheck from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/classCallCheck.js\";\nimport _createClass from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/createClass.js\";\n/**\r\n * @license\r\n * Copyright 2018 Google LLC. All Rights Reserved.\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n * http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n * =============================================================================\r\n */\nimport { AdadeltaOptimizer } from './adadelta_optimizer';\nimport { AdagradOptimizer } from './adagrad_optimizer';\nimport { AdamOptimizer } from './adam_optimizer';\nimport { AdamaxOptimizer } from './adamax_optimizer';\nimport { MomentumOptimizer } from './momentum_optimizer';\nimport { RMSPropOptimizer } from './rmsprop_optimizer';\nimport { SGDOptimizer } from './sgd_optimizer';\nexport var OptimizerConstructors = /*#__PURE__*/function () {\n  function OptimizerConstructors() {\n    _classCallCheck(this, OptimizerConstructors);\n  }\n  _createClass(OptimizerConstructors, null, [{\n    key: \"sgd\",\n    value:\n    /**\r\n     * Constructs a `tf.SGDOptimizer` that uses stochastic gradient descent.\r\n     *\r\n     * ```js\r\n     * // Fit a quadratic function by learning the coefficients a, b, c.\r\n     * const xs = tf.tensor1d([0, 1, 2, 3]);\r\n     * const ys = tf.tensor1d([1.1, 5.9, 16.8, 33.9]);\r\n     *\r\n     * const a = tf.scalar(Math.random()).variable();\r\n     * const b = tf.scalar(Math.random()).variable();\r\n     * const c = tf.scalar(Math.random()).variable();\r\n     *\r\n     * // y = a * x^2 + b * x + c.\r\n     * const f = x => a.mul(x.square()).add(b.mul(x)).add(c);\r\n     * const loss = (pred, label) => pred.sub(label).square().mean();\r\n     *\r\n     * const learningRate = 0.01;\r\n     * const optimizer = tf.train.sgd(learningRate);\r\n     *\r\n     * // Train the model.\r\n     * for (let i = 0; i < 10; i++) {\r\n     *   optimizer.minimize(() => loss(f(xs), ys));\r\n     * }\r\n     *\r\n     * // Make predictions.\r\n     * console.log(\r\n     *     `a: ${a.dataSync()}, b: ${b.dataSync()}, c: ${c.dataSync()}`);\r\n     * const preds = f(xs).dataSync();\r\n     * preds.forEach((pred, i) => {\r\n     *   console.log(`x: ${i}, pred: ${pred}`);\r\n     * });\r\n     * ```\r\n     *\r\n     * @param learningRate The learning rate to use for the SGD algorithm.\r\n     *\r\n     * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\r\n     */\n    function sgd(learningRate) {\n      return new SGDOptimizer(learningRate);\n    }\n    /**\r\n     * Constructs a `tf.MomentumOptimizer` that uses momentum gradient\r\n     * descent.\r\n     *\r\n     * See\r\n     * [http://proceedings.mlr.press/v28/sutskever13.pdf](\r\n     * http://proceedings.mlr.press/v28/sutskever13.pdf)\r\n     *\r\n     * @param learningRate The learning rate to use for the Momentum gradient\r\n     * descent algorithm.\r\n     * @param momentum The momentum to use for the momentum gradient descent\r\n     * algorithm.\r\n     *\r\n     * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\r\n     */\n  }, {\n    key: \"momentum\",\n    value: function momentum(learningRate, _momentum) {\n      var useNesterov = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : false;\n      return new MomentumOptimizer(learningRate, _momentum, useNesterov);\n    }\n    /**\r\n     * Constructs a `tf.RMSPropOptimizer` that uses RMSProp gradient\r\n     * descent. This implementation uses plain momentum and is not centered\r\n     * version of RMSProp.\r\n     *\r\n     * See\r\n     * [http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf](\r\n     * http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\r\n     *\r\n     * @param learningRate The learning rate to use for the RMSProp gradient\r\n     * descent algorithm.\r\n     * @param decay The discounting factor for the history/coming gradient.\r\n     * @param momentum The momentum to use for the RMSProp gradient descent\r\n     * algorithm.\r\n     * @param epsilon Small value to avoid zero denominator.\r\n     * @param centered If true, gradients are normalized by the estimated\r\n     * variance of the gradient.\r\n     *\r\n     * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\r\n     */\n  }, {\n    key: \"rmsprop\",\n    value: function rmsprop(learningRate) {\n      var decay = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : .9;\n      var momentum = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 0.0;\n      var epsilon = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : null;\n      var centered = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : false;\n      return new RMSPropOptimizer(learningRate, decay, momentum, epsilon, centered);\n    }\n    /**\r\n     * Constructs a `tf.AdamOptimizer` that uses the Adam algorithm.\r\n     * See [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)\r\n     *\r\n     * @param learningRate The learning rate to use for the Adam gradient\r\n     * descent algorithm.\r\n     * @param beta1 The exponential decay rate for the 1st moment estimates.\r\n     * @param beta2 The exponential decay rate for the 2nd moment estimates.\r\n     * @param epsilon A small constant for numerical stability.\r\n     *\r\n     * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\r\n     */\n  }, {\n    key: \"adam\",\n    value: function adam() {\n      var learningRate = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : 0.001;\n      var beta1 = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0.9;\n      var beta2 = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 0.999;\n      var epsilon = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : null;\n      return new AdamOptimizer(learningRate, beta1, beta2, epsilon);\n    }\n    /**\r\n     * Constructs a `tf.AdadeltaOptimizer` that uses the Adadelta algorithm.\r\n     * See [https://arxiv.org/abs/1212.5701](https://arxiv.org/abs/1212.5701)\r\n     *\r\n     * @param learningRate The learning rate to use for the Adadelta gradient\r\n     * descent algorithm.\r\n     * @param rho The learning rate decay over each update.\r\n     * @param epsilon A constant epsilon used to better condition the grad\r\n     * update.\r\n     *\r\n     * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\r\n     */\n  }, {\n    key: \"adadelta\",\n    value: function adadelta() {\n      var learningRate = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : .001;\n      var rho = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : .95;\n      var epsilon = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : null;\n      return new AdadeltaOptimizer(learningRate, rho, epsilon);\n    }\n    /**\r\n     * Constructs a `tf.AdamaxOptimizer` that uses the Adamax algorithm.\r\n     * See [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)\r\n     *\r\n     * @param learningRate The learning rate to use for the Adamax gradient\r\n     * descent algorithm.\r\n     * @param beta1 The exponential decay rate for the 1st moment estimates.\r\n     * @param beta2 The exponential decay rate for the 2nd moment estimates.\r\n     * @param epsilon A small constant for numerical stability.\r\n     * @param decay The learning rate decay over each update.\r\n     *\r\n     * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\r\n     */\n  }, {\n    key: \"adamax\",\n    value: function adamax() {\n      var learningRate = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : 0.002;\n      var beta1 = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0.9;\n      var beta2 = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 0.999;\n      var epsilon = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : null;\n      var decay = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : 0.0;\n      return new AdamaxOptimizer(learningRate, beta1, beta2, epsilon, decay);\n    }\n    /**\r\n     * Constructs a `tf.AdagradOptimizer` that uses the Adagrad algorithm.\r\n     * See\r\n     * [http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf](\r\n     * http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\r\n     * or\r\n     * [http://ruder.io/optimizing-gradient-descent/index.html#adagrad](\r\n     * http://ruder.io/optimizing-gradient-descent/index.html#adagrad)\r\n     *\r\n     * @param learningRate The learning rate to use for the Adagrad gradient\r\n     * descent algorithm.\r\n     * @param initialAccumulatorValue Starting value for the accumulators, must be\r\n     * positive.\r\n     *\r\n     * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\r\n     */\n  }, {\n    key: \"adagrad\",\n    value: function adagrad(learningRate) {\n      var initialAccumulatorValue = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0.1;\n      return new AdagradOptimizer(learningRate, initialAccumulatorValue);\n    }\n  }]);\n  return OptimizerConstructors;\n}();","map":{"version":3,"names":["AdadeltaOptimizer","AdagradOptimizer","AdamOptimizer","AdamaxOptimizer","MomentumOptimizer","RMSPropOptimizer","SGDOptimizer","OptimizerConstructors","_classCallCheck","_createClass","key","value","sgd","learningRate","momentum","useNesterov","arguments","length","undefined","rmsprop","decay","epsilon","centered","adam","beta1","beta2","adadelta","rho","adamax","adagrad","initialAccumulatorValue"],"sources":["C:\\Users\\vince\\OneDrive\\Documents\\GitHub\\tfjs-core\\src\\optimizers\\optimizer_constructors.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {AdadeltaOptimizer} from './adadelta_optimizer';\nimport {AdagradOptimizer} from './adagrad_optimizer';\nimport {AdamOptimizer} from './adam_optimizer';\nimport {AdamaxOptimizer} from './adamax_optimizer';\nimport {MomentumOptimizer} from './momentum_optimizer';\nimport {RMSPropOptimizer} from './rmsprop_optimizer';\nimport {SGDOptimizer} from './sgd_optimizer';\n\nexport class OptimizerConstructors {\n  /**\n   * Constructs a `tf.SGDOptimizer` that uses stochastic gradient descent.\n   *\n   * ```js\n   * // Fit a quadratic function by learning the coefficients a, b, c.\n   * const xs = tf.tensor1d([0, 1, 2, 3]);\n   * const ys = tf.tensor1d([1.1, 5.9, 16.8, 33.9]);\n   *\n   * const a = tf.scalar(Math.random()).variable();\n   * const b = tf.scalar(Math.random()).variable();\n   * const c = tf.scalar(Math.random()).variable();\n   *\n   * // y = a * x^2 + b * x + c.\n   * const f = x => a.mul(x.square()).add(b.mul(x)).add(c);\n   * const loss = (pred, label) => pred.sub(label).square().mean();\n   *\n   * const learningRate = 0.01;\n   * const optimizer = tf.train.sgd(learningRate);\n   *\n   * // Train the model.\n   * for (let i = 0; i < 10; i++) {\n   *   optimizer.minimize(() => loss(f(xs), ys));\n   * }\n   *\n   * // Make predictions.\n   * console.log(\n   *     `a: ${a.dataSync()}, b: ${b.dataSync()}, c: ${c.dataSync()}`);\n   * const preds = f(xs).dataSync();\n   * preds.forEach((pred, i) => {\n   *   console.log(`x: ${i}, pred: ${pred}`);\n   * });\n   * ```\n   *\n   * @param learningRate The learning rate to use for the SGD algorithm.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n  static sgd(learningRate: number): SGDOptimizer {\n    return new SGDOptimizer(learningRate);\n  }\n\n  /**\n   * Constructs a `tf.MomentumOptimizer` that uses momentum gradient\n   * descent.\n   *\n   * See\n   * [http://proceedings.mlr.press/v28/sutskever13.pdf](\n   * http://proceedings.mlr.press/v28/sutskever13.pdf)\n   *\n   * @param learningRate The learning rate to use for the Momentum gradient\n   * descent algorithm.\n   * @param momentum The momentum to use for the momentum gradient descent\n   * algorithm.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n  static momentum(learningRate: number, momentum: number, useNesterov = false):\n      MomentumOptimizer {\n    return new MomentumOptimizer(learningRate, momentum, useNesterov);\n  }\n\n  /**\n   * Constructs a `tf.RMSPropOptimizer` that uses RMSProp gradient\n   * descent. This implementation uses plain momentum and is not centered\n   * version of RMSProp.\n   *\n   * See\n   * [http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf](\n   * http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n   *\n   * @param learningRate The learning rate to use for the RMSProp gradient\n   * descent algorithm.\n   * @param decay The discounting factor for the history/coming gradient.\n   * @param momentum The momentum to use for the RMSProp gradient descent\n   * algorithm.\n   * @param epsilon Small value to avoid zero denominator.\n   * @param centered If true, gradients are normalized by the estimated\n   * variance of the gradient.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n  static rmsprop(\n      learningRate: number, decay = .9, momentum = 0.0, epsilon: number = null,\n      centered = false): RMSPropOptimizer {\n    return new RMSPropOptimizer(\n        learningRate, decay, momentum, epsilon, centered);\n  }\n\n  /**\n   * Constructs a `tf.AdamOptimizer` that uses the Adam algorithm.\n   * See [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)\n   *\n   * @param learningRate The learning rate to use for the Adam gradient\n   * descent algorithm.\n   * @param beta1 The exponential decay rate for the 1st moment estimates.\n   * @param beta2 The exponential decay rate for the 2nd moment estimates.\n   * @param epsilon A small constant for numerical stability.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n  static adam(\n      learningRate = 0.001, beta1 = 0.9, beta2 = 0.999,\n      epsilon: number = null): AdamOptimizer {\n    return new AdamOptimizer(learningRate, beta1, beta2, epsilon);\n  }\n\n  /**\n   * Constructs a `tf.AdadeltaOptimizer` that uses the Adadelta algorithm.\n   * See [https://arxiv.org/abs/1212.5701](https://arxiv.org/abs/1212.5701)\n   *\n   * @param learningRate The learning rate to use for the Adadelta gradient\n   * descent algorithm.\n   * @param rho The learning rate decay over each update.\n   * @param epsilon A constant epsilon used to better condition the grad\n   * update.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n  static adadelta(learningRate = .001, rho = .95, epsilon: number = null):\n      AdadeltaOptimizer {\n    return new AdadeltaOptimizer(learningRate, rho, epsilon);\n  }\n\n  /**\n   * Constructs a `tf.AdamaxOptimizer` that uses the Adamax algorithm.\n   * See [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)\n   *\n   * @param learningRate The learning rate to use for the Adamax gradient\n   * descent algorithm.\n   * @param beta1 The exponential decay rate for the 1st moment estimates.\n   * @param beta2 The exponential decay rate for the 2nd moment estimates.\n   * @param epsilon A small constant for numerical stability.\n   * @param decay The learning rate decay over each update.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n  static adamax(\n      learningRate = 0.002, beta1 = 0.9, beta2 = 0.999, epsilon: number = null,\n      decay = 0.0): AdamaxOptimizer {\n    return new AdamaxOptimizer(learningRate, beta1, beta2, epsilon, decay);\n  }\n\n  /**\n   * Constructs a `tf.AdagradOptimizer` that uses the Adagrad algorithm.\n   * See\n   * [http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf](\n   * http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n   * or\n   * [http://ruder.io/optimizing-gradient-descent/index.html#adagrad](\n   * http://ruder.io/optimizing-gradient-descent/index.html#adagrad)\n   *\n   * @param learningRate The learning rate to use for the Adagrad gradient\n   * descent algorithm.\n   * @param initialAccumulatorValue Starting value for the accumulators, must be\n   * positive.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n  static adagrad(learningRate: number, initialAccumulatorValue = 0.1):\n      AdagradOptimizer {\n    return new AdagradOptimizer(learningRate, initialAccumulatorValue);\n  }\n}\n"],"mappings":";;AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQA,iBAAiB,QAAO,sBAAsB;AACtD,SAAQC,gBAAgB,QAAO,qBAAqB;AACpD,SAAQC,aAAa,QAAO,kBAAkB;AAC9C,SAAQC,eAAe,QAAO,oBAAoB;AAClD,SAAQC,iBAAiB,QAAO,sBAAsB;AACtD,SAAQC,gBAAgB,QAAO,qBAAqB;AACpD,SAAQC,YAAY,QAAO,iBAAiB;AAE5C,WAAaC,qBAAqB;EAAA,SAAAA,sBAAA;IAAAC,eAAA,OAAAD,qBAAA;EAAA;EAAAE,YAAA,CAAAF,qBAAA;IAAAG,GAAA;IAAAC,KAAA;IAChC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;IAqCA,SAAAC,IAAWC,YAAoB;MAC7B,OAAO,IAAIP,YAAY,CAACO,YAAY,CAAC;IACvC;IAEA;;;;;;;;;;;;;;;EAAA;IAAAH,GAAA;IAAAC,KAAA,EAeA,SAAAG,SAAgBD,YAAoB,EAAEC,SAAgB,EAAqB;MAAA,IAAnBC,WAAW,GAAAC,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,KAAK;MAEzE,OAAO,IAAIZ,iBAAiB,CAACS,YAAY,EAAEC,SAAQ,EAAEC,WAAW,CAAC;IACnE;IAEA;;;;;;;;;;;;;;;;;;;;EAAA;IAAAL,GAAA;IAAAC,KAAA,EAoBA,SAAAQ,QACIN,YAAoB,EACJ;MAAA,IADMO,KAAK,GAAAJ,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,EAAE;MAAA,IAAEF,QAAQ,GAAAE,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,GAAG;MAAA,IAAEK,OAAA,GAAAL,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAkB,IAAI;MAAA,IACxEM,QAAQ,GAAAN,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,KAAK;MAClB,OAAO,IAAIX,gBAAgB,CACvBQ,YAAY,EAAEO,KAAK,EAAEN,QAAQ,EAAEO,OAAO,EAAEC,QAAQ,CAAC;IACvD;IAEA;;;;;;;;;;;;EAAA;IAAAZ,GAAA;IAAAC,KAAA,EAYA,SAAAY,KAAA,EAE0B;MAAA,IADtBV,YAAY,GAAAG,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,KAAK;MAAA,IAAEQ,KAAK,GAAAR,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,GAAG;MAAA,IAAES,KAAK,GAAAT,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,KAAK;MAAA,IAChDK,OAAA,GAAAL,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAkB,IAAI;MACxB,OAAO,IAAId,aAAa,CAACW,YAAY,EAAEW,KAAK,EAAEC,KAAK,EAAEJ,OAAO,CAAC;IAC/D;IAEA;;;;;;;;;;;;EAAA;IAAAX,GAAA;IAAAC,KAAA,EAYA,SAAAe,SAAA,EAAsE;MAAA,IAAtDb,YAAY,GAAAG,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,IAAI;MAAA,IAAEW,GAAG,GAAAX,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,GAAG;MAAA,IAAEK,OAAA,GAAAL,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAkB,IAAI;MAEpE,OAAO,IAAIhB,iBAAiB,CAACa,YAAY,EAAEc,GAAG,EAAEN,OAAO,CAAC;IAC1D;IAEA;;;;;;;;;;;;;EAAA;IAAAX,GAAA;IAAAC,KAAA,EAaA,SAAAiB,OAAA,EAEe;MAAA,IADXf,YAAY,GAAAG,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,KAAK;MAAA,IAAEQ,KAAK,GAAAR,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,GAAG;MAAA,IAAES,KAAK,GAAAT,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,KAAK;MAAA,IAAEK,OAAA,GAAAL,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAkB,IAAI;MAAA,IACxEI,KAAK,GAAAJ,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,GAAG;MACb,OAAO,IAAIb,eAAe,CAACU,YAAY,EAAEW,KAAK,EAAEC,KAAK,EAAEJ,OAAO,EAAED,KAAK,CAAC;IACxE;IAEA;;;;;;;;;;;;;;;;EAAA;IAAAV,GAAA;IAAAC,KAAA,EAgBA,SAAAkB,QAAehB,YAAoB,EAA+B;MAAA,IAA7BiB,uBAAuB,GAAAd,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,GAAG;MAEhE,OAAO,IAAIf,gBAAgB,CAACY,YAAY,EAAEiB,uBAAuB,CAAC;IACpE;EAAC;EAAA,OAAAvB,qBAAA;AAAA"},"metadata":{},"sourceType":"module","externalDependencies":[]}