{"ast":null,"code":"/**\r\n * @license\r\n * Copyright 2018 Google LLC\r\n *\r\n * Use of this source code is governed by an MIT-style\r\n * license that can be found in the LICENSE file or at\r\n * https://opensource.org/licenses/MIT.\r\n * =============================================================================\r\n */\n/**\r\n * Exported functions.\r\n */\nimport { CallbackConstructorRegistry } from './base_callbacks';\nimport { Input } from './engine/input_layer';\nimport { LayersModel } from './engine/training';\nimport { Sequential } from './models';\nexport { loadLayersModel } from './models';\n// TODO(cais): Add doc string to all the public static functions in this\n//   class; include exectuable JavaScript code snippets where applicable\n//   (b/74074458).\n// LayersModel and related factory methods.\n/**\r\n * A model is a data structure that consists of `Layers` and defines inputs\r\n * and outputs.\r\n *\r\n * The key difference between `tf.model` and `tf.sequential` is that\r\n * `tf.model` is more generic, supporting an arbitrary graph (without\r\n * cycles) of layers. `tf.sequential` is less generic and supports only a linear\r\n * stack of layers.\r\n *\r\n * When creating a `tf.LayersModel`, specify its input(s) and output(s). Layers\r\n * are used to wire input(s) to output(s).\r\n *\r\n * For example, the following code snippet defines a model consisting of\r\n * two `dense` layers, with 10 and 4 units, respectively.\r\n *\r\n * ```js\r\n * // Define input, which has a size of 5 (not including batch dimension).\r\n * const input = tf.input({shape: [5]});\r\n *\r\n * // First dense layer uses relu activation.\r\n * const denseLayer1 = tf.layers.dense({units: 10, activation: 'relu'});\r\n * // Second dense layer uses softmax activation.\r\n * const denseLayer2 = tf.layers.dense({units: 4, activation: 'softmax'});\r\n *\r\n * // Obtain the output symbolic tensor by applying the layers on the input.\r\n * const output = denseLayer2.apply(denseLayer1.apply(input));\r\n *\r\n * // Create the model based on the inputs.\r\n * const model = tf.model({inputs: input, outputs: output});\r\n *\r\n * // The model can be used for training, evaluation and prediction.\r\n * // For example, the following line runs prediction with the model on\r\n * // some fake data.\r\n * model.predict(tf.ones([2, 5])).print();\r\n * ```\r\n * See also:\r\n *   `tf.sequential`, `tf.loadLayersModel`.\r\n *\r\n * @doc {heading: 'Models', subheading: 'Creation'}\r\n */\nexport function model(args) {\n  return new LayersModel(args);\n}\n/**\r\n * Creates a `tf.Sequential` model.  A sequential model is any model where the\r\n * outputs of one layer are the inputs to the next layer, i.e. the model\r\n * topology is a simple 'stack' of layers, with no branching or skipping.\r\n *\r\n * This means that the first layer passed to a `tf.Sequential` model should have\r\n * a defined input shape. What that means is that it should have received an\r\n * `inputShape` or `batchInputShape` argument, or for some type of layers\r\n * (recurrent, Dense...) an `inputDim` argument.\r\n *\r\n * The key difference between `tf.model` and `tf.sequential` is that\r\n * `tf.sequential` is less generic, supporting only a linear stack of layers.\r\n * `tf.model` is more generic and supports an arbitrary graph (without\r\n * cycles) of layers.\r\n *\r\n * Examples:\r\n *\r\n * ```js\r\n * const model = tf.sequential();\r\n *\r\n * // First layer must have an input shape defined.\r\n * model.add(tf.layers.dense({units: 32, inputShape: [50]}));\r\n * // Afterwards, TF.js does automatic shape inference.\r\n * model.add(tf.layers.dense({units: 4}));\r\n *\r\n * // Inspect the inferred shape of the model's output, which equals\r\n * // `[null, 4]`. The 1st dimension is the undetermined batch dimension; the\r\n * // 2nd is the output size of the model's last layer.\r\n * console.log(JSON.stringify(model.outputs[0].shape));\r\n * ```\r\n *\r\n * It is also possible to specify a batch size (with potentially undetermined\r\n * batch dimension, denoted by \"null\") for the first layer using the\r\n * `batchInputShape` key. The following example is equivalent to the above:\r\n *\r\n * ```js\r\n * const model = tf.sequential();\r\n *\r\n * // First layer must have a defined input shape\r\n * model.add(tf.layers.dense({units: 32, batchInputShape: [null, 50]}));\r\n * // Afterwards, TF.js does automatic shape inference.\r\n * model.add(tf.layers.dense({units: 4}));\r\n *\r\n * // Inspect the inferred shape of the model's output.\r\n * console.log(JSON.stringify(model.outputs[0].shape));\r\n * ```\r\n *\r\n * You can also use an `Array` of already-constructed `Layer`s to create\r\n * a `tf.Sequential` model:\r\n *\r\n * ```js\r\n * const model = tf.sequential({\r\n *   layers: [tf.layers.dense({units: 32, inputShape: [50]}),\r\n *            tf.layers.dense({units: 4})]\r\n * });\r\n * console.log(JSON.stringify(model.outputs[0].shape));\r\n * ```\r\n *\r\n * @doc {heading: 'Models', subheading: 'Creation'}\r\n */\nexport function sequential(config) {\n  return new Sequential(config);\n}\n/**\r\n * Used to instantiate an input to a model as a `tf.SymbolicTensor`.\r\n *\r\n * Users should call the `input` factory function for\r\n * consistency with other generator functions.\r\n *\r\n * Example:\r\n *\r\n * ```js\r\n * // Defines a simple logistic regression model with 32 dimensional input\r\n * // and 3 dimensional output.\r\n * const x = tf.input({shape: [32]});\r\n * const y = tf.layers.dense({units: 3, activation: 'softmax'}).apply(x);\r\n * const model = tf.model({inputs: x, outputs: y});\r\n * model.predict(tf.ones([2, 32])).print();\r\n * ```\r\n *\r\n * Note: `input` is only necessary when using `model`. When using\r\n * `sequential`, specify `inputShape` for the first layer or use `inputLayer`\r\n * as the first layer.\r\n *\r\n * @doc {heading: 'Models', subheading: 'Inputs'}\r\n */\nexport function input(config) {\n  return Input(config);\n}\nexport function registerCallbackConstructor(verbosityLevel, callbackConstructor) {\n  CallbackConstructorRegistry.registerCallbackConstructor(verbosityLevel, callbackConstructor);\n}","map":{"version":3,"names":["CallbackConstructorRegistry","Input","LayersModel","Sequential","loadLayersModel","model","args","sequential","config","input","registerCallbackConstructor","verbosityLevel","callbackConstructor"],"sources":["C:\\Users\\vince\\OneDrive\\Documents\\GitHub\\tfjs-layers\\src\\exports.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Exported functions.\n */\n\nimport {BaseCallbackConstructor, CallbackConstructorRegistry} from './base_callbacks';\nimport {ContainerArgs} from './engine/container';\nimport {Input, InputConfig,} from './engine/input_layer';\nimport {SymbolicTensor} from './engine/topology';\nimport {LayersModel} from './engine/training';\nimport {Sequential, SequentialArgs} from './models';\n\nexport {loadLayersModel} from './models';\n\n// TODO(cais): Add doc string to all the public static functions in this\n//   class; include exectuable JavaScript code snippets where applicable\n//   (b/74074458).\n\n// LayersModel and related factory methods.\n\n/**\n * A model is a data structure that consists of `Layers` and defines inputs\n * and outputs.\n *\n * The key difference between `tf.model` and `tf.sequential` is that\n * `tf.model` is more generic, supporting an arbitrary graph (without\n * cycles) of layers. `tf.sequential` is less generic and supports only a linear\n * stack of layers.\n *\n * When creating a `tf.LayersModel`, specify its input(s) and output(s). Layers\n * are used to wire input(s) to output(s).\n *\n * For example, the following code snippet defines a model consisting of\n * two `dense` layers, with 10 and 4 units, respectively.\n *\n * ```js\n * // Define input, which has a size of 5 (not including batch dimension).\n * const input = tf.input({shape: [5]});\n *\n * // First dense layer uses relu activation.\n * const denseLayer1 = tf.layers.dense({units: 10, activation: 'relu'});\n * // Second dense layer uses softmax activation.\n * const denseLayer2 = tf.layers.dense({units: 4, activation: 'softmax'});\n *\n * // Obtain the output symbolic tensor by applying the layers on the input.\n * const output = denseLayer2.apply(denseLayer1.apply(input));\n *\n * // Create the model based on the inputs.\n * const model = tf.model({inputs: input, outputs: output});\n *\n * // The model can be used for training, evaluation and prediction.\n * // For example, the following line runs prediction with the model on\n * // some fake data.\n * model.predict(tf.ones([2, 5])).print();\n * ```\n * See also:\n *   `tf.sequential`, `tf.loadLayersModel`.\n *\n * @doc {heading: 'Models', subheading: 'Creation'}\n */\nexport function model(args: ContainerArgs): LayersModel {\n  return new LayersModel(args);\n}\n\n/**\n * Creates a `tf.Sequential` model.  A sequential model is any model where the\n * outputs of one layer are the inputs to the next layer, i.e. the model\n * topology is a simple 'stack' of layers, with no branching or skipping.\n *\n * This means that the first layer passed to a `tf.Sequential` model should have\n * a defined input shape. What that means is that it should have received an\n * `inputShape` or `batchInputShape` argument, or for some type of layers\n * (recurrent, Dense...) an `inputDim` argument.\n *\n * The key difference between `tf.model` and `tf.sequential` is that\n * `tf.sequential` is less generic, supporting only a linear stack of layers.\n * `tf.model` is more generic and supports an arbitrary graph (without\n * cycles) of layers.\n *\n * Examples:\n *\n * ```js\n * const model = tf.sequential();\n *\n * // First layer must have an input shape defined.\n * model.add(tf.layers.dense({units: 32, inputShape: [50]}));\n * // Afterwards, TF.js does automatic shape inference.\n * model.add(tf.layers.dense({units: 4}));\n *\n * // Inspect the inferred shape of the model's output, which equals\n * // `[null, 4]`. The 1st dimension is the undetermined batch dimension; the\n * // 2nd is the output size of the model's last layer.\n * console.log(JSON.stringify(model.outputs[0].shape));\n * ```\n *\n * It is also possible to specify a batch size (with potentially undetermined\n * batch dimension, denoted by \"null\") for the first layer using the\n * `batchInputShape` key. The following example is equivalent to the above:\n *\n * ```js\n * const model = tf.sequential();\n *\n * // First layer must have a defined input shape\n * model.add(tf.layers.dense({units: 32, batchInputShape: [null, 50]}));\n * // Afterwards, TF.js does automatic shape inference.\n * model.add(tf.layers.dense({units: 4}));\n *\n * // Inspect the inferred shape of the model's output.\n * console.log(JSON.stringify(model.outputs[0].shape));\n * ```\n *\n * You can also use an `Array` of already-constructed `Layer`s to create\n * a `tf.Sequential` model:\n *\n * ```js\n * const model = tf.sequential({\n *   layers: [tf.layers.dense({units: 32, inputShape: [50]}),\n *            tf.layers.dense({units: 4})]\n * });\n * console.log(JSON.stringify(model.outputs[0].shape));\n * ```\n *\n * @doc {heading: 'Models', subheading: 'Creation'}\n */\nexport function sequential(config?: SequentialArgs): Sequential {\n  return new Sequential(config);\n}\n\n/**\n * Used to instantiate an input to a model as a `tf.SymbolicTensor`.\n *\n * Users should call the `input` factory function for\n * consistency with other generator functions.\n *\n * Example:\n *\n * ```js\n * // Defines a simple logistic regression model with 32 dimensional input\n * // and 3 dimensional output.\n * const x = tf.input({shape: [32]});\n * const y = tf.layers.dense({units: 3, activation: 'softmax'}).apply(x);\n * const model = tf.model({inputs: x, outputs: y});\n * model.predict(tf.ones([2, 32])).print();\n * ```\n *\n * Note: `input` is only necessary when using `model`. When using\n * `sequential`, specify `inputShape` for the first layer or use `inputLayer`\n * as the first layer.\n *\n * @doc {heading: 'Models', subheading: 'Inputs'}\n */\nexport function input(config: InputConfig): SymbolicTensor {\n  return Input(config);\n}\n\nexport function registerCallbackConstructor(\n    verbosityLevel: number,\n    callbackConstructor: BaseCallbackConstructor): void {\n  CallbackConstructorRegistry.registerCallbackConstructor(\n      verbosityLevel, callbackConstructor);\n}\n"],"mappings":"AAAA;;;;;;;;;AAUA;;;AAIA,SAAiCA,2BAA2B,QAAO,kBAAkB;AAErF,SAAQC,KAAK,QAAqB,sBAAsB;AAExD,SAAQC,WAAW,QAAO,mBAAmB;AAC7C,SAAQC,UAAU,QAAuB,UAAU;AAEnD,SAAQC,eAAe,QAAO,UAAU;AAExC;AACA;AACA;AAEA;AAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAwCA,OAAM,SAAUC,KAAKA,CAACC,IAAmB;EACvC,OAAO,IAAIJ,WAAW,CAACI,IAAI,CAAC;AAC9B;AAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA4DA,OAAM,SAAUC,UAAUA,CAACC,MAAuB;EAChD,OAAO,IAAIL,UAAU,CAACK,MAAM,CAAC;AAC/B;AAEA;;;;;;;;;;;;;;;;;;;;;;;AAuBA,OAAM,SAAUC,KAAKA,CAACD,MAAmB;EACvC,OAAOP,KAAK,CAACO,MAAM,CAAC;AACtB;AAEA,OAAM,SAAUE,2BAA2BA,CACvCC,cAAsB,EACtBC,mBAA4C;EAC9CZ,2BAA2B,CAACU,2BAA2B,CACnDC,cAAc,EAAEC,mBAAmB,CAAC;AAC1C"},"metadata":{},"sourceType":"module","externalDependencies":[]}