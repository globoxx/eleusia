{"ast":null,"code":"import _inherits from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/inherits.js\";\nimport _createSuper from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/createSuper.js\";\nimport _regeneratorRuntime from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/regeneratorRuntime.js\";\nimport _asyncToGenerator from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/asyncToGenerator.js\";\nimport _classCallCheck from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/classCallCheck.js\";\nimport _createClass from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/createClass.js\";\n/**\r\n * @license\r\n * Copyright 2018 Google LLC. All Rights Reserved.\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n * http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n *\r\n * =============================================================================\r\n */\nimport * as tf from '@tensorflow/tfjs-core';\nimport * as seedrandom from 'seedrandom';\nimport { iteratorFromConcatenated, iteratorFromFunction, iteratorFromItems, iteratorFromZipped, ZipMismatchMode } from './iterators/lazy_iterator';\nimport { canTensorify, deepMapAndAwaitAll, isIterable } from './util/deep_map';\n// TODO(soergel): consider vectorized operations within the pipeline.\n/**\r\n * Represents a potentially large list of independent data elements (typically\r\n * 'samples' or 'examples').\r\n *\r\n * A 'data example' may be a primitive, an array, a map from string keys to\r\n * values, or any nested structure of these.\r\n *\r\n * A `Dataset` represents an ordered collection of elements, together with a\r\n * chain of transformations to be performed on those elements. Each\r\n * transformation is a method of `Dataset` that returns another `Dataset`, so\r\n * these may be chained, e.g.\r\n * `const processedDataset = rawDataset.filter(...).map(...).batch(...)`.\r\n *\r\n * Data loading and transformation is done in a lazy, streaming fashion.  The\r\n * dataset may be iterated over multiple times; each iteration starts the data\r\n * loading anew and recapitulates the transformations.\r\n *\r\n * A `Dataset` is typically processed as a stream of unbatched examples -- i.e.,\r\n * its transformations are applied one example at a time. Batching produces a\r\n * new `Dataset` where each element is a batch. Batching should usually come\r\n * last in a pipeline, because data transformations are easier to express on a\r\n * per-example basis than on a per-batch basis.\r\n *\r\n * The following code examples are calling `await dataset.forEachAsync(...)` to\r\n * iterate once over the entire dataset in order to print out the data.\r\n *\r\n * @doc {heading: 'Data', subheading: 'Classes', namespace: 'data'}\r\n */\nexport var Dataset = /*#__PURE__*/function () {\n  function Dataset() {\n    _classCallCheck(this, Dataset);\n    this.size = null;\n  }\n  // TODO(soergel): Make Datasets report whether repeated iterator() calls\n  // produce the same result (e.g., reading from a file) or different results\n  // (e.g., from the webcam).  Currently we don't make this distinction but it\n  // could be important for the user to know.\n  // abstract isDeterministic(): boolean;\n  /**\r\n   * Groups elements into batches.\r\n   *\r\n   * It is assumed that each of the incoming dataset elements has the same\r\n   * structure -- i.e. the same set of keys at each location in an object\r\n   * hierarchy.  For each key, the resulting `Dataset` provides a batched\r\n   * element collecting all of the incoming values for that key.\r\n   *\r\n   *  * Incoming primitives are grouped into a 1-D Tensor.\r\n   *  * Incoming Tensors are grouped into a new Tensor where the 0th axis is\r\n   *    the batch dimension.\r\n   *  * Incoming arrays are converted to Tensor and then batched.\r\n   *  * A nested array is interpreted as an n-D Tensor, so the batched result\r\n   *    has n+1 dimensions.\r\n   *  * An array that cannot be converted to Tensor produces an error.\r\n   *\r\n   * If an array should not be batched as a unit, it should first be converted\r\n   * to an object with integer keys.\r\n   *\r\n   * Here are a few examples:\r\n   *\r\n   * Batch a dataset of numbers:\r\n   * ```js\r\n   * const a = tf.data.array([1, 2, 3, 4, 5, 6, 7, 8]).batch(4);\r\n   * await a.forEachAsync(e => e.print());\r\n   * ```\r\n   *\r\n   * Batch a dataset of arrays:\r\n   * ```js\r\n   * const b = tf.data.array([[1], [2], [3], [4], [5], [6], [7], [8]]).batch(4);\r\n   * await b.forEachAsync(e => e.print());\r\n   * ```\r\n   *\r\n   * Batch a dataset of objects:\r\n   * ```js\r\n   * const c = tf.data.array([{a: 1, b: 11}, {a: 2, b: 12}, {a: 3, b: 13},\r\n   *   {a: 4, b: 14}, {a: 5, b: 15}, {a: 6, b: 16}, {a: 7, b: 17},\r\n   *   {a: 8, b: 18}]).batch(4);\r\n   * await c.forEachAsync(e => {\r\n   *   console.log('{');\r\n   *   for(var key in e) {\r\n   *     console.log(key+':');\r\n   *     e[key].print();\r\n   *   }\r\n   *   console.log('}');\r\n   * })\r\n   * ```\r\n   *\r\n   * @param batchSize The number of elements desired per batch.\r\n   * @param smallLastBatch Whether to emit the final batch when it has fewer\r\n   *   than batchSize elements. Default true.\r\n   * @returns A `Dataset`, from which a stream of batches can be obtained.\r\n   *\r\n   * @doc {heading: 'Data', subheading: 'Classes'}\r\n   */\n  _createClass(Dataset, [{\n    key: \"batch\",\n    value: function batch(batchSize) {\n      var smallLastBatch = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : true;\n      var base = this;\n      tf.util.assert(batchSize > 0, function () {\n        return \"batchSize needs to be positive, but it is\\n      \".concat(batchSize);\n      });\n      var size;\n      if (this.size === Infinity || this.size == null) {\n        // If the size of this dataset is infinity or null, the new size keeps the\n        // same.\n        size = this.size;\n      } else if (smallLastBatch) {\n        // If the size of this dataset is known and include small last batch, the\n        // new size is full batch count plus last batch.\n        size = Math.ceil(this.size / batchSize);\n      } else {\n        // If the size of this dataset is known and not include small last batch,\n        // the new size is full batch count.\n        size = Math.floor(this.size / batchSize);\n      }\n      return datasetFromIteratorFn( /*#__PURE__*/_asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee() {\n        return _regeneratorRuntime().wrap(function _callee$(_context) {\n          while (1) switch (_context.prev = _context.next) {\n            case 0:\n              _context.next = 2;\n              return base.iterator();\n            case 2:\n              return _context.abrupt(\"return\", _context.sent.columnMajorBatch(batchSize, smallLastBatch, deepBatchConcat));\n            case 3:\n            case \"end\":\n              return _context.stop();\n          }\n        }, _callee);\n      })), size);\n    }\n    /**\r\n     * Concatenates this `Dataset` with another.\r\n     *\r\n     * ```js\r\n     * const a = tf.data.array([1, 2, 3]);\r\n     * const b = tf.data.array([4, 5, 6]);\r\n     * const c = a.concatenate(b);\r\n     * await c.forEachAsync(e => console.log(e));\r\n     * ```\r\n     *\r\n     * @param dataset A `Dataset` to be concatenated onto this one.\r\n     * @returns A `Dataset`.\r\n     *\r\n     * @doc {heading: 'Data', subheading: 'Classes'}\r\n     */\n  }, {\n    key: \"concatenate\",\n    value: function concatenate(dataset) {\n      var base = this;\n      var size;\n      if (this.size === Infinity || dataset.size === Infinity) {\n        // If the size of any of these two dataset is infinity, new size is\n        // infinity.\n        size = Infinity;\n      } else if (this.size != null && dataset.size != null) {\n        // If the size of both datasets are known and not infinity, new size is\n        // sum the size of these two datasets.\n        size = this.size + dataset.size;\n      } else {\n        // If neither of these two datasets has infinite size and any of these two\n        // datasets' size is null, the new size is null.\n        size = null;\n      }\n      return datasetFromIteratorFn( /*#__PURE__*/_asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee2() {\n        return _regeneratorRuntime().wrap(function _callee2$(_context2) {\n          while (1) switch (_context2.prev = _context2.next) {\n            case 0:\n              _context2.next = 2;\n              return base.iterator();\n            case 2:\n              _context2.t0 = _context2.sent;\n              _context2.next = 5;\n              return dataset.iterator();\n            case 5:\n              _context2.t1 = _context2.sent;\n              return _context2.abrupt(\"return\", _context2.t0.concatenate.call(_context2.t0, _context2.t1));\n            case 7:\n            case \"end\":\n              return _context2.stop();\n          }\n        }, _callee2);\n      })), size);\n    }\n    /**\r\n     * Filters this dataset according to `predicate`.\r\n     *\r\n     * ```js\r\n     * const a = tf.data.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\r\n     *   .filter(x => x%2 === 0);\r\n     * await a.forEachAsync(e => console.log(e));\r\n     * ```\r\n     *\r\n     * @param predicate A function mapping a dataset element to a boolean or a\r\n     * `Promise` for one.\r\n     *\r\n     * @returns A `Dataset` of elements for which the predicate was true.\r\n     *\r\n     * @doc {heading: 'Data', subheading: 'Classes'}\r\n     */\n  }, {\n    key: \"filter\",\n    value: function filter(predicate) {\n      var base = this;\n      var size;\n      if (this.size === Infinity) {\n        // If the size of this dataset is infinity, new size is infinity\n        size = Infinity;\n      } else {\n        // If this dataset has limited elements, new size is null because it might\n        // exhausted randomly.\n        size = null;\n      }\n      return datasetFromIteratorFn( /*#__PURE__*/_asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee3() {\n        return _regeneratorRuntime().wrap(function _callee3$(_context3) {\n          while (1) switch (_context3.prev = _context3.next) {\n            case 0:\n              _context3.next = 2;\n              return base.iterator();\n            case 2:\n              return _context3.abrupt(\"return\", _context3.sent.filter(function (x) {\n                return tf.tidy(function () {\n                  return predicate(x);\n                });\n              }));\n            case 3:\n            case \"end\":\n              return _context3.stop();\n          }\n        }, _callee3);\n      })), size);\n    }\n    /**\r\n     * Apply a function to every element of the dataset.\r\n     *\r\n     * After the function is applied to a dataset element, any Tensors contained\r\n     * within that element are disposed.\r\n     *\r\n     * ```js\r\n     * const a = tf.data.array([1, 2, 3]);\r\n     * await a.forEachAsync(e => console.log(e));\r\n     * ```\r\n     *\r\n     * @param f A function to apply to each dataset element.\r\n     * @returns A `Promise` that resolves after all elements have been processed.\r\n     *\r\n     * @doc {heading: 'Data', subheading: 'Classes'}\r\n     */\n  }, {\n    key: \"forEachAsync\",\n    value: function () {\n      var _forEachAsync = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee4(f) {\n        return _regeneratorRuntime().wrap(function _callee4$(_context4) {\n          while (1) switch (_context4.prev = _context4.next) {\n            case 0:\n              _context4.next = 2;\n              return this.iterator();\n            case 2:\n              return _context4.abrupt(\"return\", _context4.sent.forEachAsync(f));\n            case 3:\n            case \"end\":\n              return _context4.stop();\n          }\n        }, _callee4, this);\n      }));\n      function forEachAsync(_x) {\n        return _forEachAsync.apply(this, arguments);\n      }\n      return forEachAsync;\n    }()\n    /**\r\n     * Maps this dataset through a 1-to-1 transform.\r\n     *\r\n     * ```js\r\n     * const a = tf.data.array([1, 2, 3]).map(x => x*x);\r\n     * await a.forEachAsync(e => console.log(e));\r\n     * ```\r\n     *\r\n     * @param transform A function mapping a dataset element to a transformed\r\n     *   dataset element.\r\n     *\r\n     * @returns A `Dataset` of transformed elements.\r\n     *\r\n     * @doc {heading: 'Data', subheading: 'Classes'}\r\n     */\n  }, {\n    key: \"map\",\n    value: function map(transform) {\n      var base = this;\n      return datasetFromIteratorFn( /*#__PURE__*/_asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee5() {\n        return _regeneratorRuntime().wrap(function _callee5$(_context5) {\n          while (1) switch (_context5.prev = _context5.next) {\n            case 0:\n              _context5.next = 2;\n              return base.iterator();\n            case 2:\n              return _context5.abrupt(\"return\", _context5.sent.map(function (x) {\n                return tf.tidy(function () {\n                  return transform(x);\n                });\n              }));\n            case 3:\n            case \"end\":\n              return _context5.stop();\n          }\n        }, _callee5);\n      })), this.size);\n    }\n    /**\r\n     * Maps this dataset through an async 1-to-1 transform.\r\n     *\r\n     * ```js\r\n     * const a =\r\n     *  tf.data.array([1, 2, 3]).mapAsync(x => new Promise(function(resolve){\r\n     *    setTimeout(() => {\r\n     *      resolve(x * x);\r\n     *    }, Math.random()*1000 + 500);\r\n     *  }));\r\n     * console.log(await a.toArray());\r\n     * ```\r\n     *\r\n     * @param transform A function mapping a dataset element to a `Promise` for a\r\n     *   transformed dataset element.  This transform is responsible for disposing\r\n     *   any intermediate `Tensor`s, i.e. by wrapping its computation in\r\n     *   `tf.tidy()`; that cannot be automated here (as it is in the synchronous\r\n     *   `map()` case).\r\n     *\r\n     * @returns A `Dataset` of transformed elements.\r\n     *\r\n     * @doc {heading: 'Data', subheading: 'Classes'}\r\n     */\n  }, {\n    key: \"mapAsync\",\n    value: function mapAsync(transform) {\n      var base = this;\n      return datasetFromIteratorFn( /*#__PURE__*/_asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee6() {\n        return _regeneratorRuntime().wrap(function _callee6$(_context6) {\n          while (1) switch (_context6.prev = _context6.next) {\n            case 0:\n              _context6.next = 2;\n              return base.iterator();\n            case 2:\n              return _context6.abrupt(\"return\", _context6.sent.mapAsync(transform));\n            case 3:\n            case \"end\":\n              return _context6.stop();\n          }\n        }, _callee6);\n      })), this.size);\n    }\n    /**\r\n     *  Creates a `Dataset` that prefetches elements from this dataset.\r\n     *\r\n     * @param bufferSize: An integer specifying the number of elements to be\r\n     *   prefetched.\r\n     * @returns A `Dataset`.\r\n     *\r\n     * @doc {heading: 'Data', subheading: 'Classes'}\r\n     */\n  }, {\n    key: \"prefetch\",\n    value: function prefetch(bufferSize) {\n      if (bufferSize == null) {\n        throw new RangeError('`Dataset.prefetch()` requires bufferSize to be specified.');\n      }\n      var base = this;\n      return datasetFromIteratorFn( /*#__PURE__*/_asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee7() {\n        return _regeneratorRuntime().wrap(function _callee7$(_context7) {\n          while (1) switch (_context7.prev = _context7.next) {\n            case 0:\n              _context7.next = 2;\n              return base.iterator();\n            case 2:\n              return _context7.abrupt(\"return\", _context7.sent.prefetch(bufferSize));\n            case 3:\n            case \"end\":\n              return _context7.stop();\n          }\n        }, _callee7);\n      })), this.size);\n    }\n    /**\r\n     * Repeats this dataset `count` times.\r\n     *\r\n     * NOTE: If this dataset is a function of global state (e.g. a random number\r\n     * generator), then different repetitions may produce different elements.\r\n     *\r\n     * ```js\r\n     * const a = tf.data.array([1, 2, 3]).repeat(3);\r\n     * await a.forEachAsync(e => console.log(e));\r\n     * ```\r\n     *\r\n     * @param count: (Optional) An integer, representing the number of times\r\n     *   the dataset should be repeated. The default behavior (if `count` is\r\n     *   `undefined` or negative) is for the dataset be repeated indefinitely.\r\n     * @returns A `Dataset`.\r\n     *\r\n     * @doc {heading: 'Data', subheading: 'Classes'}\r\n     */\n  }, {\n    key: \"repeat\",\n    value: function repeat(count) {\n      var base = this;\n      var size;\n      if (this.size != null && count > 0) {\n        // If this dataset has size and count is positive, new size is current\n        // size multiply count. This also covers the case that current size is\n        // infinity.\n        size = this.size * count;\n      } else if (count === 0) {\n        // If count is 0, new size is 0.\n        size = 0;\n      } else if (this.size != null && (count === undefined || count < 0)) {\n        // If this dataset has size and count is undefined or negative, the\n        // dataset will be repeated indefinitely and new size is infinity.\n        size = Infinity;\n      } else {\n        // If the size of this dataset is null, the new dataset's size is null.\n        size = null;\n      }\n      return datasetFromIteratorFn( /*#__PURE__*/_asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee9() {\n        var iteratorIterator;\n        return _regeneratorRuntime().wrap(function _callee9$(_context9) {\n          while (1) switch (_context9.prev = _context9.next) {\n            case 0:\n              iteratorIterator = iteratorFromFunction( /*#__PURE__*/_asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee8() {\n                return _regeneratorRuntime().wrap(function _callee8$(_context8) {\n                  while (1) switch (_context8.prev = _context8.next) {\n                    case 0:\n                      _context8.next = 2;\n                      return base.iterator();\n                    case 2:\n                      _context8.t0 = _context8.sent;\n                      return _context8.abrupt(\"return\", {\n                        value: _context8.t0,\n                        done: false\n                      });\n                    case 4:\n                    case \"end\":\n                      return _context8.stop();\n                  }\n                }, _callee8);\n              })));\n              return _context9.abrupt(\"return\", iteratorFromConcatenated(iteratorIterator.take(count)));\n            case 2:\n            case \"end\":\n              return _context9.stop();\n          }\n        }, _callee9);\n      })), size);\n    }\n    /**\r\n     * Creates a `Dataset` that skips `count` initial elements from this dataset.\r\n     *\r\n     * ```js\r\n     * const a = tf.data.array([1, 2, 3, 4, 5, 6]).skip(3);\r\n     * await a.forEachAsync(e => console.log(e));\r\n     * ```\r\n     *\r\n     * @param count: The number of elements of this dataset that should be skipped\r\n     *   to form the new dataset.  If `count` is greater than the size of this\r\n     *   dataset, the new dataset will contain no elements.  If `count`\r\n     *   is `undefined` or negative, skips the entire dataset.\r\n     *\r\n     * @returns A `Dataset`.\r\n     *\r\n     * @doc {heading: 'Data', subheading: 'Classes'}\r\n     */\n  }, {\n    key: \"skip\",\n    value: function skip(count) {\n      var base = this;\n      var size;\n      if (this.size != null && count >= 0 && this.size >= count) {\n        // If the size of this dataset is greater than count, the new dataset's\n        // size is current size minus skipped size.This also covers the case that\n        // current size is infinity.\n        size = this.size - count;\n      } else if (this.size != null && (this.size < count || count === undefined || count < 0)) {\n        // If the size of this dataset is smaller than count, or count is\n        // undefined or negative, skips the entire dataset and the new size is 0.\n        size = 0;\n      } else {\n        // If the size of this dataset is null, the new dataset's size is null.\n        size = null;\n      }\n      return datasetFromIteratorFn( /*#__PURE__*/_asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee10() {\n        return _regeneratorRuntime().wrap(function _callee10$(_context10) {\n          while (1) switch (_context10.prev = _context10.next) {\n            case 0:\n              _context10.next = 2;\n              return base.iterator();\n            case 2:\n              return _context10.abrupt(\"return\", _context10.sent.skip(count));\n            case 3:\n            case \"end\":\n              return _context10.stop();\n          }\n        }, _callee10);\n      })), size);\n    }\n    /**\r\n     * Pseudorandomly shuffles the elements of this dataset. This is done in a\r\n     * streaming manner, by sampling from a given number of prefetched elements.\r\n     *\r\n     * ```js\r\n     * const a = tf.data.array([1, 2, 3, 4, 5, 6]).shuffle(3);\r\n     * await a.forEachAsync(e => console.log(e));\r\n     * ```\r\n     *\r\n     * @param bufferSize: An integer specifying the number of elements from this\r\n     *   dataset from which the new dataset will sample.\r\n     * @param seed: (Optional) An integer specifying the random seed that will\r\n     *   be used to create the distribution.\r\n     * @param reshuffleEachIteration: (Optional) A boolean, which if true\r\n     *   indicates that the dataset should be pseudorandomly reshuffled each time\r\n     *   it is iterated over. If false, elements will be returned in the same\r\n     *   shuffled order on each iteration. (Defaults to `true`.)\r\n     * @returns A `Dataset`.\r\n     *\r\n     * @doc {heading: 'Data', subheading: 'Classes'}\r\n     */\n  }, {\n    key: \"shuffle\",\n    value: function shuffle(bufferSize, seed) {\n      var reshuffleEachIteration = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : true;\n      if (bufferSize == null || bufferSize < 0) {\n        if (this.size == null) {\n          throw new RangeError('`Dataset.shuffle()` requires bufferSize to be specified.');\n        } else {\n          throw new RangeError('`Dataset.shuffle()` requires bufferSize to be specified.  ' + 'If your data fits in main memory (for regular JS objects), ' + 'and/or GPU memory (for `tf.Tensor`s), consider setting ' + \"bufferSize to the dataset size (\".concat(this.size, \" elements)\"));\n        }\n      }\n      var base = this;\n      var random = seedrandom.alea(seed || tf.util.now().toString());\n      return datasetFromIteratorFn( /*#__PURE__*/_asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee11() {\n        var seed2;\n        return _regeneratorRuntime().wrap(function _callee11$(_context11) {\n          while (1) switch (_context11.prev = _context11.next) {\n            case 0:\n              seed2 = random.int32();\n              if (reshuffleEachIteration) {\n                seed2 += random.int32();\n              }\n              _context11.next = 4;\n              return base.iterator();\n            case 4:\n              return _context11.abrupt(\"return\", _context11.sent.shuffle(bufferSize, seed2.toString()));\n            case 5:\n            case \"end\":\n              return _context11.stop();\n          }\n        }, _callee11);\n      })), this.size);\n    }\n    /**\r\n     * Creates a `Dataset` with at most `count` initial elements from this\r\n     * dataset.\r\n     *\r\n     * ```js\r\n     * const a = tf.data.array([1, 2, 3, 4, 5, 6]).take(3);\r\n     * await a.forEachAsync(e => console.log(e));\r\n     * ```\r\n     *\r\n     * @param count: The number of elements of this dataset that should be taken\r\n     *   to form the new dataset.  If `count` is `undefined` or negative, or if\r\n     *   `count` is greater than the size of this dataset, the new dataset will\r\n     *   contain all elements of this dataset.\r\n     * @returns A `Dataset`.\r\n     *\r\n     * @doc {heading: 'Data', subheading: 'Classes'}\r\n     */\n  }, {\n    key: \"take\",\n    value: function take(count) {\n      var base = this;\n      var size;\n      if (this.size != null && this.size > count) {\n        // If the size of this dataset is greater than count, the new dataset's\n        // size is count.\n        size = count;\n      } else if (this.size != null && this.size <= count) {\n        // If the size of this dataset is equal or smaller than count, the new\n        // dataset's size is the size of this dataset.\n        size = this.size;\n      } else {\n        // If the size of this dataset is null, the new dataset's size is null.\n        size = null;\n      }\n      return datasetFromIteratorFn( /*#__PURE__*/_asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee12() {\n        return _regeneratorRuntime().wrap(function _callee12$(_context12) {\n          while (1) switch (_context12.prev = _context12.next) {\n            case 0:\n              _context12.next = 2;\n              return base.iterator();\n            case 2:\n              return _context12.abrupt(\"return\", _context12.sent.take(count));\n            case 3:\n            case \"end\":\n              return _context12.stop();\n          }\n        }, _callee12);\n      })), size);\n    }\n    /**\r\n     * Collect all elements of this dataset into an array.\r\n     *\r\n     * Obviously this will succeed only for small datasets that fit in memory.\r\n     * Useful for testing and generally should be avoided if possible.\r\n     *\r\n     * ```js\r\n     * const a = tf.data.array([1, 2, 3, 4, 5, 6]);\r\n     * console.log(await a.toArray());\r\n     * ```\r\n     *\r\n     * @returns A Promise for an array of elements, which will resolve\r\n     *   when a new stream has been obtained and fully consumed.\r\n     *\r\n     * @doc {heading: 'Data', subheading: 'Classes'}\r\n     */\n  }, {\n    key: \"toArray\",\n    value: function () {\n      var _toArray = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee13() {\n        return _regeneratorRuntime().wrap(function _callee13$(_context13) {\n          while (1) switch (_context13.prev = _context13.next) {\n            case 0:\n              if (!(this.size === Infinity)) {\n                _context13.next = 2;\n                break;\n              }\n              throw new Error('Can not convert infinite data stream to array.');\n            case 2:\n              _context13.next = 4;\n              return this.iterator();\n            case 4:\n              return _context13.abrupt(\"return\", _context13.sent.toArray());\n            case 5:\n            case \"end\":\n              return _context13.stop();\n          }\n        }, _callee13, this);\n      }));\n      function toArray() {\n        return _toArray.apply(this, arguments);\n      }\n      return toArray;\n    }()\n    /**\r\n     * Collect all elements of this dataset into an array with prefetching 100\r\n     * elements. This is useful for testing, because the prefetch changes the\r\n     * order in which the Promises are resolved along the processing pipeline.\r\n     * This may help expose bugs where results are dependent on the order of\r\n     * Promise resolution rather than on the logical order of the stream (i.e.,\r\n     * due to hidden mutable state).\r\n     *\r\n     * @returns A Promise for an array of elements, which will resolve\r\n     *   when a new stream has been obtained and fully consumed.\r\n     */\n  }, {\n    key: \"toArrayForTest\",\n    value: function () {\n      var _toArrayForTest = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee14() {\n        return _regeneratorRuntime().wrap(function _callee14$(_context14) {\n          while (1) switch (_context14.prev = _context14.next) {\n            case 0:\n              if (!(this.size === Infinity)) {\n                _context14.next = 2;\n                break;\n              }\n              throw new Error('Can not convert infinite data stream to array.');\n            case 2:\n              _context14.next = 4;\n              return this.iterator();\n            case 4:\n              return _context14.abrupt(\"return\", _context14.sent.toArrayForTest());\n            case 5:\n            case \"end\":\n              return _context14.stop();\n          }\n        }, _callee14, this);\n      }));\n      function toArrayForTest() {\n        return _toArrayForTest.apply(this, arguments);\n      }\n      return toArrayForTest;\n    }()\n  }]);\n  return Dataset;\n}();\n// TODO(soergel): deep sharded shuffle, where supported\nDataset.MAX_BUFFER_SIZE = 10000;\n/**\r\n * Create a `Dataset` defined by a provided iterator() function.\r\n *\r\n * ```js\r\n * let i = -1;\r\n * const func = () =>\r\n *    ++i < 5 ? {value: i, done: false} : {value: null, done: true};\r\n * const iter = tf.data.iteratorFromFunction(func);\r\n * const ds = tf.data.datasetFromIteratorFn(iter);\r\n * await ds.forEachAsync(e => console.log(e));\r\n * ```\r\n */\nexport function datasetFromIteratorFn(iteratorFn) {\n  var size = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : null;\n  return new ( /*#__PURE__*/function (_Dataset) {\n    _inherits(_class, _Dataset);\n    var _super = _createSuper(_class);\n    function _class() {\n      var _this;\n      _classCallCheck(this, _class);\n      _this = _super.apply(this, arguments);\n      _this.size = size;\n      return _this;\n    }\n    /*\r\n     * Provide a new stream of elements.  Note this will also start new streams\r\n     * from any underlying `Dataset`s.\r\n     */\n    _createClass(_class, [{\n      key: \"iterator\",\n      value: function () {\n        var _iterator = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee15() {\n          return _regeneratorRuntime().wrap(function _callee15$(_context15) {\n            while (1) switch (_context15.prev = _context15.next) {\n              case 0:\n                return _context15.abrupt(\"return\", iteratorFn());\n              case 1:\n              case \"end\":\n                return _context15.stop();\n            }\n          }, _callee15);\n        }));\n        function iterator() {\n          return _iterator.apply(this, arguments);\n        }\n        return iterator;\n      }()\n    }]);\n    return _class;\n  }(Dataset))();\n}\n/**\r\n * Create a `Dataset` from an array of elements.\r\n *\r\n * Create a Dataset from an array of objects:\r\n * ```js\r\n * const a = tf.data.array([{'item': 1}, {'item': 2}, {'item': 3}]);\r\n * await a.forEachAsync(e => console.log(e));\r\n * ```\r\n *\r\n * Create a Dataset from an array of numbers:\r\n * ```js\r\n * const a = tf.data.array([4, 5, 6]);\r\n * await a.forEachAsync(e => console.log(e));\r\n * ```\r\n * @param items An array of elements that will be parsed as items in a dataset.\r\n *\r\n * @doc {heading: 'Data', subheading: 'Creation', namespace: 'data'}\r\n */\nexport function array(items) {\n  return datasetFromIteratorFn( /*#__PURE__*/_asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee16() {\n    return _regeneratorRuntime().wrap(function _callee16$(_context16) {\n      while (1) switch (_context16.prev = _context16.next) {\n        case 0:\n          return _context16.abrupt(\"return\", iteratorFromItems(items));\n        case 1:\n        case \"end\":\n          return _context16.stop();\n      }\n    }, _callee16);\n  })), items.length);\n}\n/**\r\n * Create a `Dataset` by zipping together an array, dict, or nested\r\n * structure of `Dataset`s (and perhaps additional constants).\r\n * The underlying datasets must provide elements in a consistent order such that\r\n * they correspond.\r\n *\r\n * The number of elements in the resulting dataset is the same as the size of\r\n * the smallest dataset in datasets.\r\n *\r\n * The nested structure of the `datasets` argument determines the\r\n * structure of elements in the resulting iterator.\r\n *\r\n * Note this means that, given an array of two datasets that produce dict\r\n * elements, the result is a dataset that produces elements that are arrays\r\n * of two dicts:\r\n *\r\n * Zip an array of datasets:\r\n * ```js\r\n * console.log('Zip two datasets of objects:');\r\n * const ds1 = tf.data.array([{a: 1}, {a: 2}, {a: 3}]);\r\n * const ds2 = tf.data.array([{b: 4}, {b: 5}, {b: 6}]);\r\n * const ds3 = tf.data.zip([ds1, ds2]);\r\n * await ds3.forEachAsync(e => console.log(JSON.stringify(e)));\r\n *\r\n * // If the goal is to merge the dicts in order to produce elements like\r\n * // {a: ..., b: ...}, this requires a second step such as:\r\n * console.log('Merge the objects:');\r\n * const ds4 = ds3.map(x => {return {a: x[0].a, b: x[1].b}});\r\n * await ds4.forEachAsync(e => console.log(e));\r\n * ```\r\n *\r\n * Zip a dict of datasets:\r\n * ```js\r\n * const a = tf.data.array([{a: 1}, {a: 2}, {a: 3}]);\r\n * const b = tf.data.array([{b: 4}, {b: 5}, {b: 6}]);\r\n * const c = tf.data.zip({c: a, d: b});\r\n * await c.forEachAsync(e => console.log(JSON.stringify(e)));\r\n * ```\r\n *\r\n * @doc {heading: 'Data', subheading: 'Operations', namespace: 'data'}\r\n */\nexport function zip(datasets) {\n  // manually type-check the argument for JS users\n  if (!isIterable(datasets)) {\n    throw new Error('The argument to zip() must be an object or array.');\n  }\n  var size;\n  if (Array.isArray(datasets)) {\n    for (var i = 0; i < datasets.length; i++) {\n      size = size == null ? datasets[i].size : Math.min(size, datasets[i].size);\n    }\n  } else if (datasets instanceof Object) {\n    for (var ds in datasets) {\n      size = size == null ? datasets[ds].size : Math.min(size, datasets[ds].size);\n    }\n  }\n  return datasetFromIteratorFn( /*#__PURE__*/_asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee17() {\n    var streams;\n    return _regeneratorRuntime().wrap(function _callee17$(_context17) {\n      while (1) switch (_context17.prev = _context17.next) {\n        case 0:\n          _context17.next = 2;\n          return deepMapAndAwaitAll(datasets, function (d) {\n            if (d instanceof Dataset) {\n              return {\n                value: d.iterator(),\n                recurse: false\n              };\n            } else if (isIterable(d)) {\n              return {\n                value: null,\n                recurse: true\n              };\n            } else {\n              throw new Error('Leaves of the structure passed to zip() must be Datasets, ' + 'not primitives.');\n            }\n          });\n        case 2:\n          streams = _context17.sent;\n          return _context17.abrupt(\"return\", iteratorFromZipped(streams, ZipMismatchMode.SHORTEST));\n        case 4:\n        case \"end\":\n          return _context17.stop();\n      }\n    }, _callee17);\n  })), size);\n}\n/**\r\n * A zip function for use with deepZip, passed via the columnMajorBatch call.\r\n *\r\n * Accepts an array of identically-structured nested elements and either batches\r\n * them (if they are primitives, numeric arrays, or Tensors) or requests\r\n * recursion (if not).\r\n */\n// tslint:disable-next-line:no-any\nfunction deepBatchConcat(rows) {\n  if (rows === null) {\n    return null;\n  }\n  // use the first item to decide whether to recurse or batch here.\n  var exampleRow = rows[0];\n  if (canTensorify(exampleRow)) {\n    // rows is an array of primitives, Tensors, or arrays.  Batch them.\n    var value = batchConcat(rows);\n    return {\n      value: value,\n      recurse: false\n    };\n  }\n  // the example row is an object, so recurse into it.\n  return {\n    value: null,\n    recurse: true\n  };\n}\n/**\r\n * Assembles a list of same-shaped numbers, number arrays, or Tensors\r\n * into a single new Tensor where axis 0 is the batch dimension.\r\n */\nfunction batchConcat(arrays) {\n  if (arrays.length === 0) {\n    // We can't return an empty Tensor because we don't know the element shape.\n    throw new Error('Can\\'t make a batch of zero elements.');\n  }\n  if (arrays[0] instanceof tf.Tensor) {\n    // Input is an array of Tensors\n    return tf.stack(arrays);\n  } else {\n    // Input is a possibly-nested array of numbers.\n    return tf.tensor(arrays);\n  }\n}","map":{"version":3,"names":["tf","seedrandom","iteratorFromConcatenated","iteratorFromFunction","iteratorFromItems","iteratorFromZipped","ZipMismatchMode","canTensorify","deepMapAndAwaitAll","isIterable","Dataset","_classCallCheck","size","_createClass","key","value","batch","batchSize","smallLastBatch","arguments","length","undefined","base","util","assert","concat","Infinity","Math","ceil","floor","datasetFromIteratorFn","_asyncToGenerator","_regeneratorRuntime","mark","_callee","wrap","_callee$","_context","prev","next","iterator","abrupt","sent","columnMajorBatch","deepBatchConcat","stop","concatenate","dataset","_callee2","_callee2$","_context2","t0","t1","call","filter","predicate","_callee3","_callee3$","_context3","x","tidy","_forEachAsync","_callee4","f","_callee4$","_context4","forEachAsync","_x","apply","map","transform","_callee5","_callee5$","_context5","mapAsync","_callee6","_callee6$","_context6","prefetch","bufferSize","RangeError","_callee7","_callee7$","_context7","repeat","count","_callee9","iteratorIterator","_callee9$","_context9","_callee8","_callee8$","_context8","done","take","skip","_callee10","_callee10$","_context10","shuffle","seed","reshuffleEachIteration","random","alea","now","toString","_callee11","seed2","_callee11$","_context11","int32","_callee12","_callee12$","_context12","_toArray","_callee13","_callee13$","_context13","Error","toArray","_toArrayForTest","_callee14","_callee14$","_context14","toArrayForTest","MAX_BUFFER_SIZE","iteratorFn","_Dataset","_inherits","_class","_super","_createSuper","_this","_iterator","_callee15","_callee15$","_context15","array","items","_callee16","_callee16$","_context16","zip","datasets","Array","isArray","i","min","Object","ds","_callee17","streams","_callee17$","_context17","d","recurse","SHORTEST","rows","exampleRow","batchConcat","arrays","Tensor","stack","tensor"],"sources":["C:\\Users\\vince\\OneDrive\\Documents\\GitHub\\tfjs-data\\src\\dataset.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * =============================================================================\n */\n\nimport * as tf from '@tensorflow/tfjs-core';\nimport {TensorContainer, TensorLike} from '@tensorflow/tfjs-core';\nimport * as seedrandom from 'seedrandom';\n\nimport {iteratorFromConcatenated, iteratorFromFunction, iteratorFromItems, iteratorFromZipped, LazyIterator, ZipMismatchMode} from './iterators/lazy_iterator';\nimport {Container} from './types';\nimport {canTensorify, deepMapAndAwaitAll, DeepMapResult, isIterable} from './util/deep_map';\n\n/**\n * A nested structure of Datasets, used as the input to zip().\n */\nexport type DatasetContainer = Container<Dataset<TensorContainer>>;\n\n// TODO(soergel): consider vectorized operations within the pipeline.\n\n/**\n * Represents a potentially large list of independent data elements (typically\n * 'samples' or 'examples').\n *\n * A 'data example' may be a primitive, an array, a map from string keys to\n * values, or any nested structure of these.\n *\n * A `Dataset` represents an ordered collection of elements, together with a\n * chain of transformations to be performed on those elements. Each\n * transformation is a method of `Dataset` that returns another `Dataset`, so\n * these may be chained, e.g.\n * `const processedDataset = rawDataset.filter(...).map(...).batch(...)`.\n *\n * Data loading and transformation is done in a lazy, streaming fashion.  The\n * dataset may be iterated over multiple times; each iteration starts the data\n * loading anew and recapitulates the transformations.\n *\n * A `Dataset` is typically processed as a stream of unbatched examples -- i.e.,\n * its transformations are applied one example at a time. Batching produces a\n * new `Dataset` where each element is a batch. Batching should usually come\n * last in a pipeline, because data transformations are easier to express on a\n * per-example basis than on a per-batch basis.\n *\n * The following code examples are calling `await dataset.forEachAsync(...)` to\n * iterate once over the entire dataset in order to print out the data.\n *\n * @doc {heading: 'Data', subheading: 'Classes', namespace: 'data'}\n */\nexport abstract class Dataset<T extends tf.TensorContainer> {\n  /*\n   * Provide a new stream of elements.  Note this will also start new streams\n   * from any underlying `Dataset`s.\n   *\n   * CAUTION: Any Tensors contained within the elements returned from\n   * this stream *must* be manually disposed to avoid a GPU memory leak.\n   * The tf.tidy() approach cannot be used in an asynchronous context.\n   */\n  abstract iterator(): Promise<LazyIterator<T>>;\n\n  readonly size: number = null;\n\n  // TODO(soergel): Make Datasets report whether repeated iterator() calls\n  // produce the same result (e.g., reading from a file) or different results\n  // (e.g., from the webcam).  Currently we don't make this distinction but it\n  // could be important for the user to know.\n  // abstract isDeterministic(): boolean;\n\n  /**\n   * Groups elements into batches.\n   *\n   * It is assumed that each of the incoming dataset elements has the same\n   * structure -- i.e. the same set of keys at each location in an object\n   * hierarchy.  For each key, the resulting `Dataset` provides a batched\n   * element collecting all of the incoming values for that key.\n   *\n   *  * Incoming primitives are grouped into a 1-D Tensor.\n   *  * Incoming Tensors are grouped into a new Tensor where the 0th axis is\n   *    the batch dimension.\n   *  * Incoming arrays are converted to Tensor and then batched.\n   *  * A nested array is interpreted as an n-D Tensor, so the batched result\n   *    has n+1 dimensions.\n   *  * An array that cannot be converted to Tensor produces an error.\n   *\n   * If an array should not be batched as a unit, it should first be converted\n   * to an object with integer keys.\n   *\n   * Here are a few examples:\n   *\n   * Batch a dataset of numbers:\n   * ```js\n   * const a = tf.data.array([1, 2, 3, 4, 5, 6, 7, 8]).batch(4);\n   * await a.forEachAsync(e => e.print());\n   * ```\n   *\n   * Batch a dataset of arrays:\n   * ```js\n   * const b = tf.data.array([[1], [2], [3], [4], [5], [6], [7], [8]]).batch(4);\n   * await b.forEachAsync(e => e.print());\n   * ```\n   *\n   * Batch a dataset of objects:\n   * ```js\n   * const c = tf.data.array([{a: 1, b: 11}, {a: 2, b: 12}, {a: 3, b: 13},\n   *   {a: 4, b: 14}, {a: 5, b: 15}, {a: 6, b: 16}, {a: 7, b: 17},\n   *   {a: 8, b: 18}]).batch(4);\n   * await c.forEachAsync(e => {\n   *   console.log('{');\n   *   for(var key in e) {\n   *     console.log(key+':');\n   *     e[key].print();\n   *   }\n   *   console.log('}');\n   * })\n   * ```\n   *\n   * @param batchSize The number of elements desired per batch.\n   * @param smallLastBatch Whether to emit the final batch when it has fewer\n   *   than batchSize elements. Default true.\n   * @returns A `Dataset`, from which a stream of batches can be obtained.\n   *\n   * @doc {heading: 'Data', subheading: 'Classes'}\n   */\n  batch(batchSize: number, smallLastBatch = true): Dataset<tf.TensorContainer> {\n    const base = this;\n    tf.util.assert(\n        batchSize > 0, () => `batchSize needs to be positive, but it is\n      ${batchSize}`);\n    let size;\n    if (this.size === Infinity || this.size == null) {\n      // If the size of this dataset is infinity or null, the new size keeps the\n      // same.\n      size = this.size;\n    } else if (smallLastBatch) {\n      // If the size of this dataset is known and include small last batch, the\n      // new size is full batch count plus last batch.\n      size = Math.ceil(this.size / batchSize);\n    } else {\n      // If the size of this dataset is known and not include small last batch,\n      // the new size is full batch count.\n      size = Math.floor(this.size / batchSize);\n    }\n    return datasetFromIteratorFn(async () => {\n      return (await base.iterator())\n          .columnMajorBatch(batchSize, smallLastBatch, deepBatchConcat);\n    }, size);\n  }\n\n  /**\n   * Concatenates this `Dataset` with another.\n   *\n   * ```js\n   * const a = tf.data.array([1, 2, 3]);\n   * const b = tf.data.array([4, 5, 6]);\n   * const c = a.concatenate(b);\n   * await c.forEachAsync(e => console.log(e));\n   * ```\n   *\n   * @param dataset A `Dataset` to be concatenated onto this one.\n   * @returns A `Dataset`.\n   *\n   * @doc {heading: 'Data', subheading: 'Classes'}\n   */\n  concatenate(dataset: Dataset<T>): Dataset<T> {\n    const base = this;\n    let size;\n    if (this.size === Infinity || dataset.size === Infinity) {\n      // If the size of any of these two dataset is infinity, new size is\n      // infinity.\n      size = Infinity;\n    } else if (this.size != null && dataset.size != null) {\n      // If the size of both datasets are known and not infinity, new size is\n      // sum the size of these two datasets.\n      size = this.size + dataset.size;\n    } else {\n      // If neither of these two datasets has infinite size and any of these two\n      // datasets' size is null, the new size is null.\n      size = null;\n    }\n    return datasetFromIteratorFn(\n        async () =>\n            (await base.iterator()).concatenate(await dataset.iterator()),\n        size);\n  }\n\n  /**\n   * Filters this dataset according to `predicate`.\n   *\n   * ```js\n   * const a = tf.data.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n   *   .filter(x => x%2 === 0);\n   * await a.forEachAsync(e => console.log(e));\n   * ```\n   *\n   * @param predicate A function mapping a dataset element to a boolean or a\n   * `Promise` for one.\n   *\n   * @returns A `Dataset` of elements for which the predicate was true.\n   *\n   * @doc {heading: 'Data', subheading: 'Classes'}\n   */\n  filter(predicate: (value: T) => boolean): Dataset<T> {\n    const base = this;\n    let size;\n    if (this.size === Infinity) {\n      // If the size of this dataset is infinity, new size is infinity\n      size = Infinity;\n    } else {\n      // If this dataset has limited elements, new size is null because it might\n      // exhausted randomly.\n      size = null;\n    }\n    return datasetFromIteratorFn(async () => {\n      return (await base.iterator()).filter(x => tf.tidy(() => predicate(x)));\n    }, size);\n  }\n\n  /**\n   * Apply a function to every element of the dataset.\n   *\n   * After the function is applied to a dataset element, any Tensors contained\n   * within that element are disposed.\n   *\n   * ```js\n   * const a = tf.data.array([1, 2, 3]);\n   * await a.forEachAsync(e => console.log(e));\n   * ```\n   *\n   * @param f A function to apply to each dataset element.\n   * @returns A `Promise` that resolves after all elements have been processed.\n   *\n   * @doc {heading: 'Data', subheading: 'Classes'}\n   */\n  async forEachAsync(f: (input: T) => void): Promise<void> {\n    return (await this.iterator()).forEachAsync(f);\n  }\n\n  /**\n   * Maps this dataset through a 1-to-1 transform.\n   *\n   * ```js\n   * const a = tf.data.array([1, 2, 3]).map(x => x*x);\n   * await a.forEachAsync(e => console.log(e));\n   * ```\n   *\n   * @param transform A function mapping a dataset element to a transformed\n   *   dataset element.\n   *\n   * @returns A `Dataset` of transformed elements.\n   *\n   * @doc {heading: 'Data', subheading: 'Classes'}\n   */\n  map<O extends tf.TensorContainer>(transform: (value: T) => O): Dataset<O> {\n    const base = this;\n    return datasetFromIteratorFn(async () => {\n      return (await base.iterator()).map(x => tf.tidy(() => transform(x)));\n    }, this.size);\n  }\n\n  /**\n   * Maps this dataset through an async 1-to-1 transform.\n   *\n   * ```js\n   * const a =\n   *  tf.data.array([1, 2, 3]).mapAsync(x => new Promise(function(resolve){\n   *    setTimeout(() => {\n   *      resolve(x * x);\n   *    }, Math.random()*1000 + 500);\n   *  }));\n   * console.log(await a.toArray());\n   * ```\n   *\n   * @param transform A function mapping a dataset element to a `Promise` for a\n   *   transformed dataset element.  This transform is responsible for disposing\n   *   any intermediate `Tensor`s, i.e. by wrapping its computation in\n   *   `tf.tidy()`; that cannot be automated here (as it is in the synchronous\n   *   `map()` case).\n   *\n   * @returns A `Dataset` of transformed elements.\n   *\n   * @doc {heading: 'Data', subheading: 'Classes'}\n   */\n  mapAsync<O extends tf.TensorContainer>(transform: (value: T) => Promise<O>):\n      Dataset<O> {\n    const base = this;\n    return datasetFromIteratorFn(async () => {\n      return (await base.iterator()).mapAsync(transform);\n    }, this.size);\n  }\n\n  /**\n   *  Creates a `Dataset` that prefetches elements from this dataset.\n   *\n   * @param bufferSize: An integer specifying the number of elements to be\n   *   prefetched.\n   * @returns A `Dataset`.\n   *\n   * @doc {heading: 'Data', subheading: 'Classes'}\n   */\n  prefetch(bufferSize: number): Dataset<T> {\n    if (bufferSize == null) {\n      throw new RangeError(\n          '`Dataset.prefetch()` requires bufferSize to be specified.');\n    }\n\n    const base = this;\n    return datasetFromIteratorFn(\n        async () => (await base.iterator()).prefetch(bufferSize), this.size);\n  }\n\n  /**\n   * Repeats this dataset `count` times.\n   *\n   * NOTE: If this dataset is a function of global state (e.g. a random number\n   * generator), then different repetitions may produce different elements.\n   *\n   * ```js\n   * const a = tf.data.array([1, 2, 3]).repeat(3);\n   * await a.forEachAsync(e => console.log(e));\n   * ```\n   *\n   * @param count: (Optional) An integer, representing the number of times\n   *   the dataset should be repeated. The default behavior (if `count` is\n   *   `undefined` or negative) is for the dataset be repeated indefinitely.\n   * @returns A `Dataset`.\n   *\n   * @doc {heading: 'Data', subheading: 'Classes'}\n   */\n  repeat(count?: number): Dataset<T> {\n    const base = this;\n    let size;\n    if (this.size != null && count > 0) {\n      // If this dataset has size and count is positive, new size is current\n      // size multiply count. This also covers the case that current size is\n      // infinity.\n      size = this.size * count;\n    } else if (count === 0) {\n      // If count is 0, new size is 0.\n      size = 0;\n    } else if (this.size != null && (count === undefined || count < 0)) {\n      // If this dataset has size and count is undefined or negative, the\n      // dataset will be repeated indefinitely and new size is infinity.\n      size = Infinity;\n    } else {\n      // If the size of this dataset is null, the new dataset's size is null.\n      size = null;\n    }\n    return datasetFromIteratorFn(async () => {\n      const iteratorIterator = iteratorFromFunction(\n          async () => ({value: await base.iterator(), done: false}));\n      return iteratorFromConcatenated(iteratorIterator.take(count));\n    }, size);\n  }\n\n  /**\n   * Creates a `Dataset` that skips `count` initial elements from this dataset.\n   *\n   * ```js\n   * const a = tf.data.array([1, 2, 3, 4, 5, 6]).skip(3);\n   * await a.forEachAsync(e => console.log(e));\n   * ```\n   *\n   * @param count: The number of elements of this dataset that should be skipped\n   *   to form the new dataset.  If `count` is greater than the size of this\n   *   dataset, the new dataset will contain no elements.  If `count`\n   *   is `undefined` or negative, skips the entire dataset.\n   *\n   * @returns A `Dataset`.\n   *\n   * @doc {heading: 'Data', subheading: 'Classes'}\n   */\n  skip(count: number): Dataset<T> {\n    const base = this;\n    let size;\n    if (this.size != null && count >= 0 && this.size >= count) {\n      // If the size of this dataset is greater than count, the new dataset's\n      // size is current size minus skipped size.This also covers the case that\n      // current size is infinity.\n      size = this.size - count;\n    } else if (\n        this.size != null &&\n        (this.size < count || count === undefined || count < 0)) {\n      // If the size of this dataset is smaller than count, or count is\n      // undefined or negative, skips the entire dataset and the new size is 0.\n      size = 0;\n    } else {\n      // If the size of this dataset is null, the new dataset's size is null.\n      size = null;\n    }\n    return datasetFromIteratorFn(\n        async () => (await base.iterator()).skip(count), size);\n  }\n\n  // TODO(soergel): deep sharded shuffle, where supported\n\n  static readonly MAX_BUFFER_SIZE = 10000;\n\n  /**\n   * Pseudorandomly shuffles the elements of this dataset. This is done in a\n   * streaming manner, by sampling from a given number of prefetched elements.\n   *\n   * ```js\n   * const a = tf.data.array([1, 2, 3, 4, 5, 6]).shuffle(3);\n   * await a.forEachAsync(e => console.log(e));\n   * ```\n   *\n   * @param bufferSize: An integer specifying the number of elements from this\n   *   dataset from which the new dataset will sample.\n   * @param seed: (Optional) An integer specifying the random seed that will\n   *   be used to create the distribution.\n   * @param reshuffleEachIteration: (Optional) A boolean, which if true\n   *   indicates that the dataset should be pseudorandomly reshuffled each time\n   *   it is iterated over. If false, elements will be returned in the same\n   *   shuffled order on each iteration. (Defaults to `true`.)\n   * @returns A `Dataset`.\n   *\n   * @doc {heading: 'Data', subheading: 'Classes'}\n   */\n  shuffle(bufferSize: number, seed?: string, reshuffleEachIteration = true):\n      Dataset<T> {\n    if (bufferSize == null || bufferSize < 0) {\n      if (this.size == null) {\n        throw new RangeError(\n            '`Dataset.shuffle()` requires bufferSize to be specified.');\n      } else {\n        throw new RangeError(\n            '`Dataset.shuffle()` requires bufferSize to be specified.  ' +\n            'If your data fits in main memory (for regular JS objects), ' +\n            'and/or GPU memory (for `tf.Tensor`s), consider setting ' +\n            `bufferSize to the dataset size (${this.size} elements)`);\n      }\n    }\n    const base = this;\n    const random = seedrandom.alea(seed || tf.util.now().toString());\n    return datasetFromIteratorFn(async () => {\n      let seed2 = random.int32();\n      if (reshuffleEachIteration) {\n        seed2 += random.int32();\n      }\n      return (await base.iterator()).shuffle(bufferSize, seed2.toString());\n    }, this.size);\n  }\n\n  /**\n   * Creates a `Dataset` with at most `count` initial elements from this\n   * dataset.\n   *\n   * ```js\n   * const a = tf.data.array([1, 2, 3, 4, 5, 6]).take(3);\n   * await a.forEachAsync(e => console.log(e));\n   * ```\n   *\n   * @param count: The number of elements of this dataset that should be taken\n   *   to form the new dataset.  If `count` is `undefined` or negative, or if\n   *   `count` is greater than the size of this dataset, the new dataset will\n   *   contain all elements of this dataset.\n   * @returns A `Dataset`.\n   *\n   * @doc {heading: 'Data', subheading: 'Classes'}\n   */\n  take(count: number): Dataset<T> {\n    const base = this;\n    let size;\n    if (this.size != null && this.size > count) {\n      // If the size of this dataset is greater than count, the new dataset's\n      // size is count.\n      size = count;\n    } else if (this.size != null && this.size <= count) {\n      // If the size of this dataset is equal or smaller than count, the new\n      // dataset's size is the size of this dataset.\n      size = this.size;\n    } else {\n      // If the size of this dataset is null, the new dataset's size is null.\n      size = null;\n    }\n    return datasetFromIteratorFn(\n        async () => (await base.iterator()).take(count), size);\n  }\n\n  /**\n   * Collect all elements of this dataset into an array.\n   *\n   * Obviously this will succeed only for small datasets that fit in memory.\n   * Useful for testing and generally should be avoided if possible.\n   *\n   * ```js\n   * const a = tf.data.array([1, 2, 3, 4, 5, 6]);\n   * console.log(await a.toArray());\n   * ```\n   *\n   * @returns A Promise for an array of elements, which will resolve\n   *   when a new stream has been obtained and fully consumed.\n   *\n   * @doc {heading: 'Data', subheading: 'Classes'}\n   */\n  async toArray() {\n    if (this.size === Infinity) {\n      throw new Error('Can not convert infinite data stream to array.');\n    }\n    return (await this.iterator()).toArray();\n  }\n\n  /**\n   * Collect all elements of this dataset into an array with prefetching 100\n   * elements. This is useful for testing, because the prefetch changes the\n   * order in which the Promises are resolved along the processing pipeline.\n   * This may help expose bugs where results are dependent on the order of\n   * Promise resolution rather than on the logical order of the stream (i.e.,\n   * due to hidden mutable state).\n   *\n   * @returns A Promise for an array of elements, which will resolve\n   *   when a new stream has been obtained and fully consumed.\n   */\n  async toArrayForTest() {\n    if (this.size === Infinity) {\n      throw new Error('Can not convert infinite data stream to array.');\n    }\n    return (await this.iterator()).toArrayForTest();\n  }\n}\n\n/**\n * Create a `Dataset` defined by a provided iterator() function.\n *\n * ```js\n * let i = -1;\n * const func = () =>\n *    ++i < 5 ? {value: i, done: false} : {value: null, done: true};\n * const iter = tf.data.iteratorFromFunction(func);\n * const ds = tf.data.datasetFromIteratorFn(iter);\n * await ds.forEachAsync(e => console.log(e));\n * ```\n */\nexport function datasetFromIteratorFn<T extends tf.TensorContainer>(\n    iteratorFn: () => Promise<LazyIterator<T>>,\n    size: number = null): Dataset<T> {\n  return new class extends Dataset<T> {\n    override size = size;\n\n    /*\n     * Provide a new stream of elements.  Note this will also start new streams\n     * from any underlying `Dataset`s.\n     */\n    async iterator(): Promise<LazyIterator<T>> {\n      return iteratorFn();\n    }\n  }\n  ();\n}\n\n/**\n * Create a `Dataset` from an array of elements.\n *\n * Create a Dataset from an array of objects:\n * ```js\n * const a = tf.data.array([{'item': 1}, {'item': 2}, {'item': 3}]);\n * await a.forEachAsync(e => console.log(e));\n * ```\n *\n * Create a Dataset from an array of numbers:\n * ```js\n * const a = tf.data.array([4, 5, 6]);\n * await a.forEachAsync(e => console.log(e));\n * ```\n * @param items An array of elements that will be parsed as items in a dataset.\n *\n * @doc {heading: 'Data', subheading: 'Creation', namespace: 'data'}\n */\nexport function array<T extends tf.TensorContainer>(items: T[]): Dataset<T> {\n  return datasetFromIteratorFn(\n      async () => iteratorFromItems(items), items.length);\n}\n\n/**\n * Create a `Dataset` by zipping together an array, dict, or nested\n * structure of `Dataset`s (and perhaps additional constants).\n * The underlying datasets must provide elements in a consistent order such that\n * they correspond.\n *\n * The number of elements in the resulting dataset is the same as the size of\n * the smallest dataset in datasets.\n *\n * The nested structure of the `datasets` argument determines the\n * structure of elements in the resulting iterator.\n *\n * Note this means that, given an array of two datasets that produce dict\n * elements, the result is a dataset that produces elements that are arrays\n * of two dicts:\n *\n * Zip an array of datasets:\n * ```js\n * console.log('Zip two datasets of objects:');\n * const ds1 = tf.data.array([{a: 1}, {a: 2}, {a: 3}]);\n * const ds2 = tf.data.array([{b: 4}, {b: 5}, {b: 6}]);\n * const ds3 = tf.data.zip([ds1, ds2]);\n * await ds3.forEachAsync(e => console.log(JSON.stringify(e)));\n *\n * // If the goal is to merge the dicts in order to produce elements like\n * // {a: ..., b: ...}, this requires a second step such as:\n * console.log('Merge the objects:');\n * const ds4 = ds3.map(x => {return {a: x[0].a, b: x[1].b}});\n * await ds4.forEachAsync(e => console.log(e));\n * ```\n *\n * Zip a dict of datasets:\n * ```js\n * const a = tf.data.array([{a: 1}, {a: 2}, {a: 3}]);\n * const b = tf.data.array([{b: 4}, {b: 5}, {b: 6}]);\n * const c = tf.data.zip({c: a, d: b});\n * await c.forEachAsync(e => console.log(JSON.stringify(e)));\n * ```\n *\n * @doc {heading: 'Data', subheading: 'Operations', namespace: 'data'}\n */\nexport function zip<O extends tf.TensorContainer>(datasets: DatasetContainer):\n    Dataset<O> {\n  // manually type-check the argument for JS users\n  if (!isIterable(datasets)) {\n    throw new Error('The argument to zip() must be an object or array.');\n  }\n  let size;\n  if (Array.isArray(datasets)) {\n    for (let i = 0; i < datasets.length; i++) {\n      size = size == null ? (datasets[i] as Dataset<O>).size :\n                            Math.min(size, (datasets[i] as Dataset<O>).size);\n    }\n  } else if (datasets instanceof Object) {\n    for (const ds in datasets) {\n      size = size == null ? (datasets[ds] as Dataset<O>).size :\n                            Math.min(size, (datasets[ds] as Dataset<O>).size);\n    }\n  }\n  return datasetFromIteratorFn<O>(async () => {\n    const streams = await deepMapAndAwaitAll(datasets, d => {\n      if (d instanceof Dataset) {\n        return {value: d.iterator(), recurse: false};\n      } else if (isIterable(d)) {\n        return {value: null, recurse: true};\n      } else {\n        throw new Error(\n            'Leaves of the structure passed to zip() must be Datasets, ' +\n            'not primitives.');\n      }\n    });\n    return iteratorFromZipped<O>(streams, ZipMismatchMode.SHORTEST);\n  }, size);\n}\n\n/**\n * A zip function for use with deepZip, passed via the columnMajorBatch call.\n *\n * Accepts an array of identically-structured nested elements and either batches\n * them (if they are primitives, numeric arrays, or Tensors) or requests\n * recursion (if not).\n */\n// tslint:disable-next-line:no-any\nfunction deepBatchConcat(rows: any[]): DeepMapResult {\n  if (rows === null) {\n    return null;\n  }\n\n  // use the first item to decide whether to recurse or batch here.\n  const exampleRow = rows[0];\n\n  if (canTensorify(exampleRow)) {\n    // rows is an array of primitives, Tensors, or arrays.  Batch them.\n    const value = batchConcat(rows);\n    return {value, recurse: false};\n  }\n\n  // the example row is an object, so recurse into it.\n  return {value: null, recurse: true};\n}\n\n/**\n * Assembles a list of same-shaped numbers, number arrays, or Tensors\n * into a single new Tensor where axis 0 is the batch dimension.\n */\nfunction batchConcat<T extends(TensorLike | tf.Tensor)>(arrays: T[]):\n    tf.Tensor {\n  if (arrays.length === 0) {\n    // We can't return an empty Tensor because we don't know the element shape.\n    throw new Error('Can\\'t make a batch of zero elements.');\n  }\n\n  if (arrays[0] instanceof tf.Tensor) {\n    // Input is an array of Tensors\n    return tf.stack(arrays as tf.Tensor[]);\n  } else {\n    // Input is a possibly-nested array of numbers.\n    return tf.tensor(arrays as TensorLike);\n  }\n}\n"],"mappings":";;;;;;AAAA;;;;;;;;;;;;;;;;;AAkBA,OAAO,KAAKA,EAAE,MAAM,uBAAuB;AAE3C,OAAO,KAAKC,UAAU,MAAM,YAAY;AAExC,SAAQC,wBAAwB,EAAEC,oBAAoB,EAAEC,iBAAiB,EAAEC,kBAAkB,EAAgBC,eAAe,QAAO,2BAA2B;AAE9J,SAAQC,YAAY,EAAEC,kBAAkB,EAAiBC,UAAU,QAAO,iBAAiB;AAO3F;AAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA4BA,WAAsBC,OAAO;EAA7B,SAAAA,QAAA;IAAAC,eAAA,OAAAD,OAAA;IAWW,KAAAE,IAAI,GAAW,IAAI;EA2c9B;EAzcE;EACA;EACA;EACA;EACA;EAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;EAAAC,YAAA,CAAAH,OAAA;IAAAI,GAAA;IAAAC,KAAA,EAuDA,SAAAC,MAAMC,SAAiB,EAAuB;MAAA,IAArBC,cAAc,GAAAC,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,IAAI;MAC5C,IAAMG,IAAI,GAAG,IAAI;MACjBtB,EAAE,CAACuB,IAAI,CAACC,MAAM,CACVP,SAAS,GAAG,CAAC,EAAE;QAAA,2DAAAQ,MAAA,CACfR,SAAS;MAAA,CAAE,CAAC;MAChB,IAAIL,IAAI;MACR,IAAI,IAAI,CAACA,IAAI,KAAKc,QAAQ,IAAI,IAAI,CAACd,IAAI,IAAI,IAAI,EAAE;QAC/C;QACA;QACAA,IAAI,GAAG,IAAI,CAACA,IAAI;OACjB,MAAM,IAAIM,cAAc,EAAE;QACzB;QACA;QACAN,IAAI,GAAGe,IAAI,CAACC,IAAI,CAAC,IAAI,CAAChB,IAAI,GAAGK,SAAS,CAAC;OACxC,MAAM;QACL;QACA;QACAL,IAAI,GAAGe,IAAI,CAACE,KAAK,CAAC,IAAI,CAACjB,IAAI,GAAGK,SAAS,CAAC;;MAE1C,OAAOa,qBAAqB,eAAAC,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CAAC,SAAAC,QAAA;QAAA,OAAAF,mBAAA,GAAAG,IAAA,UAAAC,SAAAC,QAAA;UAAA,kBAAAA,QAAA,CAAAC,IAAA,GAAAD,QAAA,CAAAE,IAAA;YAAA;cAAAF,QAAA,CAAAE,IAAA;cAAA,OACbjB,IAAI,CAACkB,QAAQ,EAAE;YAAA;cAAA,OAAAH,QAAA,CAAAI,MAAA,WAAAJ,QAAA,CAAAK,IAAA,CACxBC,gBAAgB,CAAC1B,SAAS,EAAEC,cAAc,EAAE0B,eAAe;YAAA;YAAA;cAAA,OAAAP,QAAA,CAAAQ,IAAA;UAAA;QAAA,GAAAX,OAAA;MAAA,CACjE,IAAEtB,IAAI,CAAC;IACV;IAEA;;;;;;;;;;;;;;;EAAA;IAAAE,GAAA;IAAAC,KAAA,EAeA,SAAA+B,YAAYC,OAAmB;MAC7B,IAAMzB,IAAI,GAAG,IAAI;MACjB,IAAIV,IAAI;MACR,IAAI,IAAI,CAACA,IAAI,KAAKc,QAAQ,IAAIqB,OAAO,CAACnC,IAAI,KAAKc,QAAQ,EAAE;QACvD;QACA;QACAd,IAAI,GAAGc,QAAQ;OAChB,MAAM,IAAI,IAAI,CAACd,IAAI,IAAI,IAAI,IAAImC,OAAO,CAACnC,IAAI,IAAI,IAAI,EAAE;QACpD;QACA;QACAA,IAAI,GAAG,IAAI,CAACA,IAAI,GAAGmC,OAAO,CAACnC,IAAI;OAChC,MAAM;QACL;QACA;QACAA,IAAI,GAAG,IAAI;;MAEb,OAAOkB,qBAAqB,eAAAC,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CACxB,SAAAe,SAAA;QAAA,OAAAhB,mBAAA,GAAAG,IAAA,UAAAc,UAAAC,SAAA;UAAA,kBAAAA,SAAA,CAAAZ,IAAA,GAAAY,SAAA,CAAAX,IAAA;YAAA;cAAAW,SAAA,CAAAX,IAAA;cAAA,OACWjB,IAAI,CAACkB,QAAQ,EAAE;YAAA;cAAAU,SAAA,CAAAC,EAAA,GAAAD,SAAA,CAAAR,IAAA;cAAAQ,SAAA,CAAAX,IAAA;cAAA,OAAoBQ,OAAO,CAACP,QAAQ,EAAE;YAAA;cAAAU,SAAA,CAAAE,EAAA,GAAAF,SAAA,CAAAR,IAAA;cAAA,OAAAQ,SAAA,CAAAT,MAAA,WAAAS,SAAA,CAAAC,EAAA,CAApCL,WAAW,CAAAO,IAAA,CAAAH,SAAA,CAAAC,EAAA,EAAAD,SAAA,CAAAE,EAAA;YAAA;YAAA;cAAA,OAAAF,SAAA,CAAAL,IAAA;UAAA;QAAA,GAAAG,QAAA;MAAA,CAA0B,IACjEpC,IAAI,CAAC;IACX;IAEA;;;;;;;;;;;;;;;;EAAA;IAAAE,GAAA;IAAAC,KAAA,EAgBA,SAAAuC,OAAOC,SAAgC;MACrC,IAAMjC,IAAI,GAAG,IAAI;MACjB,IAAIV,IAAI;MACR,IAAI,IAAI,CAACA,IAAI,KAAKc,QAAQ,EAAE;QAC1B;QACAd,IAAI,GAAGc,QAAQ;OAChB,MAAM;QACL;QACA;QACAd,IAAI,GAAG,IAAI;;MAEb,OAAOkB,qBAAqB,eAAAC,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CAAC,SAAAuB,SAAA;QAAA,OAAAxB,mBAAA,GAAAG,IAAA,UAAAsB,UAAAC,SAAA;UAAA,kBAAAA,SAAA,CAAApB,IAAA,GAAAoB,SAAA,CAAAnB,IAAA;YAAA;cAAAmB,SAAA,CAAAnB,IAAA;cAAA,OACbjB,IAAI,CAACkB,QAAQ,EAAE;YAAA;cAAA,OAAAkB,SAAA,CAAAjB,MAAA,WAAAiB,SAAA,CAAAhB,IAAA,CAAEY,MAAM,CAAC,UAAAK,CAAC;gBAAA,OAAI3D,EAAE,CAAC4D,IAAI,CAAC;kBAAA,OAAML,SAAS,CAACI,CAAC,CAAC;gBAAA,EAAC;cAAA;YAAA;YAAA;cAAA,OAAAD,SAAA,CAAAb,IAAA;UAAA;QAAA,GAAAW,QAAA;MAAA,CACvE,IAAE5C,IAAI,CAAC;IACV;IAEA;;;;;;;;;;;;;;;;EAAA;IAAAE,GAAA;IAAAC,KAAA;MAAA,IAAA8C,aAAA,GAAA9B,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CAgBA,SAAA6B,SAAmBC,CAAqB;QAAA,OAAA/B,mBAAA,GAAAG,IAAA,UAAA6B,UAAAC,SAAA;UAAA,kBAAAA,SAAA,CAAA3B,IAAA,GAAA2B,SAAA,CAAA1B,IAAA;YAAA;cAAA0B,SAAA,CAAA1B,IAAA;cAAA,OACxB,IAAI,CAACC,QAAQ,EAAE;YAAA;cAAA,OAAAyB,SAAA,CAAAxB,MAAA,WAAAwB,SAAA,CAAAvB,IAAA,CAAEwB,YAAY,CAACH,CAAC;YAAA;YAAA;cAAA,OAAAE,SAAA,CAAApB,IAAA;UAAA;QAAA,GAAAiB,QAAA;MAAA,CAC9C;MAAA,SAAAI,aAAAC,EAAA;QAAA,OAAAN,aAAA,CAAAO,KAAA,OAAAjD,SAAA;MAAA;MAAA,OAAA+C,YAAA;IAAA;IAED;;;;;;;;;;;;;;;EAAA;IAAApD,GAAA;IAAAC,KAAA,EAeA,SAAAsD,IAAkCC,SAA0B;MAC1D,IAAMhD,IAAI,GAAG,IAAI;MACjB,OAAOQ,qBAAqB,eAAAC,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CAAC,SAAAsC,SAAA;QAAA,OAAAvC,mBAAA,GAAAG,IAAA,UAAAqC,UAAAC,SAAA;UAAA,kBAAAA,SAAA,CAAAnC,IAAA,GAAAmC,SAAA,CAAAlC,IAAA;YAAA;cAAAkC,SAAA,CAAAlC,IAAA;cAAA,OACbjB,IAAI,CAACkB,QAAQ,EAAE;YAAA;cAAA,OAAAiC,SAAA,CAAAhC,MAAA,WAAAgC,SAAA,CAAA/B,IAAA,CAAE2B,GAAG,CAAC,UAAAV,CAAC;gBAAA,OAAI3D,EAAE,CAAC4D,IAAI,CAAC;kBAAA,OAAMU,SAAS,CAACX,CAAC,CAAC;gBAAA,EAAC;cAAA;YAAA;YAAA;cAAA,OAAAc,SAAA,CAAA5B,IAAA;UAAA;QAAA,GAAA0B,QAAA;MAAA,CACpE,IAAE,IAAI,CAAC3D,IAAI,CAAC;IACf;IAEA;;;;;;;;;;;;;;;;;;;;;;;EAAA;IAAAE,GAAA;IAAAC,KAAA,EAuBA,SAAA2D,SAAuCJ,SAAmC;MAExE,IAAMhD,IAAI,GAAG,IAAI;MACjB,OAAOQ,qBAAqB,eAAAC,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CAAC,SAAA0C,SAAA;QAAA,OAAA3C,mBAAA,GAAAG,IAAA,UAAAyC,UAAAC,SAAA;UAAA,kBAAAA,SAAA,CAAAvC,IAAA,GAAAuC,SAAA,CAAAtC,IAAA;YAAA;cAAAsC,SAAA,CAAAtC,IAAA;cAAA,OACbjB,IAAI,CAACkB,QAAQ,EAAE;YAAA;cAAA,OAAAqC,SAAA,CAAApC,MAAA,WAAAoC,SAAA,CAAAnC,IAAA,CAAEgC,QAAQ,CAACJ,SAAS;YAAA;YAAA;cAAA,OAAAO,SAAA,CAAAhC,IAAA;UAAA;QAAA,GAAA8B,QAAA;MAAA,CAClD,IAAE,IAAI,CAAC/D,IAAI,CAAC;IACf;IAEA;;;;;;;;;EAAA;IAAAE,GAAA;IAAAC,KAAA,EASA,SAAA+D,SAASC,UAAkB;MACzB,IAAIA,UAAU,IAAI,IAAI,EAAE;QACtB,MAAM,IAAIC,UAAU,CAChB,2DAA2D,CAAC;;MAGlE,IAAM1D,IAAI,GAAG,IAAI;MACjB,OAAOQ,qBAAqB,eAAAC,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CACxB,SAAAgD,SAAA;QAAA,OAAAjD,mBAAA,GAAAG,IAAA,UAAA+C,UAAAC,SAAA;UAAA,kBAAAA,SAAA,CAAA7C,IAAA,GAAA6C,SAAA,CAAA5C,IAAA;YAAA;cAAA4C,SAAA,CAAA5C,IAAA;cAAA,OAAmBjB,IAAI,CAACkB,QAAQ,EAAE;YAAA;cAAA,OAAA2C,SAAA,CAAA1C,MAAA,WAAA0C,SAAA,CAAAzC,IAAA,CAAEoC,QAAQ,CAACC,UAAU;YAAA;YAAA;cAAA,OAAAI,SAAA,CAAAtC,IAAA;UAAA;QAAA,GAAAoC,QAAA;MAAA,CAAC,IAAE,IAAI,CAACrE,IAAI,CAAC;IAC1E;IAEA;;;;;;;;;;;;;;;;;;EAAA;IAAAE,GAAA;IAAAC,KAAA,EAkBA,SAAAqE,OAAOC,KAAc;MACnB,IAAM/D,IAAI,GAAG,IAAI;MACjB,IAAIV,IAAI;MACR,IAAI,IAAI,CAACA,IAAI,IAAI,IAAI,IAAIyE,KAAK,GAAG,CAAC,EAAE;QAClC;QACA;QACA;QACAzE,IAAI,GAAG,IAAI,CAACA,IAAI,GAAGyE,KAAK;OACzB,MAAM,IAAIA,KAAK,KAAK,CAAC,EAAE;QACtB;QACAzE,IAAI,GAAG,CAAC;OACT,MAAM,IAAI,IAAI,CAACA,IAAI,IAAI,IAAI,KAAKyE,KAAK,KAAKhE,SAAS,IAAIgE,KAAK,GAAG,CAAC,CAAC,EAAE;QAClE;QACA;QACAzE,IAAI,GAAGc,QAAQ;OAChB,MAAM;QACL;QACAd,IAAI,GAAG,IAAI;;MAEb,OAAOkB,qBAAqB,eAAAC,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CAAC,SAAAqD,SAAA;QAAA,IAAAC,gBAAA;QAAA,OAAAvD,mBAAA,GAAAG,IAAA,UAAAqD,UAAAC,SAAA;UAAA,kBAAAA,SAAA,CAAAnD,IAAA,GAAAmD,SAAA,CAAAlD,IAAA;YAAA;cACrBgD,gBAAgB,GAAGpF,oBAAoB,eAAA4B,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CACzC,SAAAyD,SAAA;gBAAA,OAAA1D,mBAAA,GAAAG,IAAA,UAAAwD,UAAAC,SAAA;kBAAA,kBAAAA,SAAA,CAAAtD,IAAA,GAAAsD,SAAA,CAAArD,IAAA;oBAAA;sBAAAqD,SAAA,CAAArD,IAAA;sBAAA,OAA2BjB,IAAI,CAACkB,QAAQ,EAAE;oBAAA;sBAAAoD,SAAA,CAAAzC,EAAA,GAAAyC,SAAA,CAAAlD,IAAA;sBAAA,OAAAkD,SAAA,CAAAnD,MAAA;wBAA5B1B,KAAK,EAAA6E,SAAA,CAAAzC,EAAA;wBAAyB0C,IAAI,EAAE;sBAAK;oBAAA;oBAAA;sBAAA,OAAAD,SAAA,CAAA/C,IAAA;kBAAA;gBAAA,GAAA6C,QAAA;cAAA,CAAE,GAAC;cAAA,OAAAD,SAAA,CAAAhD,MAAA,WACvDvC,wBAAwB,CAACqF,gBAAgB,CAACO,IAAI,CAACT,KAAK,CAAC,CAAC;YAAA;YAAA;cAAA,OAAAI,SAAA,CAAA5C,IAAA;UAAA;QAAA,GAAAyC,QAAA;MAAA,CAC9D,IAAE1E,IAAI,CAAC;IACV;IAEA;;;;;;;;;;;;;;;;;EAAA;IAAAE,GAAA;IAAAC,KAAA,EAiBA,SAAAgF,KAAKV,KAAa;MAChB,IAAM/D,IAAI,GAAG,IAAI;MACjB,IAAIV,IAAI;MACR,IAAI,IAAI,CAACA,IAAI,IAAI,IAAI,IAAIyE,KAAK,IAAI,CAAC,IAAI,IAAI,CAACzE,IAAI,IAAIyE,KAAK,EAAE;QACzD;QACA;QACA;QACAzE,IAAI,GAAG,IAAI,CAACA,IAAI,GAAGyE,KAAK;OACzB,MAAM,IACH,IAAI,CAACzE,IAAI,IAAI,IAAI,KAChB,IAAI,CAACA,IAAI,GAAGyE,KAAK,IAAIA,KAAK,KAAKhE,SAAS,IAAIgE,KAAK,GAAG,CAAC,CAAC,EAAE;QAC3D;QACA;QACAzE,IAAI,GAAG,CAAC;OACT,MAAM;QACL;QACAA,IAAI,GAAG,IAAI;;MAEb,OAAOkB,qBAAqB,eAAAC,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CACxB,SAAA+D,UAAA;QAAA,OAAAhE,mBAAA,GAAAG,IAAA,UAAA8D,WAAAC,UAAA;UAAA,kBAAAA,UAAA,CAAA5D,IAAA,GAAA4D,UAAA,CAAA3D,IAAA;YAAA;cAAA2D,UAAA,CAAA3D,IAAA;cAAA,OAAmBjB,IAAI,CAACkB,QAAQ,EAAE;YAAA;cAAA,OAAA0D,UAAA,CAAAzD,MAAA,WAAAyD,UAAA,CAAAxD,IAAA,CAAEqD,IAAI,CAACV,KAAK;YAAA;YAAA;cAAA,OAAAa,UAAA,CAAArD,IAAA;UAAA;QAAA,GAAAmD,SAAA;MAAA,CAAC,IAAEpF,IAAI,CAAC;IAC5D;IAMA;;;;;;;;;;;;;;;;;;;;;EAAA;IAAAE,GAAA;IAAAC,KAAA,EAqBA,SAAAoF,QAAQpB,UAAkB,EAAEqB,IAAa,EAA+B;MAAA,IAA7BC,sBAAsB,GAAAlF,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,IAAI;MAEtE,IAAI4D,UAAU,IAAI,IAAI,IAAIA,UAAU,GAAG,CAAC,EAAE;QACxC,IAAI,IAAI,CAACnE,IAAI,IAAI,IAAI,EAAE;UACrB,MAAM,IAAIoE,UAAU,CAChB,0DAA0D,CAAC;SAChE,MAAM;UACL,MAAM,IAAIA,UAAU,CAChB,4DAA4D,GAC5D,6DAA6D,GAC7D,yDAAyD,sCAAAvD,MAAA,CACtB,IAAI,CAACb,IAAI,eAAY,CAAC;;;MAGjE,IAAMU,IAAI,GAAG,IAAI;MACjB,IAAMgF,MAAM,GAAGrG,UAAU,CAACsG,IAAI,CAACH,IAAI,IAAIpG,EAAE,CAACuB,IAAI,CAACiF,GAAG,EAAE,CAACC,QAAQ,EAAE,CAAC;MAChE,OAAO3E,qBAAqB,eAAAC,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CAAC,SAAAyE,UAAA;QAAA,IAAAC,KAAA;QAAA,OAAA3E,mBAAA,GAAAG,IAAA,UAAAyE,WAAAC,UAAA;UAAA,kBAAAA,UAAA,CAAAvE,IAAA,GAAAuE,UAAA,CAAAtE,IAAA;YAAA;cACvBoE,KAAK,GAAGL,MAAM,CAACQ,KAAK,EAAE;cAC1B,IAAIT,sBAAsB,EAAE;gBAC1BM,KAAK,IAAIL,MAAM,CAACQ,KAAK,EAAE;;cACxBD,UAAA,CAAAtE,IAAA;cAAA,OACajB,IAAI,CAACkB,QAAQ,EAAE;YAAA;cAAA,OAAAqE,UAAA,CAAApE,MAAA,WAAAoE,UAAA,CAAAnE,IAAA,CAAEyD,OAAO,CAACpB,UAAU,EAAE4B,KAAK,CAACF,QAAQ,EAAE;YAAA;YAAA;cAAA,OAAAI,UAAA,CAAAhE,IAAA;UAAA;QAAA,GAAA6D,SAAA;MAAA,CACpE,IAAE,IAAI,CAAC9F,IAAI,CAAC;IACf;IAEA;;;;;;;;;;;;;;;;;EAAA;IAAAE,GAAA;IAAAC,KAAA,EAiBA,SAAA+E,KAAKT,KAAa;MAChB,IAAM/D,IAAI,GAAG,IAAI;MACjB,IAAIV,IAAI;MACR,IAAI,IAAI,CAACA,IAAI,IAAI,IAAI,IAAI,IAAI,CAACA,IAAI,GAAGyE,KAAK,EAAE;QAC1C;QACA;QACAzE,IAAI,GAAGyE,KAAK;OACb,MAAM,IAAI,IAAI,CAACzE,IAAI,IAAI,IAAI,IAAI,IAAI,CAACA,IAAI,IAAIyE,KAAK,EAAE;QAClD;QACA;QACAzE,IAAI,GAAG,IAAI,CAACA,IAAI;OACjB,MAAM;QACL;QACAA,IAAI,GAAG,IAAI;;MAEb,OAAOkB,qBAAqB,eAAAC,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CACxB,SAAA8E,UAAA;QAAA,OAAA/E,mBAAA,GAAAG,IAAA,UAAA6E,WAAAC,UAAA;UAAA,kBAAAA,UAAA,CAAA3E,IAAA,GAAA2E,UAAA,CAAA1E,IAAA;YAAA;cAAA0E,UAAA,CAAA1E,IAAA;cAAA,OAAmBjB,IAAI,CAACkB,QAAQ,EAAE;YAAA;cAAA,OAAAyE,UAAA,CAAAxE,MAAA,WAAAwE,UAAA,CAAAvE,IAAA,CAAEoD,IAAI,CAACT,KAAK;YAAA;YAAA;cAAA,OAAA4B,UAAA,CAAApE,IAAA;UAAA;QAAA,GAAAkE,SAAA;MAAA,CAAC,IAAEnG,IAAI,CAAC;IAC5D;IAEA;;;;;;;;;;;;;;;;EAAA;IAAAE,GAAA;IAAAC,KAAA;MAAA,IAAAmG,QAAA,GAAAnF,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CAgBA,SAAAkF,UAAA;QAAA,OAAAnF,mBAAA,GAAAG,IAAA,UAAAiF,WAAAC,UAAA;UAAA,kBAAAA,UAAA,CAAA/E,IAAA,GAAA+E,UAAA,CAAA9E,IAAA;YAAA;cAAA,MACM,IAAI,CAAC3B,IAAI,KAAKc,QAAQ;gBAAA2F,UAAA,CAAA9E,IAAA;gBAAA;cAAA;cAAA,MAClB,IAAI+E,KAAK,CAAC,gDAAgD,CAAC;YAAA;cAAAD,UAAA,CAAA9E,IAAA;cAAA,OAErD,IAAI,CAACC,QAAQ,EAAE;YAAA;cAAA,OAAA6E,UAAA,CAAA5E,MAAA,WAAA4E,UAAA,CAAA3E,IAAA,CAAE6E,OAAO;YAAA;YAAA;cAAA,OAAAF,UAAA,CAAAxE,IAAA;UAAA;QAAA,GAAAsE,SAAA;MAAA,CACvC;MAAA,SAAAI,QAAA;QAAA,OAAAL,QAAA,CAAA9C,KAAA,OAAAjD,SAAA;MAAA;MAAA,OAAAoG,OAAA;IAAA;IAED;;;;;;;;;;;EAAA;IAAAzG,GAAA;IAAAC,KAAA;MAAA,IAAAyG,eAAA,GAAAzF,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CAWA,SAAAwF,UAAA;QAAA,OAAAzF,mBAAA,GAAAG,IAAA,UAAAuF,WAAAC,UAAA;UAAA,kBAAAA,UAAA,CAAArF,IAAA,GAAAqF,UAAA,CAAApF,IAAA;YAAA;cAAA,MACM,IAAI,CAAC3B,IAAI,KAAKc,QAAQ;gBAAAiG,UAAA,CAAApF,IAAA;gBAAA;cAAA;cAAA,MAClB,IAAI+E,KAAK,CAAC,gDAAgD,CAAC;YAAA;cAAAK,UAAA,CAAApF,IAAA;cAAA,OAErD,IAAI,CAACC,QAAQ,EAAE;YAAA;cAAA,OAAAmF,UAAA,CAAAlF,MAAA,WAAAkF,UAAA,CAAAjF,IAAA,CAAEkF,cAAc;YAAA;YAAA;cAAA,OAAAD,UAAA,CAAA9E,IAAA;UAAA;QAAA,GAAA4E,SAAA;MAAA,CAC9C;MAAA,SAAAG,eAAA;QAAA,OAAAJ,eAAA,CAAApD,KAAA,OAAAjD,SAAA;MAAA;MAAA,OAAAyG,cAAA;IAAA;EAAA;EAAA,OAAAlH,OAAA;AAAA;AA7HD;AAEgBA,OAAA,CAAAmH,eAAe,GAAG,KAAK;AA8HzC;;;;;;;;;;;;AAYA,OAAM,SAAU/F,qBAAqBA,CACjCgG,UAA0C,EACvB;EAAA,IAAnBlH,IAAA,GAAAO,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAe,IAAI;EACrB,OAAO,6BAAA4G,QAAA;IAAAC,SAAA,CAAAC,MAAA,EAAAF,QAAA;IAAA,IAAAG,MAAA,GAAAC,YAAA,CAAAF,MAAA;IAAI,SAAAA,OAAA;MAAA,IAAAG,KAAA;MAAAzH,eAAA,OAAAsH,MAAA;;MACAG,KAAA,CAAAxH,IAAI,GAAGA,IAAI;MAAC,OAAAwH,KAAA;IASvB;IAPE;;;;IAAAvH,YAAA,CAAAoH,MAAA;MAAAnH,GAAA;MAAAC,KAAA;QAAA,IAAAsH,SAAA,GAAAtG,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CAIA,SAAAqG,UAAA;UAAA,OAAAtG,mBAAA,GAAAG,IAAA,UAAAoG,WAAAC,UAAA;YAAA,kBAAAA,UAAA,CAAAlG,IAAA,GAAAkG,UAAA,CAAAjG,IAAA;cAAA;gBAAA,OAAAiG,UAAA,CAAA/F,MAAA,WACSqF,UAAU,EAAE;cAAA;cAAA;gBAAA,OAAAU,UAAA,CAAA3F,IAAA;YAAA;UAAA,GAAAyF,SAAA;QAAA,CACpB;QAAA,SAAA9F,SAAA;UAAA,OAAA6F,SAAA,CAAAjE,KAAA,OAAAjD,SAAA;QAAA;QAAA,OAAAqB,QAAA;MAAA;IAAA;IAAA,OAAAyF,MAAA;EAAA,EATsBvH,OAAU,IAWjC;AACJ;AAEA;;;;;;;;;;;;;;;;;;AAkBA,OAAM,SAAU+H,KAAKA,CAA+BC,KAAU;EAC5D,OAAO5G,qBAAqB,eAAAC,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CACxB,SAAA0G,UAAA;IAAA,OAAA3G,mBAAA,GAAAG,IAAA,UAAAyG,WAAAC,UAAA;MAAA,kBAAAA,UAAA,CAAAvG,IAAA,GAAAuG,UAAA,CAAAtG,IAAA;QAAA;UAAA,OAAAsG,UAAA,CAAApG,MAAA,WAAYrC,iBAAiB,CAACsI,KAAK,CAAC;QAAA;QAAA;UAAA,OAAAG,UAAA,CAAAhG,IAAA;MAAA;IAAA,GAAA8F,SAAA;EAAA,KAAED,KAAK,CAACtH,MAAM,CAAC;AACzD;AAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAyCA,OAAM,SAAU0H,GAAGA,CAA+BC,QAA0B;EAE1E;EACA,IAAI,CAACtI,UAAU,CAACsI,QAAQ,CAAC,EAAE;IACzB,MAAM,IAAIzB,KAAK,CAAC,mDAAmD,CAAC;;EAEtE,IAAI1G,IAAI;EACR,IAAIoI,KAAK,CAACC,OAAO,CAACF,QAAQ,CAAC,EAAE;IAC3B,KAAK,IAAIG,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGH,QAAQ,CAAC3H,MAAM,EAAE8H,CAAC,EAAE,EAAE;MACxCtI,IAAI,GAAGA,IAAI,IAAI,IAAI,GAAImI,QAAQ,CAACG,CAAC,CAAgB,CAACtI,IAAI,GAChCe,IAAI,CAACwH,GAAG,CAACvI,IAAI,EAAGmI,QAAQ,CAACG,CAAC,CAAgB,CAACtI,IAAI,CAAC;;GAEzE,MAAM,IAAImI,QAAQ,YAAYK,MAAM,EAAE;IACrC,KAAK,IAAMC,EAAE,IAAIN,QAAQ,EAAE;MACzBnI,IAAI,GAAGA,IAAI,IAAI,IAAI,GAAImI,QAAQ,CAACM,EAAE,CAAgB,CAACzI,IAAI,GACjCe,IAAI,CAACwH,GAAG,CAACvI,IAAI,EAAGmI,QAAQ,CAACM,EAAE,CAAgB,CAACzI,IAAI,CAAC;;;EAG3E,OAAOkB,qBAAqB,eAAAC,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CAAI,SAAAqH,UAAA;IAAA,IAAAC,OAAA;IAAA,OAAAvH,mBAAA,GAAAG,IAAA,UAAAqH,WAAAC,UAAA;MAAA,kBAAAA,UAAA,CAAAnH,IAAA,GAAAmH,UAAA,CAAAlH,IAAA;QAAA;UAAAkH,UAAA,CAAAlH,IAAA;UAAA,OACR/B,kBAAkB,CAACuI,QAAQ,EAAE,UAAAW,CAAC,EAAG;YACrD,IAAIA,CAAC,YAAYhJ,OAAO,EAAE;cACxB,OAAO;gBAACK,KAAK,EAAE2I,CAAC,CAAClH,QAAQ,EAAE;gBAAEmH,OAAO,EAAE;cAAK,CAAC;aAC7C,MAAM,IAAIlJ,UAAU,CAACiJ,CAAC,CAAC,EAAE;cACxB,OAAO;gBAAC3I,KAAK,EAAE,IAAI;gBAAE4I,OAAO,EAAE;cAAI,CAAC;aACpC,MAAM;cACL,MAAM,IAAIrC,KAAK,CACX,4DAA4D,GAC5D,iBAAiB,CAAC;;UAE1B,CAAC,CAAC;QAAA;UAVIiC,OAAO,GAAAE,UAAA,CAAA/G,IAAA;UAAA,OAAA+G,UAAA,CAAAhH,MAAA,WAWNpC,kBAAkB,CAAIkJ,OAAO,EAAEjJ,eAAe,CAACsJ,QAAQ,CAAC;QAAA;QAAA;UAAA,OAAAH,UAAA,CAAA5G,IAAA;MAAA;IAAA,GAAAyG,SAAA;EAAA,CAChE,IAAE1I,IAAI,CAAC;AACV;AAEA;;;;;;;AAOA;AACA,SAASgC,eAAeA,CAACiH,IAAW;EAClC,IAAIA,IAAI,KAAK,IAAI,EAAE;IACjB,OAAO,IAAI;;EAGb;EACA,IAAMC,UAAU,GAAGD,IAAI,CAAC,CAAC,CAAC;EAE1B,IAAItJ,YAAY,CAACuJ,UAAU,CAAC,EAAE;IAC5B;IACA,IAAM/I,KAAK,GAAGgJ,WAAW,CAACF,IAAI,CAAC;IAC/B,OAAO;MAAC9I,KAAK,EAALA,KAAK;MAAE4I,OAAO,EAAE;IAAK,CAAC;;EAGhC;EACA,OAAO;IAAC5I,KAAK,EAAE,IAAI;IAAE4I,OAAO,EAAE;EAAI,CAAC;AACrC;AAEA;;;;AAIA,SAASI,WAAWA,CAAoCC,MAAW;EAEjE,IAAIA,MAAM,CAAC5I,MAAM,KAAK,CAAC,EAAE;IACvB;IACA,MAAM,IAAIkG,KAAK,CAAC,uCAAuC,CAAC;;EAG1D,IAAI0C,MAAM,CAAC,CAAC,CAAC,YAAYhK,EAAE,CAACiK,MAAM,EAAE;IAClC;IACA,OAAOjK,EAAE,CAACkK,KAAK,CAACF,MAAqB,CAAC;GACvC,MAAM;IACL;IACA,OAAOhK,EAAE,CAACmK,MAAM,CAACH,MAAoB,CAAC;;AAE1C"},"metadata":{},"sourceType":"module","externalDependencies":[]}