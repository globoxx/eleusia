{"ast":null,"code":"import _toConsumableArray from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/toConsumableArray.js\";\nimport _classCallCheck from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/classCallCheck.js\";\nimport _createClass from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/createClass.js\";\nimport _get from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/get.js\";\nimport _getPrototypeOf from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/getPrototypeOf.js\";\nimport _inherits from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/inherits.js\";\nimport _createSuper from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/createSuper.js\";\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * Layers that augment the functionality of a base layer.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy } from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport { nameScope } from '../common';\nimport { InputSpec, Layer, SymbolicTensor } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { VALID_BIDIRECTIONAL_MERGE_MODES } from '../keras_format/common';\nimport * as generic_utils from '../utils/generic_utils';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\nimport { rnn, standardizeArgs } from './recurrent';\nimport { deserialize } from './serialization';\n/**\n * Abstract wrapper base class.\n *\n * Wrappers take another layer and augment it in various ways.\n * Do not use this class as a layer, it is only an abstract base class.\n * Two usable wrappers are the `TimeDistributed` and `Bidirectional` wrappers.\n */\nexport var Wrapper = /*#__PURE__*/function (_Layer) {\n  _inherits(Wrapper, _Layer);\n  var _super = _createSuper(Wrapper);\n  function Wrapper(args) {\n    var _this;\n    _classCallCheck(this, Wrapper);\n    // Porting Note: In PyKeras, `self.layer` is set prior to the calling\n    //   `super()`. But we can't do that here due to TypeScript's restriction.\n    //   See: https://github.com/Microsoft/TypeScript/issues/8277\n    //   As a result, we have to add checks in `get trainable()` and\n    //   `set trainable()` below in order to prevent using `this.layer` when\n    //   its value is `undefined`. The super constructor does use the getter\n    //   and the setter of `this.layer`.\n    _this = _super.call(this, args);\n    _this.layer = args.layer;\n    return _this;\n  }\n  _createClass(Wrapper, [{\n    key: \"build\",\n    value: function build(inputShape) {\n      this.built = true;\n    }\n    // TODO(cais): Implement activityRegularizer getter.\n  }, {\n    key: \"trainable\",\n    get: function get() {\n      // Porting Note: the check of `this.layer` here is necessary due to the\n      //   way the `constructor` of this class is written (see Porting Note\n      //   above).\n      if (this.layer != null) {\n        return this.layer.trainable;\n      } else {\n        return false;\n      }\n    },\n    set: function set(value) {\n      // Porting Note: the check of `this.layer` here is necessary due to the\n      //   way the `constructor` of this class is written (see Porting Note\n      //   above).\n      if (this.layer != null) {\n        this.layer.trainable = value;\n      }\n    }\n  }, {\n    key: \"trainableWeights\",\n    get: function get() {\n      return this.layer.trainableWeights;\n    }\n    // TODO(cais): Implement setter for trainableWeights.\n  }, {\n    key: \"nonTrainableWeights\",\n    get: function get() {\n      return this.layer.nonTrainableWeights;\n    }\n    // TODO(cais): Implement setter for nonTrainableWeights.\n  }, {\n    key: \"updates\",\n    get: function get() {\n      // tslint:disable-next-line:no-any\n      return this.layer._updates;\n    }\n    // TODO(cais): Implement getUpdatesFor().\n  }, {\n    key: \"losses\",\n    get: function get() {\n      return this.layer.losses;\n    }\n    // TODO(cais): Implement getLossesFor().\n  }, {\n    key: \"getWeights\",\n    value: function getWeights() {\n      return this.layer.getWeights();\n    }\n  }, {\n    key: \"setWeights\",\n    value: function setWeights(weights) {\n      this.layer.setWeights(weights);\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        'layer': {\n          'className': this.layer.getClassName(),\n          'config': this.layer.getConfig()\n        }\n      };\n      var baseConfig = _get(_getPrototypeOf(Wrapper.prototype), \"getConfig\", this).call(this);\n      Object.assign(config, baseConfig);\n      return config;\n    }\n  }, {\n    key: \"setFastWeightInitDuringBuild\",\n    value: function setFastWeightInitDuringBuild(value) {\n      _get(_getPrototypeOf(Wrapper.prototype), \"setFastWeightInitDuringBuild\", this).call(this, value);\n      if (this.layer != null) {\n        this.layer.setFastWeightInitDuringBuild(value);\n      }\n    }\n    /** @nocollapse */\n  }], [{\n    key: \"fromConfig\",\n    value: function fromConfig(cls, config) {\n      var customObjects = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n      var layerConfig = config['layer'];\n      var layer = deserialize(layerConfig, customObjects);\n      delete config['layer'];\n      var newConfig = {\n        layer: layer\n      };\n      Object.assign(newConfig, config);\n      return new cls(newConfig);\n    }\n  }]);\n  return Wrapper;\n}(Layer);\nexport var TimeDistributed = /*#__PURE__*/function (_Wrapper) {\n  _inherits(TimeDistributed, _Wrapper);\n  var _super2 = _createSuper(TimeDistributed);\n  function TimeDistributed(args) {\n    var _this2;\n    _classCallCheck(this, TimeDistributed);\n    _this2 = _super2.call(this, args);\n    _this2.supportsMasking = true;\n    return _this2;\n  }\n  _createClass(TimeDistributed, [{\n    key: \"build\",\n    value: function build(inputShape) {\n      inputShape = getExactlyOneShape(inputShape);\n      if (inputShape.length < 3) {\n        throw new ValueError(\"TimeDistributed layer expects an input shape >= 3D, but received \" + \"input shape \".concat(JSON.stringify(inputShape)));\n      }\n      this.inputSpec = [{\n        shape: inputShape\n      }];\n      var childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n      if (!this.layer.built) {\n        this.layer.build(childInputShape);\n        this.layer.built = true;\n      }\n      _get(_getPrototypeOf(TimeDistributed.prototype), \"build\", this).call(this, inputShape);\n    }\n  }, {\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      inputShape = getExactlyOneShape(inputShape);\n      var childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n      var childOutputShape = this.layer.computeOutputShape(childInputShape);\n      var timesteps = inputShape[1];\n      return [childOutputShape[0], timesteps].concat(childOutputShape.slice(1));\n    }\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this3 = this;\n      return tidy(function () {\n        // TODO(cais): Add 'training' and 'useLearningPhase' to kwargs.\n        inputs = getExactlyOneTensor(inputs);\n        // Porting Note: In tfjs-layers, `inputs` are always concrete tensor\n        // values. Hence the inputs can't have an undetermined first (batch)\n        // dimension, which is why we always use the K.rnn approach here.\n        var step = function step(inputs, states) {\n          // TODO(cais): Add useLearningPhase.\n          // NOTE(cais): `layer.call` may return a length-1 array of Tensor in\n          //   some cases (e.g., `layer` is a `Sequential` instance), which is\n          //   why `getExactlyOneTensor` is used below.\n          var output = getExactlyOneTensor(_this3.layer.call(inputs, kwargs));\n          return [output, []];\n        };\n        var rnnOutputs = rnn(step, inputs, [], false /* goBackwards */, null /* mask */, null /* constants */, false /* unroll */, true /* needPerStepOutputs */);\n        var y = rnnOutputs[1];\n        // TODO(cais): Add activity regularization.\n        // TODO(cais): Add useLearningPhase.\n        return y;\n      });\n    }\n  }]);\n  return TimeDistributed;\n}(Wrapper);\n/** @nocollapse */\nTimeDistributed.className = 'TimeDistributed';\nserialization.registerClass(TimeDistributed);\nexport function checkBidirectionalMergeMode(value) {\n  generic_utils.checkStringTypeUnionValue(VALID_BIDIRECTIONAL_MERGE_MODES, 'BidirectionalMergeMode', value);\n}\nvar DEFAULT_BIDIRECTIONAL_MERGE_MODE = 'concat';\nexport var Bidirectional = /*#__PURE__*/function (_Wrapper2) {\n  _inherits(Bidirectional, _Wrapper2);\n  var _super3 = _createSuper(Bidirectional);\n  function Bidirectional(args) {\n    var _this4;\n    _classCallCheck(this, Bidirectional);\n    _this4 = _super3.call(this, args);\n    // Note: When creating `this.forwardLayer`, the original Layer object\n    //   (`config.layer`) ought to be cloned. This is why we call\n    //   `getConfig()` followed by `deserialize()`. Without this cloning,\n    //   the layer names saved during serialization will incorrectly contain\n    //   the 'forward_' prefix. In Python Keras, this is done using\n    //   `copy.copy` (shallow copy), which does not have a simple equivalent\n    //   in JavaScript. JavaScript's `Object.assign()` does not copy\n    //   methods.\n    var layerConfig = args.layer.getConfig();\n    var forwDict = {};\n    forwDict['className'] = args.layer.getClassName();\n    forwDict['config'] = layerConfig;\n    _this4.forwardLayer = deserialize(forwDict);\n    layerConfig['goBackwards'] = layerConfig['goBackwards'] === true ? false : true;\n    var backDict = {};\n    backDict['className'] = args.layer.getClassName();\n    backDict['config'] = layerConfig;\n    _this4.backwardLayer = deserialize(backDict);\n    _this4.forwardLayer.name = 'forward_' + _this4.forwardLayer.name;\n    _this4.backwardLayer.name = 'backward_' + _this4.backwardLayer.name;\n    _this4.mergeMode = args.mergeMode === undefined ? DEFAULT_BIDIRECTIONAL_MERGE_MODE : args.mergeMode;\n    checkBidirectionalMergeMode(_this4.mergeMode);\n    if (args.weights) {\n      throw new NotImplementedError('weights support is not implemented for Bidirectional layer yet.');\n    }\n    _this4._stateful = args.layer.stateful;\n    _this4.returnSequences = args.layer.returnSequences;\n    _this4.returnState = args.layer.returnState;\n    _this4.supportsMasking = true;\n    _this4._trainable = true;\n    _this4.inputSpec = args.layer.inputSpec;\n    _this4.numConstants = null;\n    return _this4;\n  }\n  _createClass(Bidirectional, [{\n    key: \"trainable\",\n    get: function get() {\n      return this._trainable;\n    },\n    set: function set(value) {\n      // Porting Note: the check of `this.layer` here is necessary due to the\n      //   way the `constructor` of this class is written (see Porting Note\n      //   above).\n      this._trainable = value;\n      if (this.forwardLayer != null) {\n        this.forwardLayer.trainable = value;\n      }\n      if (this.backwardLayer != null) {\n        this.backwardLayer.trainable = value;\n      }\n    }\n  }, {\n    key: \"getWeights\",\n    value: function getWeights() {\n      return this.forwardLayer.getWeights().concat(this.backwardLayer.getWeights());\n    }\n  }, {\n    key: \"setWeights\",\n    value: function setWeights(weights) {\n      var numWeights = weights.length;\n      var numeightsOver2 = Math.floor(numWeights / 2);\n      this.forwardLayer.setWeights(weights.slice(0, numeightsOver2));\n      this.backwardLayer.setWeights(weights.slice(numeightsOver2));\n    }\n  }, {\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      var layerShapes = this.forwardLayer.computeOutputShape(inputShape);\n      if (!(Array.isArray(layerShapes) && Array.isArray(layerShapes[0]))) {\n        layerShapes = [layerShapes];\n      }\n      layerShapes = layerShapes;\n      var outputShape;\n      var outputShapes;\n      var stateShape;\n      if (this.returnState) {\n        stateShape = layerShapes.slice(1);\n        outputShape = layerShapes[0];\n      } else {\n        outputShape = layerShapes[0];\n      }\n      outputShape = outputShape;\n      if (this.mergeMode === 'concat') {\n        outputShape[outputShape.length - 1] *= 2;\n        outputShapes = [outputShape];\n      } else if (this.mergeMode == null) {\n        outputShapes = [outputShape, outputShape.slice()];\n      } else {\n        outputShapes = [outputShape];\n      }\n      if (this.returnState) {\n        if (this.mergeMode == null) {\n          return outputShapes.concat(stateShape).concat(stateShape.slice());\n        }\n        return [outputShape].concat(stateShape).concat(stateShape.slice());\n      }\n      return generic_utils.singletonOrArray(outputShapes);\n    }\n  }, {\n    key: \"apply\",\n    value: function apply(inputs, kwargs) {\n      var initialState = kwargs == null ? null : kwargs['initialState'];\n      var constants = kwargs == null ? null : kwargs['constants'];\n      if (kwargs == null) {\n        kwargs = {};\n      }\n      var standardized = standardizeArgs(inputs, initialState, constants, this.numConstants);\n      inputs = standardized.inputs;\n      initialState = standardized.initialState;\n      constants = standardized.constants;\n      if (Array.isArray(inputs)) {\n        initialState = inputs.slice(1);\n        inputs = inputs[0];\n      }\n      if ((initialState == null || initialState.length === 0) && constants == null) {\n        return _get(_getPrototypeOf(Bidirectional.prototype), \"apply\", this).call(this, inputs, kwargs);\n      }\n      var additionalInputs = [];\n      var additionalSpecs = [];\n      if (initialState != null) {\n        var numStates = initialState.length;\n        if (numStates % 2 > 0) {\n          throw new ValueError('When passing `initialState` to a Bidrectional RNN, ' + 'the state should be an Array containing the states of ' + 'the underlying RNNs.');\n        }\n        kwargs['initialState'] = initialState;\n        additionalInputs.push.apply(additionalInputs, _toConsumableArray(initialState));\n        var stateSpecs = initialState.map(function (state) {\n          return new InputSpec({\n            shape: state.shape\n          });\n        });\n        this.forwardLayer.stateSpec = stateSpecs.slice(0, numStates / 2);\n        this.backwardLayer.stateSpec = stateSpecs.slice(numStates / 2);\n        additionalSpecs.push.apply(additionalSpecs, _toConsumableArray(stateSpecs));\n      }\n      if (constants != null) {\n        throw new NotImplementedError('Support for constants in Bidirectional layers is not ' + 'implemented yet.');\n      }\n      var isSymbolicTensor = additionalInputs[0] instanceof SymbolicTensor;\n      for (var _i = 0, _additionalInputs = additionalInputs; _i < _additionalInputs.length; _i++) {\n        var tensor = _additionalInputs[_i];\n        if (tensor instanceof SymbolicTensor !== isSymbolicTensor) {\n          throw new ValueError('The initial state of a Bidirectional layer cannot be ' + 'specified as a mix of symbolic and non-symbolic tensors');\n        }\n      }\n      if (isSymbolicTensor) {\n        // Compute the full input and specs, including the states.\n        var fullInput = [inputs].concat(additionalInputs);\n        var fullInputSpec = this.inputSpec.concat(additionalSpecs);\n        // Perform the call temporarily and replace inputSpec.\n        // Note: with initial states symbolic calls and non-symbolic calls to\n        // this method differ in how the initial states are passed. For\n        // symbolic calls, the initial states are passed in the first arg, as\n        // an Array of SymbolicTensors; for non-symbolic calls, they are\n        // passed in the second arg as a part of the kwargs. Hence the need to\n        // temporarily modify inputSpec here.\n        // TODO(cais): Make refactoring so that this hacky code below is no\n        // longer needed.\n        var originalInputSpec = this.inputSpec;\n        this.inputSpec = fullInputSpec;\n        var output = _get(_getPrototypeOf(Bidirectional.prototype), \"apply\", this).call(this, fullInput, kwargs);\n        this.inputSpec = originalInputSpec;\n        return output;\n      } else {\n        return _get(_getPrototypeOf(Bidirectional.prototype), \"apply\", this).call(this, inputs, kwargs);\n      }\n    }\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this5 = this;\n      return tidy(function () {\n        var initialState = kwargs['initialState'];\n        var y;\n        var yRev;\n        if (initialState == null) {\n          y = _this5.forwardLayer.call(inputs, kwargs);\n          yRev = _this5.backwardLayer.call(inputs, kwargs);\n        } else {\n          var forwardState = initialState.slice(0, initialState.length / 2);\n          var backwardState = initialState.slice(initialState.length / 2);\n          y = _this5.forwardLayer.call(inputs, Object.assign(kwargs, {\n            initialState: forwardState\n          }));\n          yRev = _this5.backwardLayer.call(inputs, Object.assign(kwargs, {\n            initialState: backwardState\n          }));\n        }\n        var states;\n        if (_this5.returnState) {\n          if (Array.isArray(y)) {\n            states = y.slice(1).concat(yRev.slice(1));\n          } else {}\n          y = y[0];\n          yRev = yRev[0];\n        }\n        if (_this5.returnSequences) {\n          yRev = tfc.reverse(yRev, 1);\n        }\n        var output;\n        if (_this5.mergeMode === 'concat') {\n          output = K.concatenate([y, yRev]);\n        } else if (_this5.mergeMode === 'sum') {\n          output = tfc.add(y, yRev);\n        } else if (_this5.mergeMode === 'ave') {\n          output = tfc.mul(.5, tfc.add(y, yRev));\n        } else if (_this5.mergeMode === 'mul') {\n          output = tfc.mul(y, yRev);\n        } else if (_this5.mergeMode == null) {\n          output = [y, yRev];\n        }\n        // TODO(cais): Properly set learning phase.\n        if (_this5.returnState) {\n          if (_this5.mergeMode == null) {\n            return output.concat(states);\n          }\n          return [output].concat(states);\n        }\n        return output;\n      });\n    }\n  }, {\n    key: \"resetStates\",\n    value: function resetStates(states) {\n      this.forwardLayer.resetStates();\n      this.backwardLayer.resetStates();\n    }\n  }, {\n    key: \"build\",\n    value: function build(inputShape) {\n      var _this6 = this;\n      nameScope(this.forwardLayer.name, function () {\n        _this6.forwardLayer.build(inputShape);\n      });\n      nameScope(this.backwardLayer.name, function () {\n        _this6.backwardLayer.build(inputShape);\n      });\n      this.built = true;\n    }\n  }, {\n    key: \"computeMask\",\n    value: function computeMask(inputs, mask) {\n      if (Array.isArray(mask)) {\n        mask = mask[0];\n      }\n      var outputMask;\n      if (this.returnSequences) {\n        if (this.mergeMode == null) {\n          outputMask = [mask, mask];\n        } else {\n          outputMask = mask;\n        }\n      } else {\n        if (this.mergeMode == null) {\n          outputMask = [null, null];\n        } else {\n          outputMask = null;\n        }\n      }\n      if (this.returnState) {\n        var states = this.forwardLayer.states;\n        var stateMask = states.map(function (state) {\n          return null;\n        });\n        if (Array.isArray(outputMask)) {\n          return outputMask.concat(stateMask).concat(stateMask);\n        } else {\n          return [outputMask].concat(stateMask).concat(stateMask);\n        }\n      } else {\n        return outputMask;\n      }\n    }\n  }, {\n    key: \"trainableWeights\",\n    get: function get() {\n      return this.forwardLayer.trainableWeights.concat(this.backwardLayer.trainableWeights);\n    }\n  }, {\n    key: \"nonTrainableWeights\",\n    get: function get() {\n      return this.forwardLayer.nonTrainableWeights.concat(this.backwardLayer.nonTrainableWeights);\n    }\n    // TODO(cais): Implement constraints().\n  }, {\n    key: \"setFastWeightInitDuringBuild\",\n    value: function setFastWeightInitDuringBuild(value) {\n      _get(_getPrototypeOf(Bidirectional.prototype), \"setFastWeightInitDuringBuild\", this).call(this, value);\n      if (this.forwardLayer != null) {\n        this.forwardLayer.setFastWeightInitDuringBuild(value);\n      }\n      if (this.backwardLayer != null) {\n        this.backwardLayer.setFastWeightInitDuringBuild(value);\n      }\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        'mergeMode': this.mergeMode\n      };\n      // TODO(cais): Add logic for `numConstants` once the property is added.\n      var baseConfig = _get(_getPrototypeOf(Bidirectional.prototype), \"getConfig\", this).call(this);\n      Object.assign(config, baseConfig);\n      return config;\n    }\n    /** @nocollapse */\n  }], [{\n    key: \"fromConfig\",\n    value: function fromConfig(cls, config) {\n      var rnnLayer = deserialize(config['layer']);\n      delete config['layer'];\n      // TODO(cais): Add logic for `numConstants` once the property is added.\n      if (config['numConstants'] != null) {\n        throw new NotImplementedError(\"Deserialization of a Bidirectional layer with numConstants \" + \"present is not supported yet.\");\n      }\n      // tslint:disable-next-line:no-any\n      var newConfig = config;\n      newConfig['layer'] = rnnLayer;\n      return new cls(newConfig);\n    }\n  }]);\n  return Bidirectional;\n}(Wrapper);\n/** @nocollapse */\nBidirectional.className = 'Bidirectional';\nserialization.registerClass(Bidirectional);","map":{"version":3,"names":["tfc","serialization","tidy","K","nameScope","InputSpec","Layer","SymbolicTensor","NotImplementedError","ValueError","VALID_BIDIRECTIONAL_MERGE_MODES","generic_utils","getExactlyOneShape","getExactlyOneTensor","rnn","standardizeArgs","deserialize","Wrapper","_Layer","_inherits","_super","_createSuper","args","_this","_classCallCheck","call","layer","_createClass","key","value","build","inputShape","built","get","trainable","set","trainableWeights","nonTrainableWeights","_updates","losses","getWeights","setWeights","weights","getConfig","config","getClassName","baseConfig","_get","_getPrototypeOf","prototype","Object","assign","setFastWeightInitDuringBuild","fromConfig","cls","customObjects","arguments","length","undefined","layerConfig","newConfig","TimeDistributed","_Wrapper","_super2","_this2","supportsMasking","concat","JSON","stringify","inputSpec","shape","childInputShape","slice","computeOutputShape","childOutputShape","timesteps","inputs","kwargs","_this3","step","states","output","rnnOutputs","y","className","registerClass","checkBidirectionalMergeMode","checkStringTypeUnionValue","DEFAULT_BIDIRECTIONAL_MERGE_MODE","Bidirectional","_Wrapper2","_super3","_this4","forwDict","forwardLayer","backDict","backwardLayer","name","mergeMode","_stateful","stateful","returnSequences","returnState","_trainable","numConstants","numWeights","numeightsOver2","Math","floor","layerShapes","Array","isArray","outputShape","outputShapes","stateShape","singletonOrArray","apply","initialState","constants","standardized","additionalInputs","additionalSpecs","numStates","push","_toConsumableArray","stateSpecs","map","state","stateSpec","isSymbolicTensor","_i","_additionalInputs","tensor","fullInput","fullInputSpec","originalInputSpec","_this5","yRev","forwardState","backwardState","reverse","concatenate","add","mul","resetStates","_this6","computeMask","mask","outputMask","stateMask","rnnLayer"],"sources":["C:\\Users\\vince\\OneDrive\\Documents\\GitHub\\tfjs-layers\\src\\layers\\wrappers.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Layers that augment the functionality of a base layer.\n */\n\nimport * as tfc from '@tensorflow/tfjs-core';\nimport {serialization, Tensor, tidy} from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport {nameScope} from '../common';\nimport {InputSpec, Layer, LayerArgs, SymbolicTensor} from '../engine/topology';\nimport {NotImplementedError, ValueError} from '../errors';\nimport {BidirectionalMergeMode, Shape, VALID_BIDIRECTIONAL_MERGE_MODES} from '../keras_format/common';\nimport {Kwargs} from '../types';\nimport {RegularizerFn, RnnStepFunction} from '../types';\nimport * as generic_utils from '../utils/generic_utils';\nimport {getExactlyOneShape, getExactlyOneTensor} from '../utils/types_utils';\nimport {LayerVariable} from '../variables';\n\nimport {rnn, RNN, standardizeArgs} from './recurrent';\nimport {deserialize} from './serialization';\n\nexport declare interface WrapperLayerArgs extends LayerArgs {\n  /**\n   * The layer to be wrapped.\n   */\n  layer: Layer;\n}\n\n/**\n * Abstract wrapper base class.\n *\n * Wrappers take another layer and augment it in various ways.\n * Do not use this class as a layer, it is only an abstract base class.\n * Two usable wrappers are the `TimeDistributed` and `Bidirectional` wrappers.\n */\nexport abstract class Wrapper extends Layer {\n  readonly layer: Layer;\n\n  constructor(args: WrapperLayerArgs) {\n    // Porting Note: In PyKeras, `self.layer` is set prior to the calling\n    //   `super()`. But we can't do that here due to TypeScript's restriction.\n    //   See: https://github.com/Microsoft/TypeScript/issues/8277\n    //   As a result, we have to add checks in `get trainable()` and\n    //   `set trainable()` below in order to prevent using `this.layer` when\n    //   its value is `undefined`. The super constructor does use the getter\n    //   and the setter of `this.layer`.\n    super(args);\n    this.layer = args.layer;\n  }\n\n  override build(inputShape: Shape|Shape[]): void {\n    this.built = true;\n  }\n\n  // TODO(cais): Implement activityRegularizer getter.\n\n  override get trainable(): boolean {\n    // Porting Note: the check of `this.layer` here is necessary due to the\n    //   way the `constructor` of this class is written (see Porting Note\n    //   above).\n    if (this.layer != null) {\n      return this.layer.trainable;\n    } else {\n      return false;\n    }\n  }\n\n  override set trainable(value: boolean) {\n    // Porting Note: the check of `this.layer` here is necessary due to the\n    //   way the `constructor` of this class is written (see Porting Note\n    //   above).\n    if (this.layer != null) {\n      this.layer.trainable = value;\n    }\n  }\n\n  override get trainableWeights(): LayerVariable[] {\n    return this.layer.trainableWeights;\n  }\n  // TODO(cais): Implement setter for trainableWeights.\n\n  override get nonTrainableWeights(): LayerVariable[] {\n    return this.layer.nonTrainableWeights;\n  }\n  // TODO(cais): Implement setter for nonTrainableWeights.\n\n  override get updates(): Tensor[] {\n    // tslint:disable-next-line:no-any\n    return (this.layer as any)._updates;\n  }\n\n  // TODO(cais): Implement getUpdatesFor().\n\n  override get losses(): RegularizerFn[] {\n    return this.layer.losses;\n  }\n\n  // TODO(cais): Implement getLossesFor().\n\n  override getWeights(): Tensor[] {\n    return this.layer.getWeights();\n  }\n\n  override setWeights(weights: Tensor[]): void {\n    this.layer.setWeights(weights);\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {\n      'layer': {\n        'className': this.layer.getClassName(),\n        'config': this.layer.getConfig(),\n      }\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n  override setFastWeightInitDuringBuild(value: boolean) {\n    super.setFastWeightInitDuringBuild(value);\n    if (this.layer != null) {\n      this.layer.setFastWeightInitDuringBuild(value);\n    }\n  }\n\n  /** @nocollapse */\n  static override fromConfig<T extends serialization.Serializable>(\n      cls: serialization.SerializableConstructor<T>,\n      config: serialization.ConfigDict,\n      customObjects = {} as serialization.ConfigDict): T {\n    const layerConfig = config['layer'] as serialization.ConfigDict;\n    const layer = deserialize(layerConfig, customObjects) as Layer;\n    delete config['layer'];\n    const newConfig = {layer};\n    Object.assign(newConfig, config);\n    return new cls(newConfig);\n  }\n}\n\nexport class TimeDistributed extends Wrapper {\n  /** @nocollapse */\n  static className = 'TimeDistributed';\n  constructor(args: WrapperLayerArgs) {\n    super(args);\n    this.supportsMasking = true;\n  }\n\n  override build(inputShape: Shape|Shape[]): void {\n    inputShape = getExactlyOneShape(inputShape);\n    if (inputShape.length < 3) {\n      throw new ValueError(\n          `TimeDistributed layer expects an input shape >= 3D, but received ` +\n          `input shape ${JSON.stringify(inputShape)}`);\n    }\n    this.inputSpec = [{shape: inputShape}];\n    const childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n    if (!this.layer.built) {\n      this.layer.build(childInputShape);\n      this.layer.built = true;\n    }\n    super.build(inputShape);\n  }\n\n  override computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    inputShape = getExactlyOneShape(inputShape);\n    const childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n    const childOutputShape =\n        this.layer.computeOutputShape(childInputShape) as Shape;\n    const timesteps = inputShape[1];\n    return [childOutputShape[0], timesteps].concat(childOutputShape.slice(1));\n  }\n\n  override call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      // TODO(cais): Add 'training' and 'useLearningPhase' to kwargs.\n      inputs = getExactlyOneTensor(inputs);\n      // Porting Note: In tfjs-layers, `inputs` are always concrete tensor\n      // values. Hence the inputs can't have an undetermined first (batch)\n      // dimension, which is why we always use the K.rnn approach here.\n      const step: RnnStepFunction = (inputs: Tensor, states: Tensor[]) => {\n        // TODO(cais): Add useLearningPhase.\n        // NOTE(cais): `layer.call` may return a length-1 array of Tensor in\n        //   some cases (e.g., `layer` is a `Sequential` instance), which is\n        //   why `getExactlyOneTensor` is used below.\n        const output = getExactlyOneTensor(this.layer.call(inputs, kwargs));\n        return [output, []];\n      };\n      const rnnOutputs =\n          rnn(step, inputs, [], false /* goBackwards */, null /* mask */,\n              null /* constants */, false /* unroll */,\n              true /* needPerStepOutputs */);\n      const y = rnnOutputs[1];\n      // TODO(cais): Add activity regularization.\n      // TODO(cais): Add useLearningPhase.\n      return y;\n    });\n  }\n\n  // TODO(cais): Implement detailed computeMask() logic.\n}\nserialization.registerClass(TimeDistributed);\n\nexport function checkBidirectionalMergeMode(value?: string): void {\n  generic_utils.checkStringTypeUnionValue(\n      VALID_BIDIRECTIONAL_MERGE_MODES, 'BidirectionalMergeMode', value);\n}\n\nexport declare interface BidirectionalLayerArgs extends WrapperLayerArgs {\n  /**\n   * The instance of an `RNN` layer to be wrapped.\n   */\n  layer: RNN;\n\n  /**\n   * Mode by which outputs of the forward and backward RNNs are\n   * combined. If `null` or `undefined`, the output will not be\n   * combined, they will be returned as an `Array`.\n   *\n   * If `undefined` (i.e., not provided), defaults to `'concat'`.\n   */\n  mergeMode?: BidirectionalMergeMode;\n}\n\nconst DEFAULT_BIDIRECTIONAL_MERGE_MODE: BidirectionalMergeMode = 'concat';\n\nexport class Bidirectional extends Wrapper {\n  /** @nocollapse */\n  static className = 'Bidirectional';\n  mergeMode: BidirectionalMergeMode;\n  private forwardLayer: RNN;\n  private backwardLayer: RNN;\n  private returnSequences: boolean;\n  private returnState: boolean;\n  private numConstants?: number;\n  private _trainable: boolean;\n\n  constructor(args: BidirectionalLayerArgs) {\n    super(args);\n\n    // Note: When creating `this.forwardLayer`, the original Layer object\n    //   (`config.layer`) ought to be cloned. This is why we call\n    //   `getConfig()` followed by `deserialize()`. Without this cloning,\n    //   the layer names saved during serialization will incorrectly contain\n    //   the 'forward_' prefix. In Python Keras, this is done using\n    //   `copy.copy` (shallow copy), which does not have a simple equivalent\n    //   in JavaScript. JavaScript's `Object.assign()` does not copy\n    //   methods.\n    const layerConfig = args.layer.getConfig();\n    const forwDict: serialization.ConfigDict = {};\n    forwDict['className'] = args.layer.getClassName();\n    forwDict['config'] = layerConfig;\n    this.forwardLayer = deserialize(forwDict) as RNN;\n    layerConfig['goBackwards'] =\n        layerConfig['goBackwards'] === true ? false : true;\n    const backDict: serialization.ConfigDict = {};\n    backDict['className'] = args.layer.getClassName();\n    backDict['config'] = layerConfig;\n    this.backwardLayer = deserialize(backDict) as RNN;\n    this.forwardLayer.name = 'forward_' + this.forwardLayer.name;\n    this.backwardLayer.name = 'backward_' + this.backwardLayer.name;\n\n    this.mergeMode = args.mergeMode === undefined ?\n        DEFAULT_BIDIRECTIONAL_MERGE_MODE :\n        args.mergeMode;\n    checkBidirectionalMergeMode(this.mergeMode);\n    if (args.weights) {\n      throw new NotImplementedError(\n          'weights support is not implemented for Bidirectional layer yet.');\n    }\n    this._stateful = args.layer.stateful;\n    this.returnSequences = args.layer.returnSequences;\n    this.returnState = args.layer.returnState;\n    this.supportsMasking = true;\n    this._trainable = true;\n    this.inputSpec = args.layer.inputSpec;\n    this.numConstants = null;\n  }\n\n  override get trainable(): boolean {\n    return this._trainable;\n  }\n\n  override set trainable(value: boolean) {\n    // Porting Note: the check of `this.layer` here is necessary due to the\n    //   way the `constructor` of this class is written (see Porting Note\n    //   above).\n    this._trainable = value;\n    if (this.forwardLayer != null) {\n      this.forwardLayer.trainable = value;\n    }\n    if (this.backwardLayer != null) {\n      this.backwardLayer.trainable = value;\n    }\n  }\n\n  override getWeights(): Tensor[] {\n    return this.forwardLayer.getWeights().concat(\n        this.backwardLayer.getWeights());\n  }\n\n  override setWeights(weights: Tensor[]): void {\n    const numWeights = weights.length;\n    const numeightsOver2 = Math.floor(numWeights / 2);\n    this.forwardLayer.setWeights(weights.slice(0, numeightsOver2));\n    this.backwardLayer.setWeights(weights.slice(numeightsOver2));\n  }\n\n  override computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    let layerShapes: Shape|Shape[] =\n        this.forwardLayer.computeOutputShape(inputShape);\n    if (!(Array.isArray(layerShapes) && Array.isArray(layerShapes[0]))) {\n      layerShapes = [layerShapes as Shape];\n    }\n    layerShapes = layerShapes as Shape[];\n\n    let outputShape: Shape;\n    let outputShapes: Shape[];\n    let stateShape: Shape[];\n    if (this.returnState) {\n      stateShape = layerShapes.slice(1);\n      outputShape = layerShapes[0];\n    } else {\n      outputShape = layerShapes[0];\n    }\n    outputShape = outputShape;\n    if (this.mergeMode === 'concat') {\n      outputShape[outputShape.length - 1] *= 2;\n      outputShapes = [outputShape];\n    } else if (this.mergeMode == null) {\n      outputShapes = [outputShape, outputShape.slice()];\n    } else {\n      outputShapes = [outputShape];\n    }\n\n    if (this.returnState) {\n      if (this.mergeMode == null) {\n        return outputShapes.concat(stateShape).concat(stateShape.slice());\n      }\n      return [outputShape].concat(stateShape).concat(stateShape.slice());\n    }\n    return generic_utils.singletonOrArray(outputShapes);\n  }\n\n  override apply(\n      inputs: Tensor|Tensor[]|SymbolicTensor|SymbolicTensor[],\n      kwargs?: Kwargs): Tensor|Tensor[]|SymbolicTensor|SymbolicTensor[] {\n    let initialState: Tensor[]|SymbolicTensor[] =\n        kwargs == null ? null : kwargs['initialState'];\n    let constants: Tensor[]|SymbolicTensor[] =\n        kwargs == null ? null : kwargs['constants'];\n    if (kwargs == null) {\n      kwargs = {};\n    }\n    const standardized =\n        standardizeArgs(inputs, initialState, constants, this.numConstants);\n    inputs = standardized.inputs;\n    initialState = standardized.initialState;\n    constants = standardized.constants;\n\n    if (Array.isArray(inputs)) {\n      initialState = (inputs as Tensor[] | SymbolicTensor[]).slice(1);\n      inputs = (inputs as Tensor[] | SymbolicTensor[])[0];\n    }\n\n    if ((initialState == null || initialState.length === 0) &&\n        constants == null) {\n      return super.apply(inputs, kwargs);\n    }\n    const additionalInputs: Array<Tensor|SymbolicTensor> = [];\n    const additionalSpecs: InputSpec[] = [];\n    if (initialState != null) {\n      const numStates = initialState.length;\n      if (numStates % 2 > 0) {\n        throw new ValueError(\n            'When passing `initialState` to a Bidrectional RNN, ' +\n            'the state should be an Array containing the states of ' +\n            'the underlying RNNs.');\n      }\n      kwargs['initialState'] = initialState;\n      additionalInputs.push(...initialState);\n      const stateSpecs = (initialState as Array<Tensor|SymbolicTensor>)\n                             .map(state => new InputSpec({shape: state.shape}));\n      this.forwardLayer.stateSpec = stateSpecs.slice(0, numStates / 2);\n      this.backwardLayer.stateSpec = stateSpecs.slice(numStates / 2);\n      additionalSpecs.push(...stateSpecs);\n    }\n    if (constants != null) {\n      throw new NotImplementedError(\n          'Support for constants in Bidirectional layers is not ' +\n          'implemented yet.');\n    }\n\n    const isSymbolicTensor = additionalInputs[0] instanceof SymbolicTensor;\n    for (const tensor of additionalInputs) {\n      if (tensor instanceof SymbolicTensor !== isSymbolicTensor) {\n        throw new ValueError(\n            'The initial state of a Bidirectional layer cannot be ' +\n            'specified as a mix of symbolic and non-symbolic tensors');\n      }\n    }\n\n    if (isSymbolicTensor) {\n      // Compute the full input and specs, including the states.\n      const fullInput = [inputs].concat(additionalInputs);\n      const fullInputSpec = this.inputSpec.concat(additionalSpecs);\n      // Perform the call temporarily and replace inputSpec.\n      // Note: with initial states symbolic calls and non-symbolic calls to\n      // this method differ in how the initial states are passed. For\n      // symbolic calls, the initial states are passed in the first arg, as\n      // an Array of SymbolicTensors; for non-symbolic calls, they are\n      // passed in the second arg as a part of the kwargs. Hence the need to\n      // temporarily modify inputSpec here.\n      // TODO(cais): Make refactoring so that this hacky code below is no\n      // longer needed.\n      const originalInputSpec = this.inputSpec;\n      this.inputSpec = fullInputSpec;\n      const output =\n          super.apply(fullInput as Tensor[] | SymbolicTensor[], kwargs);\n      this.inputSpec = originalInputSpec;\n      return output;\n    } else {\n      return super.apply(inputs, kwargs);\n    }\n  }\n\n  override call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      const initialState = kwargs['initialState'];\n\n      let y: Tensor|Tensor[];\n      let yRev: Tensor|Tensor[];\n      if (initialState == null) {\n        y = this.forwardLayer.call(inputs, kwargs);\n        yRev = this.backwardLayer.call(inputs, kwargs);\n      } else {\n        const forwardState = initialState.slice(0, initialState.length / 2);\n        const backwardState = initialState.slice(initialState.length / 2);\n        y = this.forwardLayer.call(\n            inputs, Object.assign(kwargs, {initialState: forwardState}));\n        yRev = this.backwardLayer.call(\n            inputs, Object.assign(kwargs, {initialState: backwardState}));\n      }\n\n      let states: Tensor[];\n      if (this.returnState) {\n        if (Array.isArray(y)) {\n          states = y.slice(1).concat((yRev as Tensor[]).slice(1));\n        } else {\n        }\n        y = (y as Tensor[])[0];\n        yRev = (yRev as Tensor[])[0];\n      }\n\n      if (this.returnSequences) {\n        yRev = tfc.reverse(yRev as Tensor, 1);\n      }\n\n      let output: Tensor|Tensor[];\n      if (this.mergeMode === 'concat') {\n        output = K.concatenate([y as Tensor, yRev as Tensor]);\n      } else if (this.mergeMode === 'sum') {\n        output = tfc.add(y as Tensor, yRev as Tensor);\n      } else if (this.mergeMode === 'ave') {\n        output = tfc.mul(.5, tfc.add(y as Tensor, yRev as Tensor));\n      } else if (this.mergeMode === 'mul') {\n        output = tfc.mul(y as Tensor, yRev as Tensor);\n      } else if (this.mergeMode == null) {\n        output = [y as Tensor, yRev as Tensor];\n      }\n\n      // TODO(cais): Properly set learning phase.\n      if (this.returnState) {\n        if (this.mergeMode == null) {\n          return (output as Tensor[]).concat(states);\n        }\n        return [output as Tensor].concat(states);\n      }\n      return output;\n    });\n  }\n\n  override resetStates(states?: Tensor|Tensor[]): void {\n    this.forwardLayer.resetStates();\n    this.backwardLayer.resetStates();\n  }\n\n  override build(inputShape: Shape|Shape[]): void {\n    nameScope(this.forwardLayer.name, () => {\n      this.forwardLayer.build(inputShape);\n    });\n    nameScope(this.backwardLayer.name, () => {\n      this.backwardLayer.build(inputShape);\n    });\n    this.built = true;\n  }\n\n  override computeMask(inputs: Tensor|Tensor[], mask?: Tensor|Tensor[]): Tensor\n      |Tensor[] {\n    if (Array.isArray(mask)) {\n      mask = mask[0];\n    }\n    let outputMask: Tensor|Tensor[];\n    if (this.returnSequences) {\n      if (this.mergeMode == null) {\n        outputMask = [mask, mask];\n      } else {\n        outputMask = mask;\n      }\n    } else {\n      if (this.mergeMode == null) {\n        outputMask = [null, null];\n      } else {\n        outputMask = null;\n      }\n    }\n    if (this.returnState) {\n      const states = this.forwardLayer.states;\n      const stateMask: Tensor[] = states.map(state => null);\n      if (Array.isArray(outputMask)) {\n        return outputMask.concat(stateMask).concat(stateMask);\n      } else {\n        return [outputMask].concat(stateMask).concat(stateMask);\n      }\n    } else {\n      return outputMask;\n    }\n  }\n\n  override get trainableWeights(): LayerVariable[] {\n    return this.forwardLayer.trainableWeights.concat(\n        this.backwardLayer.trainableWeights);\n  }\n\n  override get nonTrainableWeights(): LayerVariable[] {\n    return this.forwardLayer.nonTrainableWeights.concat(\n        this.backwardLayer.nonTrainableWeights);\n  }\n\n  // TODO(cais): Implement constraints().\n\n  override setFastWeightInitDuringBuild(value: boolean) {\n    super.setFastWeightInitDuringBuild(value);\n    if (this.forwardLayer != null) {\n      this.forwardLayer.setFastWeightInitDuringBuild(value);\n    }\n    if (this.backwardLayer != null) {\n      this.backwardLayer.setFastWeightInitDuringBuild(value);\n    }\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {\n      'mergeMode': this.mergeMode,\n    };\n    // TODO(cais): Add logic for `numConstants` once the property is added.\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n  /** @nocollapse */\n  static override fromConfig<T extends serialization.Serializable>(\n      cls: serialization.SerializableConstructor<T>,\n      config: serialization.ConfigDict): T {\n    const rnnLayer =\n        deserialize(config['layer'] as serialization.ConfigDict) as RNN;\n    delete config['layer'];\n    // TODO(cais): Add logic for `numConstants` once the property is added.\n    if (config['numConstants'] != null) {\n      throw new NotImplementedError(\n          `Deserialization of a Bidirectional layer with numConstants ` +\n          `present is not supported yet.`);\n    }\n    // tslint:disable-next-line:no-any\n    const newConfig: {[key: string]: any} = config;\n    newConfig['layer'] = rnnLayer;\n    return new cls(newConfig);\n  }\n}\nserialization.registerClass(Bidirectional);\n"],"mappings":";;;;;;;AAAA;;;;;;;;;AAUA;;;AAIA,OAAO,KAAKA,GAAG,MAAM,uBAAuB;AAC5C,SAAQC,aAAa,EAAUC,IAAI,QAAO,uBAAuB;AACjE,OAAO,KAAKC,CAAC,MAAM,yBAAyB;AAC5C,SAAQC,SAAS,QAAO,WAAW;AACnC,SAAQC,SAAS,EAAEC,KAAK,EAAaC,cAAc,QAAO,oBAAoB;AAC9E,SAAQC,mBAAmB,EAAEC,UAAU,QAAO,WAAW;AACzD,SAAuCC,+BAA+B,QAAO,wBAAwB;AAGrG,OAAO,KAAKC,aAAa,MAAM,wBAAwB;AACvD,SAAQC,kBAAkB,EAAEC,mBAAmB,QAAO,sBAAsB;AAG5E,SAAQC,GAAG,EAAOC,eAAe,QAAO,aAAa;AACrD,SAAQC,WAAW,QAAO,iBAAiB;AAS3C;;;;;;;AAOA,WAAsBC,OAAQ,0BAAAC,MAAA;EAAAC,SAAA,CAAAF,OAAA,EAAAC,MAAA;EAAA,IAAAE,MAAA,GAAAC,YAAA,CAAAJ,OAAA;EAG5B,SAAAA,QAAYK,IAAsB;IAAA,IAAAC,KAAA;IAAAC,eAAA,OAAAP,OAAA;IAChC;IACA;IACA;IACA;IACA;IACA;IACA;IACAM,KAAA,GAAAH,MAAA,CAAAK,IAAA,OAAMH,IAAI;IACVC,KAAA,CAAKG,KAAK,GAAGJ,IAAI,CAACI,KAAK;IAAC,OAAAH,KAAA;EAC1B;EAACI,YAAA,CAAAV,OAAA;IAAAW,GAAA;IAAAC,KAAA,EAEQ,SAAAC,MAAMC,UAAyB;MACtC,IAAI,CAACC,KAAK,GAAG,IAAI;IACnB;IAEA;EAAA;IAAAJ,GAAA;IAAAK,GAAA,EAEA,SAAAA,IAAA,EAAsB;MACpB;MACA;MACA;MACA,IAAI,IAAI,CAACP,KAAK,IAAI,IAAI,EAAE;QACtB,OAAO,IAAI,CAACA,KAAK,CAACQ,SAAS;OAC5B,MAAM;QACL,OAAO,KAAK;;IAEhB,CAAC;IAAAC,GAAA,EAED,SAAAA,IAAuBN,KAAc;MACnC;MACA;MACA;MACA,IAAI,IAAI,CAACH,KAAK,IAAI,IAAI,EAAE;QACtB,IAAI,CAACA,KAAK,CAACQ,SAAS,GAAGL,KAAK;;IAEhC;EAAC;IAAAD,GAAA;IAAAK,GAAA,EAED,SAAAA,IAAA,EAA6B;MAC3B,OAAO,IAAI,CAACP,KAAK,CAACU,gBAAgB;IACpC;IACA;EAAA;IAAAR,GAAA;IAAAK,GAAA,EAEA,SAAAA,IAAA,EAAgC;MAC9B,OAAO,IAAI,CAACP,KAAK,CAACW,mBAAmB;IACvC;IACA;EAAA;IAAAT,GAAA;IAAAK,GAAA,EAEA,SAAAA,IAAA,EAAoB;MAClB;MACA,OAAQ,IAAI,CAACP,KAAa,CAACY,QAAQ;IACrC;IAEA;EAAA;IAAAV,GAAA;IAAAK,GAAA,EAEA,SAAAA,IAAA,EAAmB;MACjB,OAAO,IAAI,CAACP,KAAK,CAACa,MAAM;IAC1B;IAEA;EAAA;IAAAX,GAAA;IAAAC,KAAA,EAES,SAAAW,WAAA,EAAU;MACjB,OAAO,IAAI,CAACd,KAAK,CAACc,UAAU,EAAE;IAChC;EAAC;IAAAZ,GAAA;IAAAC,KAAA,EAEQ,SAAAY,WAAWC,OAAiB;MACnC,IAAI,CAAChB,KAAK,CAACe,UAAU,CAACC,OAAO,CAAC;IAChC;EAAC;IAAAd,GAAA;IAAAC,KAAA,EAEQ,SAAAc,UAAA,EAAS;MAChB,IAAMC,MAAM,GAA6B;QACvC,OAAO,EAAE;UACP,WAAW,EAAE,IAAI,CAAClB,KAAK,CAACmB,YAAY,EAAE;UACtC,QAAQ,EAAE,IAAI,CAACnB,KAAK,CAACiB,SAAS;;OAEjC;MACD,IAAMG,UAAU,GAAAC,IAAA,CAAAC,eAAA,CAAA/B,OAAA,CAAAgC,SAAA,sBAAAxB,IAAA,MAAoB;MACpCyB,MAAM,CAACC,MAAM,CAACP,MAAM,EAAEE,UAAU,CAAC;MACjC,OAAOF,MAAM;IACf;EAAC;IAAAhB,GAAA;IAAAC,KAAA,EAEQ,SAAAuB,6BAA6BvB,KAAc;MAClDkB,IAAA,CAAAC,eAAA,CAAA/B,OAAA,CAAAgC,SAAA,yCAAAxB,IAAA,OAAmCI,KAAK;MACxC,IAAI,IAAI,CAACH,KAAK,IAAI,IAAI,EAAE;QACtB,IAAI,CAACA,KAAK,CAAC0B,4BAA4B,CAACvB,KAAK,CAAC;;IAElD;IAEA;EAAA;IAAAD,GAAA;IAAAC,KAAA,EACA,SAAAwB,WACIC,GAA6C,EAC7CV,MAAgC,EACc;MAAA,IAA9CW,aAAA,GAAAC,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAgB,EAA8B;MAChD,IAAMG,WAAW,GAAGf,MAAM,CAAC,OAAO,CAA6B;MAC/D,IAAMlB,KAAK,GAAGV,WAAW,CAAC2C,WAAW,EAAEJ,aAAa,CAAU;MAC9D,OAAOX,MAAM,CAAC,OAAO,CAAC;MACtB,IAAMgB,SAAS,GAAG;QAAClC,KAAK,EAALA;MAAK,CAAC;MACzBwB,MAAM,CAACC,MAAM,CAACS,SAAS,EAAEhB,MAAM,CAAC;MAChC,OAAO,IAAIU,GAAG,CAACM,SAAS,CAAC;IAC3B;EAAC;EAAA,OAAA3C,OAAA;AAAA,EAtGmCX,KAAK;AAyG3C,WAAauD,eAAgB,0BAAAC,QAAA;EAAA3C,SAAA,CAAA0C,eAAA,EAAAC,QAAA;EAAA,IAAAC,OAAA,GAAA1C,YAAA,CAAAwC,eAAA;EAG3B,SAAAA,gBAAYvC,IAAsB;IAAA,IAAA0C,MAAA;IAAAxC,eAAA,OAAAqC,eAAA;IAChCG,MAAA,GAAAD,OAAA,CAAAtC,IAAA,OAAMH,IAAI;IACV0C,MAAA,CAAKC,eAAe,GAAG,IAAI;IAAC,OAAAD,MAAA;EAC9B;EAACrC,YAAA,CAAAkC,eAAA;IAAAjC,GAAA;IAAAC,KAAA,EAEQ,SAAAC,MAAMC,UAAyB;MACtCA,UAAU,GAAGnB,kBAAkB,CAACmB,UAAU,CAAC;MAC3C,IAAIA,UAAU,CAAC0B,MAAM,GAAG,CAAC,EAAE;QACzB,MAAM,IAAIhD,UAAU,CAChB,qFAAAyD,MAAA,CACeC,IAAI,CAACC,SAAS,CAACrC,UAAU,CAAC,CAAE,CAAC;;MAElD,IAAI,CAACsC,SAAS,GAAG,CAAC;QAACC,KAAK,EAAEvC;MAAU,CAAC,CAAC;MACtC,IAAMwC,eAAe,GAAG,CAACxC,UAAU,CAAC,CAAC,CAAC,CAAC,CAACmC,MAAM,CAACnC,UAAU,CAACyC,KAAK,CAAC,CAAC,CAAC,CAAC;MACnE,IAAI,CAAC,IAAI,CAAC9C,KAAK,CAACM,KAAK,EAAE;QACrB,IAAI,CAACN,KAAK,CAACI,KAAK,CAACyC,eAAe,CAAC;QACjC,IAAI,CAAC7C,KAAK,CAACM,KAAK,GAAG,IAAI;;MAEzBe,IAAA,CAAAC,eAAA,CAAAa,eAAA,CAAAZ,SAAA,kBAAAxB,IAAA,OAAYM,UAAU;IACxB;EAAC;IAAAH,GAAA;IAAAC,KAAA,EAEQ,SAAA4C,mBAAmB1C,UAAyB;MACnDA,UAAU,GAAGnB,kBAAkB,CAACmB,UAAU,CAAC;MAC3C,IAAMwC,eAAe,GAAG,CAACxC,UAAU,CAAC,CAAC,CAAC,CAAC,CAACmC,MAAM,CAACnC,UAAU,CAACyC,KAAK,CAAC,CAAC,CAAC,CAAC;MACnE,IAAME,gBAAgB,GAClB,IAAI,CAAChD,KAAK,CAAC+C,kBAAkB,CAACF,eAAe,CAAU;MAC3D,IAAMI,SAAS,GAAG5C,UAAU,CAAC,CAAC,CAAC;MAC/B,OAAO,CAAC2C,gBAAgB,CAAC,CAAC,CAAC,EAAEC,SAAS,CAAC,CAACT,MAAM,CAACQ,gBAAgB,CAACF,KAAK,CAAC,CAAC,CAAC,CAAC;IAC3E;EAAC;IAAA5C,GAAA;IAAAC,KAAA,EAEQ,SAAAJ,KAAKmD,MAAuB,EAAEC,MAAc;MAAA,IAAAC,MAAA;MACnD,OAAO5E,IAAI,CAAC,YAAK;QACf;QACA0E,MAAM,GAAG/D,mBAAmB,CAAC+D,MAAM,CAAC;QACpC;QACA;QACA;QACA,IAAMG,IAAI,GAAoB,SAAxBA,IAAIA,CAAqBH,MAAc,EAAEI,MAAgB,EAAI;UACjE;UACA;UACA;UACA;UACA,IAAMC,MAAM,GAAGpE,mBAAmB,CAACiE,MAAI,CAACpD,KAAK,CAACD,IAAI,CAACmD,MAAM,EAAEC,MAAM,CAAC,CAAC;UACnE,OAAO,CAACI,MAAM,EAAE,EAAE,CAAC;QACrB,CAAC;QACD,IAAMC,UAAU,GACZpE,GAAG,CAACiE,IAAI,EAAEH,MAAM,EAAE,EAAE,EAAE,KAAK,CAAC,mBAAmB,IAAI,CAAC,YAChD,IAAI,CAAC,iBAAiB,KAAK,CAAC,cAC5B,IAAI,CAAC,yBAAyB;QACtC,IAAMO,CAAC,GAAGD,UAAU,CAAC,CAAC,CAAC;QACvB;QACA;QACA,OAAOC,CAAC;MACV,CAAC,CAAC;IACJ;EAAC;EAAA,OAAAtB,eAAA;AAAA,EAzDkC5C,OAAO;AAC1C;AACO4C,eAAA,CAAAuB,SAAS,GAAG,iBAAiB;AA2DtCnF,aAAa,CAACoF,aAAa,CAACxB,eAAe,CAAC;AAE5C,OAAM,SAAUyB,2BAA2BA,CAACzD,KAAc;EACxDlB,aAAa,CAAC4E,yBAAyB,CACnC7E,+BAA+B,EAAE,wBAAwB,EAAEmB,KAAK,CAAC;AACvE;AAkBA,IAAM2D,gCAAgC,GAA2B,QAAQ;AAEzE,WAAaC,aAAc,0BAAAC,SAAA;EAAAvE,SAAA,CAAAsE,aAAA,EAAAC,SAAA;EAAA,IAAAC,OAAA,GAAAtE,YAAA,CAAAoE,aAAA;EAWzB,SAAAA,cAAYnE,IAA4B;IAAA,IAAAsE,MAAA;IAAApE,eAAA,OAAAiE,aAAA;IACtCG,MAAA,GAAAD,OAAA,CAAAlE,IAAA,OAAMH,IAAI;IAEV;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA,IAAMqC,WAAW,GAAGrC,IAAI,CAACI,KAAK,CAACiB,SAAS,EAAE;IAC1C,IAAMkD,QAAQ,GAA6B,EAAE;IAC7CA,QAAQ,CAAC,WAAW,CAAC,GAAGvE,IAAI,CAACI,KAAK,CAACmB,YAAY,EAAE;IACjDgD,QAAQ,CAAC,QAAQ,CAAC,GAAGlC,WAAW;IAChCiC,MAAA,CAAKE,YAAY,GAAG9E,WAAW,CAAC6E,QAAQ,CAAQ;IAChDlC,WAAW,CAAC,aAAa,CAAC,GACtBA,WAAW,CAAC,aAAa,CAAC,KAAK,IAAI,GAAG,KAAK,GAAG,IAAI;IACtD,IAAMoC,QAAQ,GAA6B,EAAE;IAC7CA,QAAQ,CAAC,WAAW,CAAC,GAAGzE,IAAI,CAACI,KAAK,CAACmB,YAAY,EAAE;IACjDkD,QAAQ,CAAC,QAAQ,CAAC,GAAGpC,WAAW;IAChCiC,MAAA,CAAKI,aAAa,GAAGhF,WAAW,CAAC+E,QAAQ,CAAQ;IACjDH,MAAA,CAAKE,YAAY,CAACG,IAAI,GAAG,UAAU,GAAGL,MAAA,CAAKE,YAAY,CAACG,IAAI;IAC5DL,MAAA,CAAKI,aAAa,CAACC,IAAI,GAAG,WAAW,GAAGL,MAAA,CAAKI,aAAa,CAACC,IAAI;IAE/DL,MAAA,CAAKM,SAAS,GAAG5E,IAAI,CAAC4E,SAAS,KAAKxC,SAAS,GACzC8B,gCAAgC,GAChClE,IAAI,CAAC4E,SAAS;IAClBZ,2BAA2B,CAACM,MAAA,CAAKM,SAAS,CAAC;IAC3C,IAAI5E,IAAI,CAACoB,OAAO,EAAE;MAChB,MAAM,IAAIlC,mBAAmB,CACzB,iEAAiE,CAAC;;IAExEoF,MAAA,CAAKO,SAAS,GAAG7E,IAAI,CAACI,KAAK,CAAC0E,QAAQ;IACpCR,MAAA,CAAKS,eAAe,GAAG/E,IAAI,CAACI,KAAK,CAAC2E,eAAe;IACjDT,MAAA,CAAKU,WAAW,GAAGhF,IAAI,CAACI,KAAK,CAAC4E,WAAW;IACzCV,MAAA,CAAK3B,eAAe,GAAG,IAAI;IAC3B2B,MAAA,CAAKW,UAAU,GAAG,IAAI;IACtBX,MAAA,CAAKvB,SAAS,GAAG/C,IAAI,CAACI,KAAK,CAAC2C,SAAS;IACrCuB,MAAA,CAAKY,YAAY,GAAG,IAAI;IAAC,OAAAZ,MAAA;EAC3B;EAACjE,YAAA,CAAA8D,aAAA;IAAA7D,GAAA;IAAAK,GAAA,EAED,SAAAA,IAAA,EAAsB;MACpB,OAAO,IAAI,CAACsE,UAAU;IACxB,CAAC;IAAApE,GAAA,EAED,SAAAA,IAAuBN,KAAc;MACnC;MACA;MACA;MACA,IAAI,CAAC0E,UAAU,GAAG1E,KAAK;MACvB,IAAI,IAAI,CAACiE,YAAY,IAAI,IAAI,EAAE;QAC7B,IAAI,CAACA,YAAY,CAAC5D,SAAS,GAAGL,KAAK;;MAErC,IAAI,IAAI,CAACmE,aAAa,IAAI,IAAI,EAAE;QAC9B,IAAI,CAACA,aAAa,CAAC9D,SAAS,GAAGL,KAAK;;IAExC;EAAC;IAAAD,GAAA;IAAAC,KAAA,EAEQ,SAAAW,WAAA,EAAU;MACjB,OAAO,IAAI,CAACsD,YAAY,CAACtD,UAAU,EAAE,CAAC0B,MAAM,CACxC,IAAI,CAAC8B,aAAa,CAACxD,UAAU,EAAE,CAAC;IACtC;EAAC;IAAAZ,GAAA;IAAAC,KAAA,EAEQ,SAAAY,WAAWC,OAAiB;MACnC,IAAM+D,UAAU,GAAG/D,OAAO,CAACe,MAAM;MACjC,IAAMiD,cAAc,GAAGC,IAAI,CAACC,KAAK,CAACH,UAAU,GAAG,CAAC,CAAC;MACjD,IAAI,CAACX,YAAY,CAACrD,UAAU,CAACC,OAAO,CAAC8B,KAAK,CAAC,CAAC,EAAEkC,cAAc,CAAC,CAAC;MAC9D,IAAI,CAACV,aAAa,CAACvD,UAAU,CAACC,OAAO,CAAC8B,KAAK,CAACkC,cAAc,CAAC,CAAC;IAC9D;EAAC;IAAA9E,GAAA;IAAAC,KAAA,EAEQ,SAAA4C,mBAAmB1C,UAAyB;MACnD,IAAI8E,WAAW,GACX,IAAI,CAACf,YAAY,CAACrB,kBAAkB,CAAC1C,UAAU,CAAC;MACpD,IAAI,EAAE+E,KAAK,CAACC,OAAO,CAACF,WAAW,CAAC,IAAIC,KAAK,CAACC,OAAO,CAACF,WAAW,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE;QAClEA,WAAW,GAAG,CAACA,WAAoB,CAAC;;MAEtCA,WAAW,GAAGA,WAAsB;MAEpC,IAAIG,WAAkB;MACtB,IAAIC,YAAqB;MACzB,IAAIC,UAAmB;MACvB,IAAI,IAAI,CAACZ,WAAW,EAAE;QACpBY,UAAU,GAAGL,WAAW,CAACrC,KAAK,CAAC,CAAC,CAAC;QACjCwC,WAAW,GAAGH,WAAW,CAAC,CAAC,CAAC;OAC7B,MAAM;QACLG,WAAW,GAAGH,WAAW,CAAC,CAAC,CAAC;;MAE9BG,WAAW,GAAGA,WAAW;MACzB,IAAI,IAAI,CAACd,SAAS,KAAK,QAAQ,EAAE;QAC/Bc,WAAW,CAACA,WAAW,CAACvD,MAAM,GAAG,CAAC,CAAC,IAAI,CAAC;QACxCwD,YAAY,GAAG,CAACD,WAAW,CAAC;OAC7B,MAAM,IAAI,IAAI,CAACd,SAAS,IAAI,IAAI,EAAE;QACjCe,YAAY,GAAG,CAACD,WAAW,EAAEA,WAAW,CAACxC,KAAK,EAAE,CAAC;OAClD,MAAM;QACLyC,YAAY,GAAG,CAACD,WAAW,CAAC;;MAG9B,IAAI,IAAI,CAACV,WAAW,EAAE;QACpB,IAAI,IAAI,CAACJ,SAAS,IAAI,IAAI,EAAE;UAC1B,OAAOe,YAAY,CAAC/C,MAAM,CAACgD,UAAU,CAAC,CAAChD,MAAM,CAACgD,UAAU,CAAC1C,KAAK,EAAE,CAAC;;QAEnE,OAAO,CAACwC,WAAW,CAAC,CAAC9C,MAAM,CAACgD,UAAU,CAAC,CAAChD,MAAM,CAACgD,UAAU,CAAC1C,KAAK,EAAE,CAAC;;MAEpE,OAAO7D,aAAa,CAACwG,gBAAgB,CAACF,YAAY,CAAC;IACrD;EAAC;IAAArF,GAAA;IAAAC,KAAA,EAEQ,SAAAuF,MACLxC,MAAuD,EACvDC,MAAe;MACjB,IAAIwC,YAAY,GACZxC,MAAM,IAAI,IAAI,GAAG,IAAI,GAAGA,MAAM,CAAC,cAAc,CAAC;MAClD,IAAIyC,SAAS,GACTzC,MAAM,IAAI,IAAI,GAAG,IAAI,GAAGA,MAAM,CAAC,WAAW,CAAC;MAC/C,IAAIA,MAAM,IAAI,IAAI,EAAE;QAClBA,MAAM,GAAG,EAAE;;MAEb,IAAM0C,YAAY,GACdxG,eAAe,CAAC6D,MAAM,EAAEyC,YAAY,EAAEC,SAAS,EAAE,IAAI,CAACd,YAAY,CAAC;MACvE5B,MAAM,GAAG2C,YAAY,CAAC3C,MAAM;MAC5ByC,YAAY,GAAGE,YAAY,CAACF,YAAY;MACxCC,SAAS,GAAGC,YAAY,CAACD,SAAS;MAElC,IAAIR,KAAK,CAACC,OAAO,CAACnC,MAAM,CAAC,EAAE;QACzByC,YAAY,GAAIzC,MAAsC,CAACJ,KAAK,CAAC,CAAC,CAAC;QAC/DI,MAAM,GAAIA,MAAsC,CAAC,CAAC,CAAC;;MAGrD,IAAI,CAACyC,YAAY,IAAI,IAAI,IAAIA,YAAY,CAAC5D,MAAM,KAAK,CAAC,KAClD6D,SAAS,IAAI,IAAI,EAAE;QACrB,OAAAvE,IAAA,CAAAC,eAAA,CAAAyC,aAAA,CAAAxC,SAAA,kBAAAxB,IAAA,OAAmBmD,MAAM,EAAEC,MAAM;;MAEnC,IAAM2C,gBAAgB,GAAiC,EAAE;MACzD,IAAMC,eAAe,GAAgB,EAAE;MACvC,IAAIJ,YAAY,IAAI,IAAI,EAAE;QACxB,IAAMK,SAAS,GAAGL,YAAY,CAAC5D,MAAM;QACrC,IAAIiE,SAAS,GAAG,CAAC,GAAG,CAAC,EAAE;UACrB,MAAM,IAAIjH,UAAU,CAChB,qDAAqD,GACrD,wDAAwD,GACxD,sBAAsB,CAAC;;QAE7BoE,MAAM,CAAC,cAAc,CAAC,GAAGwC,YAAY;QACrCG,gBAAgB,CAACG,IAAI,CAAAP,KAAA,CAArBI,gBAAgB,EAAAI,kBAAA,CAASP,YAAY,EAAC;QACtC,IAAMQ,UAAU,GAAIR,YAA6C,CACzCS,GAAG,CAAC,UAAAC,KAAK;UAAA,OAAI,IAAI1H,SAAS,CAAC;YAACiE,KAAK,EAAEyD,KAAK,CAACzD;UAAK,CAAC,CAAC;QAAA,EAAC;QACzE,IAAI,CAACwB,YAAY,CAACkC,SAAS,GAAGH,UAAU,CAACrD,KAAK,CAAC,CAAC,EAAEkD,SAAS,GAAG,CAAC,CAAC;QAChE,IAAI,CAAC1B,aAAa,CAACgC,SAAS,GAAGH,UAAU,CAACrD,KAAK,CAACkD,SAAS,GAAG,CAAC,CAAC;QAC9DD,eAAe,CAACE,IAAI,CAAAP,KAAA,CAApBK,eAAe,EAAAG,kBAAA,CAASC,UAAU,EAAC;;MAErC,IAAIP,SAAS,IAAI,IAAI,EAAE;QACrB,MAAM,IAAI9G,mBAAmB,CACzB,uDAAuD,GACvD,kBAAkB,CAAC;;MAGzB,IAAMyH,gBAAgB,GAAGT,gBAAgB,CAAC,CAAC,CAAC,YAAYjH,cAAc;MACtE,SAAA2H,EAAA,MAAAC,iBAAA,GAAqBX,gBAAgB,EAAAU,EAAA,GAAAC,iBAAA,CAAA1E,MAAA,EAAAyE,EAAA,IAAE;QAAlC,IAAME,MAAM,GAAAD,iBAAA,CAAAD,EAAA;QACf,IAAIE,MAAM,YAAY7H,cAAc,KAAK0H,gBAAgB,EAAE;UACzD,MAAM,IAAIxH,UAAU,CAChB,uDAAuD,GACvD,yDAAyD,CAAC;;;MAIlE,IAAIwH,gBAAgB,EAAE;QACpB;QACA,IAAMI,SAAS,GAAG,CAACzD,MAAM,CAAC,CAACV,MAAM,CAACsD,gBAAgB,CAAC;QACnD,IAAMc,aAAa,GAAG,IAAI,CAACjE,SAAS,CAACH,MAAM,CAACuD,eAAe,CAAC;QAC5D;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA,IAAMc,iBAAiB,GAAG,IAAI,CAAClE,SAAS;QACxC,IAAI,CAACA,SAAS,GAAGiE,aAAa;QAC9B,IAAMrD,MAAM,GAAAlC,IAAA,CAAAC,eAAA,CAAAyC,aAAA,CAAAxC,SAAA,kBAAAxB,IAAA,OACI4G,SAAwC,EAAExD,MAAM,CAAC;QACjE,IAAI,CAACR,SAAS,GAAGkE,iBAAiB;QAClC,OAAOtD,MAAM;OACd,MAAM;QACL,OAAAlC,IAAA,CAAAC,eAAA,CAAAyC,aAAA,CAAAxC,SAAA,kBAAAxB,IAAA,OAAmBmD,MAAM,EAAEC,MAAM;;IAErC;EAAC;IAAAjD,GAAA;IAAAC,KAAA,EAEQ,SAAAJ,KAAKmD,MAAuB,EAAEC,MAAc;MAAA,IAAA2D,MAAA;MACnD,OAAOtI,IAAI,CAAC,YAAK;QACf,IAAMmH,YAAY,GAAGxC,MAAM,CAAC,cAAc,CAAC;QAE3C,IAAIM,CAAkB;QACtB,IAAIsD,IAAqB;QACzB,IAAIpB,YAAY,IAAI,IAAI,EAAE;UACxBlC,CAAC,GAAGqD,MAAI,CAAC1C,YAAY,CAACrE,IAAI,CAACmD,MAAM,EAAEC,MAAM,CAAC;UAC1C4D,IAAI,GAAGD,MAAI,CAACxC,aAAa,CAACvE,IAAI,CAACmD,MAAM,EAAEC,MAAM,CAAC;SAC/C,MAAM;UACL,IAAM6D,YAAY,GAAGrB,YAAY,CAAC7C,KAAK,CAAC,CAAC,EAAE6C,YAAY,CAAC5D,MAAM,GAAG,CAAC,CAAC;UACnE,IAAMkF,aAAa,GAAGtB,YAAY,CAAC7C,KAAK,CAAC6C,YAAY,CAAC5D,MAAM,GAAG,CAAC,CAAC;UACjE0B,CAAC,GAAGqD,MAAI,CAAC1C,YAAY,CAACrE,IAAI,CACtBmD,MAAM,EAAE1B,MAAM,CAACC,MAAM,CAAC0B,MAAM,EAAE;YAACwC,YAAY,EAAEqB;UAAY,CAAC,CAAC,CAAC;UAChED,IAAI,GAAGD,MAAI,CAACxC,aAAa,CAACvE,IAAI,CAC1BmD,MAAM,EAAE1B,MAAM,CAACC,MAAM,CAAC0B,MAAM,EAAE;YAACwC,YAAY,EAAEsB;UAAa,CAAC,CAAC,CAAC;;QAGnE,IAAI3D,MAAgB;QACpB,IAAIwD,MAAI,CAAClC,WAAW,EAAE;UACpB,IAAIQ,KAAK,CAACC,OAAO,CAAC5B,CAAC,CAAC,EAAE;YACpBH,MAAM,GAAGG,CAAC,CAACX,KAAK,CAAC,CAAC,CAAC,CAACN,MAAM,CAAEuE,IAAiB,CAACjE,KAAK,CAAC,CAAC,CAAC,CAAC;WACxD,MAAM,C;UAEPW,CAAC,GAAIA,CAAc,CAAC,CAAC,CAAC;UACtBsD,IAAI,GAAIA,IAAiB,CAAC,CAAC,CAAC;;QAG9B,IAAID,MAAI,CAACnC,eAAe,EAAE;UACxBoC,IAAI,GAAGzI,GAAG,CAAC4I,OAAO,CAACH,IAAc,EAAE,CAAC,CAAC;;QAGvC,IAAIxD,MAAuB;QAC3B,IAAIuD,MAAI,CAACtC,SAAS,KAAK,QAAQ,EAAE;UAC/BjB,MAAM,GAAG9E,CAAC,CAAC0I,WAAW,CAAC,CAAC1D,CAAW,EAAEsD,IAAc,CAAC,CAAC;SACtD,MAAM,IAAID,MAAI,CAACtC,SAAS,KAAK,KAAK,EAAE;UACnCjB,MAAM,GAAGjF,GAAG,CAAC8I,GAAG,CAAC3D,CAAW,EAAEsD,IAAc,CAAC;SAC9C,MAAM,IAAID,MAAI,CAACtC,SAAS,KAAK,KAAK,EAAE;UACnCjB,MAAM,GAAGjF,GAAG,CAAC+I,GAAG,CAAC,EAAE,EAAE/I,GAAG,CAAC8I,GAAG,CAAC3D,CAAW,EAAEsD,IAAc,CAAC,CAAC;SAC3D,MAAM,IAAID,MAAI,CAACtC,SAAS,KAAK,KAAK,EAAE;UACnCjB,MAAM,GAAGjF,GAAG,CAAC+I,GAAG,CAAC5D,CAAW,EAAEsD,IAAc,CAAC;SAC9C,MAAM,IAAID,MAAI,CAACtC,SAAS,IAAI,IAAI,EAAE;UACjCjB,MAAM,GAAG,CAACE,CAAW,EAAEsD,IAAc,CAAC;;QAGxC;QACA,IAAID,MAAI,CAAClC,WAAW,EAAE;UACpB,IAAIkC,MAAI,CAACtC,SAAS,IAAI,IAAI,EAAE;YAC1B,OAAQjB,MAAmB,CAACf,MAAM,CAACc,MAAM,CAAC;;UAE5C,OAAO,CAACC,MAAgB,CAAC,CAACf,MAAM,CAACc,MAAM,CAAC;;QAE1C,OAAOC,MAAM;MACf,CAAC,CAAC;IACJ;EAAC;IAAArD,GAAA;IAAAC,KAAA,EAEQ,SAAAmH,YAAYhE,MAAwB;MAC3C,IAAI,CAACc,YAAY,CAACkD,WAAW,EAAE;MAC/B,IAAI,CAAChD,aAAa,CAACgD,WAAW,EAAE;IAClC;EAAC;IAAApH,GAAA;IAAAC,KAAA,EAEQ,SAAAC,MAAMC,UAAyB;MAAA,IAAAkH,MAAA;MACtC7I,SAAS,CAAC,IAAI,CAAC0F,YAAY,CAACG,IAAI,EAAE,YAAK;QACrCgD,MAAI,CAACnD,YAAY,CAAChE,KAAK,CAACC,UAAU,CAAC;MACrC,CAAC,CAAC;MACF3B,SAAS,CAAC,IAAI,CAAC4F,aAAa,CAACC,IAAI,EAAE,YAAK;QACtCgD,MAAI,CAACjD,aAAa,CAAClE,KAAK,CAACC,UAAU,CAAC;MACtC,CAAC,CAAC;MACF,IAAI,CAACC,KAAK,GAAG,IAAI;IACnB;EAAC;IAAAJ,GAAA;IAAAC,KAAA,EAEQ,SAAAqH,YAAYtE,MAAuB,EAAEuE,IAAsB;MAElE,IAAIrC,KAAK,CAACC,OAAO,CAACoC,IAAI,CAAC,EAAE;QACvBA,IAAI,GAAGA,IAAI,CAAC,CAAC,CAAC;;MAEhB,IAAIC,UAA2B;MAC/B,IAAI,IAAI,CAAC/C,eAAe,EAAE;QACxB,IAAI,IAAI,CAACH,SAAS,IAAI,IAAI,EAAE;UAC1BkD,UAAU,GAAG,CAACD,IAAI,EAAEA,IAAI,CAAC;SAC1B,MAAM;UACLC,UAAU,GAAGD,IAAI;;OAEpB,MAAM;QACL,IAAI,IAAI,CAACjD,SAAS,IAAI,IAAI,EAAE;UAC1BkD,UAAU,GAAG,CAAC,IAAI,EAAE,IAAI,CAAC;SAC1B,MAAM;UACLA,UAAU,GAAG,IAAI;;;MAGrB,IAAI,IAAI,CAAC9C,WAAW,EAAE;QACpB,IAAMtB,MAAM,GAAG,IAAI,CAACc,YAAY,CAACd,MAAM;QACvC,IAAMqE,SAAS,GAAarE,MAAM,CAAC8C,GAAG,CAAC,UAAAC,KAAK;UAAA,OAAI,IAAI;QAAA,EAAC;QACrD,IAAIjB,KAAK,CAACC,OAAO,CAACqC,UAAU,CAAC,EAAE;UAC7B,OAAOA,UAAU,CAAClF,MAAM,CAACmF,SAAS,CAAC,CAACnF,MAAM,CAACmF,SAAS,CAAC;SACtD,MAAM;UACL,OAAO,CAACD,UAAU,CAAC,CAAClF,MAAM,CAACmF,SAAS,CAAC,CAACnF,MAAM,CAACmF,SAAS,CAAC;;OAE1D,MAAM;QACL,OAAOD,UAAU;;IAErB;EAAC;IAAAxH,GAAA;IAAAK,GAAA,EAED,SAAAA,IAAA,EAA6B;MAC3B,OAAO,IAAI,CAAC6D,YAAY,CAAC1D,gBAAgB,CAAC8B,MAAM,CAC5C,IAAI,CAAC8B,aAAa,CAAC5D,gBAAgB,CAAC;IAC1C;EAAC;IAAAR,GAAA;IAAAK,GAAA,EAED,SAAAA,IAAA,EAAgC;MAC9B,OAAO,IAAI,CAAC6D,YAAY,CAACzD,mBAAmB,CAAC6B,MAAM,CAC/C,IAAI,CAAC8B,aAAa,CAAC3D,mBAAmB,CAAC;IAC7C;IAEA;EAAA;IAAAT,GAAA;IAAAC,KAAA,EAES,SAAAuB,6BAA6BvB,KAAc;MAClDkB,IAAA,CAAAC,eAAA,CAAAyC,aAAA,CAAAxC,SAAA,yCAAAxB,IAAA,OAAmCI,KAAK;MACxC,IAAI,IAAI,CAACiE,YAAY,IAAI,IAAI,EAAE;QAC7B,IAAI,CAACA,YAAY,CAAC1C,4BAA4B,CAACvB,KAAK,CAAC;;MAEvD,IAAI,IAAI,CAACmE,aAAa,IAAI,IAAI,EAAE;QAC9B,IAAI,CAACA,aAAa,CAAC5C,4BAA4B,CAACvB,KAAK,CAAC;;IAE1D;EAAC;IAAAD,GAAA;IAAAC,KAAA,EAEQ,SAAAc,UAAA,EAAS;MAChB,IAAMC,MAAM,GAA6B;QACvC,WAAW,EAAE,IAAI,CAACsD;OACnB;MACD;MACA,IAAMpD,UAAU,GAAAC,IAAA,CAAAC,eAAA,CAAAyC,aAAA,CAAAxC,SAAA,sBAAAxB,IAAA,MAAoB;MACpCyB,MAAM,CAACC,MAAM,CAACP,MAAM,EAAEE,UAAU,CAAC;MACjC,OAAOF,MAAM;IACf;IAEA;EAAA;IAAAhB,GAAA;IAAAC,KAAA,EACA,SAAAwB,WACIC,GAA6C,EAC7CV,MAAgC;MAClC,IAAM0G,QAAQ,GACVtI,WAAW,CAAC4B,MAAM,CAAC,OAAO,CAA6B,CAAQ;MACnE,OAAOA,MAAM,CAAC,OAAO,CAAC;MACtB;MACA,IAAIA,MAAM,CAAC,cAAc,CAAC,IAAI,IAAI,EAAE;QAClC,MAAM,IAAIpC,mBAAmB,CACzB,+FAC+B,CAAC;;MAEtC;MACA,IAAMoD,SAAS,GAAyBhB,MAAM;MAC9CgB,SAAS,CAAC,OAAO,CAAC,GAAG0F,QAAQ;MAC7B,OAAO,IAAIhG,GAAG,CAACM,SAAS,CAAC;IAC3B;EAAC;EAAA,OAAA6B,aAAA;AAAA,EAhWgCxE,OAAO;AACxC;AACOwE,aAAA,CAAAL,SAAS,GAAG,eAAe;AAgWpCnF,aAAa,CAACoF,aAAa,CAACI,aAAa,CAAC"},"metadata":{},"sourceType":"module","externalDependencies":[]}