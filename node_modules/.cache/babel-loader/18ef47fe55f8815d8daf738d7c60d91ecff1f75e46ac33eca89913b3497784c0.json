{"ast":null,"code":"import _slicedToArray from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/slicedToArray.js\";\n/**\n * @license\n * Copyright 2020 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { customGrad } from '../gradients';\nimport { convertToTensor } from '../tensor_util_env';\nimport { cast } from './cast';\nimport { exp } from './exp';\nimport { log } from './log';\nimport { max } from './max';\nimport { mul } from './mul';\nimport { op } from './operation';\nimport { sub } from './sub';\nimport { sum } from './sum';\n/**\n * Computes the log softmax.\n *\n * ```js\n * const a = tf.tensor1d([1, 2, 3]);\n *\n * a.logSoftmax().print();  // or tf.logSoftmax(a)\n * ```\n *\n * ```js\n * const a = tf.tensor2d([2, 4, 6, 1, 2, 3], [2, 3]);\n *\n * a.logSoftmax().print();  // or tf.logSoftmax(a)\n * ```\n *\n * @param logits The logits array.\n * @param axis The dimension softmax would be performed on. Defaults to `-1`\n *     which indicates the last dimension.\n *\n * @doc {heading: 'Operations', subheading: 'Normalization'}\n */\nfunction logSoftmax_(logits) {\n  var axis = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : -1;\n  var $logits = convertToTensor(logits, 'logits', 'logSoftmax');\n  if (axis === -1) {\n    axis = $logits.rank - 1;\n  }\n  if (axis !== $logits.rank - 1) {\n    throw Error('Log Softmax along a non-last dimension is not yet supported. ' + \"Logits was rank \".concat($logits.rank, \" and axis was \").concat(axis));\n  }\n  // const forward: ForwardFunc<Tensor> = (backend, save) => {\n  //   const keepDims = true;\n  //   const xMax = max(logits, axis, true);\n  //   const shifted = sub(logits, xMax);\n  //   const value =\n  //       sub(cast(shifted, 'float32'), log(sum(exp(shifted), axis,\n  //       keepDims)));\n  //   save([value]);\n  //   return value;\n  // };\n  // Use a custom gradient for numerical stability.\n  var customOp = customGrad(function (logits, save) {\n    var keepDims = true;\n    var xMax = max(logits, axis, true);\n    var shifted = sub(logits, xMax);\n    var value = sub(cast(shifted, 'float32'), log(sum(exp(shifted), axis, keepDims)));\n    save([value]);\n    var gradFunc = function gradFunc(dy, saved) {\n      var _saved = _slicedToArray(saved, 1),\n        value = _saved[0];\n      var keepDims = true;\n      var softmax = exp(value);\n      return sub(dy, mul(sum(dy, axis, keepDims), softmax));\n    };\n    return {\n      value: value,\n      gradFunc: gradFunc\n    };\n  });\n  return customOp($logits);\n  // TODO Use Engine.runKernel when CPU/WebGL/WASM backends implement this.\n  // const inputs: LogSoftmaxInputs = {logits: $logits};\n  // const attrs: LogSoftmaxAttrs = {axis};\n  // return ENGINE.runKernel(\n  //            LogSoftmax, inputs as unknown as NamedTensorMap,\n  //            attrs as unknown as NamedAttrMap);\n}\n\nexport var logSoftmax = /* @__PURE__ */op({\n  logSoftmax_: logSoftmax_\n});","map":{"version":3,"names":["customGrad","convertToTensor","cast","exp","log","max","mul","op","sub","sum","logSoftmax_","logits","axis","arguments","length","undefined","$logits","rank","Error","concat","customOp","save","keepDims","xMax","shifted","value","gradFunc","dy","saved","_saved","_slicedToArray","softmax","logSoftmax"],"sources":["C:\\Users\\vince\\OneDrive\\Documents\\GitHub\\tfjs-core\\src\\ops\\log_softmax.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2020 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {customGrad} from '../gradients';\n\nimport {Tensor} from '../tensor';\nimport {GradSaveFunc} from '../tensor_types';\nimport {convertToTensor} from '../tensor_util_env';\nimport {TensorLike} from '../types';\n\nimport {cast} from './cast';\nimport {exp} from './exp';\nimport {log} from './log';\nimport {max} from './max';\nimport {mul} from './mul';\nimport {op} from './operation';\nimport {sub} from './sub';\nimport {sum} from './sum';\n\n/**\n * Computes the log softmax.\n *\n * ```js\n * const a = tf.tensor1d([1, 2, 3]);\n *\n * a.logSoftmax().print();  // or tf.logSoftmax(a)\n * ```\n *\n * ```js\n * const a = tf.tensor2d([2, 4, 6, 1, 2, 3], [2, 3]);\n *\n * a.logSoftmax().print();  // or tf.logSoftmax(a)\n * ```\n *\n * @param logits The logits array.\n * @param axis The dimension softmax would be performed on. Defaults to `-1`\n *     which indicates the last dimension.\n *\n * @doc {heading: 'Operations', subheading: 'Normalization'}\n */\nfunction logSoftmax_<T extends Tensor>(logits: T|TensorLike, axis = -1): T {\n  const $logits = convertToTensor(logits, 'logits', 'logSoftmax');\n\n  if (axis === -1) {\n    axis = $logits.rank - 1;\n  }\n  if (axis !== $logits.rank - 1) {\n    throw Error(\n        'Log Softmax along a non-last dimension is not yet supported. ' +\n        `Logits was rank ${$logits.rank} and axis was ${axis}`);\n  }\n\n  // const forward: ForwardFunc<Tensor> = (backend, save) => {\n  //   const keepDims = true;\n  //   const xMax = max(logits, axis, true);\n  //   const shifted = sub(logits, xMax);\n  //   const value =\n  //       sub(cast(shifted, 'float32'), log(sum(exp(shifted), axis,\n  //       keepDims)));\n  //   save([value]);\n  //   return value;\n  // };\n\n  // Use a custom gradient for numerical stability.\n  const customOp = customGrad((logits: Tensor, save: GradSaveFunc) => {\n    const keepDims = true;\n    const xMax = max(logits, axis, true);\n    const shifted = sub(logits, xMax);\n    const value =\n        sub(cast(shifted, 'float32'), log(sum(exp(shifted), axis, keepDims)));\n    save([value]);\n\n    const gradFunc = (dy: Tensor, saved: Tensor[]) => {\n      const [value] = saved;\n      const keepDims = true;\n      const softmax = exp(value);\n      return sub(dy, mul(sum(dy, axis, keepDims), softmax));\n    };\n    return {value, gradFunc};\n  });\n\n  return customOp($logits) as T;\n\n  // TODO Use Engine.runKernel when CPU/WebGL/WASM backends implement this.\n  // const inputs: LogSoftmaxInputs = {logits: $logits};\n  // const attrs: LogSoftmaxAttrs = {axis};\n  // return ENGINE.runKernel(\n  //            LogSoftmax, inputs as unknown as NamedTensorMap,\n  //            attrs as unknown as NamedAttrMap);\n}\n\nexport const logSoftmax = /* @__PURE__ */ op({logSoftmax_});\n"],"mappings":";AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQA,UAAU,QAAO,cAAc;AAIvC,SAAQC,eAAe,QAAO,oBAAoB;AAGlD,SAAQC,IAAI,QAAO,QAAQ;AAC3B,SAAQC,GAAG,QAAO,OAAO;AACzB,SAAQC,GAAG,QAAO,OAAO;AACzB,SAAQC,GAAG,QAAO,OAAO;AACzB,SAAQC,GAAG,QAAO,OAAO;AACzB,SAAQC,EAAE,QAAO,aAAa;AAC9B,SAAQC,GAAG,QAAO,OAAO;AACzB,SAAQC,GAAG,QAAO,OAAO;AAEzB;;;;;;;;;;;;;;;;;;;;;AAqBA,SAASC,WAAWA,CAAmBC,MAAoB,EAAW;EAAA,IAATC,IAAI,GAAAC,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,CAAC,CAAC;EACpE,IAAMG,OAAO,GAAGf,eAAe,CAACU,MAAM,EAAE,QAAQ,EAAE,YAAY,CAAC;EAE/D,IAAIC,IAAI,KAAK,CAAC,CAAC,EAAE;IACfA,IAAI,GAAGI,OAAO,CAACC,IAAI,GAAG,CAAC;;EAEzB,IAAIL,IAAI,KAAKI,OAAO,CAACC,IAAI,GAAG,CAAC,EAAE;IAC7B,MAAMC,KAAK,CACP,+DAA+D,sBAAAC,MAAA,CAC5CH,OAAO,CAACC,IAAI,oBAAAE,MAAA,CAAiBP,IAAI,CAAE,CAAC;;EAG7D;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EAEA;EACA,IAAMQ,QAAQ,GAAGpB,UAAU,CAAC,UAACW,MAAc,EAAEU,IAAkB,EAAI;IACjE,IAAMC,QAAQ,GAAG,IAAI;IACrB,IAAMC,IAAI,GAAGlB,GAAG,CAACM,MAAM,EAAEC,IAAI,EAAE,IAAI,CAAC;IACpC,IAAMY,OAAO,GAAGhB,GAAG,CAACG,MAAM,EAAEY,IAAI,CAAC;IACjC,IAAME,KAAK,GACPjB,GAAG,CAACN,IAAI,CAACsB,OAAO,EAAE,SAAS,CAAC,EAAEpB,GAAG,CAACK,GAAG,CAACN,GAAG,CAACqB,OAAO,CAAC,EAAEZ,IAAI,EAAEU,QAAQ,CAAC,CAAC,CAAC;IACzED,IAAI,CAAC,CAACI,KAAK,CAAC,CAAC;IAEb,IAAMC,QAAQ,GAAG,SAAXA,QAAQA,CAAIC,EAAU,EAAEC,KAAe,EAAI;MAC/C,IAAAC,MAAA,GAAAC,cAAA,CAAgBF,KAAK;QAAdH,KAAK,GAAAI,MAAA;MACZ,IAAMP,QAAQ,GAAG,IAAI;MACrB,IAAMS,OAAO,GAAG5B,GAAG,CAACsB,KAAK,CAAC;MAC1B,OAAOjB,GAAG,CAACmB,EAAE,EAAErB,GAAG,CAACG,GAAG,CAACkB,EAAE,EAAEf,IAAI,EAAEU,QAAQ,CAAC,EAAES,OAAO,CAAC,CAAC;IACvD,CAAC;IACD,OAAO;MAACN,KAAK,EAALA,KAAK;MAAEC,QAAQ,EAARA;IAAQ,CAAC;EAC1B,CAAC,CAAC;EAEF,OAAON,QAAQ,CAACJ,OAAO,CAAM;EAE7B;EACA;EACA;EACA;EACA;EACA;AACF;;AAEA,OAAO,IAAMgB,UAAU,GAAG,eAAgBzB,EAAE,CAAC;EAACG,WAAW,EAAXA;AAAW,CAAC,CAAC"},"metadata":{},"sourceType":"module","externalDependencies":[]}