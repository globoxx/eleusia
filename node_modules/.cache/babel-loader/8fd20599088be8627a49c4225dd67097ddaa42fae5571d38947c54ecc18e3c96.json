{"ast":null,"code":"import _createForOfIteratorHelper from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/createForOfIteratorHelper.js\";\nimport _classCallCheck from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/classCallCheck.js\";\nimport _createClass from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/createClass.js\";\nimport _get from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/get.js\";\nimport _getPrototypeOf from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/getPrototypeOf.js\";\nimport _inherits from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/inherits.js\";\nimport _createSuper from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/createSuper.js\";\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n *  Advanced activation layers.\n */\nimport { cast, clipByValue, elu, greater, leakyRelu, mul, prelu, relu, serialization } from '@tensorflow/tfjs-core';\nimport { Softmax as softmaxActivation } from '../activations';\nimport { getConstraint, serializeConstraint } from '../constraints';\nimport { InputSpec, Layer } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { getInitializer, serializeInitializer } from '../initializers';\nimport { getRegularizer, serializeRegularizer } from '../regularizers';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\nexport var ReLU = /*#__PURE__*/function (_Layer) {\n  _inherits(ReLU, _Layer);\n  var _super = _createSuper(ReLU);\n  function ReLU(args) {\n    var _this;\n    _classCallCheck(this, ReLU);\n    _this = _super.call(this, args == null ? {} : args);\n    _this.supportsMasking = true;\n    if (args != null) {\n      _this.maxValue = args.maxValue;\n    }\n    return _this;\n  }\n  _createClass(ReLU, [{\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      inputs = getExactlyOneTensor(inputs);\n      var output = relu(inputs);\n      if (this.maxValue != null) {\n        output = clipByValue(output, 0, this.maxValue);\n      }\n      return output;\n    }\n  }, {\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      return inputShape;\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        maxValue: this.maxValue\n      };\n      var baseConfig = _get(_getPrototypeOf(ReLU.prototype), \"getConfig\", this).call(this);\n      Object.assign(config, baseConfig);\n      return config;\n    }\n  }]);\n  return ReLU;\n}(Layer);\n/** @nocollapse */\nReLU.className = 'ReLU';\nserialization.registerClass(ReLU);\nexport var LeakyReLU = /*#__PURE__*/function (_Layer2) {\n  _inherits(LeakyReLU, _Layer2);\n  var _super2 = _createSuper(LeakyReLU);\n  function LeakyReLU(args) {\n    var _this2;\n    _classCallCheck(this, LeakyReLU);\n    _this2 = _super2.call(this, args == null ? {} : args);\n    _this2.DEFAULT_ALPHA = 0.3;\n    if (args == null) {\n      args = {};\n    }\n    _this2.alpha = args.alpha == null ? _this2.DEFAULT_ALPHA : args.alpha;\n    return _this2;\n  }\n  _createClass(LeakyReLU, [{\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var x = getExactlyOneTensor(inputs);\n      return leakyRelu(x, this.alpha);\n    }\n  }, {\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      return inputShape;\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        alpha: this.alpha\n      };\n      var baseConfig = _get(_getPrototypeOf(LeakyReLU.prototype), \"getConfig\", this).call(this);\n      Object.assign(config, baseConfig);\n      return config;\n    }\n  }]);\n  return LeakyReLU;\n}(Layer);\n/** @nocollapse */\nLeakyReLU.className = 'LeakyReLU';\nserialization.registerClass(LeakyReLU);\nexport var PReLU = /*#__PURE__*/function (_Layer3) {\n  _inherits(PReLU, _Layer3);\n  var _super3 = _createSuper(PReLU);\n  function PReLU(args) {\n    var _this3;\n    _classCallCheck(this, PReLU);\n    _this3 = _super3.call(this, args == null ? {} : args);\n    _this3.DEFAULT_ALPHA_INITIALIZER = 'zeros';\n    if (args == null) {\n      args = {};\n    }\n    _this3.supportsMasking = true;\n    _this3.alphaInitializer = getInitializer(args.alphaInitializer || _this3.DEFAULT_ALPHA_INITIALIZER);\n    _this3.alphaRegularizer = getRegularizer(args.alphaRegularizer);\n    _this3.alphaConstraint = getConstraint(args.alphaConstraint);\n    if (args.sharedAxes == null) {\n      _this3.sharedAxes = null;\n    } else if (Array.isArray(args.sharedAxes)) {\n      _this3.sharedAxes = args.sharedAxes;\n    } else if (typeof args.sharedAxes === 'number') {\n      _this3.sharedAxes = [args.sharedAxes];\n    } else {\n      throw new ValueError(\"Expected sharedAxes to be a number or an array of numbers, \" + \"but got \".concat(args.sharedAxes));\n    }\n    return _this3;\n  }\n  _createClass(PReLU, [{\n    key: \"build\",\n    value: function build(inputShape) {\n      inputShape = getExactlyOneShape(inputShape);\n      var paramShape = inputShape.slice(1);\n      if (this.sharedAxes != null) {\n        var _iterator = _createForOfIteratorHelper(this.sharedAxes),\n          _step;\n        try {\n          for (_iterator.s(); !(_step = _iterator.n()).done;) {\n            var i = _step.value;\n            paramShape[i - 1] = 1;\n          }\n        } catch (err) {\n          _iterator.e(err);\n        } finally {\n          _iterator.f();\n        }\n      }\n      this.alpha = this.addWeight('alpha', paramShape, 'float32', this.alphaInitializer, this.alphaRegularizer, true, this.alphaConstraint);\n      // Set input spec.\n      var axes = {};\n      if (this.sharedAxes != null) {\n        for (var _i = 1; _i < inputShape.length; ++_i) {\n          axes[_i] = inputShape[_i];\n        }\n      }\n      this.inputSpec = [new InputSpec({\n        ndim: inputShape.length,\n        axes: axes\n      })];\n      this.built = true;\n    }\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      inputs = getExactlyOneTensor(inputs);\n      return prelu(inputs, this.alpha.read());\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        alphaInitializer: serializeInitializer(this.alphaInitializer),\n        alphaRegularizer: serializeRegularizer(this.alphaRegularizer),\n        alphaConstraint: serializeConstraint(this.alphaConstraint),\n        sharedAxes: this.sharedAxes\n      };\n      var baseConfig = _get(_getPrototypeOf(PReLU.prototype), \"getConfig\", this).call(this);\n      Object.assign(config, baseConfig);\n      return config;\n    }\n  }]);\n  return PReLU;\n}(Layer);\n/** @nocollapse */\nPReLU.className = 'PReLU';\nserialization.registerClass(PReLU);\nexport var ELU = /*#__PURE__*/function (_Layer4) {\n  _inherits(ELU, _Layer4);\n  var _super4 = _createSuper(ELU);\n  function ELU(args) {\n    var _this4;\n    _classCallCheck(this, ELU);\n    _this4 = _super4.call(this, args == null ? {} : args);\n    _this4.DEFAULT_ALPHA = 1.0;\n    if (args == null) {\n      args = {};\n    }\n    if (args.alpha != null && args.alpha !== _this4.DEFAULT_ALPHA) {\n      throw new NotImplementedError(\"Non-default alpha value (\".concat(args.alpha, \") is not supported by the \") + \"ELU layer yet.\");\n    }\n    _this4.alpha = args.alpha == null ? _this4.DEFAULT_ALPHA : args.alpha;\n    return _this4;\n  }\n  _createClass(ELU, [{\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var x = getExactlyOneTensor(inputs);\n      return elu(x);\n    }\n  }, {\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      return inputShape;\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        alpha: this.alpha\n      };\n      var baseConfig = _get(_getPrototypeOf(ELU.prototype), \"getConfig\", this).call(this);\n      Object.assign(config, baseConfig);\n      return config;\n    }\n  }]);\n  return ELU;\n}(Layer);\n/** @nocollapse */\nELU.className = 'ELU';\nserialization.registerClass(ELU);\nexport var ThresholdedReLU = /*#__PURE__*/function (_Layer5) {\n  _inherits(ThresholdedReLU, _Layer5);\n  var _super5 = _createSuper(ThresholdedReLU);\n  function ThresholdedReLU(args) {\n    var _this5;\n    _classCallCheck(this, ThresholdedReLU);\n    _this5 = _super5.call(this, args == null ? {} : args);\n    _this5.DEFAULT_THETA = 1.0;\n    if (args == null) {\n      args = {};\n    }\n    _this5.theta = args.theta == null ? _this5.DEFAULT_THETA : args.theta;\n    return _this5;\n  }\n  _createClass(ThresholdedReLU, [{\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var x = getExactlyOneTensor(inputs);\n      return mul(x, cast(greater(x, this.theta), 'float32'));\n    }\n  }, {\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      return inputShape;\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        theta: this.theta\n      };\n      var baseConfig = _get(_getPrototypeOf(ThresholdedReLU.prototype), \"getConfig\", this).call(this);\n      Object.assign(config, baseConfig);\n      return config;\n    }\n  }]);\n  return ThresholdedReLU;\n}(Layer);\n/** @nocollapse */\nThresholdedReLU.className = 'ThresholdedReLU';\nserialization.registerClass(ThresholdedReLU);\nexport var Softmax = /*#__PURE__*/function (_Layer6) {\n  _inherits(Softmax, _Layer6);\n  var _super6 = _createSuper(Softmax);\n  function Softmax(args) {\n    var _this6;\n    _classCallCheck(this, Softmax);\n    _this6 = _super6.call(this, args == null ? {} : args);\n    _this6.DEFAULT_AXIS = 1.0;\n    if (args == null) {\n      args = {};\n    }\n    _this6.softmax = new softmaxActivation().apply;\n    _this6.axis = args.axis == null ? _this6.DEFAULT_AXIS : args.axis;\n    return _this6;\n  }\n  _createClass(Softmax, [{\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var x = getExactlyOneTensor(inputs);\n      return this.softmax(x, this.axis);\n    }\n  }, {\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      return inputShape;\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        axis: this.axis\n      };\n      var baseConfig = _get(_getPrototypeOf(Softmax.prototype), \"getConfig\", this).call(this);\n      Object.assign(config, baseConfig);\n      return config;\n    }\n  }]);\n  return Softmax;\n}(Layer);\n/** @nocollapse */\nSoftmax.className = 'Softmax';\nserialization.registerClass(Softmax);","map":{"version":3,"names":["cast","clipByValue","elu","greater","leakyRelu","mul","prelu","relu","serialization","Softmax","softmaxActivation","getConstraint","serializeConstraint","InputSpec","Layer","NotImplementedError","ValueError","getInitializer","serializeInitializer","getRegularizer","serializeRegularizer","getExactlyOneShape","getExactlyOneTensor","ReLU","_Layer","_inherits","_super","_createSuper","args","_this","_classCallCheck","call","supportsMasking","maxValue","_createClass","key","value","inputs","kwargs","output","computeOutputShape","inputShape","getConfig","config","baseConfig","_get","_getPrototypeOf","prototype","Object","assign","className","registerClass","LeakyReLU","_Layer2","_super2","_this2","DEFAULT_ALPHA","alpha","x","PReLU","_Layer3","_super3","_this3","DEFAULT_ALPHA_INITIALIZER","alphaInitializer","alphaRegularizer","alphaConstraint","sharedAxes","Array","isArray","concat","build","paramShape","slice","_iterator","_createForOfIteratorHelper","_step","s","n","done","i","err","e","f","addWeight","axes","length","inputSpec","ndim","built","read","ELU","_Layer4","_super4","_this4","ThresholdedReLU","_Layer5","_super5","_this5","DEFAULT_THETA","theta","_Layer6","_super6","_this6","DEFAULT_AXIS","softmax","apply","axis"],"sources":["C:\\Users\\vince\\OneDrive\\Documents\\GitHub\\tfjs-layers\\src\\layers\\advanced_activations.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n *  Advanced activation layers.\n */\n\nimport {cast, clipByValue, elu, greater, leakyRelu, mul, prelu, relu, serialization, Tensor} from '@tensorflow/tfjs-core';\n\nimport {Softmax as softmaxActivation} from '../activations';\nimport {Constraint, getConstraint, serializeConstraint} from '../constraints';\nimport {InputSpec, Layer, LayerArgs} from '../engine/topology';\nimport {NotImplementedError, ValueError} from '../errors';\nimport {getInitializer, Initializer, InitializerIdentifier, serializeInitializer} from '../initializers';\nimport {Shape} from '../keras_format/common';\nimport {getRegularizer, Regularizer, serializeRegularizer} from '../regularizers';\nimport {Kwargs} from '../types';\nimport {getExactlyOneShape, getExactlyOneTensor} from '../utils/types_utils';\nimport {LayerVariable} from '../variables';\n\nexport declare interface ReLULayerArgs extends LayerArgs {\n  /**\n   * Float, the maximum output value.\n   */\n  maxValue?: number;\n}\n\nexport class ReLU extends Layer {\n  /** @nocollapse */\n  static className = 'ReLU';\n  maxValue: number;\n\n  constructor(args?: ReLULayerArgs) {\n    super(args == null ? {} : args);\n    this.supportsMasking = true;\n    if (args != null) {\n      this.maxValue = args.maxValue;\n    }\n  }\n\n  override call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    inputs = getExactlyOneTensor(inputs);\n    let output = relu(inputs);\n    if (this.maxValue != null) {\n      output = clipByValue(output, 0, this.maxValue);\n    }\n    return output;\n  }\n\n  override computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    return inputShape;\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {maxValue: this.maxValue};\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(ReLU);\n\nexport declare interface LeakyReLULayerArgs extends LayerArgs {\n  /**\n   * Float `>= 0`. Negative slope coefficient. Defaults to `0.3`.\n   */\n  alpha?: number;\n}\n\nexport class LeakyReLU extends Layer {\n  /** @nocollapse */\n  static className = 'LeakyReLU';\n  readonly alpha: number;\n\n  readonly DEFAULT_ALPHA = 0.3;\n\n  constructor(args?: LeakyReLULayerArgs) {\n    super(args == null ? {} : args);\n    if (args == null) {\n      args = {};\n    }\n    this.alpha = args.alpha == null ? this.DEFAULT_ALPHA : args.alpha;\n  }\n\n  override call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    const x = getExactlyOneTensor(inputs);\n    return leakyRelu(x, this.alpha);\n  }\n\n  override computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    return inputShape;\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {alpha: this.alpha};\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(LeakyReLU);\n\nexport declare interface PReLULayerArgs extends LayerArgs {\n  /**\n   * Initializer for the learnable alpha.\n   */\n  alphaInitializer?: Initializer|InitializerIdentifier;\n\n  /**\n   * Regularizer for the learnable alpha.\n   */\n  alphaRegularizer?: Regularizer;\n\n  /**\n   * Constraint for the learnable alpha.\n   */\n  alphaConstraint?: Constraint;\n\n  /**\n   * The axes along which to share learnable parameters for the activation\n   * function. For example, if the incoming feature maps are from a 2D\n   * convolution with output shape `[numExamples, height, width, channels]`,\n   * and you wish to share parameters across space (height and width) so that\n   * each filter channels has only one set of parameters, set\n   * `shared_axes: [1, 2]`.\n   */\n  sharedAxes?: number|number[];\n}\n\nexport class PReLU extends Layer {\n  /** @nocollapse */\n  static className = 'PReLU';\n  private readonly alphaInitializer: Initializer;\n  private readonly alphaRegularizer: Regularizer;\n  private readonly alphaConstraint: Constraint;\n  private readonly sharedAxes: number[];\n  private alpha: LayerVariable;\n\n  readonly DEFAULT_ALPHA_INITIALIZER: InitializerIdentifier = 'zeros';\n\n  constructor(args?: PReLULayerArgs) {\n    super(args == null ? {} : args);\n    if (args == null) {\n      args = {};\n    }\n\n    this.supportsMasking = true;\n    this.alphaInitializer =\n        getInitializer(args.alphaInitializer || this.DEFAULT_ALPHA_INITIALIZER);\n    this.alphaRegularizer = getRegularizer(args.alphaRegularizer);\n    this.alphaConstraint = getConstraint(args.alphaConstraint);\n    if (args.sharedAxes == null) {\n      this.sharedAxes = null;\n    } else if (Array.isArray(args.sharedAxes)) {\n      this.sharedAxes = args.sharedAxes;\n    } else if (typeof args.sharedAxes === 'number') {\n      this.sharedAxes = [args.sharedAxes];\n    } else {\n      throw new ValueError(\n          `Expected sharedAxes to be a number or an array of numbers, ` +\n          `but got ${args.sharedAxes}`);\n    }\n  }\n\n  override build(inputShape: Shape|Shape[]) {\n    inputShape = getExactlyOneShape(inputShape);\n    const paramShape: Shape = inputShape.slice(1);\n    if (this.sharedAxes != null) {\n      for (const i of this.sharedAxes) {\n        paramShape[i - 1] = 1;\n      }\n    }\n    this.alpha = this.addWeight(\n        'alpha', paramShape, 'float32', this.alphaInitializer,\n        this.alphaRegularizer, true, this.alphaConstraint);\n    // Set input spec.\n    const axes: {[axis: number]: number} = {};\n    if (this.sharedAxes != null) {\n      for (let i = 1; i < inputShape.length; ++i) {\n        axes[i] = inputShape[i];\n      }\n    }\n    this.inputSpec = [new InputSpec({\n      ndim: inputShape.length,\n      axes,\n    })];\n    this.built = true;\n  }\n\n  override call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    inputs = getExactlyOneTensor(inputs);\n    return prelu(inputs, this.alpha.read());\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {\n      alphaInitializer: serializeInitializer(this.alphaInitializer),\n      alphaRegularizer: serializeRegularizer(this.alphaRegularizer),\n      alphaConstraint: serializeConstraint(this.alphaConstraint),\n      sharedAxes: this.sharedAxes\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(PReLU);\n\nexport declare interface ELULayerArgs extends LayerArgs {\n  /**\n   * Float `>= 0`. Negative slope coefficient. Defaults to `1.0`.\n   */\n  alpha?: number;\n}\n\nexport class ELU extends Layer {\n  /** @nocollapse */\n  static className = 'ELU';\n  readonly alpha: number;\n\n  readonly DEFAULT_ALPHA = 1.0;\n\n  constructor(args?: ELULayerArgs) {\n    super(args == null ? {} : args);\n    if (args == null) {\n      args = {};\n    }\n\n    if (args.alpha != null && args.alpha !== this.DEFAULT_ALPHA) {\n      throw new NotImplementedError(\n          `Non-default alpha value (${args.alpha}) is not supported by the ` +\n          `ELU layer yet.`);\n    }\n\n    this.alpha = args.alpha == null ? this.DEFAULT_ALPHA : args.alpha;\n  }\n\n  override call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    const x = getExactlyOneTensor(inputs);\n    return elu(x);\n  }\n\n  override computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    return inputShape;\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {alpha: this.alpha};\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(ELU);\n\nexport declare interface ThresholdedReLULayerArgs extends LayerArgs {\n  /**\n   * Float >= 0. Threshold location of activation.\n   */\n  theta?: number;\n}\n\nexport class ThresholdedReLU extends Layer {\n  /** @nocollapse */\n  static className = 'ThresholdedReLU';\n  readonly theta: number;\n\n  readonly DEFAULT_THETA = 1.0;\n\n  constructor(args?: ThresholdedReLULayerArgs) {\n    super(args == null ? {} : args);\n    if (args == null) {\n      args = {};\n    }\n\n    this.theta = args.theta == null ? this.DEFAULT_THETA : args.theta;\n  }\n\n  override call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    const x = getExactlyOneTensor(inputs);\n    return mul(x, cast(greater(x, this.theta), 'float32'));\n  }\n\n  override computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    return inputShape;\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {theta: this.theta};\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(ThresholdedReLU);\n\nexport declare interface SoftmaxLayerArgs extends LayerArgs {\n  /**\n   * Integer, axis along which the softmax normalization is applied.\n   * Defaults to `-1` (i.e., the last axis).\n   */\n  axis?: number;\n}\n\nexport class Softmax extends Layer {\n  /** @nocollapse */\n  static className = 'Softmax';\n  readonly axis: number;\n  readonly softmax: (t: Tensor, a?: number) => Tensor;\n  readonly DEFAULT_AXIS = 1.0;\n\n  constructor(args?: SoftmaxLayerArgs) {\n    super(args == null ? {} : args);\n    if (args == null) {\n      args = {};\n    }\n    this.softmax = new softmaxActivation().apply;\n    this.axis = args.axis == null ? this.DEFAULT_AXIS : args.axis;\n  }\n\n  override call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    const x = getExactlyOneTensor(inputs);\n    return this.softmax(x, this.axis);\n  }\n\n  override computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    return inputShape;\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {axis: this.axis};\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(Softmax);\n"],"mappings":";;;;;;;AAAA;;;;;;;;;AAUA;;;AAIA,SAAQA,IAAI,EAAEC,WAAW,EAAEC,GAAG,EAAEC,OAAO,EAAEC,SAAS,EAAEC,GAAG,EAAEC,KAAK,EAAEC,IAAI,EAAEC,aAAa,QAAe,uBAAuB;AAEzH,SAAQC,OAAO,IAAIC,iBAAiB,QAAO,gBAAgB;AAC3D,SAAoBC,aAAa,EAAEC,mBAAmB,QAAO,gBAAgB;AAC7E,SAAQC,SAAS,EAAEC,KAAK,QAAkB,oBAAoB;AAC9D,SAAQC,mBAAmB,EAAEC,UAAU,QAAO,WAAW;AACzD,SAAQC,cAAc,EAAsCC,oBAAoB,QAAO,iBAAiB;AAExG,SAAQC,cAAc,EAAeC,oBAAoB,QAAO,iBAAiB;AAEjF,SAAQC,kBAAkB,EAAEC,mBAAmB,QAAO,sBAAsB;AAU5E,WAAaC,IAAK,0BAAAC,MAAA;EAAAC,SAAA,CAAAF,IAAA,EAAAC,MAAA;EAAA,IAAAE,MAAA,GAAAC,YAAA,CAAAJ,IAAA;EAKhB,SAAAA,KAAYK,IAAoB;IAAA,IAAAC,KAAA;IAAAC,eAAA,OAAAP,IAAA;IAC9BM,KAAA,GAAAH,MAAA,CAAAK,IAAA,OAAMH,IAAI,IAAI,IAAI,GAAG,EAAE,GAAGA,IAAI;IAC9BC,KAAA,CAAKG,eAAe,GAAG,IAAI;IAC3B,IAAIJ,IAAI,IAAI,IAAI,EAAE;MAChBC,KAAA,CAAKI,QAAQ,GAAGL,IAAI,CAACK,QAAQ;;IAC9B,OAAAJ,KAAA;EACH;EAACK,YAAA,CAAAX,IAAA;IAAAY,GAAA;IAAAC,KAAA,EAEQ,SAAAL,KAAKM,MAAuB,EAAEC,MAAc;MACnDD,MAAM,GAAGf,mBAAmB,CAACe,MAAM,CAAC;MACpC,IAAIE,MAAM,GAAGhC,IAAI,CAAC8B,MAAM,CAAC;MACzB,IAAI,IAAI,CAACJ,QAAQ,IAAI,IAAI,EAAE;QACzBM,MAAM,GAAGtC,WAAW,CAACsC,MAAM,EAAE,CAAC,EAAE,IAAI,CAACN,QAAQ,CAAC;;MAEhD,OAAOM,MAAM;IACf;EAAC;IAAAJ,GAAA;IAAAC,KAAA,EAEQ,SAAAI,mBAAmBC,UAAyB;MACnD,OAAOA,UAAU;IACnB;EAAC;IAAAN,GAAA;IAAAC,KAAA,EAEQ,SAAAM,UAAA,EAAS;MAChB,IAAMC,MAAM,GAA6B;QAACV,QAAQ,EAAE,IAAI,CAACA;MAAQ,CAAC;MAClE,IAAMW,UAAU,GAAAC,IAAA,CAAAC,eAAA,CAAAvB,IAAA,CAAAwB,SAAA,sBAAAhB,IAAA,MAAoB;MACpCiB,MAAM,CAACC,MAAM,CAACN,MAAM,EAAEC,UAAU,CAAC;MACjC,OAAOD,MAAM;IACf;EAAC;EAAA,OAAApB,IAAA;AAAA,EA/BuBT,KAAK;AAC7B;AACOS,IAAA,CAAA2B,SAAS,GAAG,MAAM;AA+B3B1C,aAAa,CAAC2C,aAAa,CAAC5B,IAAI,CAAC;AASjC,WAAa6B,SAAU,0BAAAC,OAAA;EAAA5B,SAAA,CAAA2B,SAAA,EAAAC,OAAA;EAAA,IAAAC,OAAA,GAAA3B,YAAA,CAAAyB,SAAA;EAOrB,SAAAA,UAAYxB,IAAyB;IAAA,IAAA2B,MAAA;IAAAzB,eAAA,OAAAsB,SAAA;IACnCG,MAAA,GAAAD,OAAA,CAAAvB,IAAA,OAAMH,IAAI,IAAI,IAAI,GAAG,EAAE,GAAGA,IAAI;IAHvB2B,MAAA,CAAAC,aAAa,GAAG,GAAG;IAI1B,IAAI5B,IAAI,IAAI,IAAI,EAAE;MAChBA,IAAI,GAAG,EAAE;;IAEX2B,MAAA,CAAKE,KAAK,GAAG7B,IAAI,CAAC6B,KAAK,IAAI,IAAI,GAAGF,MAAA,CAAKC,aAAa,GAAG5B,IAAI,CAAC6B,KAAK;IAAC,OAAAF,MAAA;EACpE;EAACrB,YAAA,CAAAkB,SAAA;IAAAjB,GAAA;IAAAC,KAAA,EAEQ,SAAAL,KAAKM,MAAuB,EAAEC,MAAc;MACnD,IAAMoB,CAAC,GAAGpC,mBAAmB,CAACe,MAAM,CAAC;MACrC,OAAOjC,SAAS,CAACsD,CAAC,EAAE,IAAI,CAACD,KAAK,CAAC;IACjC;EAAC;IAAAtB,GAAA;IAAAC,KAAA,EAEQ,SAAAI,mBAAmBC,UAAyB;MACnD,OAAOA,UAAU;IACnB;EAAC;IAAAN,GAAA;IAAAC,KAAA,EAEQ,SAAAM,UAAA,EAAS;MAChB,IAAMC,MAAM,GAA6B;QAACc,KAAK,EAAE,IAAI,CAACA;MAAK,CAAC;MAC5D,IAAMb,UAAU,GAAAC,IAAA,CAAAC,eAAA,CAAAM,SAAA,CAAAL,SAAA,sBAAAhB,IAAA,MAAoB;MACpCiB,MAAM,CAACC,MAAM,CAACN,MAAM,EAAEC,UAAU,CAAC;MACjC,OAAOD,MAAM;IACf;EAAC;EAAA,OAAAS,SAAA;AAAA,EA7B4BtC,KAAK;AAClC;AACOsC,SAAA,CAAAF,SAAS,GAAG,WAAW;AA6BhC1C,aAAa,CAAC2C,aAAa,CAACC,SAAS,CAAC;AA6BtC,WAAaO,KAAM,0BAAAC,OAAA;EAAAnC,SAAA,CAAAkC,KAAA,EAAAC,OAAA;EAAA,IAAAC,OAAA,GAAAlC,YAAA,CAAAgC,KAAA;EAWjB,SAAAA,MAAY/B,IAAqB;IAAA,IAAAkC,MAAA;IAAAhC,eAAA,OAAA6B,KAAA;IAC/BG,MAAA,GAAAD,OAAA,CAAA9B,IAAA,OAAMH,IAAI,IAAI,IAAI,GAAG,EAAE,GAAGA,IAAI;IAHvBkC,MAAA,CAAAC,yBAAyB,GAA0B,OAAO;IAIjE,IAAInC,IAAI,IAAI,IAAI,EAAE;MAChBA,IAAI,GAAG,EAAE;;IAGXkC,MAAA,CAAK9B,eAAe,GAAG,IAAI;IAC3B8B,MAAA,CAAKE,gBAAgB,GACjB/C,cAAc,CAACW,IAAI,CAACoC,gBAAgB,IAAIF,MAAA,CAAKC,yBAAyB,CAAC;IAC3ED,MAAA,CAAKG,gBAAgB,GAAG9C,cAAc,CAACS,IAAI,CAACqC,gBAAgB,CAAC;IAC7DH,MAAA,CAAKI,eAAe,GAAGvD,aAAa,CAACiB,IAAI,CAACsC,eAAe,CAAC;IAC1D,IAAItC,IAAI,CAACuC,UAAU,IAAI,IAAI,EAAE;MAC3BL,MAAA,CAAKK,UAAU,GAAG,IAAI;KACvB,MAAM,IAAIC,KAAK,CAACC,OAAO,CAACzC,IAAI,CAACuC,UAAU,CAAC,EAAE;MACzCL,MAAA,CAAKK,UAAU,GAAGvC,IAAI,CAACuC,UAAU;KAClC,MAAM,IAAI,OAAOvC,IAAI,CAACuC,UAAU,KAAK,QAAQ,EAAE;MAC9CL,MAAA,CAAKK,UAAU,GAAG,CAACvC,IAAI,CAACuC,UAAU,CAAC;KACpC,MAAM;MACL,MAAM,IAAInD,UAAU,CAChB,2EAAAsD,MAAA,CACW1C,IAAI,CAACuC,UAAU,CAAE,CAAC;;IAClC,OAAAL,MAAA;EACH;EAAC5B,YAAA,CAAAyB,KAAA;IAAAxB,GAAA;IAAAC,KAAA,EAEQ,SAAAmC,MAAM9B,UAAyB;MACtCA,UAAU,GAAGpB,kBAAkB,CAACoB,UAAU,CAAC;MAC3C,IAAM+B,UAAU,GAAU/B,UAAU,CAACgC,KAAK,CAAC,CAAC,CAAC;MAC7C,IAAI,IAAI,CAACN,UAAU,IAAI,IAAI,EAAE;QAAA,IAAAO,SAAA,GAAAC,0BAAA,CACX,IAAI,CAACR,UAAU;UAAAS,KAAA;QAAA;UAA/B,KAAAF,SAAA,CAAAG,CAAA,MAAAD,KAAA,GAAAF,SAAA,CAAAI,CAAA,IAAAC,IAAA,GAAiC;YAAA,IAAtBC,CAAC,GAAAJ,KAAA,CAAAxC,KAAA;YACVoC,UAAU,CAACQ,CAAC,GAAG,CAAC,CAAC,GAAG,CAAC;;QACtB,SAAAC,GAAA;UAAAP,SAAA,CAAAQ,CAAA,CAAAD,GAAA;QAAA;UAAAP,SAAA,CAAAS,CAAA;QAAA;;MAEH,IAAI,CAAC1B,KAAK,GAAG,IAAI,CAAC2B,SAAS,CACvB,OAAO,EAAEZ,UAAU,EAAE,SAAS,EAAE,IAAI,CAACR,gBAAgB,EACrD,IAAI,CAACC,gBAAgB,EAAE,IAAI,EAAE,IAAI,CAACC,eAAe,CAAC;MACtD;MACA,IAAMmB,IAAI,GAA6B,EAAE;MACzC,IAAI,IAAI,CAAClB,UAAU,IAAI,IAAI,EAAE;QAC3B,KAAK,IAAIa,EAAC,GAAG,CAAC,EAAEA,EAAC,GAAGvC,UAAU,CAAC6C,MAAM,EAAE,EAAEN,EAAC,EAAE;UAC1CK,IAAI,CAACL,EAAC,CAAC,GAAGvC,UAAU,CAACuC,EAAC,CAAC;;;MAG3B,IAAI,CAACO,SAAS,GAAG,CAAC,IAAI1E,SAAS,CAAC;QAC9B2E,IAAI,EAAE/C,UAAU,CAAC6C,MAAM;QACvBD,IAAI,EAAJA;OACD,CAAC,CAAC;MACH,IAAI,CAACI,KAAK,GAAG,IAAI;IACnB;EAAC;IAAAtD,GAAA;IAAAC,KAAA,EAEQ,SAAAL,KAAKM,MAAuB,EAAEC,MAAc;MACnDD,MAAM,GAAGf,mBAAmB,CAACe,MAAM,CAAC;MACpC,OAAO/B,KAAK,CAAC+B,MAAM,EAAE,IAAI,CAACoB,KAAK,CAACiC,IAAI,EAAE,CAAC;IACzC;EAAC;IAAAvD,GAAA;IAAAC,KAAA,EAEQ,SAAAM,UAAA,EAAS;MAChB,IAAMC,MAAM,GAA6B;QACvCqB,gBAAgB,EAAE9C,oBAAoB,CAAC,IAAI,CAAC8C,gBAAgB,CAAC;QAC7DC,gBAAgB,EAAE7C,oBAAoB,CAAC,IAAI,CAAC6C,gBAAgB,CAAC;QAC7DC,eAAe,EAAEtD,mBAAmB,CAAC,IAAI,CAACsD,eAAe,CAAC;QAC1DC,UAAU,EAAE,IAAI,CAACA;OAClB;MACD,IAAMvB,UAAU,GAAAC,IAAA,CAAAC,eAAA,CAAAa,KAAA,CAAAZ,SAAA,sBAAAhB,IAAA,MAAoB;MACpCiB,MAAM,CAACC,MAAM,CAACN,MAAM,EAAEC,UAAU,CAAC;MACjC,OAAOD,MAAM;IACf;EAAC;EAAA,OAAAgB,KAAA;AAAA,EA3EwB7C,KAAK;AAC9B;AACO6C,KAAA,CAAAT,SAAS,GAAG,OAAO;AA2E5B1C,aAAa,CAAC2C,aAAa,CAACQ,KAAK,CAAC;AASlC,WAAagC,GAAI,0BAAAC,OAAA;EAAAnE,SAAA,CAAAkE,GAAA,EAAAC,OAAA;EAAA,IAAAC,OAAA,GAAAlE,YAAA,CAAAgE,GAAA;EAOf,SAAAA,IAAY/D,IAAmB;IAAA,IAAAkE,MAAA;IAAAhE,eAAA,OAAA6D,GAAA;IAC7BG,MAAA,GAAAD,OAAA,CAAA9D,IAAA,OAAMH,IAAI,IAAI,IAAI,GAAG,EAAE,GAAGA,IAAI;IAHvBkE,MAAA,CAAAtC,aAAa,GAAG,GAAG;IAI1B,IAAI5B,IAAI,IAAI,IAAI,EAAE;MAChBA,IAAI,GAAG,EAAE;;IAGX,IAAIA,IAAI,CAAC6B,KAAK,IAAI,IAAI,IAAI7B,IAAI,CAAC6B,KAAK,KAAKqC,MAAA,CAAKtC,aAAa,EAAE;MAC3D,MAAM,IAAIzC,mBAAmB,CACzB,4BAAAuD,MAAA,CAA4B1C,IAAI,CAAC6B,KAAK,kDACtB,CAAC;;IAGvBqC,MAAA,CAAKrC,KAAK,GAAG7B,IAAI,CAAC6B,KAAK,IAAI,IAAI,GAAGqC,MAAA,CAAKtC,aAAa,GAAG5B,IAAI,CAAC6B,KAAK;IAAC,OAAAqC,MAAA;EACpE;EAAC5D,YAAA,CAAAyD,GAAA;IAAAxD,GAAA;IAAAC,KAAA,EAEQ,SAAAL,KAAKM,MAAuB,EAAEC,MAAc;MACnD,IAAMoB,CAAC,GAAGpC,mBAAmB,CAACe,MAAM,CAAC;MACrC,OAAOnC,GAAG,CAACwD,CAAC,CAAC;IACf;EAAC;IAAAvB,GAAA;IAAAC,KAAA,EAEQ,SAAAI,mBAAmBC,UAAyB;MACnD,OAAOA,UAAU;IACnB;EAAC;IAAAN,GAAA;IAAAC,KAAA,EAEQ,SAAAM,UAAA,EAAS;MAChB,IAAMC,MAAM,GAA6B;QAACc,KAAK,EAAE,IAAI,CAACA;MAAK,CAAC;MAC5D,IAAMb,UAAU,GAAAC,IAAA,CAAAC,eAAA,CAAA6C,GAAA,CAAA5C,SAAA,sBAAAhB,IAAA,MAAoB;MACpCiB,MAAM,CAACC,MAAM,CAACN,MAAM,EAAEC,UAAU,CAAC;MACjC,OAAOD,MAAM;IACf;EAAC;EAAA,OAAAgD,GAAA;AAAA,EApCsB7E,KAAK;AAC5B;AACO6E,GAAA,CAAAzC,SAAS,GAAG,KAAK;AAoC1B1C,aAAa,CAAC2C,aAAa,CAACwC,GAAG,CAAC;AAShC,WAAaI,eAAgB,0BAAAC,OAAA;EAAAvE,SAAA,CAAAsE,eAAA,EAAAC,OAAA;EAAA,IAAAC,OAAA,GAAAtE,YAAA,CAAAoE,eAAA;EAO3B,SAAAA,gBAAYnE,IAA+B;IAAA,IAAAsE,MAAA;IAAApE,eAAA,OAAAiE,eAAA;IACzCG,MAAA,GAAAD,OAAA,CAAAlE,IAAA,OAAMH,IAAI,IAAI,IAAI,GAAG,EAAE,GAAGA,IAAI;IAHvBsE,MAAA,CAAAC,aAAa,GAAG,GAAG;IAI1B,IAAIvE,IAAI,IAAI,IAAI,EAAE;MAChBA,IAAI,GAAG,EAAE;;IAGXsE,MAAA,CAAKE,KAAK,GAAGxE,IAAI,CAACwE,KAAK,IAAI,IAAI,GAAGF,MAAA,CAAKC,aAAa,GAAGvE,IAAI,CAACwE,KAAK;IAAC,OAAAF,MAAA;EACpE;EAAChE,YAAA,CAAA6D,eAAA;IAAA5D,GAAA;IAAAC,KAAA,EAEQ,SAAAL,KAAKM,MAAuB,EAAEC,MAAc;MACnD,IAAMoB,CAAC,GAAGpC,mBAAmB,CAACe,MAAM,CAAC;MACrC,OAAOhC,GAAG,CAACqD,CAAC,EAAE1D,IAAI,CAACG,OAAO,CAACuD,CAAC,EAAE,IAAI,CAAC0C,KAAK,CAAC,EAAE,SAAS,CAAC,CAAC;IACxD;EAAC;IAAAjE,GAAA;IAAAC,KAAA,EAEQ,SAAAI,mBAAmBC,UAAyB;MACnD,OAAOA,UAAU;IACnB;EAAC;IAAAN,GAAA;IAAAC,KAAA,EAEQ,SAAAM,UAAA,EAAS;MAChB,IAAMC,MAAM,GAA6B;QAACyD,KAAK,EAAE,IAAI,CAACA;MAAK,CAAC;MAC5D,IAAMxD,UAAU,GAAAC,IAAA,CAAAC,eAAA,CAAAiD,eAAA,CAAAhD,SAAA,sBAAAhB,IAAA,MAAoB;MACpCiB,MAAM,CAACC,MAAM,CAACN,MAAM,EAAEC,UAAU,CAAC;MACjC,OAAOD,MAAM;IACf;EAAC;EAAA,OAAAoD,eAAA;AAAA,EA9BkCjF,KAAK;AACxC;AACOiF,eAAA,CAAA7C,SAAS,GAAG,iBAAiB;AA8BtC1C,aAAa,CAAC2C,aAAa,CAAC4C,eAAe,CAAC;AAU5C,WAAatF,OAAQ,0BAAA4F,OAAA;EAAA5E,SAAA,CAAAhB,OAAA,EAAA4F,OAAA;EAAA,IAAAC,OAAA,GAAA3E,YAAA,CAAAlB,OAAA;EAOnB,SAAAA,QAAYmB,IAAuB;IAAA,IAAA2E,MAAA;IAAAzE,eAAA,OAAArB,OAAA;IACjC8F,MAAA,GAAAD,OAAA,CAAAvE,IAAA,OAAMH,IAAI,IAAI,IAAI,GAAG,EAAE,GAAGA,IAAI;IAHvB2E,MAAA,CAAAC,YAAY,GAAG,GAAG;IAIzB,IAAI5E,IAAI,IAAI,IAAI,EAAE;MAChBA,IAAI,GAAG,EAAE;;IAEX2E,MAAA,CAAKE,OAAO,GAAG,IAAI/F,iBAAiB,EAAE,CAACgG,KAAK;IAC5CH,MAAA,CAAKI,IAAI,GAAG/E,IAAI,CAAC+E,IAAI,IAAI,IAAI,GAAGJ,MAAA,CAAKC,YAAY,GAAG5E,IAAI,CAAC+E,IAAI;IAAC,OAAAJ,MAAA;EAChE;EAACrE,YAAA,CAAAzB,OAAA;IAAA0B,GAAA;IAAAC,KAAA,EAEQ,SAAAL,KAAKM,MAAuB,EAAEC,MAAc;MACnD,IAAMoB,CAAC,GAAGpC,mBAAmB,CAACe,MAAM,CAAC;MACrC,OAAO,IAAI,CAACoE,OAAO,CAAC/C,CAAC,EAAE,IAAI,CAACiD,IAAI,CAAC;IACnC;EAAC;IAAAxE,GAAA;IAAAC,KAAA,EAEQ,SAAAI,mBAAmBC,UAAyB;MACnD,OAAOA,UAAU;IACnB;EAAC;IAAAN,GAAA;IAAAC,KAAA,EAEQ,SAAAM,UAAA,EAAS;MAChB,IAAMC,MAAM,GAA6B;QAACgE,IAAI,EAAE,IAAI,CAACA;MAAI,CAAC;MAC1D,IAAM/D,UAAU,GAAAC,IAAA,CAAAC,eAAA,CAAArC,OAAA,CAAAsC,SAAA,sBAAAhB,IAAA,MAAoB;MACpCiB,MAAM,CAACC,MAAM,CAACN,MAAM,EAAEC,UAAU,CAAC;MACjC,OAAOD,MAAM;IACf;EAAC;EAAA,OAAAlC,OAAA;AAAA,EA9B0BK,KAAK;AAChC;AACOL,OAAA,CAAAyC,SAAS,GAAG,SAAS;AA8B9B1C,aAAa,CAAC2C,aAAa,CAAC1C,OAAO,CAAC"},"metadata":{},"sourceType":"module","externalDependencies":[]}