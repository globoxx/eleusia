{"ast":null,"code":"import _classCallCheck from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/classCallCheck.js\";\nimport _createClass from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/createClass.js\";\nimport _get from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/get.js\";\nimport _getPrototypeOf from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/getPrototypeOf.js\";\nimport _inherits from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/inherits.js\";\nimport _createSuper from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/createSuper.js\";\n/**\r\n * @license\r\n * Copyright 2018 Google LLC\r\n *\r\n * Use of this source code is governed by an MIT-style\r\n * license that can be found in the LICENSE file or at\r\n * https://opensource.org/licenses/MIT.\r\n * =============================================================================\r\n */\n/**\r\n * TensorFlow.js Layers: Noise Layers.\r\n */\nimport { add, greaterEqual, mul, randomUniform, serialization, tidy } from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport { Layer } from '../engine/topology';\nimport { getExactlyOneTensor } from '../utils/types_utils';\nexport var GaussianNoise = /*#__PURE__*/function (_Layer) {\n  _inherits(GaussianNoise, _Layer);\n  var _super = _createSuper(GaussianNoise);\n  function GaussianNoise(args) {\n    var _this;\n    _classCallCheck(this, GaussianNoise);\n    _this = _super.call(this, args);\n    _this.supportsMasking = true;\n    _this.stddev = args.stddev;\n    return _this;\n  }\n  _createClass(GaussianNoise, [{\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      return inputShape;\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var baseConfig = _get(_getPrototypeOf(GaussianNoise.prototype), \"getConfig\", this).call(this);\n      var config = {\n        stddev: this.stddev\n      };\n      Object.assign(config, baseConfig);\n      return config;\n    }\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this2 = this;\n      return tidy(function () {\n        _this2.invokeCallHook(inputs, kwargs);\n        var input = getExactlyOneTensor(inputs);\n        var noised = function noised() {\n          return add(K.randomNormal(input.shape, 0, _this2.stddev), input);\n        };\n        var output = K.inTrainPhase(noised, function () {\n          return input;\n        }, kwargs['training'] || false);\n        return output;\n      });\n    }\n  }]);\n  return GaussianNoise;\n}(Layer);\n/** @nocollapse */\nGaussianNoise.className = 'GaussianNoise';\nserialization.registerClass(GaussianNoise);\nexport var GaussianDropout = /*#__PURE__*/function (_Layer2) {\n  _inherits(GaussianDropout, _Layer2);\n  var _super2 = _createSuper(GaussianDropout);\n  function GaussianDropout(args) {\n    var _this3;\n    _classCallCheck(this, GaussianDropout);\n    _this3 = _super2.call(this, args);\n    _this3.supportsMasking = true;\n    _this3.rate = args.rate;\n    return _this3;\n  }\n  _createClass(GaussianDropout, [{\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      return inputShape;\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var baseConfig = _get(_getPrototypeOf(GaussianDropout.prototype), \"getConfig\", this).call(this);\n      var config = {\n        rate: this.rate\n      };\n      Object.assign(config, baseConfig);\n      return config;\n    }\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this4 = this;\n      return tidy(function () {\n        _this4.invokeCallHook(inputs, kwargs);\n        var input = getExactlyOneTensor(inputs);\n        if (_this4.rate > 0 && _this4.rate < 1) {\n          var noised = function noised() {\n            var stddev = Math.sqrt(_this4.rate / (1 - _this4.rate));\n            return mul(input, K.randomNormal(input.shape, 1, stddev));\n          };\n          return K.inTrainPhase(noised, function () {\n            return input;\n          }, kwargs['training'] || false);\n        }\n        return input;\n      });\n    }\n  }]);\n  return GaussianDropout;\n}(Layer);\n/** @nocollapse */\nGaussianDropout.className = 'GaussianDropout';\nserialization.registerClass(GaussianDropout);\n/**\r\n * Applies Alpha Dropout to the input.\r\n *\r\n * As it is a regularization layer, it is only active at training time.\r\n *\r\n * Alpha Dropout is a `Dropout` that keeps mean and variance of inputs\r\n * to their original values, in order to ensure the self-normalizing property\r\n * even after this dropout.\r\n * Alpha Dropout fits well to Scaled Exponential Linear Units\r\n * by randomly setting activations to the negative saturation value.\r\n *\r\n * Arguments:\r\n *   - `rate`: float, drop probability (as with `Dropout`).\r\n *     The multiplicative noise will have\r\n *     standard deviation `sqrt(rate / (1 - rate))`.\r\n *   - `noise_shape`: A 1-D `Tensor` of type `int32`, representing the\r\n *     shape for randomly generated keep/drop flags.\r\n *\r\n * Input shape:\r\n *   Arbitrary. Use the keyword argument `inputShape`\r\n *   (tuple of integers, does not include the samples axis)\r\n *   when using this layer as the first layer in a model.\r\n *\r\n * Output shape:\r\n *   Same shape as input.\r\n *\r\n * References:\r\n *   - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\r\n */\nexport var AlphaDropout = /*#__PURE__*/function (_Layer3) {\n  _inherits(AlphaDropout, _Layer3);\n  var _super3 = _createSuper(AlphaDropout);\n  function AlphaDropout(args) {\n    var _this5;\n    _classCallCheck(this, AlphaDropout);\n    _this5 = _super3.call(this, args);\n    _this5.supportsMasking = true;\n    _this5.rate = args.rate;\n    _this5.noiseShape = args.noiseShape;\n    return _this5;\n  }\n  _createClass(AlphaDropout, [{\n    key: \"_getNoiseShape\",\n    value: function _getNoiseShape(inputs) {\n      return this.noiseShape || getExactlyOneTensor(inputs).shape;\n    }\n  }, {\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      return inputShape;\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var baseConfig = _get(_getPrototypeOf(AlphaDropout.prototype), \"getConfig\", this).call(this);\n      var config = {\n        rate: this.rate\n      };\n      Object.assign(config, baseConfig);\n      return config;\n    }\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this6 = this;\n      return tidy(function () {\n        if (_this6.rate < 1 && _this6.rate > 0) {\n          var noiseShape = _this6._getNoiseShape(inputs);\n          var droppedInputs = function droppedInputs() {\n            var input = getExactlyOneTensor(inputs);\n            var alpha = 1.6732632423543772848170429916717;\n            var scale = 1.0507009873554804934193349852946;\n            var alphaP = -alpha * scale;\n            var keptIdx = greaterEqual(randomUniform(noiseShape), _this6.rate);\n            keptIdx = K.cast(keptIdx, 'float32'); // get default dtype.\n            // Get affine transformation params.\n            var a = Math.pow((1 - _this6.rate) * (1 + _this6.rate * Math.pow(alphaP, 2)), -0.5);\n            var b = -a * alphaP * _this6.rate;\n            // Apply mask.\n            var x = add(mul(input, keptIdx), mul(add(keptIdx, -1), alphaP));\n            return add(mul(x, a), b);\n          };\n          return K.inTrainPhase(droppedInputs, function () {\n            return getExactlyOneTensor(inputs);\n          }, kwargs['training'] || false);\n        }\n        return inputs;\n      });\n    }\n  }]);\n  return AlphaDropout;\n}(Layer);\n/** @nocollapse */\nAlphaDropout.className = 'AlphaDropout';\nserialization.registerClass(AlphaDropout);","map":{"version":3,"names":["add","greaterEqual","mul","randomUniform","serialization","tidy","K","Layer","getExactlyOneTensor","GaussianNoise","_Layer","_inherits","_super","_createSuper","args","_this","_classCallCheck","call","supportsMasking","stddev","_createClass","key","value","computeOutputShape","inputShape","getConfig","baseConfig","_get","_getPrototypeOf","prototype","config","Object","assign","inputs","kwargs","_this2","invokeCallHook","input","noised","randomNormal","shape","output","inTrainPhase","className","registerClass","GaussianDropout","_Layer2","_super2","_this3","rate","_this4","Math","sqrt","AlphaDropout","_Layer3","_super3","_this5","noiseShape","_getNoiseShape","_this6","droppedInputs","alpha","scale","alphaP","keptIdx","cast","a","pow","b","x"],"sources":["C:\\Users\\vince\\OneDrive\\Documents\\GitHub\\tfjs-layers\\src\\layers\\noise.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * TensorFlow.js Layers: Noise Layers.\n */\n\nimport {add, greaterEqual, mul, randomUniform, serialization, Tensor, tidy} from '@tensorflow/tfjs-core';\n\nimport * as K from '../backend/tfjs_backend';\nimport {Layer, LayerArgs} from '../engine/topology';\nimport {Shape} from '../keras_format/common';\nimport {Kwargs} from '../types';\nimport {getExactlyOneTensor} from '../utils/types_utils';\n\nexport declare interface GaussianNoiseArgs extends LayerArgs {\n  /** Standard Deviation.  */\n  stddev: number;\n}\n\nexport class GaussianNoise extends Layer {\n  /** @nocollapse */\n  static className = 'GaussianNoise';\n  readonly stddev: number;\n\n  constructor(args: GaussianNoiseArgs) {\n    super(args);\n    this.supportsMasking = true;\n    this.stddev = args.stddev;\n  }\n\n  override computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    return inputShape;\n  }\n\n  override getConfig() {\n    const baseConfig = super.getConfig();\n    const config = {stddev: this.stddev};\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n  override call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      this.invokeCallHook(inputs, kwargs);\n      const input = getExactlyOneTensor(inputs);\n      const noised = () =>\n          add(K.randomNormal(input.shape, 0, this.stddev), input);\n      const output =\n          K.inTrainPhase(noised, () => input, kwargs['training'] || false);\n      return output;\n    });\n  }\n}\nserialization.registerClass(GaussianNoise);\n\nexport declare interface GaussianDropoutArgs extends LayerArgs {\n  /** drop probability.  */\n  rate: number;\n}\n\nexport class GaussianDropout extends Layer {\n  /** @nocollapse */\n  static className = 'GaussianDropout';\n  readonly rate: number;\n\n  constructor(args: GaussianDropoutArgs) {\n    super(args);\n    this.supportsMasking = true;\n    this.rate = args.rate;\n  }\n\n  override computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    return inputShape;\n  }\n\n  override getConfig() {\n    const baseConfig = super.getConfig();\n    const config = {rate: this.rate};\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n  override call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      this.invokeCallHook(inputs, kwargs);\n      const input = getExactlyOneTensor(inputs);\n      if (this.rate > 0 && this.rate < 1) {\n        const noised = () => {\n          const stddev = Math.sqrt(this.rate / (1 - this.rate));\n          return mul(input, K.randomNormal(input.shape, 1, stddev));\n        };\n        return K.inTrainPhase(noised, () => input, kwargs['training'] || false);\n      }\n      return input;\n    });\n  }\n}\nserialization.registerClass(GaussianDropout);\n\nexport declare interface AlphaDropoutArgs extends LayerArgs {\n  /** drop probability.  */\n  rate: number;\n  /**\n   * A 1-D `Tensor` of type `int32`, representing the\n   * shape for randomly generated keep/drop flags.\n   */\n  noiseShape?: Shape;\n}\n\n/**\n * Applies Alpha Dropout to the input.\n *\n * As it is a regularization layer, it is only active at training time.\n *\n * Alpha Dropout is a `Dropout` that keeps mean and variance of inputs\n * to their original values, in order to ensure the self-normalizing property\n * even after this dropout.\n * Alpha Dropout fits well to Scaled Exponential Linear Units\n * by randomly setting activations to the negative saturation value.\n *\n * Arguments:\n *   - `rate`: float, drop probability (as with `Dropout`).\n *     The multiplicative noise will have\n *     standard deviation `sqrt(rate / (1 - rate))`.\n *   - `noise_shape`: A 1-D `Tensor` of type `int32`, representing the\n *     shape for randomly generated keep/drop flags.\n *\n * Input shape:\n *   Arbitrary. Use the keyword argument `inputShape`\n *   (tuple of integers, does not include the samples axis)\n *   when using this layer as the first layer in a model.\n *\n * Output shape:\n *   Same shape as input.\n *\n * References:\n *   - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\n */\nexport class AlphaDropout extends Layer {\n  /** @nocollapse */\n  static className = 'AlphaDropout';\n  readonly rate: number;\n  readonly noiseShape: Shape;\n\n  constructor(args: AlphaDropoutArgs) {\n    super(args);\n    this.supportsMasking = true;\n    this.rate = args.rate;\n    this.noiseShape = args.noiseShape;\n  }\n\n  _getNoiseShape(inputs: Tensor|Tensor[]) {\n    return this.noiseShape || getExactlyOneTensor(inputs).shape;\n  }\n\n  override computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    return inputShape;\n  }\n\n  override getConfig() {\n    const baseConfig = super.getConfig();\n    const config = {rate: this.rate};\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n  override call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      if (this.rate < 1 && this.rate > 0) {\n        const noiseShape = this._getNoiseShape(inputs);\n\n        const droppedInputs = () => {\n          const input = getExactlyOneTensor(inputs);\n\n          const alpha = 1.6732632423543772848170429916717;\n          const scale = 1.0507009873554804934193349852946;\n\n          const alphaP = -alpha * scale;\n\n          let keptIdx = greaterEqual(randomUniform(noiseShape), this.rate);\n\n          keptIdx = K.cast(keptIdx, 'float32');  // get default dtype.\n\n          // Get affine transformation params.\n          const a = ((1 - this.rate) * (1 + this.rate * alphaP ** 2)) ** -0.5;\n          const b = -a * alphaP * this.rate;\n\n          // Apply mask.\n          const x = add(mul(input, keptIdx), mul(add(keptIdx, -1), alphaP));\n\n          return add(mul(x, a), b);\n        };\n        return K.inTrainPhase(\n            droppedInputs, () => getExactlyOneTensor(inputs),\n            kwargs['training'] || false);\n      }\n      return inputs;\n    });\n  }\n}\nserialization.registerClass(AlphaDropout);\n"],"mappings":";;;;;;AAAA;;;;;;;;;AAUA;;;AAIA,SAAQA,GAAG,EAAEC,YAAY,EAAEC,GAAG,EAAEC,aAAa,EAAEC,aAAa,EAAUC,IAAI,QAAO,uBAAuB;AAExG,OAAO,KAAKC,CAAC,MAAM,yBAAyB;AAC5C,SAAQC,KAAK,QAAkB,oBAAoB;AAGnD,SAAQC,mBAAmB,QAAO,sBAAsB;AAOxD,WAAaC,aAAc,0BAAAC,MAAA;EAAAC,SAAA,CAAAF,aAAA,EAAAC,MAAA;EAAA,IAAAE,MAAA,GAAAC,YAAA,CAAAJ,aAAA;EAKzB,SAAAA,cAAYK,IAAuB;IAAA,IAAAC,KAAA;IAAAC,eAAA,OAAAP,aAAA;IACjCM,KAAA,GAAAH,MAAA,CAAAK,IAAA,OAAMH,IAAI;IACVC,KAAA,CAAKG,eAAe,GAAG,IAAI;IAC3BH,KAAA,CAAKI,MAAM,GAAGL,IAAI,CAACK,MAAM;IAAC,OAAAJ,KAAA;EAC5B;EAACK,YAAA,CAAAX,aAAA;IAAAY,GAAA;IAAAC,KAAA,EAEQ,SAAAC,mBAAmBC,UAAyB;MACnD,OAAOA,UAAU;IACnB;EAAC;IAAAH,GAAA;IAAAC,KAAA,EAEQ,SAAAG,UAAA,EAAS;MAChB,IAAMC,UAAU,GAAAC,IAAA,CAAAC,eAAA,CAAAnB,aAAA,CAAAoB,SAAA,sBAAAZ,IAAA,MAAoB;MACpC,IAAMa,MAAM,GAAG;QAACX,MAAM,EAAE,IAAI,CAACA;MAAM,CAAC;MACpCY,MAAM,CAACC,MAAM,CAACF,MAAM,EAAEJ,UAAU,CAAC;MACjC,OAAOI,MAAM;IACf;EAAC;IAAAT,GAAA;IAAAC,KAAA,EAEQ,SAAAL,KAAKgB,MAAuB,EAAEC,MAAc;MAAA,IAAAC,MAAA;MACnD,OAAO9B,IAAI,CAAC,YAAK;QACf8B,MAAI,CAACC,cAAc,CAACH,MAAM,EAAEC,MAAM,CAAC;QACnC,IAAMG,KAAK,GAAG7B,mBAAmB,CAACyB,MAAM,CAAC;QACzC,IAAMK,MAAM,GAAG,SAATA,MAAMA,CAAA;UAAA,OACRtC,GAAG,CAACM,CAAC,CAACiC,YAAY,CAACF,KAAK,CAACG,KAAK,EAAE,CAAC,EAAEL,MAAI,CAAChB,MAAM,CAAC,EAAEkB,KAAK,CAAC;QAAA;QAC3D,IAAMI,MAAM,GACRnC,CAAC,CAACoC,YAAY,CAACJ,MAAM,EAAE;UAAA,OAAMD,KAAK;QAAA,GAAEH,MAAM,CAAC,UAAU,CAAC,IAAI,KAAK,CAAC;QACpE,OAAOO,MAAM;MACf,CAAC,CAAC;IACJ;EAAC;EAAA,OAAAhC,aAAA;AAAA,EAhCgCF,KAAK;AACtC;AACOE,aAAA,CAAAkC,SAAS,GAAG,eAAe;AAgCpCvC,aAAa,CAACwC,aAAa,CAACnC,aAAa,CAAC;AAO1C,WAAaoC,eAAgB,0BAAAC,OAAA;EAAAnC,SAAA,CAAAkC,eAAA,EAAAC,OAAA;EAAA,IAAAC,OAAA,GAAAlC,YAAA,CAAAgC,eAAA;EAK3B,SAAAA,gBAAY/B,IAAyB;IAAA,IAAAkC,MAAA;IAAAhC,eAAA,OAAA6B,eAAA;IACnCG,MAAA,GAAAD,OAAA,CAAA9B,IAAA,OAAMH,IAAI;IACVkC,MAAA,CAAK9B,eAAe,GAAG,IAAI;IAC3B8B,MAAA,CAAKC,IAAI,GAAGnC,IAAI,CAACmC,IAAI;IAAC,OAAAD,MAAA;EACxB;EAAC5B,YAAA,CAAAyB,eAAA;IAAAxB,GAAA;IAAAC,KAAA,EAEQ,SAAAC,mBAAmBC,UAAyB;MACnD,OAAOA,UAAU;IACnB;EAAC;IAAAH,GAAA;IAAAC,KAAA,EAEQ,SAAAG,UAAA,EAAS;MAChB,IAAMC,UAAU,GAAAC,IAAA,CAAAC,eAAA,CAAAiB,eAAA,CAAAhB,SAAA,sBAAAZ,IAAA,MAAoB;MACpC,IAAMa,MAAM,GAAG;QAACmB,IAAI,EAAE,IAAI,CAACA;MAAI,CAAC;MAChClB,MAAM,CAACC,MAAM,CAACF,MAAM,EAAEJ,UAAU,CAAC;MACjC,OAAOI,MAAM;IACf;EAAC;IAAAT,GAAA;IAAAC,KAAA,EAEQ,SAAAL,KAAKgB,MAAuB,EAAEC,MAAc;MAAA,IAAAgB,MAAA;MACnD,OAAO7C,IAAI,CAAC,YAAK;QACf6C,MAAI,CAACd,cAAc,CAACH,MAAM,EAAEC,MAAM,CAAC;QACnC,IAAMG,KAAK,GAAG7B,mBAAmB,CAACyB,MAAM,CAAC;QACzC,IAAIiB,MAAI,CAACD,IAAI,GAAG,CAAC,IAAIC,MAAI,CAACD,IAAI,GAAG,CAAC,EAAE;UAClC,IAAMX,MAAM,GAAG,SAATA,MAAMA,CAAA,EAAQ;YAClB,IAAMnB,MAAM,GAAGgC,IAAI,CAACC,IAAI,CAACF,MAAI,CAACD,IAAI,IAAI,CAAC,GAAGC,MAAI,CAACD,IAAI,CAAC,CAAC;YACrD,OAAO/C,GAAG,CAACmC,KAAK,EAAE/B,CAAC,CAACiC,YAAY,CAACF,KAAK,CAACG,KAAK,EAAE,CAAC,EAAErB,MAAM,CAAC,CAAC;UAC3D,CAAC;UACD,OAAOb,CAAC,CAACoC,YAAY,CAACJ,MAAM,EAAE;YAAA,OAAMD,KAAK;UAAA,GAAEH,MAAM,CAAC,UAAU,CAAC,IAAI,KAAK,CAAC;;QAEzE,OAAOG,KAAK;MACd,CAAC,CAAC;IACJ;EAAC;EAAA,OAAAQ,eAAA;AAAA,EAnCkCtC,KAAK;AACxC;AACOsC,eAAA,CAAAF,SAAS,GAAG,iBAAiB;AAmCtCvC,aAAa,CAACwC,aAAa,CAACC,eAAe,CAAC;AAY5C;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA6BA,WAAaQ,YAAa,0BAAAC,OAAA;EAAA3C,SAAA,CAAA0C,YAAA,EAAAC,OAAA;EAAA,IAAAC,OAAA,GAAA1C,YAAA,CAAAwC,YAAA;EAMxB,SAAAA,aAAYvC,IAAsB;IAAA,IAAA0C,MAAA;IAAAxC,eAAA,OAAAqC,YAAA;IAChCG,MAAA,GAAAD,OAAA,CAAAtC,IAAA,OAAMH,IAAI;IACV0C,MAAA,CAAKtC,eAAe,GAAG,IAAI;IAC3BsC,MAAA,CAAKP,IAAI,GAAGnC,IAAI,CAACmC,IAAI;IACrBO,MAAA,CAAKC,UAAU,GAAG3C,IAAI,CAAC2C,UAAU;IAAC,OAAAD,MAAA;EACpC;EAACpC,YAAA,CAAAiC,YAAA;IAAAhC,GAAA;IAAAC,KAAA,EAED,SAAAoC,eAAezB,MAAuB;MACpC,OAAO,IAAI,CAACwB,UAAU,IAAIjD,mBAAmB,CAACyB,MAAM,CAAC,CAACO,KAAK;IAC7D;EAAC;IAAAnB,GAAA;IAAAC,KAAA,EAEQ,SAAAC,mBAAmBC,UAAyB;MACnD,OAAOA,UAAU;IACnB;EAAC;IAAAH,GAAA;IAAAC,KAAA,EAEQ,SAAAG,UAAA,EAAS;MAChB,IAAMC,UAAU,GAAAC,IAAA,CAAAC,eAAA,CAAAyB,YAAA,CAAAxB,SAAA,sBAAAZ,IAAA,MAAoB;MACpC,IAAMa,MAAM,GAAG;QAACmB,IAAI,EAAE,IAAI,CAACA;MAAI,CAAC;MAChClB,MAAM,CAACC,MAAM,CAACF,MAAM,EAAEJ,UAAU,CAAC;MACjC,OAAOI,MAAM;IACf;EAAC;IAAAT,GAAA;IAAAC,KAAA,EAEQ,SAAAL,KAAKgB,MAAuB,EAAEC,MAAc;MAAA,IAAAyB,MAAA;MACnD,OAAOtD,IAAI,CAAC,YAAK;QACf,IAAIsD,MAAI,CAACV,IAAI,GAAG,CAAC,IAAIU,MAAI,CAACV,IAAI,GAAG,CAAC,EAAE;UAClC,IAAMQ,UAAU,GAAGE,MAAI,CAACD,cAAc,CAACzB,MAAM,CAAC;UAE9C,IAAM2B,aAAa,GAAG,SAAhBA,aAAaA,CAAA,EAAQ;YACzB,IAAMvB,KAAK,GAAG7B,mBAAmB,CAACyB,MAAM,CAAC;YAEzC,IAAM4B,KAAK,GAAG,iCAAiC;YAC/C,IAAMC,KAAK,GAAG,iCAAiC;YAE/C,IAAMC,MAAM,GAAG,CAACF,KAAK,GAAGC,KAAK;YAE7B,IAAIE,OAAO,GAAG/D,YAAY,CAACE,aAAa,CAACsD,UAAU,CAAC,EAAEE,MAAI,CAACV,IAAI,CAAC;YAEhEe,OAAO,GAAG1D,CAAC,CAAC2D,IAAI,CAACD,OAAO,EAAE,SAAS,CAAC,CAAC,CAAE;YAEvC;YACA,IAAME,CAAC,GAAAf,IAAA,CAAAgB,GAAA,CAAI,CAAC,CAAC,GAAGR,MAAI,CAACV,IAAI,KAAK,CAAC,GAAGU,MAAI,CAACV,IAAI,GAAAE,IAAA,CAAAgB,GAAA,CAAGJ,MAAM,EAAI,CAAC,EAAC,EAAK,CAAC,GAAG;YACnE,IAAMK,CAAC,GAAG,CAACF,CAAC,GAAGH,MAAM,GAAGJ,MAAI,CAACV,IAAI;YAEjC;YACA,IAAMoB,CAAC,GAAGrE,GAAG,CAACE,GAAG,CAACmC,KAAK,EAAE2B,OAAO,CAAC,EAAE9D,GAAG,CAACF,GAAG,CAACgE,OAAO,EAAE,CAAC,CAAC,CAAC,EAAED,MAAM,CAAC,CAAC;YAEjE,OAAO/D,GAAG,CAACE,GAAG,CAACmE,CAAC,EAAEH,CAAC,CAAC,EAAEE,CAAC,CAAC;UAC1B,CAAC;UACD,OAAO9D,CAAC,CAACoC,YAAY,CACjBkB,aAAa,EAAE;YAAA,OAAMpD,mBAAmB,CAACyB,MAAM,CAAC;UAAA,GAChDC,MAAM,CAAC,UAAU,CAAC,IAAI,KAAK,CAAC;;QAElC,OAAOD,MAAM;MACf,CAAC,CAAC;IACJ;EAAC;EAAA,OAAAoB,YAAA;AAAA,EA5D+B9C,KAAK;AACrC;AACO8C,YAAA,CAAAV,SAAS,GAAG,cAAc;AA4DnCvC,aAAa,CAACwC,aAAa,CAACS,YAAY,CAAC"},"metadata":{},"sourceType":"module","externalDependencies":[]}