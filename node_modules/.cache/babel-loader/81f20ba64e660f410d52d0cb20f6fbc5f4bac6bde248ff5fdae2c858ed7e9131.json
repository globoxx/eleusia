{"ast":null,"code":"import _slicedToArray from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/slicedToArray.js\";\n/**\r\n * @license\r\n * Copyright 2020 Google LLC. All Rights Reserved.\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n * http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n * =============================================================================\r\n */\nimport { customGrad } from '../../gradients';\nimport { convertToTensor } from '../../tensor_util_env';\nimport { assertShapesMatch } from '../../util';\nimport { add } from '../add';\nimport { expandShapeToKeepDim } from '../axis_util';\nimport { cast } from '../cast';\nimport { div } from '../div';\nimport { exp } from '../exp';\nimport { logSumExp } from '../log_sum_exp';\nimport { Reduction } from '../loss_ops_utils';\nimport { mul } from '../mul';\nimport { neg } from '../neg';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\nimport { scalar } from '../scalar';\nimport { sub } from '../sub';\nimport { sum } from '../sum';\nimport { computeWeightedLoss } from './compute_weighted_loss';\n/**\r\n * Computes softmax cross entropy between logits and labels.\r\n *\r\n * Measures the probability error in discrete classification tasks in which\r\n * the classes are mutually exclusive (each entry is in exactly one class).\r\n * For example, each CIFAR-10 image is labeled with one and only one label: an\r\n * image can be a dog or a truck, but not both.\r\n *\r\n * `NOTE`: While the classes are mutually exclusive, their probabilities need\r\n * not be. All that is required is that each row of labels is a valid\r\n * probability distribution. If they are not, the computation of the gradient\r\n * will be incorrect.\r\n *\r\n * `WARNING`: This op expects unscaled logits, since it performs a softmax on\r\n * logits internally for efficiency. Do not call this op with the output of\r\n * softmax, as it will produce incorrect results.\r\n *\r\n * logits and labels must have the same shape, e.g. [batch_size, num_classes]\r\n * and the same dtype.\r\n * @param labels The labels array.\r\n * @param logits The logits array.\r\n * @param dim The dimension softmax would be performed on. Defaults to `-1`\r\n *     which indicates the last dimension.\r\n */\nfunction softmaxCrossEntropyWithLogits_(labels, logits) {\n  var dim = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : -1;\n  if (dim === -1) {\n    dim = logits.rank - 1;\n  }\n  if (dim !== logits.rank - 1) {\n    throw Error(\"Softmax cross entropy along a non-last dimension is not yet \" + \"supported. Labels / logits was rank \".concat(logits.rank, \" \") + \"and dim was \".concat(dim));\n  }\n  // Use a custom gradient for numerical stability.\n  var customOp = customGrad(function (labels, logits, save) {\n    // Reference:\n    //   1. http://cs231n.github.io/linear-classify/#softmax\n    //   2. https://blog.feedly.com/tricks-of-the-trade-logsumexp/\n    var keepDims = true;\n    var lse = logSumExp(logits, [dim], keepDims);\n    var logResult = sub(cast(logits, 'float32'), lse);\n    save([labels, logResult]);\n    var costVector = neg(mul(logResult, labels));\n    var value = sum(costVector, [dim]);\n    var gradFunc = function gradFunc(dy, saved) {\n      var _saved = _slicedToArray(saved, 2),\n        labels = _saved[0],\n        logResult = _saved[1];\n      var dyShape = expandShapeToKeepDim(dy.shape, [dim]);\n      return [mul(reshape(dy, dyShape), sub(cast(labels, 'float32'), exp(logResult))), mul(reshape(dy, dyShape), sub(exp(logResult), cast(labels, 'float32')))];\n    };\n    return {\n      value: value,\n      gradFunc: gradFunc\n    };\n  });\n  return customOp(labels, logits);\n}\n/**\r\n * Computes the softmax cross entropy loss between two tensors.\r\n *\r\n * If labelSmoothing is nonzero, smooth the labels towards 1/2:\r\n *\r\n *   newOnehotLabels = onehotLabels * (1 - labelSmoothing)\r\n *                         + labelSmoothing / numClasses\r\n *\r\n * @param onehotLabels One hot encoded labels\r\n *    [batch_size, num_classes], same dimensions as 'predictions'.\r\n * @param logits The predicted outputs.\r\n * @param weights Tensor whose rank is either 0, or 1, and must be\r\n *    broadcastable to `loss`  of shape [batch_size]\r\n * @param labelSmoothing If greater than 0, then smooth the labels.\r\n * @param reduction Type of reduction to apply to loss. Should be of type\r\n *    `Reduction`\r\n *\r\n * @doc { heading: 'Training', subheading: 'Losses', namespace: 'losses' }\r\n */\nfunction softmaxCrossEntropy_(onehotLabels, logits, weights) {\n  var labelSmoothing = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : 0;\n  var reduction = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : Reduction.SUM_BY_NONZERO_WEIGHTS;\n  var $onehotLabels = convertToTensor(onehotLabels, 'onehotLabels', 'softmaxCrossEntropy');\n  var $logits = convertToTensor(logits, 'logits', 'softmaxCrossEntropy');\n  var $weights = null;\n  if (weights != null) {\n    $weights = convertToTensor(weights, 'weights', 'softmaxCrossEntropy');\n  }\n  assertShapesMatch($onehotLabels.shape, $logits.shape, 'Error in softmaxCrossEntropy: ');\n  if (labelSmoothing > 0) {\n    var labelSmoothingScalar = scalar(labelSmoothing);\n    var one = scalar(1);\n    var numClasses = scalar($onehotLabels.shape[1]);\n    $onehotLabels = add(mul($onehotLabels, sub(one, labelSmoothingScalar)), div(labelSmoothingScalar, numClasses));\n  }\n  var losses = softmaxCrossEntropyWithLogits_($onehotLabels, $logits);\n  return computeWeightedLoss(losses, $weights, reduction);\n}\nexport var softmaxCrossEntropy = /* @__PURE__ */op({\n  softmaxCrossEntropy_: softmaxCrossEntropy_\n});","map":{"version":3,"names":["customGrad","convertToTensor","assertShapesMatch","add","expandShapeToKeepDim","cast","div","exp","logSumExp","Reduction","mul","neg","op","reshape","scalar","sub","sum","computeWeightedLoss","softmaxCrossEntropyWithLogits_","labels","logits","dim","arguments","length","undefined","rank","Error","concat","customOp","save","keepDims","lse","logResult","costVector","value","gradFunc","dy","saved","_saved","_slicedToArray","dyShape","shape","softmaxCrossEntropy_","onehotLabels","weights","labelSmoothing","reduction","SUM_BY_NONZERO_WEIGHTS","$onehotLabels","$logits","$weights","labelSmoothingScalar","one","numClasses","losses","softmaxCrossEntropy"],"sources":["C:\\Users\\vince\\OneDrive\\Documents\\GitHub\\tfjs-core\\src\\ops\\losses\\softmax_cross_entropy.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport {customGrad} from '../../gradients';\nimport {Tensor} from '../../tensor';\nimport {GradSaveFunc} from '../../tensor_types';\nimport {convertToTensor} from '../../tensor_util_env';\nimport {TensorLike} from '../../types';\nimport {assertShapesMatch} from '../../util';\nimport {add} from '../add';\nimport {expandShapeToKeepDim} from '../axis_util';\nimport {cast} from '../cast';\nimport {div} from '../div';\nimport {exp} from '../exp';\nimport {logSumExp} from '../log_sum_exp';\nimport {Reduction} from '../loss_ops_utils';\nimport {mul} from '../mul';\nimport {neg} from '../neg';\nimport {op} from '../operation';\nimport {reshape} from '../reshape';\nimport {scalar} from '../scalar';\nimport {sub} from '../sub';\nimport {sum} from '../sum';\n\nimport {computeWeightedLoss} from './compute_weighted_loss';\n\n/**\n * Computes softmax cross entropy between logits and labels.\n *\n * Measures the probability error in discrete classification tasks in which\n * the classes are mutually exclusive (each entry is in exactly one class).\n * For example, each CIFAR-10 image is labeled with one and only one label: an\n * image can be a dog or a truck, but not both.\n *\n * `NOTE`: While the classes are mutually exclusive, their probabilities need\n * not be. All that is required is that each row of labels is a valid\n * probability distribution. If they are not, the computation of the gradient\n * will be incorrect.\n *\n * `WARNING`: This op expects unscaled logits, since it performs a softmax on\n * logits internally for efficiency. Do not call this op with the output of\n * softmax, as it will produce incorrect results.\n *\n * logits and labels must have the same shape, e.g. [batch_size, num_classes]\n * and the same dtype.\n * @param labels The labels array.\n * @param logits The logits array.\n * @param dim The dimension softmax would be performed on. Defaults to `-1`\n *     which indicates the last dimension.\n */\nfunction softmaxCrossEntropyWithLogits_<T extends Tensor, O extends Tensor>(\n    labels: T, logits: T, dim = -1): O {\n  if (dim === -1) {\n    dim = logits.rank - 1;\n  }\n\n  if (dim !== logits.rank - 1) {\n    throw Error(\n        `Softmax cross entropy along a non-last dimension is not yet ` +\n        `supported. Labels / logits was rank ${logits.rank} ` +\n        `and dim was ${dim}`);\n  }\n  // Use a custom gradient for numerical stability.\n  const customOp =\n      customGrad((labels: Tensor, logits: Tensor, save: GradSaveFunc) => {\n        // Reference:\n        //   1. http://cs231n.github.io/linear-classify/#softmax\n        //   2. https://blog.feedly.com/tricks-of-the-trade-logsumexp/\n        const keepDims = true;\n        const lse = logSumExp(logits, [dim], keepDims);\n        const logResult = sub(cast(logits, 'float32'), lse);\n        save([labels, logResult]);\n\n        const costVector = neg(mul(logResult, labels));\n        const value: O = sum(costVector, [dim]);\n\n        const gradFunc = (dy: O, saved: Tensor[]) => {\n          const [labels, logResult] = saved;\n          const dyShape = expandShapeToKeepDim(dy.shape, [dim]);\n          return [\n            mul(reshape(dy, dyShape),\n                sub(cast(labels, 'float32'), exp(logResult))),\n            mul(reshape(dy, dyShape),\n                sub(exp(logResult), cast(labels, 'float32'))),\n          ];\n        };\n        return {value, gradFunc};\n      });\n\n  return customOp(labels, logits);\n}\n\n/**\n * Computes the softmax cross entropy loss between two tensors.\n *\n * If labelSmoothing is nonzero, smooth the labels towards 1/2:\n *\n *   newOnehotLabels = onehotLabels * (1 - labelSmoothing)\n *                         + labelSmoothing / numClasses\n *\n * @param onehotLabels One hot encoded labels\n *    [batch_size, num_classes], same dimensions as 'predictions'.\n * @param logits The predicted outputs.\n * @param weights Tensor whose rank is either 0, or 1, and must be\n *    broadcastable to `loss`  of shape [batch_size]\n * @param labelSmoothing If greater than 0, then smooth the labels.\n * @param reduction Type of reduction to apply to loss. Should be of type\n *    `Reduction`\n *\n * @doc { heading: 'Training', subheading: 'Losses', namespace: 'losses' }\n */\nfunction softmaxCrossEntropy_<T extends Tensor, O extends Tensor>(\n    onehotLabels: T|TensorLike, logits: T|TensorLike,\n    weights?: Tensor|TensorLike, labelSmoothing = 0,\n    reduction = Reduction.SUM_BY_NONZERO_WEIGHTS): O {\n  let $onehotLabels =\n      convertToTensor(onehotLabels, 'onehotLabels', 'softmaxCrossEntropy');\n  const $logits = convertToTensor(logits, 'logits', 'softmaxCrossEntropy');\n  let $weights: Tensor = null;\n\n  if (weights != null) {\n    $weights = convertToTensor(weights, 'weights', 'softmaxCrossEntropy');\n  }\n\n  assertShapesMatch(\n      $onehotLabels.shape, $logits.shape, 'Error in softmaxCrossEntropy: ');\n\n  if (labelSmoothing > 0) {\n    const labelSmoothingScalar = scalar(labelSmoothing);\n    const one = scalar(1);\n    const numClasses = scalar($onehotLabels.shape[1]);\n\n    $onehotLabels =\n        add(mul($onehotLabels, sub(one, labelSmoothingScalar)),\n            div(labelSmoothingScalar, numClasses));\n  }\n\n  const losses = softmaxCrossEntropyWithLogits_($onehotLabels, $logits);\n\n  return computeWeightedLoss(losses, $weights, reduction);\n}\n\nexport const softmaxCrossEntropy = /* @__PURE__ */ op({softmaxCrossEntropy_});\n"],"mappings":";AAAA;;;;;;;;;;;;;;;;AAgBA,SAAQA,UAAU,QAAO,iBAAiB;AAG1C,SAAQC,eAAe,QAAO,uBAAuB;AAErD,SAAQC,iBAAiB,QAAO,YAAY;AAC5C,SAAQC,GAAG,QAAO,QAAQ;AAC1B,SAAQC,oBAAoB,QAAO,cAAc;AACjD,SAAQC,IAAI,QAAO,SAAS;AAC5B,SAAQC,GAAG,QAAO,QAAQ;AAC1B,SAAQC,GAAG,QAAO,QAAQ;AAC1B,SAAQC,SAAS,QAAO,gBAAgB;AACxC,SAAQC,SAAS,QAAO,mBAAmB;AAC3C,SAAQC,GAAG,QAAO,QAAQ;AAC1B,SAAQC,GAAG,QAAO,QAAQ;AAC1B,SAAQC,EAAE,QAAO,cAAc;AAC/B,SAAQC,OAAO,QAAO,YAAY;AAClC,SAAQC,MAAM,QAAO,WAAW;AAChC,SAAQC,GAAG,QAAO,QAAQ;AAC1B,SAAQC,GAAG,QAAO,QAAQ;AAE1B,SAAQC,mBAAmB,QAAO,yBAAyB;AAE3D;;;;;;;;;;;;;;;;;;;;;;;;AAwBA,SAASC,8BAA8BA,CACnCC,MAAS,EAAEC,MAAS,EAAU;EAAA,IAARC,GAAG,GAAAC,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,CAAC,CAAC;EAChC,IAAID,GAAG,KAAK,CAAC,CAAC,EAAE;IACdA,GAAG,GAAGD,MAAM,CAACK,IAAI,GAAG,CAAC;;EAGvB,IAAIJ,GAAG,KAAKD,MAAM,CAACK,IAAI,GAAG,CAAC,EAAE;IAC3B,MAAMC,KAAK,CACP,wGAAAC,MAAA,CACuCP,MAAM,CAACK,IAAI,MAAG,kBAAAE,MAAA,CACtCN,GAAG,CAAE,CAAC;;EAE3B;EACA,IAAMO,QAAQ,GACV5B,UAAU,CAAC,UAACmB,MAAc,EAAEC,MAAc,EAAES,IAAkB,EAAI;IAChE;IACA;IACA;IACA,IAAMC,QAAQ,GAAG,IAAI;IACrB,IAAMC,GAAG,GAAGvB,SAAS,CAACY,MAAM,EAAE,CAACC,GAAG,CAAC,EAAES,QAAQ,CAAC;IAC9C,IAAME,SAAS,GAAGjB,GAAG,CAACV,IAAI,CAACe,MAAM,EAAE,SAAS,CAAC,EAAEW,GAAG,CAAC;IACnDF,IAAI,CAAC,CAACV,MAAM,EAAEa,SAAS,CAAC,CAAC;IAEzB,IAAMC,UAAU,GAAGtB,GAAG,CAACD,GAAG,CAACsB,SAAS,EAAEb,MAAM,CAAC,CAAC;IAC9C,IAAMe,KAAK,GAAMlB,GAAG,CAACiB,UAAU,EAAE,CAACZ,GAAG,CAAC,CAAC;IAEvC,IAAMc,QAAQ,GAAG,SAAXA,QAAQA,CAAIC,EAAK,EAAEC,KAAe,EAAI;MAC1C,IAAAC,MAAA,GAAAC,cAAA,CAA4BF,KAAK;QAA1BlB,MAAM,GAAAmB,MAAA;QAAEN,SAAS,GAAAM,MAAA;MACxB,IAAME,OAAO,GAAGpC,oBAAoB,CAACgC,EAAE,CAACK,KAAK,EAAE,CAACpB,GAAG,CAAC,CAAC;MACrD,OAAO,CACLX,GAAG,CAACG,OAAO,CAACuB,EAAE,EAAEI,OAAO,CAAC,EACpBzB,GAAG,CAACV,IAAI,CAACc,MAAM,EAAE,SAAS,CAAC,EAAEZ,GAAG,CAACyB,SAAS,CAAC,CAAC,CAAC,EACjDtB,GAAG,CAACG,OAAO,CAACuB,EAAE,EAAEI,OAAO,CAAC,EACpBzB,GAAG,CAACR,GAAG,CAACyB,SAAS,CAAC,EAAE3B,IAAI,CAACc,MAAM,EAAE,SAAS,CAAC,CAAC,CAAC,CAClD;IACH,CAAC;IACD,OAAO;MAACe,KAAK,EAALA,KAAK;MAAEC,QAAQ,EAARA;IAAQ,CAAC;EAC1B,CAAC,CAAC;EAEN,OAAOP,QAAQ,CAACT,MAAM,EAAEC,MAAM,CAAC;AACjC;AAEA;;;;;;;;;;;;;;;;;;;AAmBA,SAASsB,oBAAoBA,CACzBC,YAA0B,EAAEvB,MAAoB,EAChDwB,OAA2B,EACiB;EAAA,IADfC,cAAc,GAAAvB,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,CAAC;EAAA,IAC/CwB,SAAS,GAAAxB,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAGb,SAAS,CAACsC,sBAAsB;EAC9C,IAAIC,aAAa,GACb/C,eAAe,CAAC0C,YAAY,EAAE,cAAc,EAAE,qBAAqB,CAAC;EACxE,IAAMM,OAAO,GAAGhD,eAAe,CAACmB,MAAM,EAAE,QAAQ,EAAE,qBAAqB,CAAC;EACxE,IAAI8B,QAAQ,GAAW,IAAI;EAE3B,IAAIN,OAAO,IAAI,IAAI,EAAE;IACnBM,QAAQ,GAAGjD,eAAe,CAAC2C,OAAO,EAAE,SAAS,EAAE,qBAAqB,CAAC;;EAGvE1C,iBAAiB,CACb8C,aAAa,CAACP,KAAK,EAAEQ,OAAO,CAACR,KAAK,EAAE,gCAAgC,CAAC;EAEzE,IAAII,cAAc,GAAG,CAAC,EAAE;IACtB,IAAMM,oBAAoB,GAAGrC,MAAM,CAAC+B,cAAc,CAAC;IACnD,IAAMO,GAAG,GAAGtC,MAAM,CAAC,CAAC,CAAC;IACrB,IAAMuC,UAAU,GAAGvC,MAAM,CAACkC,aAAa,CAACP,KAAK,CAAC,CAAC,CAAC,CAAC;IAEjDO,aAAa,GACT7C,GAAG,CAACO,GAAG,CAACsC,aAAa,EAAEjC,GAAG,CAACqC,GAAG,EAAED,oBAAoB,CAAC,CAAC,EAClD7C,GAAG,CAAC6C,oBAAoB,EAAEE,UAAU,CAAC,CAAC;;EAGhD,IAAMC,MAAM,GAAGpC,8BAA8B,CAAC8B,aAAa,EAAEC,OAAO,CAAC;EAErE,OAAOhC,mBAAmB,CAACqC,MAAM,EAAEJ,QAAQ,EAAEJ,SAAS,CAAC;AACzD;AAEA,OAAO,IAAMS,mBAAmB,GAAG,eAAgB3C,EAAE,CAAC;EAAC8B,oBAAoB,EAApBA;AAAoB,CAAC,CAAC"},"metadata":{},"sourceType":"module","externalDependencies":[]}