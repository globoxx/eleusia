{"ast":null,"code":"import _classCallCheck from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/classCallCheck.js\";\nimport _createClass from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/createClass.js\";\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { variableGrads } from '@tensorflow/tfjs-core';\nimport { getNextUniqueTensorId } from './backend/state';\nimport { getScopedTensorName, getUniqueTensorName } from './common';\nimport { NotImplementedError } from './errors';\nvar DEFAULT_VARIABLE_NAME_PREFIX = 'Variable';\n/**\n * A `tf.layers.LayerVariable` is similar to a `tf.Tensor` in that it has a\n * dtype and shape, but its value is mutable.  The value is itself represented\n * as a`tf.Tensor`, and can be read with the `read()` method and updated with\n * the `write()` method.\n */\nexport var LayerVariable = /*#__PURE__*/function () {\n  /**\n   * Construct Variable from a `tf.Tensor`.\n   *\n   * If not explicitly named, the Variable will be given a name with the\n   * prefix 'Variable'. Variable names are unique. In the case of name\n   * collision, suffixies '_<num>' will be added to the name.\n   *\n   * @param val Initial value of the Variable.\n   * @param name Name of the variable. If `null` or `undefined` is provided, it\n   *   will default a name with the prefix 'Variable'.\n   * @param constraint Optional, projection function to be applied to the\n   * variable after optimize updates\n   * @throws ValueError if `name` is `null` or `undefined`.\n   */\n  function LayerVariable(val) {\n    var dtype = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 'float32';\n    var name = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : DEFAULT_VARIABLE_NAME_PREFIX;\n    var trainable = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : true;\n    var constraint = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : null;\n    _classCallCheck(this, LayerVariable);\n    this.dtype = dtype == null ? 'float32' : dtype;\n    this.shape = val.shape;\n    this.id = getNextUniqueTensorId();\n    name = name == null ? DEFAULT_VARIABLE_NAME_PREFIX : name;\n    this.originalName = getScopedTensorName(name);\n    this.name = getUniqueTensorName(this.originalName);\n    this.trainable_ = trainable;\n    this.constraint = constraint;\n    this.val = tfc.variable(val, this.trainable_, this.name, this.dtype);\n  }\n  /**\n   * Get a snapshot of the Variable's value.\n   *\n   * The returned value is a snapshot of the Variable's value at the time of\n   * the invocation. Future mutations in the value of the tensor will only\n   * be reflected by future calls to this method.\n   */\n  _createClass(LayerVariable, [{\n    key: \"read\",\n    value: function read() {\n      this.assertNotDisposed();\n      return this.val;\n    }\n    /**\n     * Update the value of the Variable.\n     *\n     * @param newVal: The new value to update to. Must be consistent with the\n     *   dtype and shape of the Variable.\n     * @return This Variable.\n     */\n  }, {\n    key: \"write\",\n    value: function write(newVal) {\n      // TODO(cais): Once  TF.js Core supports Tensor.dtype, check dtype match.\n      this.assertNotDisposed();\n      checkShapesMatch(this.val, newVal);\n      // Skip updating if this is the exact same tensor.\n      if (this.val.id !== newVal.id) {\n        this.val.assign(newVal);\n        if (this.constraint != null) {\n          this.val.assign(this.constraint.apply(this.val));\n        }\n      }\n      return this;\n    }\n    /**\n     * Dispose this LayersVariable instance from memory.\n     */\n  }, {\n    key: \"dispose\",\n    value: function dispose() {\n      this.assertNotDisposed();\n      this.val.dispose();\n    }\n  }, {\n    key: \"assertNotDisposed\",\n    value: function assertNotDisposed() {\n      if (this.val.isDisposed) {\n        throw new Error(\"LayersVariable \".concat(this.name, \" is already disposed.\"));\n      }\n    }\n  }, {\n    key: \"trainable\",\n    get: function get() {\n      return this.trainable_;\n    },\n    set: function set(trainable) {\n      this.trainable_ = trainable;\n      this.val.trainable = trainable;\n    }\n  }]);\n  return LayerVariable;\n}();\nfunction checkShapesMatch(x, y) {\n  if (x.shape.toString() !== y.shape.toString()) {\n    throw new Error('Shape mismatch: ' + JSON.stringify(x.shape) + ' vs. ' + JSON.stringify(y.shape));\n  }\n}\n/**\n * Create a Variable.\n * @param x The initial value of the `Variable`.\n * @param dtype optional, the type of the variable.\n * @param name optional, the name of the variable, default provided by\n * Variable.\n * @param constraint optional, a constraint to be applied after every update.\n * @return The newly instantiated `Variable`.\n */\nexport function variable(x, dtype, name, constraint) {\n  return new LayerVariable(x, dtype, name, true, constraint);\n}\n/**\n * Instantiates an all-zeros Variable and returns it.\n *\n * @param shape Shape of the tensor.\n * @param dtype DType of the tensor.\n * @param name Name of the tensor.\n * @return An all-zero Variable.\n */\nexport function zerosVariable(shape, dtype, name) {\n  // TODO(cais): Implement logic for dtype.\n  return new LayerVariable(tfc.zeros(shape), dtype, name);\n}\n/**\n * Instantiates an all-zeros tensor of the same shape as another tensor.\n *\n * @param x The other tensor.\n * @param dtype DType of the tensor.\n * @param name Name of the tensor.\n * @return A newly instantiated Variable.\n */\nexport function zerosLike(x, dtype, name) {\n  return new LayerVariable(tfc.zerosLike(x), dtype, name);\n}\n/**\n * Instantiates an all-ones tensor and returns it.\n *\n * @param shape Shape of the tensor.\n * @param dtype DType of the tensor.\n * @param name Name of the tensor.\n * @return An all-ones Variable.\n */\nexport function onesVariable(shape, dtype, name) {\n  // TODO(cais): Implement logic for dtype.\n  var allocated = tfc.ones(shape);\n  return new LayerVariable(allocated, dtype, name);\n}\n/**\n * Instantiates an all-ones tensor of the same shape as another tensor.\n *\n * @param x The other tensor.\n * @param dtype DType of the tensor.\n * @param name Name of the tensor.\n * @return A newly instantiated Variable.\n */\nexport function onesLike(x, dtype, name) {\n  var allocated = tfc.onesLike(x);\n  return new LayerVariable(allocated, dtype, name);\n}\n/**\n * Instantiate an identity matrix and returns it, as a Variable\n *\n * @param size Number of rows/columns.\n * @param dtype Data type of returned Variable.\n * @param name Name of returned Variable.\n * @return A Variable, an identity matrix.\n */\nexport function eyeVariable(size, dtype, name) {\n  return new LayerVariable(tfc.eye(size), dtype, name);\n}\n/**\n * Get a Variable with uniform distribution of values.\n * @param shape Shape of the tensor.\n * @param minval Lower bound of the uniform distribution.\n * @param maxval Upper bound of the uniform distribution.\n * @param dtype\n * @param seed\n * @param name Optional name.\n * @return The uniform-random Variable.\n */\nexport function randomUniformVariable(shape, minval, maxval, dtype, seed) {\n  var name = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : 'randomUniform';\n  return new LayerVariable(tfc.randomUniform(shape, minval, maxval, dtype), dtype, name);\n}\n/**\n * Get a Variable with truncated-normal distribution of values.\n * @param shape Shape of the tensor.\n * @param mean mean value of the normal distribution.\n * @param stddev standard deviation of the normal distribution.\n * @param dtype\n * @param seed\n * @param name Optional name.\n * @return The truncated-normal-random Variable.\n */\nexport function truncatedNormalVariable(shape) {\n  var mean = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0.0;\n  var stddev = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 1.0;\n  var dtype = arguments.length > 3 ? arguments[3] : undefined;\n  var seed = arguments.length > 4 ? arguments[4] : undefined;\n  var name = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : 'truncatedNormal';\n  // TODO(cais): Implement logic for dtype and seed once they are supported\n  // by deeplearn.js.\n  dtype = dtype || 'float32';\n  if (dtype !== 'float32' && dtype !== 'int32') {\n    throw new NotImplementedError(\"randomNormal does not support dType \".concat(dtype, \".\"));\n  }\n  return new LayerVariable(tfc.truncatedNormal(shape, mean, stddev, dtype, seed), dtype, name);\n}\n/**\n * Get a Variable with normal distribution of values.\n * @param shape Shape of the tensor.\n * @param mean mean value of the normal distribution.\n * @param stddev standard deviation of the normal distribution.\n * @param dtype\n * @param seed\n * @param name Optional name.\n * @return The truncated-normal-random Variable.\n */\nexport function randomNormalVariable(shape) {\n  var mean = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0.0;\n  var stddev = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 1.0;\n  var dtype = arguments.length > 3 ? arguments[3] : undefined;\n  var seed = arguments.length > 4 ? arguments[4] : undefined;\n  var name = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : 'randomNormal';\n  dtype = dtype || 'float32';\n  if (dtype !== 'float32' && dtype !== 'int32') {\n    throw new NotImplementedError(\"randomNormalVariable does not support dType \".concat(dtype, \".\"));\n  }\n  return new LayerVariable(tfc.randomNormal(shape, mean, stddev, dtype, seed), dtype, name);\n}\n/**\n * Update the value of a Variable.\n * @param x The Variable to be updated.\n * @param xNew The new value to update to.\n * @return The Variable updated.\n */\nexport function update(x, xNew) {\n  return x.write(xNew);\n}\n/**\n * Update the value of a Variable by adding an increment.\n * @param x The Variable to be updated.\n * @param increment The incrment to add to `x`.\n * @return The Variable updated.\n */\nexport function updateAdd(x, increment) {\n  return x.write(tfc.add(x.read(), increment));\n}\n/**\n * Update the value of a Variable by subtracting a decrement.\n * @param x The Variable to be updated.\n * @param decrement The decrement to subtract from `x`.\n * @return The Variable updated.\n */\nexport function updateSub(x, decrement) {\n  return x.write(tfc.sub(x.read(), decrement));\n}\n/**\n * Get the values of an array of Variables.\n *\n * @param tensors An `Array` of `Variable`s to get the values of.\n * @return The values of the inputs, as an `Array` of`tf.Tensor`s.\n */\nexport function batchGetValue(xs) {\n  return xs.map(function (x) {\n    return x.read();\n  });\n}\n/**\n * Update the value of multiple Variables at once.\n *\n * @param variablesAndValues An `Array`, each element is of type\n *   [Variable, Tensor]. The first item is the\n *   `Variable` of which the value is to be updated. The second item\n *   carries the new value.\n */\nexport function batchSetValue(variablesAndValues) {\n  variablesAndValues.forEach(function (variableAndValue) {\n    var variable = variableAndValue[0];\n    variable.write(variableAndValue[1]);\n  });\n}\n/**\n * Returns the gradients of `variables` w.r.t. the return value of `lossFn`.\n * @param lossFn A function which returns a Scalar to be used as the function\n *   value (i.e., numerator) for differentiation.\n * @param variables List of variables to be used as the independent variables\n *   (i.e., denominator) for differentiation.\n * @returns An Array of gradients tensors.\n */\nexport function gradients(lossFn, variables) {\n  // TODO(cais): The return type signature can be simplified if deeplearn makes\n  //   the corresponding type public.\n  var variableList = variables.map(function (variable) {\n    return variable.read();\n  });\n  var valudAndGrads = variableGrads(lossFn, variableList);\n  return variables.map(function (variable) {\n    return valudAndGrads.grads[variable.name];\n  });\n}","map":{"version":3,"names":["tfc","variableGrads","getNextUniqueTensorId","getScopedTensorName","getUniqueTensorName","NotImplementedError","DEFAULT_VARIABLE_NAME_PREFIX","LayerVariable","val","dtype","arguments","length","undefined","name","trainable","constraint","_classCallCheck","shape","id","originalName","trainable_","variable","_createClass","key","value","read","assertNotDisposed","write","newVal","checkShapesMatch","assign","apply","dispose","isDisposed","Error","concat","get","set","x","y","toString","JSON","stringify","zerosVariable","zeros","zerosLike","onesVariable","allocated","ones","onesLike","eyeVariable","size","eye","randomUniformVariable","minval","maxval","seed","randomUniform","truncatedNormalVariable","mean","stddev","truncatedNormal","randomNormalVariable","randomNormal","update","xNew","updateAdd","increment","add","updateSub","decrement","sub","batchGetValue","xs","map","batchSetValue","variablesAndValues","forEach","variableAndValue","gradients","lossFn","variables","variableList","valudAndGrads","grads"],"sources":["C:\\Users\\vince\\OneDrive\\Documents\\GitHub\\tfjs-layers\\src\\variables.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\nimport * as tfc from '@tensorflow/tfjs-core';\nimport {DataType, Tensor, variableGrads} from '@tensorflow/tfjs-core';\n\nimport {getNextUniqueTensorId} from './backend/state';\nimport {getScopedTensorName, getUniqueTensorName} from './common';\nimport {Constraint} from './constraints';\nimport {NotImplementedError} from './errors';\nimport {Shape} from './keras_format/common';\nimport {HasShape} from './types';\n\nconst DEFAULT_VARIABLE_NAME_PREFIX = 'Variable';\n\n/**\n * A `tf.layers.LayerVariable` is similar to a `tf.Tensor` in that it has a\n * dtype and shape, but its value is mutable.  The value is itself represented\n * as a`tf.Tensor`, and can be read with the `read()` method and updated with\n * the `write()` method.\n */\nexport class LayerVariable {\n  readonly dtype: DataType;\n  readonly shape: Shape;\n\n  readonly id: number;\n  // The fully scoped name of this Variable, including a unique suffix if needed\n  readonly name: string;\n  // The originally requested fully scoped name of this Variable, not including\n  // any unique suffix.  This may be needed when restoring weights because this\n  // original name is used as a key.\n  readonly originalName: string;\n  private trainable_: boolean;\n\n  protected readonly val: tfc.Variable;\n  readonly constraint: Constraint;\n  /**\n   * Construct Variable from a `tf.Tensor`.\n   *\n   * If not explicitly named, the Variable will be given a name with the\n   * prefix 'Variable'. Variable names are unique. In the case of name\n   * collision, suffixies '_<num>' will be added to the name.\n   *\n   * @param val Initial value of the Variable.\n   * @param name Name of the variable. If `null` or `undefined` is provided, it\n   *   will default a name with the prefix 'Variable'.\n   * @param constraint Optional, projection function to be applied to the\n   * variable after optimize updates\n   * @throws ValueError if `name` is `null` or `undefined`.\n   */\n  constructor(\n      val: Tensor, dtype: DataType = 'float32',\n      name = DEFAULT_VARIABLE_NAME_PREFIX, trainable = true,\n      constraint: Constraint = null) {\n    this.dtype = dtype == null ? 'float32' : dtype;\n    this.shape = val.shape;\n    this.id = getNextUniqueTensorId();\n\n    name = name == null ? DEFAULT_VARIABLE_NAME_PREFIX : name;\n    this.originalName = getScopedTensorName(name);\n    this.name = getUniqueTensorName(this.originalName);\n\n    this.trainable_ = trainable;\n    this.constraint = constraint;\n\n    this.val = tfc.variable(val, this.trainable_, this.name, this.dtype);\n  }\n\n  /**\n   * Get a snapshot of the Variable's value.\n   *\n   * The returned value is a snapshot of the Variable's value at the time of\n   * the invocation. Future mutations in the value of the tensor will only\n   * be reflected by future calls to this method.\n   */\n  read(): Tensor {\n    this.assertNotDisposed();\n    return this.val;\n  }\n\n  /**\n   * Update the value of the Variable.\n   *\n   * @param newVal: The new value to update to. Must be consistent with the\n   *   dtype and shape of the Variable.\n   * @return This Variable.\n   */\n  write(newVal: Tensor) {\n    // TODO(cais): Once  TF.js Core supports Tensor.dtype, check dtype match.\n    this.assertNotDisposed();\n    checkShapesMatch(this.val, newVal);\n    // Skip updating if this is the exact same tensor.\n    if (this.val.id !== newVal.id) {\n      this.val.assign(newVal);\n      if (this.constraint != null) {\n        this.val.assign(this.constraint.apply(this.val));\n      }\n    }\n    return this;\n  }\n\n  /**\n   * Dispose this LayersVariable instance from memory.\n   */\n  dispose(): void {\n    this.assertNotDisposed();\n    this.val.dispose();\n  }\n\n  protected assertNotDisposed(): void {\n    if (this.val.isDisposed) {\n      throw new Error(`LayersVariable ${this.name} is already disposed.`);\n    }\n  }\n\n  get trainable(): boolean {\n    return this.trainable_;\n  }\n\n  set trainable(trainable: boolean) {\n    this.trainable_ = trainable;\n    this.val.trainable = trainable;\n  }\n}\n\nfunction checkShapesMatch(x: HasShape, y: HasShape): void {\n  if (x.shape.toString() !== y.shape.toString()) {\n    throw new Error(\n        'Shape mismatch: ' + JSON.stringify(x.shape) + ' vs. ' +\n        JSON.stringify(y.shape));\n  }\n}\n\n/**\n * Create a Variable.\n * @param x The initial value of the `Variable`.\n * @param dtype optional, the type of the variable.\n * @param name optional, the name of the variable, default provided by\n * Variable.\n * @param constraint optional, a constraint to be applied after every update.\n * @return The newly instantiated `Variable`.\n */\nexport function variable(\n    x: Tensor, dtype?: DataType, name?: string,\n    constraint?: Constraint): LayerVariable {\n  return new LayerVariable(x, dtype, name, true, constraint);\n}\n\n/**\n * Instantiates an all-zeros Variable and returns it.\n *\n * @param shape Shape of the tensor.\n * @param dtype DType of the tensor.\n * @param name Name of the tensor.\n * @return An all-zero Variable.\n */\nexport function zerosVariable(\n    shape: Shape, dtype?: DataType, name?: string): LayerVariable {\n  // TODO(cais): Implement logic for dtype.\n  return new LayerVariable(tfc.zeros(shape), dtype, name);\n}\n\n/**\n * Instantiates an all-zeros tensor of the same shape as another tensor.\n *\n * @param x The other tensor.\n * @param dtype DType of the tensor.\n * @param name Name of the tensor.\n * @return A newly instantiated Variable.\n */\nexport function zerosLike(\n    x: Tensor, dtype?: DataType, name?: string): LayerVariable {\n  return new LayerVariable(tfc.zerosLike(x), dtype, name);\n}\n\n/**\n * Instantiates an all-ones tensor and returns it.\n *\n * @param shape Shape of the tensor.\n * @param dtype DType of the tensor.\n * @param name Name of the tensor.\n * @return An all-ones Variable.\n */\nexport function onesVariable(\n    shape: Shape, dtype?: DataType, name?: string): LayerVariable {\n  // TODO(cais): Implement logic for dtype.\n  const allocated = tfc.ones(shape);\n  return new LayerVariable(allocated, dtype, name);\n}\n\n/**\n * Instantiates an all-ones tensor of the same shape as another tensor.\n *\n * @param x The other tensor.\n * @param dtype DType of the tensor.\n * @param name Name of the tensor.\n * @return A newly instantiated Variable.\n */\nexport function onesLike(\n    x: Tensor, dtype?: DataType, name?: string): LayerVariable {\n  const allocated = tfc.onesLike(x);\n  return new LayerVariable(allocated, dtype, name);\n}\n\n/**\n * Instantiate an identity matrix and returns it, as a Variable\n *\n * @param size Number of rows/columns.\n * @param dtype Data type of returned Variable.\n * @param name Name of returned Variable.\n * @return A Variable, an identity matrix.\n */\nexport function eyeVariable(\n    size: number, dtype?: DataType, name?: string): LayerVariable {\n  return new LayerVariable(tfc.eye(size), dtype, name);\n}\n\n/**\n * Get a Variable with uniform distribution of values.\n * @param shape Shape of the tensor.\n * @param minval Lower bound of the uniform distribution.\n * @param maxval Upper bound of the uniform distribution.\n * @param dtype\n * @param seed\n * @param name Optional name.\n * @return The uniform-random Variable.\n */\nexport function randomUniformVariable(\n    shape: Shape, minval: number, maxval: number, dtype?: DataType,\n    seed?: number, name = 'randomUniform'): LayerVariable {\n  return new LayerVariable(\n      tfc.randomUniform(shape, minval, maxval, dtype), dtype, name);\n}\n\n/**\n * Get a Variable with truncated-normal distribution of values.\n * @param shape Shape of the tensor.\n * @param mean mean value of the normal distribution.\n * @param stddev standard deviation of the normal distribution.\n * @param dtype\n * @param seed\n * @param name Optional name.\n * @return The truncated-normal-random Variable.\n */\nexport function truncatedNormalVariable(\n    shape: Shape, mean = 0.0, stddev = 1.0, dtype?: DataType, seed?: number,\n    name = 'truncatedNormal'): LayerVariable {\n  // TODO(cais): Implement logic for dtype and seed once they are supported\n  // by deeplearn.js.\n  dtype = dtype || 'float32';\n  if (dtype !== 'float32' && dtype !== 'int32') {\n    throw new NotImplementedError(\n        `randomNormal does not support dType ${dtype}.`);\n  }\n  return new LayerVariable(\n      tfc.truncatedNormal(shape, mean, stddev, dtype, seed), dtype, name);\n}\n/**\n * Get a Variable with normal distribution of values.\n * @param shape Shape of the tensor.\n * @param mean mean value of the normal distribution.\n * @param stddev standard deviation of the normal distribution.\n * @param dtype\n * @param seed\n * @param name Optional name.\n * @return The truncated-normal-random Variable.\n */\nexport function randomNormalVariable(\n    shape: Shape, mean = 0.0, stddev = 1.0, dtype?: DataType, seed?: number,\n    name = 'randomNormal'): LayerVariable {\n  dtype = dtype || 'float32';\n  if (dtype !== 'float32' && dtype !== 'int32') {\n    throw new NotImplementedError(\n        `randomNormalVariable does not support dType ${dtype}.`);\n  }\n  return new LayerVariable(\n      tfc.randomNormal(shape, mean, stddev, dtype, seed), dtype, name);\n}\n\n/**\n * Update the value of a Variable.\n * @param x The Variable to be updated.\n * @param xNew The new value to update to.\n * @return The Variable updated.\n */\nexport function update(x: LayerVariable, xNew: Tensor): LayerVariable {\n  return x.write(xNew);\n}\n\n/**\n * Update the value of a Variable by adding an increment.\n * @param x The Variable to be updated.\n * @param increment The incrment to add to `x`.\n * @return The Variable updated.\n */\nexport function updateAdd(x: LayerVariable, increment: Tensor): LayerVariable {\n  return x.write(tfc.add(x.read(), increment));\n}\n\n/**\n * Update the value of a Variable by subtracting a decrement.\n * @param x The Variable to be updated.\n * @param decrement The decrement to subtract from `x`.\n * @return The Variable updated.\n */\nexport function updateSub(x: LayerVariable, decrement: Tensor): LayerVariable {\n  return x.write(tfc.sub(x.read(), decrement));\n}\n\n/**\n * Get the values of an array of Variables.\n *\n * @param tensors An `Array` of `Variable`s to get the values of.\n * @return The values of the inputs, as an `Array` of`tf.Tensor`s.\n */\nexport function batchGetValue(xs: LayerVariable[]): Tensor[] {\n  return xs.map(x => x.read());\n}\n\n/**\n * Update the value of multiple Variables at once.\n *\n * @param variablesAndValues An `Array`, each element is of type\n *   [Variable, Tensor]. The first item is the\n *   `Variable` of which the value is to be updated. The second item\n *   carries the new value.\n */\nexport function batchSetValue(\n    variablesAndValues: Array<[LayerVariable, Tensor]>): void {\n  variablesAndValues.forEach(variableAndValue => {\n    const variable: LayerVariable = variableAndValue[0];\n    variable.write(variableAndValue[1]);\n  });\n}\n\n/**\n * Returns the gradients of `variables` w.r.t. the return value of `lossFn`.\n * @param lossFn A function which returns a Scalar to be used as the function\n *   value (i.e., numerator) for differentiation.\n * @param variables List of variables to be used as the independent variables\n *   (i.e., denominator) for differentiation.\n * @returns An Array of gradients tensors.\n */\nexport function gradients(\n    lossFn: () => tfc.Scalar, variables: LayerVariable[]): Tensor[] {\n  // TODO(cais): The return type signature can be simplified if deeplearn makes\n  //   the corresponding type public.\n  const variableList =\n      variables.map(variable => variable.read() as tfc.Variable);\n  const valudAndGrads = variableGrads(lossFn, variableList);\n  return variables.map(variable => valudAndGrads.grads[variable.name]);\n}\n"],"mappings":";;AAAA;;;;;;;;;AAUA,OAAO,KAAKA,GAAG,MAAM,uBAAuB;AAC5C,SAA0BC,aAAa,QAAO,uBAAuB;AAErE,SAAQC,qBAAqB,QAAO,iBAAiB;AACrD,SAAQC,mBAAmB,EAAEC,mBAAmB,QAAO,UAAU;AAEjE,SAAQC,mBAAmB,QAAO,UAAU;AAI5C,IAAMC,4BAA4B,GAAG,UAAU;AAE/C;;;;;;AAMA,WAAaC,aAAa;EAexB;;;;;;;;;;;;;;EAcA,SAAAA,cACIC,GAAW,EAEkB;IAAA,IAFhBC,KAAA,GAAAC,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAkB,SAAS;IAAA,IACxCG,IAAI,GAAAH,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAGJ,4BAA4B;IAAA,IAAEQ,SAAS,GAAAJ,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,IAAI;IAAA,IACrDK,UAAA,GAAAL,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAyB,IAAI;IAAAM,eAAA,OAAAT,aAAA;IAC/B,IAAI,CAACE,KAAK,GAAGA,KAAK,IAAI,IAAI,GAAG,SAAS,GAAGA,KAAK;IAC9C,IAAI,CAACQ,KAAK,GAAGT,GAAG,CAACS,KAAK;IACtB,IAAI,CAACC,EAAE,GAAGhB,qBAAqB,EAAE;IAEjCW,IAAI,GAAGA,IAAI,IAAI,IAAI,GAAGP,4BAA4B,GAAGO,IAAI;IACzD,IAAI,CAACM,YAAY,GAAGhB,mBAAmB,CAACU,IAAI,CAAC;IAC7C,IAAI,CAACA,IAAI,GAAGT,mBAAmB,CAAC,IAAI,CAACe,YAAY,CAAC;IAElD,IAAI,CAACC,UAAU,GAAGN,SAAS;IAC3B,IAAI,CAACC,UAAU,GAAGA,UAAU;IAE5B,IAAI,CAACP,GAAG,GAAGR,GAAG,CAACqB,QAAQ,CAACb,GAAG,EAAE,IAAI,CAACY,UAAU,EAAE,IAAI,CAACP,IAAI,EAAE,IAAI,CAACJ,KAAK,CAAC;EACtE;EAEA;;;;;;;EAAAa,YAAA,CAAAf,aAAA;IAAAgB,GAAA;IAAAC,KAAA,EAOA,SAAAC,KAAA,EAAI;MACF,IAAI,CAACC,iBAAiB,EAAE;MACxB,OAAO,IAAI,CAAClB,GAAG;IACjB;IAEA;;;;;;;EAAA;IAAAe,GAAA;IAAAC,KAAA,EAOA,SAAAG,MAAMC,MAAc;MAClB;MACA,IAAI,CAACF,iBAAiB,EAAE;MACxBG,gBAAgB,CAAC,IAAI,CAACrB,GAAG,EAAEoB,MAAM,CAAC;MAClC;MACA,IAAI,IAAI,CAACpB,GAAG,CAACU,EAAE,KAAKU,MAAM,CAACV,EAAE,EAAE;QAC7B,IAAI,CAACV,GAAG,CAACsB,MAAM,CAACF,MAAM,CAAC;QACvB,IAAI,IAAI,CAACb,UAAU,IAAI,IAAI,EAAE;UAC3B,IAAI,CAACP,GAAG,CAACsB,MAAM,CAAC,IAAI,CAACf,UAAU,CAACgB,KAAK,CAAC,IAAI,CAACvB,GAAG,CAAC,CAAC;;;MAGpD,OAAO,IAAI;IACb;IAEA;;;EAAA;IAAAe,GAAA;IAAAC,KAAA,EAGA,SAAAQ,QAAA,EAAO;MACL,IAAI,CAACN,iBAAiB,EAAE;MACxB,IAAI,CAAClB,GAAG,CAACwB,OAAO,EAAE;IACpB;EAAC;IAAAT,GAAA;IAAAC,KAAA,EAES,SAAAE,kBAAA,EAAiB;MACzB,IAAI,IAAI,CAAClB,GAAG,CAACyB,UAAU,EAAE;QACvB,MAAM,IAAIC,KAAK,mBAAAC,MAAA,CAAmB,IAAI,CAACtB,IAAI,2BAAwB;;IAEvE;EAAC;IAAAU,GAAA;IAAAa,GAAA,EAED,SAAAA,IAAA,EAAa;MACX,OAAO,IAAI,CAAChB,UAAU;IACxB,CAAC;IAAAiB,GAAA,EAED,SAAAA,IAAcvB,SAAkB;MAC9B,IAAI,CAACM,UAAU,GAAGN,SAAS;MAC3B,IAAI,CAACN,GAAG,CAACM,SAAS,GAAGA,SAAS;IAChC;EAAC;EAAA,OAAAP,aAAA;AAAA;AAGH,SAASsB,gBAAgBA,CAACS,CAAW,EAAEC,CAAW;EAChD,IAAID,CAAC,CAACrB,KAAK,CAACuB,QAAQ,EAAE,KAAKD,CAAC,CAACtB,KAAK,CAACuB,QAAQ,EAAE,EAAE;IAC7C,MAAM,IAAIN,KAAK,CACX,kBAAkB,GAAGO,IAAI,CAACC,SAAS,CAACJ,CAAC,CAACrB,KAAK,CAAC,GAAG,OAAO,GACtDwB,IAAI,CAACC,SAAS,CAACH,CAAC,CAACtB,KAAK,CAAC,CAAC;;AAEhC;AAEA;;;;;;;;;AASA,OAAM,SAAUI,QAAQA,CACpBiB,CAAS,EAAE7B,KAAgB,EAAEI,IAAa,EAC1CE,UAAuB;EACzB,OAAO,IAAIR,aAAa,CAAC+B,CAAC,EAAE7B,KAAK,EAAEI,IAAI,EAAE,IAAI,EAAEE,UAAU,CAAC;AAC5D;AAEA;;;;;;;;AAQA,OAAM,SAAU4B,aAAaA,CACzB1B,KAAY,EAAER,KAAgB,EAAEI,IAAa;EAC/C;EACA,OAAO,IAAIN,aAAa,CAACP,GAAG,CAAC4C,KAAK,CAAC3B,KAAK,CAAC,EAAER,KAAK,EAAEI,IAAI,CAAC;AACzD;AAEA;;;;;;;;AAQA,OAAM,SAAUgC,SAASA,CACrBP,CAAS,EAAE7B,KAAgB,EAAEI,IAAa;EAC5C,OAAO,IAAIN,aAAa,CAACP,GAAG,CAAC6C,SAAS,CAACP,CAAC,CAAC,EAAE7B,KAAK,EAAEI,IAAI,CAAC;AACzD;AAEA;;;;;;;;AAQA,OAAM,SAAUiC,YAAYA,CACxB7B,KAAY,EAAER,KAAgB,EAAEI,IAAa;EAC/C;EACA,IAAMkC,SAAS,GAAG/C,GAAG,CAACgD,IAAI,CAAC/B,KAAK,CAAC;EACjC,OAAO,IAAIV,aAAa,CAACwC,SAAS,EAAEtC,KAAK,EAAEI,IAAI,CAAC;AAClD;AAEA;;;;;;;;AAQA,OAAM,SAAUoC,QAAQA,CACpBX,CAAS,EAAE7B,KAAgB,EAAEI,IAAa;EAC5C,IAAMkC,SAAS,GAAG/C,GAAG,CAACiD,QAAQ,CAACX,CAAC,CAAC;EACjC,OAAO,IAAI/B,aAAa,CAACwC,SAAS,EAAEtC,KAAK,EAAEI,IAAI,CAAC;AAClD;AAEA;;;;;;;;AAQA,OAAM,SAAUqC,WAAWA,CACvBC,IAAY,EAAE1C,KAAgB,EAAEI,IAAa;EAC/C,OAAO,IAAIN,aAAa,CAACP,GAAG,CAACoD,GAAG,CAACD,IAAI,CAAC,EAAE1C,KAAK,EAAEI,IAAI,CAAC;AACtD;AAEA;;;;;;;;;;AAUA,OAAM,SAAUwC,qBAAqBA,CACjCpC,KAAY,EAAEqC,MAAc,EAAEC,MAAc,EAAE9C,KAAgB,EAC9D+C,IAAa,EAAwB;EAAA,IAAtB3C,IAAI,GAAAH,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,eAAe;EACvC,OAAO,IAAIH,aAAa,CACpBP,GAAG,CAACyD,aAAa,CAACxC,KAAK,EAAEqC,MAAM,EAAEC,MAAM,EAAE9C,KAAK,CAAC,EAAEA,KAAK,EAAEI,IAAI,CAAC;AACnE;AAEA;;;;;;;;;;AAUA,OAAM,SAAU6C,uBAAuBA,CACnCzC,KAAY,EACY;EAAA,IADV0C,IAAI,GAAAjD,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,GAAG;EAAA,IAAEkD,MAAM,GAAAlD,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,GAAG;EAAA,IAAED,KAAgB,GAAAC,SAAA,CAAAC,MAAA,OAAAD,SAAA,MAAAE,SAAA;EAAA,IAAE4C,IAAa,GAAA9C,SAAA,CAAAC,MAAA,OAAAD,SAAA,MAAAE,SAAA;EAAA,IACvEC,IAAI,GAAAH,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,iBAAiB;EAC1B;EACA;EACAD,KAAK,GAAGA,KAAK,IAAI,SAAS;EAC1B,IAAIA,KAAK,KAAK,SAAS,IAAIA,KAAK,KAAK,OAAO,EAAE;IAC5C,MAAM,IAAIJ,mBAAmB,wCAAA8B,MAAA,CACc1B,KAAK,OAAI;;EAEtD,OAAO,IAAIF,aAAa,CACpBP,GAAG,CAAC6D,eAAe,CAAC5C,KAAK,EAAE0C,IAAI,EAAEC,MAAM,EAAEnD,KAAK,EAAE+C,IAAI,CAAC,EAAE/C,KAAK,EAAEI,IAAI,CAAC;AACzE;AACA;;;;;;;;;;AAUA,OAAM,SAAUiD,oBAAoBA,CAChC7C,KAAY,EACS;EAAA,IADP0C,IAAI,GAAAjD,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,GAAG;EAAA,IAAEkD,MAAM,GAAAlD,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,GAAG;EAAA,IAAED,KAAgB,GAAAC,SAAA,CAAAC,MAAA,OAAAD,SAAA,MAAAE,SAAA;EAAA,IAAE4C,IAAa,GAAA9C,SAAA,CAAAC,MAAA,OAAAD,SAAA,MAAAE,SAAA;EAAA,IACvEC,IAAI,GAAAH,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,cAAc;EACvBD,KAAK,GAAGA,KAAK,IAAI,SAAS;EAC1B,IAAIA,KAAK,KAAK,SAAS,IAAIA,KAAK,KAAK,OAAO,EAAE;IAC5C,MAAM,IAAIJ,mBAAmB,gDAAA8B,MAAA,CACsB1B,KAAK,OAAI;;EAE9D,OAAO,IAAIF,aAAa,CACpBP,GAAG,CAAC+D,YAAY,CAAC9C,KAAK,EAAE0C,IAAI,EAAEC,MAAM,EAAEnD,KAAK,EAAE+C,IAAI,CAAC,EAAE/C,KAAK,EAAEI,IAAI,CAAC;AACtE;AAEA;;;;;;AAMA,OAAM,SAAUmD,MAAMA,CAAC1B,CAAgB,EAAE2B,IAAY;EACnD,OAAO3B,CAAC,CAACX,KAAK,CAACsC,IAAI,CAAC;AACtB;AAEA;;;;;;AAMA,OAAM,SAAUC,SAASA,CAAC5B,CAAgB,EAAE6B,SAAiB;EAC3D,OAAO7B,CAAC,CAACX,KAAK,CAAC3B,GAAG,CAACoE,GAAG,CAAC9B,CAAC,CAACb,IAAI,EAAE,EAAE0C,SAAS,CAAC,CAAC;AAC9C;AAEA;;;;;;AAMA,OAAM,SAAUE,SAASA,CAAC/B,CAAgB,EAAEgC,SAAiB;EAC3D,OAAOhC,CAAC,CAACX,KAAK,CAAC3B,GAAG,CAACuE,GAAG,CAACjC,CAAC,CAACb,IAAI,EAAE,EAAE6C,SAAS,CAAC,CAAC;AAC9C;AAEA;;;;;;AAMA,OAAM,SAAUE,aAAaA,CAACC,EAAmB;EAC/C,OAAOA,EAAE,CAACC,GAAG,CAAC,UAAApC,CAAC;IAAA,OAAIA,CAAC,CAACb,IAAI,EAAE;EAAA,EAAC;AAC9B;AAEA;;;;;;;;AAQA,OAAM,SAAUkD,aAAaA,CACzBC,kBAAkD;EACpDA,kBAAkB,CAACC,OAAO,CAAC,UAAAC,gBAAgB,EAAG;IAC5C,IAAMzD,QAAQ,GAAkByD,gBAAgB,CAAC,CAAC,CAAC;IACnDzD,QAAQ,CAACM,KAAK,CAACmD,gBAAgB,CAAC,CAAC,CAAC,CAAC;EACrC,CAAC,CAAC;AACJ;AAEA;;;;;;;;AAQA,OAAM,SAAUC,SAASA,CACrBC,MAAwB,EAAEC,SAA0B;EACtD;EACA;EACA,IAAMC,YAAY,GACdD,SAAS,CAACP,GAAG,CAAC,UAAArD,QAAQ;IAAA,OAAIA,QAAQ,CAACI,IAAI,EAAkB;EAAA,EAAC;EAC9D,IAAM0D,aAAa,GAAGlF,aAAa,CAAC+E,MAAM,EAAEE,YAAY,CAAC;EACzD,OAAOD,SAAS,CAACP,GAAG,CAAC,UAAArD,QAAQ;IAAA,OAAI8D,aAAa,CAACC,KAAK,CAAC/D,QAAQ,CAACR,IAAI,CAAC;EAAA,EAAC;AACtE"},"metadata":{},"sourceType":"module","externalDependencies":[]}