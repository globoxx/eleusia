{"ast":null,"code":"import _regeneratorRuntime from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/regeneratorRuntime.js\";\nimport _toConsumableArray from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/toConsumableArray.js\";\nimport _asyncToGenerator from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/asyncToGenerator.js\";\nimport _classCallCheck from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/classCallCheck.js\";\nimport _createClass from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/createClass.js\";\nimport _inherits from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/inherits.js\";\nimport _createSuper from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/createSuper.js\";\n/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { dispose as _dispose, tidy } from '../globals';\nimport { add } from '../ops/add';\nimport { div } from '../ops/div';\nimport { mul } from '../ops/mul';\nimport { pow } from '../ops/pow';\nimport { scalar } from '../ops/scalar';\nimport { sqrt } from '../ops/sqrt';\nimport { square } from '../ops/square';\nimport { sub } from '../ops/sub';\nimport { zerosLike } from '../ops/zeros_like';\nimport { Optimizer } from './optimizer';\nexport var AdamOptimizer = /*#__PURE__*/function (_Optimizer) {\n  _inherits(AdamOptimizer, _Optimizer);\n  var _super = _createSuper(AdamOptimizer);\n  function AdamOptimizer(learningRate, beta1, beta2) {\n    var _this;\n    var epsilon = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : null;\n    _classCallCheck(this, AdamOptimizer);\n    _this = _super.call(this);\n    _this.learningRate = learningRate;\n    _this.beta1 = beta1;\n    _this.beta2 = beta2;\n    _this.epsilon = epsilon;\n    _this.accumulatedFirstMoment = [];\n    _this.accumulatedSecondMoment = [];\n    tidy(function () {\n      // accB* will be updated by batch.\n      _this.accBeta1 = scalar(beta1).variable();\n      _this.accBeta2 = scalar(beta2).variable();\n    });\n    if (epsilon == null) {\n      _this.epsilon = ENGINE.backend.epsilon();\n    }\n    return _this;\n  }\n  _createClass(AdamOptimizer, [{\n    key: \"applyGradients\",\n    value: function applyGradients(variableGradients) {\n      var _this2 = this;\n      var varNames = Array.isArray(variableGradients) ? variableGradients.map(function (v) {\n        return v.name;\n      }) : Object.keys(variableGradients);\n      tidy(function () {\n        var oneMinusAccBeta1 = sub(1, _this2.accBeta1);\n        var oneMinusAccBeta2 = sub(1, _this2.accBeta2);\n        varNames.forEach(function (name, i) {\n          var value = ENGINE.registeredVariables[name];\n          var trainable = false;\n          if (_this2.accumulatedFirstMoment[i] == null) {\n            _this2.accumulatedFirstMoment[i] = {\n              originalName: \"\".concat(name, \"/m\"),\n              variable: tidy(function () {\n                return zerosLike(value).variable(trainable);\n              })\n            };\n          }\n          if (_this2.accumulatedSecondMoment[i] == null) {\n            _this2.accumulatedSecondMoment[i] = {\n              originalName: \"\".concat(name, \"/v\"),\n              variable: tidy(function () {\n                return zerosLike(value).variable(trainable);\n              })\n            };\n          }\n          var gradient = Array.isArray(variableGradients) ? variableGradients[i].tensor : variableGradients[name];\n          if (gradient == null) {\n            return;\n          }\n          var firstMoment = _this2.accumulatedFirstMoment[i].variable;\n          var secondMoment = _this2.accumulatedSecondMoment[i].variable;\n          var newFirstMoment = add(mul(firstMoment, _this2.beta1), mul(gradient, 1 - _this2.beta1));\n          var newSecondMoment = add(mul(secondMoment, _this2.beta2), mul(square(gradient), 1 - _this2.beta2));\n          var biasCorrectedFirstMoment = div(newFirstMoment, oneMinusAccBeta1);\n          var biasCorrectedSecondMoment = div(newSecondMoment, oneMinusAccBeta2);\n          firstMoment.assign(newFirstMoment);\n          secondMoment.assign(newSecondMoment);\n          var newValue = add(mul(div(biasCorrectedFirstMoment, add(sqrt(biasCorrectedSecondMoment), _this2.epsilon)), -_this2.learningRate), value);\n          value.assign(newValue);\n        });\n        _this2.accBeta1.assign(mul(_this2.accBeta1, _this2.beta1));\n        _this2.accBeta2.assign(mul(_this2.accBeta2, _this2.beta2));\n      });\n      this.incrementIterations();\n    }\n  }, {\n    key: \"dispose\",\n    value: function dispose() {\n      this.accBeta1.dispose();\n      this.accBeta2.dispose();\n      if (this.accumulatedFirstMoment != null) {\n        _dispose(this.accumulatedFirstMoment.map(function (v) {\n          return v.variable;\n        }));\n      }\n      if (this.accumulatedSecondMoment != null) {\n        _dispose(this.accumulatedSecondMoment.map(function (v) {\n          return v.variable;\n        }));\n      }\n    }\n  }, {\n    key: \"getWeights\",\n    value: function () {\n      var _getWeights = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee() {\n        var variables;\n        return _regeneratorRuntime().wrap(function _callee$(_context) {\n          while (1) switch (_context.prev = _context.next) {\n            case 0:\n              // Order matters for Python compatibility.\n              variables = [].concat(_toConsumableArray(this.accumulatedFirstMoment), _toConsumableArray(this.accumulatedSecondMoment));\n              _context.next = 3;\n              return this.saveIterations();\n            case 3:\n              _context.t0 = _context.sent;\n              return _context.abrupt(\"return\", [_context.t0].concat(variables.map(function (v) {\n                return {\n                  name: v.originalName,\n                  tensor: v.variable\n                };\n              })));\n            case 5:\n            case \"end\":\n              return _context.stop();\n          }\n        }, _callee, this);\n      }));\n      function getWeights() {\n        return _getWeights.apply(this, arguments);\n      }\n      return getWeights;\n    }()\n  }, {\n    key: \"setWeights\",\n    value: function () {\n      var _setWeights = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee2(weightValues) {\n        var _this3 = this;\n        var variableCount, trainable;\n        return _regeneratorRuntime().wrap(function _callee2$(_context2) {\n          while (1) switch (_context2.prev = _context2.next) {\n            case 0:\n              _context2.next = 2;\n              return this.extractIterations(weightValues);\n            case 2:\n              weightValues = _context2.sent;\n              tidy(function () {\n                _this3.accBeta1.assign(pow(_this3.beta1, _this3.iterations_ + 1));\n                _this3.accBeta2.assign(pow(_this3.beta2, _this3.iterations_ + 1));\n              });\n              variableCount = weightValues.length / 2;\n              trainable = false;\n              this.accumulatedFirstMoment = weightValues.slice(0, variableCount).map(function (v) {\n                return {\n                  originalName: v.name,\n                  variable: v.tensor.variable(trainable)\n                };\n              });\n              this.accumulatedSecondMoment = weightValues.slice(variableCount, variableCount * 2).map(function (v) {\n                return {\n                  originalName: v.name,\n                  variable: v.tensor.variable(trainable)\n                };\n              });\n            case 8:\n            case \"end\":\n              return _context2.stop();\n          }\n        }, _callee2, this);\n      }));\n      function setWeights(_x) {\n        return _setWeights.apply(this, arguments);\n      }\n      return setWeights;\n    }()\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      return {\n        'learningRate': this.learningRate,\n        'beta1': this.beta1,\n        'beta2': this.beta2,\n        'epsilon': this.epsilon\n      };\n    }\n    /** @nocollapse */\n  }], [{\n    key: \"className\",\n    get: /** @nocollapse */\n    function get() {\n      // Name matters for Python compatibility.\n      // This is a getter instead of a property because when it's a property, it\n      // prevents the entire class from being tree-shaken.\n      return 'Adam';\n    }\n  }, {\n    key: \"fromConfig\",\n    value: function fromConfig(cls, config) {\n      return new cls(config['learningRate'], config['beta1'], config['beta2'], config['epsilon']);\n    }\n  }]);\n  return AdamOptimizer;\n}(Optimizer);","map":{"version":3,"names":["ENGINE","dispose","tidy","add","div","mul","pow","scalar","sqrt","square","sub","zerosLike","Optimizer","AdamOptimizer","_Optimizer","_inherits","_super","_createSuper","learningRate","beta1","beta2","_this","epsilon","arguments","length","undefined","_classCallCheck","call","accumulatedFirstMoment","accumulatedSecondMoment","accBeta1","variable","accBeta2","backend","_createClass","key","value","applyGradients","variableGradients","_this2","varNames","Array","isArray","map","v","name","Object","keys","oneMinusAccBeta1","oneMinusAccBeta2","forEach","i","registeredVariables","trainable","originalName","concat","gradient","tensor","firstMoment","secondMoment","newFirstMoment","newSecondMoment","biasCorrectedFirstMoment","biasCorrectedSecondMoment","assign","newValue","incrementIterations","_getWeights","_asyncToGenerator","_regeneratorRuntime","mark","_callee","variables","wrap","_callee$","_context","prev","next","_toConsumableArray","saveIterations","t0","sent","abrupt","stop","getWeights","apply","_setWeights","_callee2","weightValues","_this3","variableCount","_callee2$","_context2","extractIterations","iterations_","slice","setWeights","_x","getConfig","get","fromConfig","cls","config"],"sources":["C:\\Users\\vince\\OneDrive\\Documents\\GitHub\\tfjs-core\\src\\optimizers\\adam_optimizer.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from '../engine';\nimport {dispose, tidy} from '../globals';\nimport {add} from '../ops/add';\nimport {div} from '../ops/div';\nimport {mul} from '../ops/mul';\nimport {pow} from '../ops/pow';\nimport {scalar} from '../ops/scalar';\nimport {sqrt} from '../ops/sqrt';\nimport {square} from '../ops/square';\nimport {sub} from '../ops/sub';\nimport {zerosLike} from '../ops/zeros_like';\nimport {ConfigDict, Serializable, SerializableConstructor} from '../serialization';\nimport {Variable} from '../tensor';\nimport {NamedTensor, NamedVariableMap} from '../tensor_types';\n\nimport {Optimizer, OptimizerVariable} from './optimizer';\n\nexport class AdamOptimizer extends Optimizer {\n  /** @nocollapse */\n  static get className() {\n    // Name matters for Python compatibility.\n    // This is a getter instead of a property because when it's a property, it\n    // prevents the entire class from being tree-shaken.\n    return 'Adam';\n  }\n  private accBeta1: Variable;\n  private accBeta2: Variable;\n\n  private accumulatedFirstMoment: OptimizerVariable[] = [];\n  private accumulatedSecondMoment: OptimizerVariable[] = [];\n\n  constructor(\n      protected learningRate: number, protected beta1: number,\n      protected beta2: number, protected epsilon: number = null) {\n    super();\n    tidy(() => {\n      // accB* will be updated by batch.\n      this.accBeta1 = scalar(beta1).variable();\n      this.accBeta2 = scalar(beta2).variable();\n    });\n\n    if (epsilon == null) {\n      this.epsilon = ENGINE.backend.epsilon();\n    }\n  }\n\n  applyGradients(variableGradients: NamedVariableMap|NamedTensor[]) {\n    const varNames = Array.isArray(variableGradients) ?\n        variableGradients.map(v => v.name) :\n        Object.keys(variableGradients);\n    tidy(() => {\n      const oneMinusAccBeta1 = sub(1, this.accBeta1);\n      const oneMinusAccBeta2 = sub(1, this.accBeta2);\n\n      varNames.forEach((name, i) => {\n        const value = ENGINE.registeredVariables[name];\n        const trainable = false;\n        if (this.accumulatedFirstMoment[i] == null) {\n          this.accumulatedFirstMoment[i] = {\n            originalName: `${name}/m`,\n            variable: tidy(() => zerosLike(value).variable(trainable))\n          };\n        }\n        if (this.accumulatedSecondMoment[i] == null) {\n          this.accumulatedSecondMoment[i] = {\n            originalName: `${name}/v`,\n            variable: tidy(() => zerosLike(value).variable(trainable))\n          };\n        }\n\n        const gradient = Array.isArray(variableGradients) ?\n            variableGradients[i].tensor :\n            variableGradients[name];\n        if (gradient == null) {\n          return;\n        }\n\n        const firstMoment = this.accumulatedFirstMoment[i].variable;\n        const secondMoment = this.accumulatedSecondMoment[i].variable;\n\n        const newFirstMoment =\n            add(mul(firstMoment, this.beta1), mul(gradient, 1 - this.beta1));\n        const newSecondMoment =\n            add(mul(secondMoment, this.beta2),\n                mul(square(gradient), 1 - this.beta2));\n\n        const biasCorrectedFirstMoment = div(newFirstMoment, oneMinusAccBeta1);\n        const biasCorrectedSecondMoment =\n            div(newSecondMoment, oneMinusAccBeta2);\n\n        firstMoment.assign(newFirstMoment);\n        secondMoment.assign(newSecondMoment);\n\n        const newValue =\n            add(mul(div(biasCorrectedFirstMoment,\n                        add(sqrt(biasCorrectedSecondMoment), this.epsilon)),\n                    -this.learningRate),\n                value);\n        value.assign(newValue);\n      });\n\n      this.accBeta1.assign(mul(this.accBeta1, this.beta1));\n      this.accBeta2.assign(mul(this.accBeta2, this.beta2));\n    });\n    this.incrementIterations();\n  }\n\n  override dispose(): void {\n    this.accBeta1.dispose();\n    this.accBeta2.dispose();\n\n    if (this.accumulatedFirstMoment != null) {\n      dispose(this.accumulatedFirstMoment.map(v => v.variable));\n    }\n    if (this.accumulatedSecondMoment != null) {\n      dispose(this.accumulatedSecondMoment.map(v => v.variable));\n    }\n  }\n\n  override async getWeights(): Promise<NamedTensor[]> {\n    // Order matters for Python compatibility.\n    const variables: OptimizerVariable[] =\n        [...this.accumulatedFirstMoment, ...this.accumulatedSecondMoment];\n    return [await this.saveIterations()].concat(\n        variables.map(v => ({name: v.originalName, tensor: v.variable})));\n  }\n\n  override async setWeights(weightValues: NamedTensor[]): Promise<void> {\n    weightValues = await this.extractIterations(weightValues);\n    tidy(() => {\n      this.accBeta1.assign(pow(this.beta1, this.iterations_ + 1));\n      this.accBeta2.assign(pow(this.beta2, this.iterations_ + 1));\n    });\n\n    const variableCount = weightValues.length / 2;\n    const trainable = false;\n    this.accumulatedFirstMoment =\n        weightValues.slice(0, variableCount).map(v => ({\n                                                   originalName: v.name,\n                                                   variable: v.tensor.variable(\n                                                       trainable)\n                                                 }));\n    this.accumulatedSecondMoment =\n        weightValues.slice(variableCount, variableCount * 2)\n            .map(v => ({\n                   originalName: v.name,\n                   variable: v.tensor.variable(trainable)\n                 }));\n  }\n\n  getConfig(): ConfigDict {\n    return {\n      'learningRate': this.learningRate,\n      'beta1': this.beta1,\n      'beta2': this.beta2,\n      'epsilon': this.epsilon,\n    };\n  }\n\n  /** @nocollapse */\n  static override fromConfig<T extends Serializable>(\n      cls: SerializableConstructor<T>, config: ConfigDict): T {\n    return new cls(\n        config['learningRate'], config['beta1'], config['beta2'],\n        config['epsilon']);\n  }\n}\n"],"mappings":";;;;;;;AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQA,MAAM,QAAO,WAAW;AAChC,SAAQC,OAAO,IAAPA,QAAO,EAAEC,IAAI,QAAO,YAAY;AACxC,SAAQC,GAAG,QAAO,YAAY;AAC9B,SAAQC,GAAG,QAAO,YAAY;AAC9B,SAAQC,GAAG,QAAO,YAAY;AAC9B,SAAQC,GAAG,QAAO,YAAY;AAC9B,SAAQC,MAAM,QAAO,eAAe;AACpC,SAAQC,IAAI,QAAO,aAAa;AAChC,SAAQC,MAAM,QAAO,eAAe;AACpC,SAAQC,GAAG,QAAO,YAAY;AAC9B,SAAQC,SAAS,QAAO,mBAAmB;AAK3C,SAAQC,SAAS,QAA0B,aAAa;AAExD,WAAaC,aAAc,0BAAAC,UAAA;EAAAC,SAAA,CAAAF,aAAA,EAAAC,UAAA;EAAA,IAAAE,MAAA,GAAAC,YAAA,CAAAJ,aAAA;EAczB,SAAAA,cACcK,YAAoB,EAAYC,KAAa,EAC7CC,KAAa,EAAkC;IAAA,IAAAC,KAAA;IAAA,IAAtBC,OAAA,GAAAC,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAkB,IAAI;IAAAG,eAAA,OAAAb,aAAA;IAC3DQ,KAAA,GAAAL,MAAA,CAAAW,IAAA;IAFYN,KAAA,CAAAH,YAAY,GAAZA,YAAY;IAAoBG,KAAA,CAAAF,KAAK,GAALA,KAAK;IACrCE,KAAA,CAAAD,KAAK,GAALA,KAAK;IAAoBC,KAAA,CAAAC,OAAO,GAAPA,OAAO;IALtCD,KAAA,CAAAO,sBAAsB,GAAwB,EAAE;IAChDP,KAAA,CAAAQ,uBAAuB,GAAwB,EAAE;IAMvD3B,IAAI,CAAC,YAAK;MACR;MACAmB,KAAA,CAAKS,QAAQ,GAAGvB,MAAM,CAACY,KAAK,CAAC,CAACY,QAAQ,EAAE;MACxCV,KAAA,CAAKW,QAAQ,GAAGzB,MAAM,CAACa,KAAK,CAAC,CAACW,QAAQ,EAAE;IAC1C,CAAC,CAAC;IAEF,IAAIT,OAAO,IAAI,IAAI,EAAE;MACnBD,KAAA,CAAKC,OAAO,GAAGtB,MAAM,CAACiC,OAAO,CAACX,OAAO,EAAE;;IACxC,OAAAD,KAAA;EACH;EAACa,YAAA,CAAArB,aAAA;IAAAsB,GAAA;IAAAC,KAAA,EAED,SAAAC,eAAeC,iBAAiD;MAAA,IAAAC,MAAA;MAC9D,IAAMC,QAAQ,GAAGC,KAAK,CAACC,OAAO,CAACJ,iBAAiB,CAAC,GAC7CA,iBAAiB,CAACK,GAAG,CAAC,UAAAC,CAAC;QAAA,OAAIA,CAAC,CAACC,IAAI;MAAA,EAAC,GAClCC,MAAM,CAACC,IAAI,CAACT,iBAAiB,CAAC;MAClCpC,IAAI,CAAC,YAAK;QACR,IAAM8C,gBAAgB,GAAGtC,GAAG,CAAC,CAAC,EAAE6B,MAAI,CAACT,QAAQ,CAAC;QAC9C,IAAMmB,gBAAgB,GAAGvC,GAAG,CAAC,CAAC,EAAE6B,MAAI,CAACP,QAAQ,CAAC;QAE9CQ,QAAQ,CAACU,OAAO,CAAC,UAACL,IAAI,EAAEM,CAAC,EAAI;UAC3B,IAAMf,KAAK,GAAGpC,MAAM,CAACoD,mBAAmB,CAACP,IAAI,CAAC;UAC9C,IAAMQ,SAAS,GAAG,KAAK;UACvB,IAAId,MAAI,CAACX,sBAAsB,CAACuB,CAAC,CAAC,IAAI,IAAI,EAAE;YAC1CZ,MAAI,CAACX,sBAAsB,CAACuB,CAAC,CAAC,GAAG;cAC/BG,YAAY,KAAAC,MAAA,CAAKV,IAAI,OAAI;cACzBd,QAAQ,EAAE7B,IAAI,CAAC;gBAAA,OAAMS,SAAS,CAACyB,KAAK,CAAC,CAACL,QAAQ,CAACsB,SAAS,CAAC;cAAA;aAC1D;;UAEH,IAAId,MAAI,CAACV,uBAAuB,CAACsB,CAAC,CAAC,IAAI,IAAI,EAAE;YAC3CZ,MAAI,CAACV,uBAAuB,CAACsB,CAAC,CAAC,GAAG;cAChCG,YAAY,KAAAC,MAAA,CAAKV,IAAI,OAAI;cACzBd,QAAQ,EAAE7B,IAAI,CAAC;gBAAA,OAAMS,SAAS,CAACyB,KAAK,CAAC,CAACL,QAAQ,CAACsB,SAAS,CAAC;cAAA;aAC1D;;UAGH,IAAMG,QAAQ,GAAGf,KAAK,CAACC,OAAO,CAACJ,iBAAiB,CAAC,GAC7CA,iBAAiB,CAACa,CAAC,CAAC,CAACM,MAAM,GAC3BnB,iBAAiB,CAACO,IAAI,CAAC;UAC3B,IAAIW,QAAQ,IAAI,IAAI,EAAE;YACpB;;UAGF,IAAME,WAAW,GAAGnB,MAAI,CAACX,sBAAsB,CAACuB,CAAC,CAAC,CAACpB,QAAQ;UAC3D,IAAM4B,YAAY,GAAGpB,MAAI,CAACV,uBAAuB,CAACsB,CAAC,CAAC,CAACpB,QAAQ;UAE7D,IAAM6B,cAAc,GAChBzD,GAAG,CAACE,GAAG,CAACqD,WAAW,EAAEnB,MAAI,CAACpB,KAAK,CAAC,EAAEd,GAAG,CAACmD,QAAQ,EAAE,CAAC,GAAGjB,MAAI,CAACpB,KAAK,CAAC,CAAC;UACpE,IAAM0C,eAAe,GACjB1D,GAAG,CAACE,GAAG,CAACsD,YAAY,EAAEpB,MAAI,CAACnB,KAAK,CAAC,EAC7Bf,GAAG,CAACI,MAAM,CAAC+C,QAAQ,CAAC,EAAE,CAAC,GAAGjB,MAAI,CAACnB,KAAK,CAAC,CAAC;UAE9C,IAAM0C,wBAAwB,GAAG1D,GAAG,CAACwD,cAAc,EAAEZ,gBAAgB,CAAC;UACtE,IAAMe,yBAAyB,GAC3B3D,GAAG,CAACyD,eAAe,EAAEZ,gBAAgB,CAAC;UAE1CS,WAAW,CAACM,MAAM,CAACJ,cAAc,CAAC;UAClCD,YAAY,CAACK,MAAM,CAACH,eAAe,CAAC;UAEpC,IAAMI,QAAQ,GACV9D,GAAG,CAACE,GAAG,CAACD,GAAG,CAAC0D,wBAAwB,EACxB3D,GAAG,CAACK,IAAI,CAACuD,yBAAyB,CAAC,EAAExB,MAAI,CAACjB,OAAO,CAAC,CAAC,EACvD,CAACiB,MAAI,CAACrB,YAAY,CAAC,EACvBkB,KAAK,CAAC;UACdA,KAAK,CAAC4B,MAAM,CAACC,QAAQ,CAAC;QACxB,CAAC,CAAC;QAEF1B,MAAI,CAACT,QAAQ,CAACkC,MAAM,CAAC3D,GAAG,CAACkC,MAAI,CAACT,QAAQ,EAAES,MAAI,CAACpB,KAAK,CAAC,CAAC;QACpDoB,MAAI,CAACP,QAAQ,CAACgC,MAAM,CAAC3D,GAAG,CAACkC,MAAI,CAACP,QAAQ,EAAEO,MAAI,CAACnB,KAAK,CAAC,CAAC;MACtD,CAAC,CAAC;MACF,IAAI,CAAC8C,mBAAmB,EAAE;IAC5B;EAAC;IAAA/B,GAAA;IAAAC,KAAA,EAEQ,SAAAnC,QAAA,EAAO;MACd,IAAI,CAAC6B,QAAQ,CAAC7B,OAAO,EAAE;MACvB,IAAI,CAAC+B,QAAQ,CAAC/B,OAAO,EAAE;MAEvB,IAAI,IAAI,CAAC2B,sBAAsB,IAAI,IAAI,EAAE;QACvC3B,QAAO,CAAC,IAAI,CAAC2B,sBAAsB,CAACe,GAAG,CAAC,UAAAC,CAAC;UAAA,OAAIA,CAAC,CAACb,QAAQ;QAAA,EAAC,CAAC;;MAE3D,IAAI,IAAI,CAACF,uBAAuB,IAAI,IAAI,EAAE;QACxC5B,QAAO,CAAC,IAAI,CAAC4B,uBAAuB,CAACc,GAAG,CAAC,UAAAC,CAAC;UAAA,OAAIA,CAAC,CAACb,QAAQ;QAAA,EAAC,CAAC;;IAE9D;EAAC;IAAAI,GAAA;IAAAC,KAAA;MAAA,IAAA+B,WAAA,GAAAC,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CAEQ,SAAAC,QAAA;QAAA,IAAAC,SAAA;QAAA,OAAAH,mBAAA,GAAAI,IAAA,UAAAC,SAAAC,QAAA;UAAA,kBAAAA,QAAA,CAAAC,IAAA,GAAAD,QAAA,CAAAE,IAAA;YAAA;cACP;cACML,SAAS,MAAAjB,MAAA,CAAAuB,kBAAA,CACP,IAAI,CAAClD,sBAAsB,GAAAkD,kBAAA,CAAK,IAAI,CAACjD,uBAAuB;cAAA8C,QAAA,CAAAE,IAAA;cAAA,OACtD,IAAI,CAACE,cAAc,EAAE;YAAA;cAAAJ,QAAA,CAAAK,EAAA,GAAAL,QAAA,CAAAM,IAAA;cAAA,OAAAN,QAAA,CAAAO,MAAA,YAAAP,QAAA,CAAAK,EAAA,EAAEzB,MAAM,CACvCiB,SAAS,CAAC7B,GAAG,CAAC,UAAAC,CAAC;gBAAA,OAAK;kBAACC,IAAI,EAAED,CAAC,CAACU,YAAY;kBAAEG,MAAM,EAAEb,CAAC,CAACb;gBAAQ,CAAC;cAAA,CAAC,CAAC;YAAA;YAAA;cAAA,OAAA4C,QAAA,CAAAQ,IAAA;UAAA;QAAA,GAAAZ,OAAA;MAAA,CACrE;MAAA,SAAAa,WAAA;QAAA,OAAAjB,WAAA,CAAAkB,KAAA,OAAA9D,SAAA;MAAA;MAAA,OAAA6D,UAAA;IAAA;EAAA;IAAAjD,GAAA;IAAAC,KAAA;MAAA,IAAAkD,WAAA,GAAAlB,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CAEQ,SAAAiB,SAAiBC,YAA2B;QAAA,IAAAC,MAAA;QAAA,IAAAC,aAAA,EAAArC,SAAA;QAAA,OAAAgB,mBAAA,GAAAI,IAAA,UAAAkB,UAAAC,SAAA;UAAA,kBAAAA,SAAA,CAAAhB,IAAA,GAAAgB,SAAA,CAAAf,IAAA;YAAA;cAAAe,SAAA,CAAAf,IAAA;cAAA,OAC9B,IAAI,CAACgB,iBAAiB,CAACL,YAAY,CAAC;YAAA;cAAzDA,YAAY,GAAAI,SAAA,CAAAX,IAAA;cACZ/E,IAAI,CAAC,YAAK;gBACRuF,MAAI,CAAC3D,QAAQ,CAACkC,MAAM,CAAC1D,GAAG,CAACmF,MAAI,CAACtE,KAAK,EAAEsE,MAAI,CAACK,WAAW,GAAG,CAAC,CAAC,CAAC;gBAC3DL,MAAI,CAACzD,QAAQ,CAACgC,MAAM,CAAC1D,GAAG,CAACmF,MAAI,CAACrE,KAAK,EAAEqE,MAAI,CAACK,WAAW,GAAG,CAAC,CAAC,CAAC;cAC7D,CAAC,CAAC;cAEIJ,aAAa,GAAGF,YAAY,CAAChE,MAAM,GAAG,CAAC;cACvC6B,SAAS,GAAG,KAAK;cACvB,IAAI,CAACzB,sBAAsB,GACvB4D,YAAY,CAACO,KAAK,CAAC,CAAC,EAAEL,aAAa,CAAC,CAAC/C,GAAG,CAAC,UAAAC,CAAC;gBAAA,OAAK;kBACJU,YAAY,EAAEV,CAAC,CAACC,IAAI;kBACpBd,QAAQ,EAAEa,CAAC,CAACa,MAAM,CAAC1B,QAAQ,CACvBsB,SAAS;iBACd;cAAA,CAAC,CAAC;cAChD,IAAI,CAACxB,uBAAuB,GACxB2D,YAAY,CAACO,KAAK,CAACL,aAAa,EAAEA,aAAa,GAAG,CAAC,CAAC,CAC/C/C,GAAG,CAAC,UAAAC,CAAC;gBAAA,OAAK;kBACJU,YAAY,EAAEV,CAAC,CAACC,IAAI;kBACpBd,QAAQ,EAAEa,CAAC,CAACa,MAAM,CAAC1B,QAAQ,CAACsB,SAAS;iBACtC;cAAA,CAAC,CAAC;YAAC;YAAA;cAAA,OAAAuC,SAAA,CAAAT,IAAA;UAAA;QAAA,GAAAI,QAAA;MAAA,CAClB;MAAA,SAAAS,WAAAC,EAAA;QAAA,OAAAX,WAAA,CAAAD,KAAA,OAAA9D,SAAA;MAAA;MAAA,OAAAyE,UAAA;IAAA;EAAA;IAAA7D,GAAA;IAAAC,KAAA,EAED,SAAA8D,UAAA,EAAS;MACP,OAAO;QACL,cAAc,EAAE,IAAI,CAAChF,YAAY;QACjC,OAAO,EAAE,IAAI,CAACC,KAAK;QACnB,OAAO,EAAE,IAAI,CAACC,KAAK;QACnB,SAAS,EAAE,IAAI,CAACE;OACjB;IACH;IAEA;EAAA;IAAAa,GAAA;IAAAgE,GAAA,EA7IA;IACA,SAAAA,IAAA,EAAoB;MAClB;MACA;MACA;MACA,OAAO,MAAM;IACf;EAAC;IAAAhE,GAAA;IAAAC,KAAA,EAwID,SAAAgE,WACIC,GAA+B,EAAEC,MAAkB;MACrD,OAAO,IAAID,GAAG,CACVC,MAAM,CAAC,cAAc,CAAC,EAAEA,MAAM,CAAC,OAAO,CAAC,EAAEA,MAAM,CAAC,OAAO,CAAC,EACxDA,MAAM,CAAC,SAAS,CAAC,CAAC;IACxB;EAAC;EAAA,OAAAzF,aAAA;AAAA,EApJgCD,SAAS"},"metadata":{},"sourceType":"module","externalDependencies":[]}