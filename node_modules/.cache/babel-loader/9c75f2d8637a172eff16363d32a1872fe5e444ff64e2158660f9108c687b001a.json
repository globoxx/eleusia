{"ast":null,"code":"import _regeneratorRuntime from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/regeneratorRuntime.js\";\nimport _asyncToGenerator from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/asyncToGenerator.js\";\nimport _classCallCheck from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/classCallCheck.js\";\nimport _createClass from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/createClass.js\";\nimport _inherits from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/inherits.js\";\nimport _createSuper from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/createSuper.js\";\n/**\r\n * @license\r\n * Copyright 2018 Google LLC. All Rights Reserved.\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n * http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n * =============================================================================\r\n */\nimport { dispose as _dispose } from '../globals';\nimport { variableGrads } from '../gradients';\nimport { scalar } from '../ops/ops';\nimport { Serializable } from '../serialization';\n/** @doc {heading: 'Training', subheading: 'Classes', namespace: 'train'} */\nexport var Optimizer = /*#__PURE__*/function (_Serializable) {\n  _inherits(Optimizer, _Serializable);\n  var _super = _createSuper(Optimizer);\n  function Optimizer() {\n    _classCallCheck(this, Optimizer);\n    return _super.apply(this, arguments);\n  }\n  _createClass(Optimizer, [{\n    key: \"minimize\",\n    value:\n    /**\r\n     * Executes `f()` and minimizes the scalar output of `f()` by computing\r\n     * gradients of y with respect to the list of trainable variables provided by\r\n     * `varList`. If no list is provided, it defaults to all trainable variables.\r\n     *\r\n     * @param f The function to execute and whose output to minimize.\r\n     * @param returnCost Whether to return the scalar cost value produced by\r\n     * executing `f()`.\r\n     * @param varList An optional list of variables to update. If specified, only\r\n     * the trainable variables in varList will be updated by minimize. Defaults to\r\n     * all trainable variables.\r\n     *\r\n     * @doc {heading: 'Training', subheading: 'Optimizers'}\r\n     */\n    function minimize(f) {\n      var returnCost = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : false;\n      var varList = arguments.length > 2 ? arguments[2] : undefined;\n      var _this$computeGradient = this.computeGradients(f, varList),\n        value = _this$computeGradient.value,\n        grads = _this$computeGradient.grads;\n      if (varList != null) {\n        var gradArray = varList.map(function (v) {\n          return {\n            name: v.name,\n            tensor: grads[v.name]\n          };\n        });\n        this.applyGradients(gradArray);\n      } else {\n        this.applyGradients(grads);\n      }\n      // Dispose gradients.\n      _dispose(grads);\n      if (returnCost) {\n        return value;\n      } else {\n        value.dispose();\n        return null;\n      }\n    }\n    /**\r\n     * The number of iterations that this optimizer instance has been invoked for.\r\n     */\n  }, {\n    key: \"iterations\",\n    get: function get() {\n      if (this.iterations_ == null) {\n        this.iterations_ = 0;\n      }\n      return this.iterations_;\n    }\n  }, {\n    key: \"incrementIterations\",\n    value: function incrementIterations() {\n      this.iterations_ = this.iterations + 1;\n    }\n    /**\r\n     * Executes f() and computes the gradient of the scalar output of f() with\r\n     * respect to the list of trainable variables provided by `varList`. If no\r\n     * list is provided, it defaults to all trainable variables.\r\n     *\r\n     * @param f The function to execute and whose output to use for computing\r\n     * gradients with respect to variables.\r\n     * @param varList An optional list of variables to compute gradients with\r\n     * respect to. If specified, only the trainable variables in varList will have\r\n     * gradients computed with respect to. Defaults to all trainable variables.\r\n     *\r\n     * @doc {heading: 'Training', subheading: 'Optimizers'}\r\n     */\n  }, {\n    key: \"computeGradients\",\n    value: function computeGradients(f, varList) {\n      return variableGrads(f, varList);\n    }\n    /**\r\n     * Dispose the variables (if any) owned by this optimizer instance.\r\n     */\n  }, {\n    key: \"dispose\",\n    value: function dispose() {\n      if (this.iterations_ != null) {\n        _dispose(this.iterations_);\n      }\n    }\n  }, {\n    key: \"saveIterations\",\n    value: function () {\n      var _saveIterations = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee() {\n        return _regeneratorRuntime().wrap(function _callee$(_context) {\n          while (1) switch (_context.prev = _context.next) {\n            case 0:\n              if (this.iterations_ == null) {\n                this.iterations_ = 0;\n              }\n              return _context.abrupt(\"return\", {\n                name: 'iter',\n                // TODO(cais): Use 'int64' type when available.\n                tensor: scalar(this.iterations_, 'int32')\n              });\n            case 2:\n            case \"end\":\n              return _context.stop();\n          }\n        }, _callee, this);\n      }));\n      function saveIterations() {\n        return _saveIterations.apply(this, arguments);\n      }\n      return saveIterations;\n    }()\n  }, {\n    key: \"getWeights\",\n    value: function () {\n      var _getWeights = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee2() {\n        return _regeneratorRuntime().wrap(function _callee2$(_context2) {\n          while (1) switch (_context2.prev = _context2.next) {\n            case 0:\n              throw new Error('getWeights() is not implemented for this optimizer yet.');\n            case 1:\n            case \"end\":\n              return _context2.stop();\n          }\n        }, _callee2);\n      }));\n      function getWeights() {\n        return _getWeights.apply(this, arguments);\n      }\n      return getWeights;\n    }()\n  }, {\n    key: \"setWeights\",\n    value: function () {\n      var _setWeights = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee3(weightValues) {\n        return _regeneratorRuntime().wrap(function _callee3$(_context3) {\n          while (1) switch (_context3.prev = _context3.next) {\n            case 0:\n              throw new Error(\"setWeights() is not implemented for this optimizer class \" + \"\".concat(this.getClassName()));\n            case 1:\n            case \"end\":\n              return _context3.stop();\n          }\n        }, _callee3, this);\n      }));\n      function setWeights(_x) {\n        return _setWeights.apply(this, arguments);\n      }\n      return setWeights;\n    }()\n    /**\r\n     * Extract the first element of the weight values and set it\r\n     * as the iterations counter variable of this instance of optimizer.\r\n     *\r\n     * @param weightValues\r\n     * @returns Weight values with the first element consumed and excluded.\r\n     */\n  }, {\n    key: \"extractIterations\",\n    value: function () {\n      var _extractIterations = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee4(weightValues) {\n        return _regeneratorRuntime().wrap(function _callee4$(_context4) {\n          while (1) switch (_context4.prev = _context4.next) {\n            case 0:\n              _context4.next = 2;\n              return weightValues[0].tensor.data();\n            case 2:\n              this.iterations_ = _context4.sent[0];\n              return _context4.abrupt(\"return\", weightValues.slice(1));\n            case 4:\n            case \"end\":\n              return _context4.stop();\n          }\n        }, _callee4, this);\n      }));\n      function extractIterations(_x2) {\n        return _extractIterations.apply(this, arguments);\n      }\n      return extractIterations;\n    }()\n  }]);\n  return Optimizer;\n}(Serializable);\nObject.defineProperty(Optimizer, Symbol.hasInstance, {\n  value: function value(instance) {\n    return instance.minimize != null && instance.computeGradients != null && instance.applyGradients != null;\n  }\n});","map":{"version":3,"names":["dispose","variableGrads","scalar","Serializable","Optimizer","_Serializable","_inherits","_super","_createSuper","_classCallCheck","apply","arguments","_createClass","key","value","minimize","f","returnCost","length","undefined","varList","_this$computeGradient","computeGradients","grads","gradArray","map","v","name","tensor","applyGradients","get","iterations_","incrementIterations","iterations","_saveIterations","_asyncToGenerator","_regeneratorRuntime","mark","_callee","wrap","_callee$","_context","prev","next","abrupt","stop","saveIterations","_getWeights","_callee2","_callee2$","_context2","Error","getWeights","_setWeights","_callee3","weightValues","_callee3$","_context3","concat","getClassName","setWeights","_x","_extractIterations","_callee4","_callee4$","_context4","data","sent","slice","extractIterations","_x2","Object","defineProperty","Symbol","hasInstance","instance"],"sources":["C:\\Users\\vince\\OneDrive\\Documents\\GitHub\\tfjs-core\\src\\optimizers\\optimizer.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {dispose} from '../globals';\nimport {variableGrads} from '../gradients';\nimport {scalar} from '../ops/ops';\nimport {Serializable} from '../serialization';\nimport {Scalar, Variable} from '../tensor';\nimport {NamedTensor, NamedTensorMap} from '../tensor_types';\n\n/**\n * A variable that belongs to an optimizer.\n *\n * The `originalName` field is required for keeping track of the canonical\n * name of the variable, which is usually the name of the model weight that\n * the variable is related to plus a suffix, e.g., 'dense1/kernel/momentum'.\n * The name of the `Variable` object itself cannot be used directly due to\n * possible deduplication: Every `Variable` must have a unique name but more\n * than one optimizer objects of the same type may be created for the same model\n * or the same `Variable`.\n */\nexport interface OptimizerVariable {\n  originalName: string;\n  variable: Variable;\n}\n\n/** @doc {heading: 'Training', subheading: 'Classes', namespace: 'train'} */\nexport abstract class Optimizer extends Serializable {\n  protected iterations_: number;\n\n  /**\n   * Executes `f()` and minimizes the scalar output of `f()` by computing\n   * gradients of y with respect to the list of trainable variables provided by\n   * `varList`. If no list is provided, it defaults to all trainable variables.\n   *\n   * @param f The function to execute and whose output to minimize.\n   * @param returnCost Whether to return the scalar cost value produced by\n   * executing `f()`.\n   * @param varList An optional list of variables to update. If specified, only\n   * the trainable variables in varList will be updated by minimize. Defaults to\n   * all trainable variables.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers'}\n   */\n  minimize(f: () => Scalar, returnCost = false, varList?: Variable[]): Scalar\n      |null {\n    const {value, grads} = this.computeGradients(f, varList);\n\n    if (varList != null) {\n      const gradArray: NamedTensor[] =\n          varList.map(v => ({name: v.name, tensor: grads[v.name]}));\n      this.applyGradients(gradArray);\n    } else {\n      this.applyGradients(grads);\n    }\n\n    // Dispose gradients.\n    dispose(grads);\n\n    if (returnCost) {\n      return value;\n    } else {\n      value.dispose();\n      return null;\n    }\n  }\n\n  /**\n   * The number of iterations that this optimizer instance has been invoked for.\n   */\n  get iterations(): number {\n    if (this.iterations_ == null) {\n      this.iterations_ = 0;\n    }\n    return this.iterations_;\n  }\n\n  protected incrementIterations() {\n    this.iterations_ = this.iterations + 1;\n  }\n\n  /**\n   * Executes f() and computes the gradient of the scalar output of f() with\n   * respect to the list of trainable variables provided by `varList`. If no\n   * list is provided, it defaults to all trainable variables.\n   *\n   * @param f The function to execute and whose output to use for computing\n   * gradients with respect to variables.\n   * @param varList An optional list of variables to compute gradients with\n   * respect to. If specified, only the trainable variables in varList will have\n   * gradients computed with respect to. Defaults to all trainable variables.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers'}\n   */\n  computeGradients(f: () => Scalar, varList?: Variable[]):\n      {value: Scalar, grads: NamedTensorMap} {\n    return variableGrads(f, varList);\n  }\n\n  /**\n   * Updates variables by using the computed gradients.\n   *\n   * @param variableGradients A mapping of variable name to its gradient value.\n   *\n   * @doc {heading: 'Training', subheading: 'Optimizers'}\n   */\n  abstract applyGradients(variableGradients: NamedTensorMap|\n                          NamedTensor[]): void;\n\n  /**\n   * Dispose the variables (if any) owned by this optimizer instance.\n   */\n  dispose(): void {\n    if (this.iterations_ != null) {\n      dispose(this.iterations_);\n    }\n  }\n\n  async saveIterations(): Promise<NamedTensor> {\n    if (this.iterations_ == null) {\n      this.iterations_ = 0;\n    }\n    return {\n      name: 'iter',  // Named for Python compatibility.\n      // TODO(cais): Use 'int64' type when available.\n      tensor: scalar(this.iterations_, 'int32')\n    };\n  }\n\n  async getWeights(): Promise<NamedTensor[]> {\n    throw new Error('getWeights() is not implemented for this optimizer yet.');\n  }\n\n  async setWeights(weightValues: NamedTensor[]): Promise<void> {\n    throw new Error(\n        `setWeights() is not implemented for this optimizer class ` +\n        `${this.getClassName()}`);\n  }\n\n  /**\n   * Extract the first element of the weight values and set it\n   * as the iterations counter variable of this instance of optimizer.\n   *\n   * @param weightValues\n   * @returns Weight values with the first element consumed and excluded.\n   */\n  protected async extractIterations(weightValues: NamedTensor[]):\n      Promise<NamedTensor[]> {\n    this.iterations_ = (await weightValues[0].tensor.data())[0];\n    return weightValues.slice(1);\n  }\n}\n\nObject.defineProperty(Optimizer, Symbol.hasInstance, {\n  value: (instance: Optimizer) => {\n    return instance.minimize != null && instance.computeGradients != null &&\n        instance.applyGradients != null;\n  }\n});\n"],"mappings":";;;;;;AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQA,OAAO,IAAPA,QAAO,QAAO,YAAY;AAClC,SAAQC,aAAa,QAAO,cAAc;AAC1C,SAAQC,MAAM,QAAO,YAAY;AACjC,SAAQC,YAAY,QAAO,kBAAkB;AAoB7C;AACA,WAAsBC,SAAU,0BAAAC,aAAA;EAAAC,SAAA,CAAAF,SAAA,EAAAC,aAAA;EAAA,IAAAE,MAAA,GAAAC,YAAA,CAAAJ,SAAA;EAAA,SAAAA,UAAA;IAAAK,eAAA,OAAAL,SAAA;IAAA,OAAAG,MAAA,CAAAG,KAAA,OAAAC,SAAA;EAAA;EAAAC,YAAA,CAAAR,SAAA;IAAAS,GAAA;IAAAC,KAAA;IAG9B;;;;;;;;;;;;;;IAcA,SAAAC,SAASC,CAAe,EAA0C;MAAA,IAAxCC,UAAU,GAAAN,SAAA,CAAAO,MAAA,QAAAP,SAAA,QAAAQ,SAAA,GAAAR,SAAA,MAAG,KAAK;MAAA,IAAES,OAAoB,GAAAT,SAAA,CAAAO,MAAA,OAAAP,SAAA,MAAAQ,SAAA;MAEhE,IAAAE,qBAAA,GAAuB,IAAI,CAACC,gBAAgB,CAACN,CAAC,EAAEI,OAAO,CAAC;QAAjDN,KAAK,GAAAO,qBAAA,CAALP,KAAK;QAAES,KAAK,GAAAF,qBAAA,CAALE,KAAK;MAEnB,IAAIH,OAAO,IAAI,IAAI,EAAE;QACnB,IAAMI,SAAS,GACXJ,OAAO,CAACK,GAAG,CAAC,UAAAC,CAAC;UAAA,OAAK;YAACC,IAAI,EAAED,CAAC,CAACC,IAAI;YAAEC,MAAM,EAAEL,KAAK,CAACG,CAAC,CAACC,IAAI;UAAC,CAAC;QAAA,CAAC,CAAC;QAC7D,IAAI,CAACE,cAAc,CAACL,SAAS,CAAC;OAC/B,MAAM;QACL,IAAI,CAACK,cAAc,CAACN,KAAK,CAAC;;MAG5B;MACAvB,QAAO,CAACuB,KAAK,CAAC;MAEd,IAAIN,UAAU,EAAE;QACd,OAAOH,KAAK;OACb,MAAM;QACLA,KAAK,CAACd,OAAO,EAAE;QACf,OAAO,IAAI;;IAEf;IAEA;;;EAAA;IAAAa,GAAA;IAAAiB,GAAA,EAGA,SAAAA,IAAA,EAAc;MACZ,IAAI,IAAI,CAACC,WAAW,IAAI,IAAI,EAAE;QAC5B,IAAI,CAACA,WAAW,GAAG,CAAC;;MAEtB,OAAO,IAAI,CAACA,WAAW;IACzB;EAAC;IAAAlB,GAAA;IAAAC,KAAA,EAES,SAAAkB,oBAAA,EAAmB;MAC3B,IAAI,CAACD,WAAW,GAAG,IAAI,CAACE,UAAU,GAAG,CAAC;IACxC;IAEA;;;;;;;;;;;;;EAAA;IAAApB,GAAA;IAAAC,KAAA,EAaA,SAAAQ,iBAAiBN,CAAe,EAAEI,OAAoB;MAEpD,OAAOnB,aAAa,CAACe,CAAC,EAAEI,OAAO,CAAC;IAClC;IAYA;;;EAAA;IAAAP,GAAA;IAAAC,KAAA,EAGA,SAAAd,QAAA,EAAO;MACL,IAAI,IAAI,CAAC+B,WAAW,IAAI,IAAI,EAAE;QAC5B/B,QAAO,CAAC,IAAI,CAAC+B,WAAW,CAAC;;IAE7B;EAAC;IAAAlB,GAAA;IAAAC,KAAA;MAAA,IAAAoB,eAAA,GAAAC,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CAED,SAAAC,QAAA;QAAA,OAAAF,mBAAA,GAAAG,IAAA,UAAAC,SAAAC,QAAA;UAAA,kBAAAA,QAAA,CAAAC,IAAA,GAAAD,QAAA,CAAAE,IAAA;YAAA;cACE,IAAI,IAAI,CAACZ,WAAW,IAAI,IAAI,EAAE;gBAC5B,IAAI,CAACA,WAAW,GAAG,CAAC;;cACrB,OAAAU,QAAA,CAAAG,MAAA,WACM;gBACLjB,IAAI,EAAE,MAAM;gBACZ;gBACAC,MAAM,EAAE1B,MAAM,CAAC,IAAI,CAAC6B,WAAW,EAAE,OAAO;eACzC;YAAA;YAAA;cAAA,OAAAU,QAAA,CAAAI,IAAA;UAAA;QAAA,GAAAP,OAAA;MAAA,CACF;MAAA,SAAAQ,eAAA;QAAA,OAAAZ,eAAA,CAAAxB,KAAA,OAAAC,SAAA;MAAA;MAAA,OAAAmC,cAAA;IAAA;EAAA;IAAAjC,GAAA;IAAAC,KAAA;MAAA,IAAAiC,WAAA,GAAAZ,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CAED,SAAAW,SAAA;QAAA,OAAAZ,mBAAA,GAAAG,IAAA,UAAAU,UAAAC,SAAA;UAAA,kBAAAA,SAAA,CAAAR,IAAA,GAAAQ,SAAA,CAAAP,IAAA;YAAA;cAAA,MACQ,IAAIQ,KAAK,CAAC,yDAAyD,CAAC;YAAA;YAAA;cAAA,OAAAD,SAAA,CAAAL,IAAA;UAAA;QAAA,GAAAG,QAAA;MAAA,CAC3E;MAAA,SAAAI,WAAA;QAAA,OAAAL,WAAA,CAAArC,KAAA,OAAAC,SAAA;MAAA;MAAA,OAAAyC,UAAA;IAAA;EAAA;IAAAvC,GAAA;IAAAC,KAAA;MAAA,IAAAuC,WAAA,GAAAlB,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CAED,SAAAiB,SAAiBC,YAA2B;QAAA,OAAAnB,mBAAA,GAAAG,IAAA,UAAAiB,UAAAC,SAAA;UAAA,kBAAAA,SAAA,CAAAf,IAAA,GAAAe,SAAA,CAAAd,IAAA;YAAA;cAAA,MACpC,IAAIQ,KAAK,CACX,iEAAAO,MAAA,CACG,IAAI,CAACC,YAAY,EAAE,CAAE,CAAC;YAAA;YAAA;cAAA,OAAAF,SAAA,CAAAZ,IAAA;UAAA;QAAA,GAAAS,QAAA;MAAA,CAC9B;MAAA,SAAAM,WAAAC,EAAA;QAAA,OAAAR,WAAA,CAAA3C,KAAA,OAAAC,SAAA;MAAA;MAAA,OAAAiD,UAAA;IAAA;IAED;;;;;;;EAAA;IAAA/C,GAAA;IAAAC,KAAA;MAAA,IAAAgD,kBAAA,GAAA3B,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CAOU,SAAA0B,SAAwBR,YAA2B;QAAA,OAAAnB,mBAAA,GAAAG,IAAA,UAAAyB,UAAAC,SAAA;UAAA,kBAAAA,SAAA,CAAAvB,IAAA,GAAAuB,SAAA,CAAAtB,IAAA;YAAA;cAAAsB,SAAA,CAAAtB,IAAA;cAAA,OAEjCY,YAAY,CAAC,CAAC,CAAC,CAAC3B,MAAM,CAACsC,IAAI,EAAE;YAAA;cAAvD,IAAI,CAACnC,WAAW,GAAAkC,SAAA,CAAAE,IAAA,CAAyC,CAAC;cAAA,OAAAF,SAAA,CAAArB,MAAA,WACnDW,YAAY,CAACa,KAAK,CAAC,CAAC,CAAC;YAAA;YAAA;cAAA,OAAAH,SAAA,CAAApB,IAAA;UAAA;QAAA,GAAAkB,QAAA;MAAA,CAC7B;MAAA,SAAAM,kBAAAC,GAAA;QAAA,OAAAR,kBAAA,CAAApD,KAAA,OAAAC,SAAA;MAAA;MAAA,OAAA0D,iBAAA;IAAA;EAAA;EAAA,OAAAjE,SAAA;AAAA,EA3HqCD,YAAY;AA8HpDoE,MAAM,CAACC,cAAc,CAACpE,SAAS,EAAEqE,MAAM,CAACC,WAAW,EAAE;EACnD5D,KAAK,EAAE,SAAAA,MAAC6D,QAAmB,EAAI;IAC7B,OAAOA,QAAQ,CAAC5D,QAAQ,IAAI,IAAI,IAAI4D,QAAQ,CAACrD,gBAAgB,IAAI,IAAI,IACjEqD,QAAQ,CAAC9C,cAAc,IAAI,IAAI;EACrC;CACD,CAAC"},"metadata":{},"sourceType":"module","externalDependencies":[]}