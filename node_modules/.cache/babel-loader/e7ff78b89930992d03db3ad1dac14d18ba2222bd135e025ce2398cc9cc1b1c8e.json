{"ast":null,"code":"import _slicedToArray from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/slicedToArray.js\";\nimport _regeneratorRuntime from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/regeneratorRuntime.js\";\nimport _asyncToGenerator from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/asyncToGenerator.js\";\nimport _classCallCheck from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/classCallCheck.js\";\nimport _createClass from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/createClass.js\";\n/**\r\n * @license\r\n * Copyright 2018 Google LLC. All Rights Reserved.\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n * http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n * =============================================================================\r\n */\nimport { dispose as _dispose, io, Tensor, util } from '@tensorflow/tfjs-core';\nimport { OperationMapper } from '../operations/operation_mapper';\nimport { GraphExecutor } from './graph_executor';\nimport { ResourceManager } from './resource_manager';\nexport var TFHUB_SEARCH_PARAM = '?tfjs-format=file';\nexport var DEFAULT_MODEL_NAME = 'model.json';\n/**\r\n * A `tf.GraphModel` is a directed, acyclic graph built from a\r\n * SavedModel GraphDef and allows inference execution.\r\n *\r\n * A `tf.GraphModel` can only be created by loading from a model converted from\r\n * a [TensorFlow SavedModel](https://www.tensorflow.org/guide/saved_model) using\r\n * the command line converter tool and loaded via `tf.loadGraphModel`.\r\n *\r\n * @doc {heading: 'Models', subheading: 'Classes'}\r\n */\nexport var GraphModel = /*#__PURE__*/function () {\n  /**\r\n   * @param modelUrl url for the model, or an `io.IOHandler`.\r\n   * @param weightManifestUrl url for the weight file generated by\r\n   * scripts/convert.py script.\r\n   * @param requestOption options for Request, which allows to send credentials\r\n   * and custom headers.\r\n   * @param onProgress Optional, progress callback function, fired periodically\r\n   * before the load is completed.\r\n   */\n  function GraphModel(modelUrl) {\n    var loadOptions = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n    var tfio = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : io;\n    _classCallCheck(this, GraphModel);\n    this.modelUrl = modelUrl;\n    this.loadOptions = loadOptions;\n    this.version = 'n/a';\n    this.io = tfio;\n    if (loadOptions == null) {\n      this.loadOptions = {};\n    }\n    this.resourceManager = new ResourceManager();\n  }\n  _createClass(GraphModel, [{\n    key: \"modelVersion\",\n    get:\n    // Returns the version information for the tensorflow model GraphDef.\n    function get() {\n      return this.version;\n    }\n  }, {\n    key: \"inputNodes\",\n    get: function get() {\n      return this.executor.inputNodes;\n    }\n  }, {\n    key: \"outputNodes\",\n    get: function get() {\n      return this.executor.outputNodes;\n    }\n  }, {\n    key: \"inputs\",\n    get: function get() {\n      return this.executor.inputs;\n    }\n  }, {\n    key: \"outputs\",\n    get: function get() {\n      return this.executor.outputs;\n    }\n  }, {\n    key: \"weights\",\n    get: function get() {\n      return this.executor.weightMap;\n    }\n  }, {\n    key: \"metadata\",\n    get: function get() {\n      return this.artifacts.userDefinedMetadata;\n    }\n  }, {\n    key: \"modelSignature\",\n    get: function get() {\n      return this.signature;\n    }\n  }, {\n    key: \"modelStructuredOutputKeys\",\n    get: function get() {\n      return this.structuredOutputKeys;\n    }\n  }, {\n    key: \"findIOHandler\",\n    value: function findIOHandler() {\n      var path = this.modelUrl;\n      if (path.load != null) {\n        // Path is an IO Handler.\n        this.handler = path;\n      } else if (this.loadOptions.requestInit != null) {\n        this.handler = this.io.browserHTTPRequest(path, this.loadOptions);\n      } else {\n        var handlers = this.io.getLoadHandlers(path, this.loadOptions);\n        if (handlers.length === 0) {\n          // For backward compatibility: if no load handler can be found,\n          // assume it is a relative http path.\n          handlers.push(this.io.browserHTTPRequest(path, this.loadOptions));\n        } else if (handlers.length > 1) {\n          throw new Error(\"Found more than one (\".concat(handlers.length, \") load handlers for \") + \"URL '\".concat([path], \"'\"));\n        }\n        this.handler = handlers[0];\n      }\n    }\n    /**\r\n     * Loads the model and weight files, construct the in memory weight map and\r\n     * compile the inference graph.\r\n     */\n  }, {\n    key: \"load\",\n    value: function load() {\n      var _this = this;\n      this.findIOHandler();\n      if (this.handler.load == null) {\n        throw new Error('Cannot proceed with model loading because the IOHandler provided ' + 'does not have the `load` method implemented.');\n      }\n      var loadResult = this.handler.load();\n      if (util.isPromise(loadResult)) {\n        return loadResult.then(function (artifacts) {\n          return _this.loadSync(artifacts);\n        });\n      }\n      return this.loadSync(loadResult);\n    }\n    /**\r\n     * Synchronously construct the in memory weight map and\r\n     * compile the inference graph.\r\n     *\r\n     * @doc {heading: 'Models', subheading: 'Classes', ignoreCI: true}\r\n     */\n  }, {\n    key: \"loadSync\",\n    value: function loadSync(artifacts) {\n      this.artifacts = artifacts;\n      var graph = this.artifacts.modelTopology;\n      var signature = this.artifacts.signature;\n      if (this.artifacts.userDefinedMetadata != null) {\n        var metadata = this.artifacts.userDefinedMetadata;\n        if (metadata.signature != null) {\n          signature = metadata.signature;\n        }\n        if (metadata.structuredOutputKeys != null) {\n          this.structuredOutputKeys = metadata.structuredOutputKeys;\n        }\n      }\n      this.signature = signature;\n      this.version = \"\".concat(graph.versions.producer, \".\").concat(graph.versions.minConsumer);\n      var weightMap = this.io.decodeWeights(this.artifacts.weightData, this.artifacts.weightSpecs);\n      this.executor = new GraphExecutor(OperationMapper.Instance.transformGraph(graph, this.signature));\n      this.executor.weightMap = this.convertTensorMapToTensorsMap(weightMap);\n      // Attach a model-level resourceManager to each executor to share resources,\n      // such as `HashTable`.\n      this.executor.resourceManager = this.resourceManager;\n      if (artifacts.modelInitializer != null && artifacts.modelInitializer.node != null) {\n        var initializer = OperationMapper.Instance.transformGraph(artifacts.modelInitializer);\n        this.initializer = new GraphExecutor(initializer);\n        this.initializer.weightMap = this.executor.weightMap;\n        // Attach a model-level resourceManager to the initializer, the\n        // hashTables created from when executing the initializer will be stored\n        // in the resourceManager.\n        this.initializer.resourceManager = this.resourceManager;\n        this.initializerSignature = artifacts.initializerSignature;\n      }\n      return true;\n    }\n    /**\r\n     * Save the configuration and/or weights of the GraphModel.\r\n     *\r\n     * An `IOHandler` is an object that has a `save` method of the proper\r\n     * signature defined. The `save` method manages the storing or\r\n     * transmission of serialized data (\"artifacts\") that represent the\r\n     * model's topology and weights onto or via a specific medium, such as\r\n     * file downloads, local storage, IndexedDB in the web browser and HTTP\r\n     * requests to a server. TensorFlow.js provides `IOHandler`\r\n     * implementations for a number of frequently used saving mediums, such as\r\n     * `tf.io.browserDownloads` and `tf.io.browserLocalStorage`. See `tf.io`\r\n     * for more details.\r\n     *\r\n     * This method also allows you to refer to certain types of `IOHandler`s\r\n     * as URL-like string shortcuts, such as 'localstorage://' and\r\n     * 'indexeddb://'.\r\n     *\r\n     * Example 1: Save `model`'s topology and weights to browser [local\r\n     * storage](https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage);\r\n     * then load it back.\r\n     *\r\n     * ```js\r\n     * const modelUrl =\r\n     *    'https://storage.googleapis.com/tfjs-models/savedmodel/mobilenet_v2_1.0_224/model.json';\r\n     * const model = await tf.loadGraphModel(modelUrl);\r\n     * const zeros = tf.zeros([1, 224, 224, 3]);\r\n     * model.predict(zeros).print();\r\n     *\r\n     * const saveResults = await model.save('localstorage://my-model-1');\r\n     *\r\n     * const loadedModel = await tf.loadGraphModel('localstorage://my-model-1');\r\n     * console.log('Prediction from loaded model:');\r\n     * model.predict(zeros).print();\r\n     * ```\r\n     *\r\n     * @param handlerOrURL An instance of `IOHandler` or a URL-like,\r\n     * scheme-based string shortcut for `IOHandler`.\r\n     * @param config Options for saving the model.\r\n     * @returns A `Promise` of `SaveResult`, which summarizes the result of\r\n     * the saving, such as byte sizes of the saved artifacts for the model's\r\n     *   topology and weight values.\r\n     *\r\n     * @doc {heading: 'Models', subheading: 'Classes', ignoreCI: true}\r\n     */\n  }, {\n    key: \"save\",\n    value: function () {\n      var _save = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee(handlerOrURL, config) {\n        var handlers;\n        return _regeneratorRuntime().wrap(function _callee$(_context) {\n          while (1) switch (_context.prev = _context.next) {\n            case 0:\n              if (!(typeof handlerOrURL === 'string')) {\n                _context.next = 9;\n                break;\n              }\n              handlers = this.io.getSaveHandlers(handlerOrURL);\n              if (!(handlers.length === 0)) {\n                _context.next = 6;\n                break;\n              }\n              throw new Error(\"Cannot find any save handlers for URL '\".concat(handlerOrURL, \"'\"));\n            case 6:\n              if (!(handlers.length > 1)) {\n                _context.next = 8;\n                break;\n              }\n              throw new Error(\"Found more than one (\".concat(handlers.length, \") save handlers for \") + \"URL '\".concat(handlerOrURL, \"'\"));\n            case 8:\n              handlerOrURL = handlers[0];\n            case 9:\n              if (!(handlerOrURL.save == null)) {\n                _context.next = 11;\n                break;\n              }\n              throw new Error('GraphModel.save() cannot proceed because the IOHandler ' + 'provided does not have the `save` attribute defined.');\n            case 11:\n              return _context.abrupt(\"return\", handlerOrURL.save(this.artifacts));\n            case 12:\n            case \"end\":\n              return _context.stop();\n          }\n        }, _callee, this);\n      }));\n      function save(_x, _x2) {\n        return _save.apply(this, arguments);\n      }\n      return save;\n    }()\n  }, {\n    key: \"addStructuredOutputNames\",\n    value: function addStructuredOutputNames(outputTensors) {\n      var _this2 = this;\n      if (this.structuredOutputKeys) {\n        var outputTensorsArray = outputTensors instanceof Tensor ? [outputTensors] : outputTensors;\n        var outputTensorMap = {};\n        outputTensorsArray.forEach(function (outputTensor, i) {\n          return outputTensorMap[_this2.structuredOutputKeys[i]] = outputTensor;\n        });\n        return outputTensorMap;\n      }\n      return outputTensors;\n    }\n    /**\r\n     * Execute the inference for the input tensors.\r\n     *\r\n     * @param input The input tensors, when there is single input for the model,\r\n     * inputs param should be a `tf.Tensor`. For models with mutliple inputs,\r\n     * inputs params should be in either `tf.Tensor`[] if the input order is\r\n     * fixed, or otherwise NamedTensorMap format.\r\n     *\r\n     * For model with multiple inputs, we recommend you use NamedTensorMap as the\r\n     * input type, if you use `tf.Tensor`[], the order of the array needs to\r\n     * follow the\r\n     * order of inputNodes array. @see {@link GraphModel.inputNodes}\r\n     *\r\n     * You can also feed any intermediate nodes using the NamedTensorMap as the\r\n     * input type. For example, given the graph\r\n     *    InputNode => Intermediate => OutputNode,\r\n     * you can execute the subgraph Intermediate => OutputNode by calling\r\n     *    model.execute('IntermediateNode' : tf.tensor(...));\r\n     *\r\n     * This is useful for models that uses tf.dynamic_rnn, where the intermediate\r\n     * state needs to be fed manually.\r\n     *\r\n     * For batch inference execution, the tensors for each input need to be\r\n     * concatenated together. For example with mobilenet, the required input shape\r\n     * is [1, 244, 244, 3], which represents the [batch, height, width, channel].\r\n     * If we are provide a batched data of 100 images, the input tensor should be\r\n     * in the shape of [100, 244, 244, 3].\r\n     *\r\n     * @param config Prediction configuration for specifying the batch size.\r\n     * Currently the batch size option is ignored for graph model.\r\n     *\r\n     * @returns Inference result tensors. If the model is converted and it\r\n     * originally had structured_outputs in tensorflow, then a NamedTensorMap\r\n     * will be returned matching the structured_outputs. If no structured_outputs\r\n     * are present, the output will be single `tf.Tensor` if the model has single\r\n     * output node, otherwise Tensor[].\r\n     *\r\n     * @doc {heading: 'Models', subheading: 'Classes'}\r\n     */\n  }, {\n    key: \"predict\",\n    value: function predict(inputs, config) {\n      var outputTensors = this.execute(inputs, this.outputNodes);\n      return this.addStructuredOutputNames(outputTensors);\n    }\n    /**\r\n     * Execute the inference for the input tensors in async fashion, use this\r\n     * method when your model contains control flow ops.\r\n     *\r\n     * @param input The input tensors, when there is single input for the model,\r\n     * inputs param should be a `tf.Tensor`. For models with mutliple inputs,\r\n     * inputs params should be in either `tf.Tensor`[] if the input order is\r\n     * fixed, or otherwise NamedTensorMap format.\r\n     *\r\n     * For model with multiple inputs, we recommend you use NamedTensorMap as the\r\n     * input type, if you use `tf.Tensor`[], the order of the array needs to\r\n     * follow the\r\n     * order of inputNodes array. @see {@link GraphModel.inputNodes}\r\n     *\r\n     * You can also feed any intermediate nodes using the NamedTensorMap as the\r\n     * input type. For example, given the graph\r\n     *    InputNode => Intermediate => OutputNode,\r\n     * you can execute the subgraph Intermediate => OutputNode by calling\r\n     *    model.execute('IntermediateNode' : tf.tensor(...));\r\n     *\r\n     * This is useful for models that uses tf.dynamic_rnn, where the intermediate\r\n     * state needs to be fed manually.\r\n     *\r\n     * For batch inference execution, the tensors for each input need to be\r\n     * concatenated together. For example with mobilenet, the required input shape\r\n     * is [1, 244, 244, 3], which represents the [batch, height, width, channel].\r\n     * If we are provide a batched data of 100 images, the input tensor should be\r\n     * in the shape of [100, 244, 244, 3].\r\n     *\r\n     * @param config Prediction configuration for specifying the batch size.\r\n     * Currently the batch size option is ignored for graph model.\r\n     *\r\n     * @returns A Promise of inference result tensors. If the model is converted\r\n     * and it originally had structured_outputs in tensorflow, then a\r\n     * NamedTensorMap will be returned matching the structured_outputs. If no\r\n     * structured_outputs are present, the output will be single `tf.Tensor` if\r\n     * the model has single output node, otherwise Tensor[].\r\n     *\r\n     * @doc {heading: 'Models', subheading: 'Classes'}\r\n     */\n  }, {\n    key: \"predictAsync\",\n    value: function () {\n      var _predictAsync = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee2(inputs, config) {\n        var outputTensors;\n        return _regeneratorRuntime().wrap(function _callee2$(_context2) {\n          while (1) switch (_context2.prev = _context2.next) {\n            case 0:\n              _context2.next = 2;\n              return this.executeAsync(inputs, this.outputNodes);\n            case 2:\n              outputTensors = _context2.sent;\n              return _context2.abrupt(\"return\", this.addStructuredOutputNames(outputTensors));\n            case 4:\n            case \"end\":\n              return _context2.stop();\n          }\n        }, _callee2, this);\n      }));\n      function predictAsync(_x3, _x4) {\n        return _predictAsync.apply(this, arguments);\n      }\n      return predictAsync;\n    }()\n  }, {\n    key: \"normalizeInputs\",\n    value: function normalizeInputs(inputs) {\n      var _this3 = this;\n      var _a;\n      if (!(inputs instanceof Tensor) && !Array.isArray(inputs)) {\n        // The input is already a NamedTensorMap.\n        var signatureInputs = (_a = this.signature) === null || _a === void 0 ? void 0 : _a.inputs;\n        if (signatureInputs != null) {\n          for (var input in signatureInputs) {\n            var tensor = signatureInputs[input];\n            if (tensor.resourceId != null) {\n              inputs[input] = this.resourceIdToCapturedInput[tensor.resourceId];\n            }\n          }\n        }\n        return inputs;\n      }\n      inputs = Array.isArray(inputs) ? inputs : [inputs];\n      var numCapturedInputs = Object.keys(this.resourceIdToCapturedInput).length;\n      if (inputs.length + numCapturedInputs !== this.inputNodes.length) {\n        throw new Error(\"Input tensor count mismatch, the graph model has \".concat(this.inputNodes.length - numCapturedInputs, \" non-resource placeholders, while there are \").concat(inputs.length, \" input tensors provided.\"));\n      }\n      var inputIndex = 0;\n      return this.inputNodes.reduce(function (map, inputName) {\n        var _a, _b, _c;\n        var resourceId = (_c = (_b = (_a = _this3.signature) === null || _a === void 0 ? void 0 : _a.inputs) === null || _b === void 0 ? void 0 : _b[inputName]) === null || _c === void 0 ? void 0 : _c.resourceId;\n        if (resourceId != null) {\n          map[inputName] = _this3.resourceIdToCapturedInput[resourceId];\n        } else {\n          map[inputName] = inputs[inputIndex++];\n        }\n        return map;\n      }, {});\n    }\n  }, {\n    key: \"normalizeOutputs\",\n    value: function normalizeOutputs(outputs) {\n      outputs = outputs || this.outputNodes;\n      return !Array.isArray(outputs) ? [outputs] : outputs;\n    }\n  }, {\n    key: \"executeInitializerGraph\",\n    value: function executeInitializerGraph() {\n      if (this.initializer == null) {\n        return [];\n      }\n      if (this.initializerSignature == null) {\n        return this.initializer.execute({}, []);\n      } else {\n        return this.initializer.execute({}, Object.keys(this.initializerSignature.outputs));\n      }\n    }\n  }, {\n    key: \"executeInitializerGraphAsync\",\n    value: function () {\n      var _executeInitializerGraphAsync = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee3() {\n        return _regeneratorRuntime().wrap(function _callee3$(_context3) {\n          while (1) switch (_context3.prev = _context3.next) {\n            case 0:\n              if (!(this.initializer == null)) {\n                _context3.next = 2;\n                break;\n              }\n              return _context3.abrupt(\"return\", []);\n            case 2:\n              if (!(this.initializerSignature == null)) {\n                _context3.next = 6;\n                break;\n              }\n              return _context3.abrupt(\"return\", this.initializer.executeAsync({}, []));\n            case 6:\n              return _context3.abrupt(\"return\", this.initializer.executeAsync({}, Object.keys(this.initializerSignature.outputs)));\n            case 7:\n            case \"end\":\n              return _context3.stop();\n          }\n        }, _callee3, this);\n      }));\n      function executeInitializerGraphAsync() {\n        return _executeInitializerGraphAsync.apply(this, arguments);\n      }\n      return executeInitializerGraphAsync;\n    }()\n  }, {\n    key: \"setResourceIdToCapturedInput\",\n    value: function setResourceIdToCapturedInput(outputs) {\n      this.resourceIdToCapturedInput = {};\n      if (this.initializerSignature) {\n        var signatureOutputs = this.initializerSignature.outputs;\n        var outputNames = Object.keys(signatureOutputs);\n        for (var i = 0; i < outputNames.length; i++) {\n          var outputName = outputNames[i];\n          var tensorInfo = signatureOutputs[outputName];\n          this.resourceIdToCapturedInput[tensorInfo.resourceId] = outputs[i];\n        }\n      }\n    }\n    /**\r\n     * Executes inference for the model for given input tensors.\r\n     * @param inputs tensor, tensor array or tensor map of the inputs for the\r\n     * model, keyed by the input node names.\r\n     * @param outputs output node name from the TensorFlow model, if no\r\n     * outputs are specified, the default outputs of the model would be used.\r\n     * You can inspect intermediate nodes of the model by adding them to the\r\n     * outputs array.\r\n     *\r\n     * @returns A single tensor if provided with a single output or no outputs\r\n     * are provided and there is only one default output, otherwise return a\r\n     * tensor array. The order of the tensor array is the same as the outputs\r\n     * if provided, otherwise the order of outputNodes attribute of the model.\r\n     *\r\n     * @doc {heading: 'Models', subheading: 'Classes'}\r\n     */\n  }, {\n    key: \"execute\",\n    value: function execute(inputs, outputs) {\n      if (this.resourceIdToCapturedInput == null) {\n        this.setResourceIdToCapturedInput(this.executeInitializerGraph());\n      }\n      inputs = this.normalizeInputs(inputs);\n      outputs = this.normalizeOutputs(outputs);\n      var result = this.executor.execute(inputs, outputs);\n      return result.length > 1 ? result : result[0];\n    }\n    /**\r\n     * Executes inference for the model for given input tensors in async\r\n     * fashion, use this method when your model contains control flow ops.\r\n     * @param inputs tensor, tensor array or tensor map of the inputs for the\r\n     * model, keyed by the input node names.\r\n     * @param outputs output node name from the TensorFlow model, if no outputs\r\n     * are specified, the default outputs of the model would be used. You can\r\n     * inspect intermediate nodes of the model by adding them to the outputs\r\n     * array.\r\n     *\r\n     * @returns A Promise of single tensor if provided with a single output or\r\n     * no outputs are provided and there is only one default output, otherwise\r\n     * return a tensor map.\r\n     *\r\n     * @doc {heading: 'Models', subheading: 'Classes'}\r\n     */\n  }, {\n    key: \"executeAsync\",\n    value: function () {\n      var _executeAsync = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee4(inputs, outputs) {\n        var result;\n        return _regeneratorRuntime().wrap(function _callee4$(_context4) {\n          while (1) switch (_context4.prev = _context4.next) {\n            case 0:\n              if (!(this.resourceIdToCapturedInput == null)) {\n                _context4.next = 6;\n                break;\n              }\n              _context4.t0 = this;\n              _context4.next = 4;\n              return this.executeInitializerGraphAsync();\n            case 4:\n              _context4.t1 = _context4.sent;\n              _context4.t0.setResourceIdToCapturedInput.call(_context4.t0, _context4.t1);\n            case 6:\n              inputs = this.normalizeInputs(inputs);\n              outputs = this.normalizeOutputs(outputs);\n              _context4.next = 10;\n              return this.executor.executeAsync(inputs, outputs);\n            case 10:\n              result = _context4.sent;\n              return _context4.abrupt(\"return\", result.length > 1 ? result : result[0]);\n            case 12:\n            case \"end\":\n              return _context4.stop();\n          }\n        }, _callee4, this);\n      }));\n      function executeAsync(_x5, _x6) {\n        return _executeAsync.apply(this, arguments);\n      }\n      return executeAsync;\n    }()\n    /**\r\n     * Get intermediate tensors for model debugging mode (flag\r\n     * KEEP_INTERMEDIATE_TENSORS is true).\r\n     *\r\n     * @doc {heading: 'Models', subheading: 'Classes'}\r\n     */\n  }, {\n    key: \"getIntermediateTensors\",\n    value: function getIntermediateTensors() {\n      return this.executor.getIntermediateTensors();\n    }\n    /**\r\n     * Dispose intermediate tensors for model debugging mode (flag\r\n     * KEEP_INTERMEDIATE_TENSORS is true).\r\n     *\r\n     * @doc {heading: 'Models', subheading: 'Classes'}\r\n     */\n  }, {\n    key: \"disposeIntermediateTensors\",\n    value: function disposeIntermediateTensors() {\n      this.executor.disposeIntermediateTensors();\n    }\n  }, {\n    key: \"convertTensorMapToTensorsMap\",\n    value: function convertTensorMapToTensorsMap(map) {\n      return Object.keys(map).reduce(function (newMap, key) {\n        newMap[key] = [map[key]];\n        return newMap;\n      }, {});\n    }\n    /**\r\n     * Releases the memory used by the weight tensors and resourceManager.\r\n     *\r\n     * @doc {heading: 'Models', subheading: 'Classes'}\r\n     */\n  }, {\n    key: \"dispose\",\n    value: function dispose() {\n      this.executor.dispose();\n      if (this.initializer) {\n        this.initializer.dispose();\n        if (this.resourceIdToCapturedInput) {\n          _dispose(this.resourceIdToCapturedInput);\n        }\n      }\n      this.resourceManager.dispose();\n    }\n  }]);\n  return GraphModel;\n}();\n/**\r\n * Load a graph model given a URL to the model definition.\r\n *\r\n * Example of loading MobileNetV2 from a URL and making a prediction with a\r\n * zeros input:\r\n *\r\n * ```js\r\n * const modelUrl =\r\n *    'https://storage.googleapis.com/tfjs-models/savedmodel/mobilenet_v2_1.0_224/model.json';\r\n * const model = await tf.loadGraphModel(modelUrl);\r\n * const zeros = tf.zeros([1, 224, 224, 3]);\r\n * model.predict(zeros).print();\r\n * ```\r\n *\r\n * Example of loading MobileNetV2 from a TF Hub URL and making a prediction\r\n * with a zeros input:\r\n *\r\n * ```js\r\n * const modelUrl =\r\n *    'https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/classification/2';\r\n * const model = await tf.loadGraphModel(modelUrl, {fromTFHub: true});\r\n * const zeros = tf.zeros([1, 224, 224, 3]);\r\n * model.predict(zeros).print();\r\n * ```\r\n * @param modelUrl The url or an `io.IOHandler` that loads the model.\r\n * @param options Options for the HTTP request, which allows to send\r\n *     credentials\r\n *    and custom headers.\r\n *\r\n * @doc {heading: 'Models', subheading: 'Loading'}\r\n */\nexport function loadGraphModel(_x7) {\n  return _loadGraphModel.apply(this, arguments);\n}\n/**\r\n * Load a graph model given a synchronous IO handler with a 'load' method.\r\n *\r\n * @param modelSource The `io.IOHandlerSync` that loads the model, or the\r\n *     `io.ModelArtifacts` that encode the model, or a tuple of\r\n *     `[io.ModelJSON, ArrayBuffer]` of which the first element encodes the\r\n *      model and the second contains the weights.\r\n *\r\n * @doc {heading: 'Models', subheading: 'Loading'}\r\n */\nfunction _loadGraphModel() {\n  _loadGraphModel = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee5(modelUrl) {\n    var options,\n      tfio,\n      model,\n      _args5 = arguments;\n    return _regeneratorRuntime().wrap(function _callee5$(_context5) {\n      while (1) switch (_context5.prev = _context5.next) {\n        case 0:\n          options = _args5.length > 1 && _args5[1] !== undefined ? _args5[1] : {};\n          tfio = _args5.length > 2 && _args5[2] !== undefined ? _args5[2] : io;\n          if (!(modelUrl == null)) {\n            _context5.next = 4;\n            break;\n          }\n          throw new Error('modelUrl in loadGraphModel() cannot be null. Please provide a url ' + 'or an IOHandler that loads the model');\n        case 4:\n          if (options == null) {\n            options = {};\n          }\n          if (options.fromTFHub && typeof modelUrl === 'string') {\n            modelUrl = getTFHubUrl(modelUrl);\n          }\n          model = new GraphModel(modelUrl, options, tfio);\n          _context5.next = 9;\n          return model.load();\n        case 9:\n          return _context5.abrupt(\"return\", model);\n        case 10:\n        case \"end\":\n          return _context5.stop();\n      }\n    }, _callee5);\n  }));\n  return _loadGraphModel.apply(this, arguments);\n}\nexport function loadGraphModelSync(modelSource) {\n  if (modelSource == null) {\n    throw new Error('modelUrl in loadGraphModelSync() cannot be null. Please provide ' + 'model artifacts or an IOHandler that loads the model');\n  }\n  var ioHandler;\n  if (modelSource instanceof Array) {\n    var _modelSource = _slicedToArray(modelSource, 2),\n      modelJSON = _modelSource[0],\n      weights = _modelSource[1];\n    if (!modelJSON) {\n      throw new Error('modelJSON must be the first element of the array');\n    }\n    if (!weights || !(weights instanceof ArrayBuffer)) {\n      throw new Error('An ArrayBuffer of weights must be the second element of' + ' the array');\n    }\n    if (!('modelTopology' in modelJSON)) {\n      throw new Error('Model JSON is missing \\'modelTopology\\'');\n    }\n    if (!('weightsManifest' in modelJSON)) {\n      throw new Error('Model JSON is missing \\'weightsManifest\\'');\n    }\n    var weightSpecs = io.getWeightSpecs(modelJSON.weightsManifest);\n    var modelArtifacts = io.getModelArtifactsForJSONSync(modelJSON, weightSpecs, weights);\n    ioHandler = io.fromMemorySync(modelArtifacts);\n  } else if ('load' in modelSource) {\n    // Then modelSource is already an IOHandlerSync.\n    ioHandler = modelSource;\n  } else if ('modelTopology' in modelSource && 'weightSpecs' in modelSource && 'weightData' in modelSource) {\n    // modelSource is of type ModelArtifacts.\n    ioHandler = io.fromMemorySync(modelSource);\n  } else {\n    throw new Error('Unknown model format');\n  }\n  var model = new GraphModel(ioHandler);\n  model.load();\n  return model;\n}\nfunction getTFHubUrl(modelUrl) {\n  if (!modelUrl.endsWith('/')) {\n    modelUrl = modelUrl + '/';\n  }\n  return \"\".concat(modelUrl).concat(DEFAULT_MODEL_NAME).concat(TFHUB_SEARCH_PARAM);\n}","map":{"version":3,"names":["dispose","io","Tensor","util","OperationMapper","GraphExecutor","ResourceManager","TFHUB_SEARCH_PARAM","DEFAULT_MODEL_NAME","GraphModel","modelUrl","loadOptions","arguments","length","undefined","tfio","_classCallCheck","version","resourceManager","_createClass","key","get","executor","inputNodes","outputNodes","inputs","outputs","weightMap","artifacts","userDefinedMetadata","signature","structuredOutputKeys","value","findIOHandler","path","load","handler","requestInit","browserHTTPRequest","handlers","getLoadHandlers","push","Error","concat","_this","loadResult","isPromise","then","loadSync","graph","modelTopology","metadata","versions","producer","minConsumer","decodeWeights","weightData","weightSpecs","Instance","transformGraph","convertTensorMapToTensorsMap","modelInitializer","node","initializer","initializerSignature","_save","_asyncToGenerator","_regeneratorRuntime","mark","_callee","handlerOrURL","config","wrap","_callee$","_context","prev","next","getSaveHandlers","save","abrupt","stop","_x","_x2","apply","addStructuredOutputNames","outputTensors","_this2","outputTensorsArray","outputTensorMap","forEach","outputTensor","i","predict","execute","_predictAsync","_callee2","_callee2$","_context2","executeAsync","sent","predictAsync","_x3","_x4","normalizeInputs","_this3","Array","isArray","signatureInputs","_a","input","tensor","resourceId","resourceIdToCapturedInput","numCapturedInputs","Object","keys","inputIndex","reduce","map","inputName","_c","_b","normalizeOutputs","executeInitializerGraph","_executeInitializerGraphAsync","_callee3","_callee3$","_context3","executeInitializerGraphAsync","setResourceIdToCapturedInput","signatureOutputs","outputNames","outputName","tensorInfo","result","_executeAsync","_callee4","_callee4$","_context4","t0","t1","call","_x5","_x6","getIntermediateTensors","disposeIntermediateTensors","newMap","loadGraphModel","_x7","_loadGraphModel","_callee5","options","model","_args5","_callee5$","_context5","fromTFHub","getTFHubUrl","loadGraphModelSync","modelSource","ioHandler","_modelSource","_slicedToArray","modelJSON","weights","ArrayBuffer","getWeightSpecs","weightsManifest","modelArtifacts","getModelArtifactsForJSONSync","fromMemorySync","endsWith"],"sources":["C:\\Users\\vince\\OneDrive\\Documents\\GitHub\\tfjs-converter\\src\\executor\\graph_model.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {dispose, InferenceModel, io, ModelPredictConfig, NamedTensorMap, Tensor, util} from '@tensorflow/tfjs-core';\n\nimport * as tensorflow from '../data/compiled_api';\nimport {NamedTensorsMap, TensorInfo} from '../data/types';\nimport {OperationMapper} from '../operations/operation_mapper';\n\nimport {GraphExecutor} from './graph_executor';\nimport {ResourceManager} from './resource_manager';\n\nexport const TFHUB_SEARCH_PARAM = '?tfjs-format=file';\nexport const DEFAULT_MODEL_NAME = 'model.json';\ntype Url = string|io.IOHandler|io.IOHandlerSync;\ntype UrlIOHandler<T extends Url> = T extends string ? io.IOHandler : T;\n\n/**\n * A `tf.GraphModel` is a directed, acyclic graph built from a\n * SavedModel GraphDef and allows inference execution.\n *\n * A `tf.GraphModel` can only be created by loading from a model converted from\n * a [TensorFlow SavedModel](https://www.tensorflow.org/guide/saved_model) using\n * the command line converter tool and loaded via `tf.loadGraphModel`.\n *\n * @doc {heading: 'Models', subheading: 'Classes'}\n */\nexport class GraphModel<ModelURL extends Url = string | io.IOHandler> implements\n    InferenceModel {\n  private executor: GraphExecutor;\n  private version = 'n/a';\n  private handler: UrlIOHandler<ModelURL>;\n  private artifacts: io.ModelArtifacts;\n  private initializer: GraphExecutor;\n  private resourceIdToCapturedInput: {[key: number]: Tensor};\n  private resourceManager: ResourceManager;\n  private signature: tensorflow.ISignatureDef;\n  private initializerSignature: tensorflow.ISignatureDef;\n  private structuredOutputKeys: string[];\n  private readonly io: typeof io;\n\n  // Returns the version information for the tensorflow model GraphDef.\n  get modelVersion(): string {\n    return this.version;\n  }\n\n  get inputNodes(): string[] {\n    return this.executor.inputNodes;\n  }\n\n  get outputNodes(): string[] {\n    return this.executor.outputNodes;\n  }\n\n  get inputs(): TensorInfo[] {\n    return this.executor.inputs;\n  }\n\n  get outputs(): TensorInfo[] {\n    return this.executor.outputs;\n  }\n\n  get weights(): NamedTensorsMap {\n    return this.executor.weightMap;\n  }\n\n  get metadata(): {} {\n    return this.artifacts.userDefinedMetadata;\n  }\n\n  get modelSignature(): {} {\n    return this.signature;\n  }\n\n  get modelStructuredOutputKeys(): {} {\n    return this.structuredOutputKeys;\n  }\n\n  /**\n   * @param modelUrl url for the model, or an `io.IOHandler`.\n   * @param weightManifestUrl url for the weight file generated by\n   * scripts/convert.py script.\n   * @param requestOption options for Request, which allows to send credentials\n   * and custom headers.\n   * @param onProgress Optional, progress callback function, fired periodically\n   * before the load is completed.\n   */\n  constructor(\n      private modelUrl: ModelURL, private loadOptions: io.LoadOptions = {},\n      tfio = io) {\n    this.io = tfio;\n    if (loadOptions == null) {\n      this.loadOptions = {};\n    }\n    this.resourceManager = new ResourceManager();\n  }\n\n  private findIOHandler() {\n    type IOHandler = UrlIOHandler<ModelURL>;\n    const path = this.modelUrl;\n    if ((path as io.IOHandler).load != null) {\n      // Path is an IO Handler.\n      this.handler = path as IOHandler;\n    } else if (this.loadOptions.requestInit != null) {\n      this.handler = this.io.browserHTTPRequest(\n                         path as string, this.loadOptions) as IOHandler;\n    } else {\n      const handlers =\n          this.io.getLoadHandlers(path as string, this.loadOptions);\n      if (handlers.length === 0) {\n        // For backward compatibility: if no load handler can be found,\n        // assume it is a relative http path.\n        handlers.push(\n            this.io.browserHTTPRequest(path as string, this.loadOptions));\n      } else if (handlers.length > 1) {\n        throw new Error(\n            `Found more than one (${handlers.length}) load handlers for ` +\n            `URL '${[path]}'`);\n      }\n      this.handler = handlers[0] as IOHandler;\n    }\n  }\n\n  /**\n   * Loads the model and weight files, construct the in memory weight map and\n   * compile the inference graph.\n   */\n  load(): UrlIOHandler<ModelURL> extends io.IOHandlerSync? boolean:\n                                             Promise<boolean> {\n    type IOHandler = UrlIOHandler<ModelURL>;\n    this.findIOHandler();\n    if (this.handler.load == null) {\n      throw new Error(\n          'Cannot proceed with model loading because the IOHandler provided ' +\n          'does not have the `load` method implemented.');\n    }\n\n    type Result =\n        IOHandler extends io.IOHandlerSync ? boolean : Promise<boolean>;\n\n    const loadResult = this.handler.load() as ReturnType<IOHandler['load']>;\n    if (util.isPromise(loadResult)) {\n      return loadResult.then(artifacts => this.loadSync(artifacts)) as Result;\n    }\n\n    return this.loadSync(loadResult) as Result;\n  }\n\n  /**\n   * Synchronously construct the in memory weight map and\n   * compile the inference graph.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes', ignoreCI: true}\n   */\n  loadSync(artifacts: io.ModelArtifacts) {\n    this.artifacts = artifacts;\n    const graph = this.artifacts.modelTopology as tensorflow.IGraphDef;\n\n    let signature = this.artifacts.signature;\n    if (this.artifacts.userDefinedMetadata != null) {\n      const metadata = this.artifacts.userDefinedMetadata;\n      if (metadata.signature != null) {\n        signature = metadata.signature;\n      }\n\n      if (metadata.structuredOutputKeys != null) {\n        this.structuredOutputKeys = metadata.structuredOutputKeys as string[];\n      }\n    }\n    this.signature = signature;\n\n    this.version = `${graph.versions.producer}.${graph.versions.minConsumer}`;\n    const weightMap = this.io.decodeWeights(\n        this.artifacts.weightData, this.artifacts.weightSpecs);\n    this.executor = new GraphExecutor(\n        OperationMapper.Instance.transformGraph(graph, this.signature));\n    this.executor.weightMap = this.convertTensorMapToTensorsMap(weightMap);\n    // Attach a model-level resourceManager to each executor to share resources,\n    // such as `HashTable`.\n    this.executor.resourceManager = this.resourceManager;\n\n    if (artifacts.modelInitializer != null &&\n        (artifacts.modelInitializer as tensorflow.IGraphDef).node != null) {\n      const initializer =\n          OperationMapper.Instance.transformGraph(artifacts.modelInitializer);\n      this.initializer = new GraphExecutor(initializer);\n      this.initializer.weightMap = this.executor.weightMap;\n      // Attach a model-level resourceManager to the initializer, the\n      // hashTables created from when executing the initializer will be stored\n      // in the resourceManager.\n      this.initializer.resourceManager = this.resourceManager;\n      this.initializerSignature = artifacts.initializerSignature;\n    }\n\n    return true;\n  }\n\n  /**\n   * Save the configuration and/or weights of the GraphModel.\n   *\n   * An `IOHandler` is an object that has a `save` method of the proper\n   * signature defined. The `save` method manages the storing or\n   * transmission of serialized data (\"artifacts\") that represent the\n   * model's topology and weights onto or via a specific medium, such as\n   * file downloads, local storage, IndexedDB in the web browser and HTTP\n   * requests to a server. TensorFlow.js provides `IOHandler`\n   * implementations for a number of frequently used saving mediums, such as\n   * `tf.io.browserDownloads` and `tf.io.browserLocalStorage`. See `tf.io`\n   * for more details.\n   *\n   * This method also allows you to refer to certain types of `IOHandler`s\n   * as URL-like string shortcuts, such as 'localstorage://' and\n   * 'indexeddb://'.\n   *\n   * Example 1: Save `model`'s topology and weights to browser [local\n   * storage](https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage);\n   * then load it back.\n   *\n   * ```js\n   * const modelUrl =\n   *    'https://storage.googleapis.com/tfjs-models/savedmodel/mobilenet_v2_1.0_224/model.json';\n   * const model = await tf.loadGraphModel(modelUrl);\n   * const zeros = tf.zeros([1, 224, 224, 3]);\n   * model.predict(zeros).print();\n   *\n   * const saveResults = await model.save('localstorage://my-model-1');\n   *\n   * const loadedModel = await tf.loadGraphModel('localstorage://my-model-1');\n   * console.log('Prediction from loaded model:');\n   * model.predict(zeros).print();\n   * ```\n   *\n   * @param handlerOrURL An instance of `IOHandler` or a URL-like,\n   * scheme-based string shortcut for `IOHandler`.\n   * @param config Options for saving the model.\n   * @returns A `Promise` of `SaveResult`, which summarizes the result of\n   * the saving, such as byte sizes of the saved artifacts for the model's\n   *   topology and weight values.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes', ignoreCI: true}\n   */\n  async save(handlerOrURL: io.IOHandler|string, config?: io.SaveConfig):\n      Promise<io.SaveResult> {\n    if (typeof handlerOrURL === 'string') {\n      const handlers = this.io.getSaveHandlers(handlerOrURL);\n      if (handlers.length === 0) {\n        throw new Error(\n            `Cannot find any save handlers for URL '${handlerOrURL}'`);\n      } else if (handlers.length > 1) {\n        throw new Error(\n            `Found more than one (${handlers.length}) save handlers for ` +\n            `URL '${handlerOrURL}'`);\n      }\n      handlerOrURL = handlers[0];\n    }\n    if (handlerOrURL.save == null) {\n      throw new Error(\n          'GraphModel.save() cannot proceed because the IOHandler ' +\n          'provided does not have the `save` attribute defined.');\n    }\n\n    return handlerOrURL.save(this.artifacts);\n  }\n\n  private addStructuredOutputNames(outputTensors: Tensor|Tensor[]) {\n    if (this.structuredOutputKeys) {\n      const outputTensorsArray =\n          outputTensors instanceof Tensor ? [outputTensors] : outputTensors;\n      const outputTensorMap: NamedTensorMap = {};\n\n      outputTensorsArray.forEach(\n          (outputTensor, i) => outputTensorMap[this.structuredOutputKeys[i]] =\n              outputTensor);\n\n      return outputTensorMap;\n    }\n    return outputTensors;\n  }\n\n  /**\n   * Execute the inference for the input tensors.\n   *\n   * @param input The input tensors, when there is single input for the model,\n   * inputs param should be a `tf.Tensor`. For models with mutliple inputs,\n   * inputs params should be in either `tf.Tensor`[] if the input order is\n   * fixed, or otherwise NamedTensorMap format.\n   *\n   * For model with multiple inputs, we recommend you use NamedTensorMap as the\n   * input type, if you use `tf.Tensor`[], the order of the array needs to\n   * follow the\n   * order of inputNodes array. @see {@link GraphModel.inputNodes}\n   *\n   * You can also feed any intermediate nodes using the NamedTensorMap as the\n   * input type. For example, given the graph\n   *    InputNode => Intermediate => OutputNode,\n   * you can execute the subgraph Intermediate => OutputNode by calling\n   *    model.execute('IntermediateNode' : tf.tensor(...));\n   *\n   * This is useful for models that uses tf.dynamic_rnn, where the intermediate\n   * state needs to be fed manually.\n   *\n   * For batch inference execution, the tensors for each input need to be\n   * concatenated together. For example with mobilenet, the required input shape\n   * is [1, 244, 244, 3], which represents the [batch, height, width, channel].\n   * If we are provide a batched data of 100 images, the input tensor should be\n   * in the shape of [100, 244, 244, 3].\n   *\n   * @param config Prediction configuration for specifying the batch size.\n   * Currently the batch size option is ignored for graph model.\n   *\n   * @returns Inference result tensors. If the model is converted and it\n   * originally had structured_outputs in tensorflow, then a NamedTensorMap\n   * will be returned matching the structured_outputs. If no structured_outputs\n   * are present, the output will be single `tf.Tensor` if the model has single\n   * output node, otherwise Tensor[].\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  predict(inputs: Tensor|Tensor[]|NamedTensorMap, config?: ModelPredictConfig):\n      Tensor|Tensor[]|NamedTensorMap {\n    const outputTensors = this.execute(inputs, this.outputNodes);\n    return this.addStructuredOutputNames(outputTensors);\n  }\n\n  /**\n   * Execute the inference for the input tensors in async fashion, use this\n   * method when your model contains control flow ops.\n   *\n   * @param input The input tensors, when there is single input for the model,\n   * inputs param should be a `tf.Tensor`. For models with mutliple inputs,\n   * inputs params should be in either `tf.Tensor`[] if the input order is\n   * fixed, or otherwise NamedTensorMap format.\n   *\n   * For model with multiple inputs, we recommend you use NamedTensorMap as the\n   * input type, if you use `tf.Tensor`[], the order of the array needs to\n   * follow the\n   * order of inputNodes array. @see {@link GraphModel.inputNodes}\n   *\n   * You can also feed any intermediate nodes using the NamedTensorMap as the\n   * input type. For example, given the graph\n   *    InputNode => Intermediate => OutputNode,\n   * you can execute the subgraph Intermediate => OutputNode by calling\n   *    model.execute('IntermediateNode' : tf.tensor(...));\n   *\n   * This is useful for models that uses tf.dynamic_rnn, where the intermediate\n   * state needs to be fed manually.\n   *\n   * For batch inference execution, the tensors for each input need to be\n   * concatenated together. For example with mobilenet, the required input shape\n   * is [1, 244, 244, 3], which represents the [batch, height, width, channel].\n   * If we are provide a batched data of 100 images, the input tensor should be\n   * in the shape of [100, 244, 244, 3].\n   *\n   * @param config Prediction configuration for specifying the batch size.\n   * Currently the batch size option is ignored for graph model.\n   *\n   * @returns A Promise of inference result tensors. If the model is converted\n   * and it originally had structured_outputs in tensorflow, then a\n   * NamedTensorMap will be returned matching the structured_outputs. If no\n   * structured_outputs are present, the output will be single `tf.Tensor` if\n   * the model has single output node, otherwise Tensor[].\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  async predictAsync(\n      inputs: Tensor|Tensor[]|NamedTensorMap,\n      config?: ModelPredictConfig): Promise<Tensor|Tensor[]|NamedTensorMap> {\n    const outputTensors = await this.executeAsync(inputs, this.outputNodes);\n    return this.addStructuredOutputNames(outputTensors);\n  }\n\n  private normalizeInputs(inputs: Tensor|Tensor[]|\n                          NamedTensorMap): NamedTensorMap {\n    if (!(inputs instanceof Tensor) && !Array.isArray(inputs)) {\n      // The input is already a NamedTensorMap.\n      const signatureInputs = this.signature?.inputs;\n      if (signatureInputs != null) {\n        for (const input in signatureInputs) {\n          const tensor = signatureInputs[input];\n          if (tensor.resourceId != null) {\n            inputs[input] = this.resourceIdToCapturedInput[tensor.resourceId];\n          }\n        }\n      }\n      return inputs;\n    }\n    inputs = Array.isArray(inputs) ? inputs : [inputs];\n\n    const numCapturedInputs =\n        Object.keys(this.resourceIdToCapturedInput).length;\n    if (inputs.length + numCapturedInputs !== this.inputNodes.length) {\n      throw new Error(`Input tensor count mismatch, the graph model has ${\n          this.inputNodes.length -\n          numCapturedInputs} non-resource placeholders, while there are ${\n          inputs.length} input tensors provided.`);\n    }\n\n    let inputIndex = 0;\n    return this.inputNodes.reduce((map, inputName) => {\n      const resourceId = this.signature?.inputs?.[inputName]?.resourceId;\n      if (resourceId != null) {\n        map[inputName] = this.resourceIdToCapturedInput[resourceId];\n      } else {\n        map[inputName] = (inputs as Tensor[])[inputIndex++];\n      }\n      return map;\n    }, {} as NamedTensorMap);\n  }\n\n  private normalizeOutputs(outputs: string|string[]): string[] {\n    outputs = outputs || this.outputNodes;\n    return !Array.isArray(outputs) ? [outputs] : outputs;\n  }\n\n  private executeInitializerGraph() {\n    if (this.initializer == null) {\n      return [];\n    }\n    if (this.initializerSignature == null) {\n      return this.initializer.execute({}, []);\n    } else {\n      return this.initializer.execute(\n          {}, Object.keys(this.initializerSignature.outputs));\n    }\n  }\n\n  private async executeInitializerGraphAsync() {\n    if (this.initializer == null) {\n      return [];\n    }\n    if (this.initializerSignature == null) {\n      return this.initializer.executeAsync({}, []);\n    } else {\n      return this.initializer.executeAsync(\n          {}, Object.keys(this.initializerSignature.outputs));\n    }\n  }\n\n  private setResourceIdToCapturedInput(outputs: Tensor[]) {\n    this.resourceIdToCapturedInput = {};\n\n    if (this.initializerSignature) {\n      const signatureOutputs = this.initializerSignature.outputs;\n      const outputNames = Object.keys(signatureOutputs);\n      for (let i = 0; i < outputNames.length; i++) {\n        const outputName = outputNames[i];\n        const tensorInfo = signatureOutputs[outputName];\n        this.resourceIdToCapturedInput[tensorInfo.resourceId] = outputs[i];\n      }\n    }\n  }\n\n  /**\n   * Executes inference for the model for given input tensors.\n   * @param inputs tensor, tensor array or tensor map of the inputs for the\n   * model, keyed by the input node names.\n   * @param outputs output node name from the TensorFlow model, if no\n   * outputs are specified, the default outputs of the model would be used.\n   * You can inspect intermediate nodes of the model by adding them to the\n   * outputs array.\n   *\n   * @returns A single tensor if provided with a single output or no outputs\n   * are provided and there is only one default output, otherwise return a\n   * tensor array. The order of the tensor array is the same as the outputs\n   * if provided, otherwise the order of outputNodes attribute of the model.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  execute(inputs: Tensor|Tensor[]|NamedTensorMap, outputs?: string|string[]):\n      Tensor|Tensor[] {\n    if (this.resourceIdToCapturedInput == null) {\n      this.setResourceIdToCapturedInput(this.executeInitializerGraph());\n    }\n    inputs = this.normalizeInputs(inputs);\n    outputs = this.normalizeOutputs(outputs);\n    const result = this.executor.execute(inputs, outputs);\n    return result.length > 1 ? result : result[0];\n  }\n\n  /**\n   * Executes inference for the model for given input tensors in async\n   * fashion, use this method when your model contains control flow ops.\n   * @param inputs tensor, tensor array or tensor map of the inputs for the\n   * model, keyed by the input node names.\n   * @param outputs output node name from the TensorFlow model, if no outputs\n   * are specified, the default outputs of the model would be used. You can\n   * inspect intermediate nodes of the model by adding them to the outputs\n   * array.\n   *\n   * @returns A Promise of single tensor if provided with a single output or\n   * no outputs are provided and there is only one default output, otherwise\n   * return a tensor map.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  async executeAsync(\n      inputs: Tensor|Tensor[]|NamedTensorMap,\n      outputs?: string|string[]): Promise<Tensor|Tensor[]> {\n    if (this.resourceIdToCapturedInput == null) {\n      this.setResourceIdToCapturedInput(\n          await this.executeInitializerGraphAsync());\n    }\n    inputs = this.normalizeInputs(inputs);\n    outputs = this.normalizeOutputs(outputs);\n    const result = await this.executor.executeAsync(inputs, outputs);\n    return result.length > 1 ? result : result[0];\n  }\n\n  /**\n   * Get intermediate tensors for model debugging mode (flag\n   * KEEP_INTERMEDIATE_TENSORS is true).\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  getIntermediateTensors(): NamedTensorsMap {\n    return this.executor.getIntermediateTensors();\n  }\n\n  /**\n   * Dispose intermediate tensors for model debugging mode (flag\n   * KEEP_INTERMEDIATE_TENSORS is true).\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  disposeIntermediateTensors() {\n    this.executor.disposeIntermediateTensors();\n  }\n\n  private convertTensorMapToTensorsMap(map: NamedTensorMap): NamedTensorsMap {\n    return Object.keys(map).reduce((newMap: NamedTensorsMap, key) => {\n      newMap[key] = [map[key]];\n      return newMap;\n    }, {});\n  }\n\n  /**\n   * Releases the memory used by the weight tensors and resourceManager.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  dispose() {\n    this.executor.dispose();\n\n    if (this.initializer) {\n      this.initializer.dispose();\n      if (this.resourceIdToCapturedInput) {\n        dispose(this.resourceIdToCapturedInput);\n      }\n    }\n\n    this.resourceManager.dispose();\n  }\n}\n\n/**\n * Load a graph model given a URL to the model definition.\n *\n * Example of loading MobileNetV2 from a URL and making a prediction with a\n * zeros input:\n *\n * ```js\n * const modelUrl =\n *    'https://storage.googleapis.com/tfjs-models/savedmodel/mobilenet_v2_1.0_224/model.json';\n * const model = await tf.loadGraphModel(modelUrl);\n * const zeros = tf.zeros([1, 224, 224, 3]);\n * model.predict(zeros).print();\n * ```\n *\n * Example of loading MobileNetV2 from a TF Hub URL and making a prediction\n * with a zeros input:\n *\n * ```js\n * const modelUrl =\n *    'https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/classification/2';\n * const model = await tf.loadGraphModel(modelUrl, {fromTFHub: true});\n * const zeros = tf.zeros([1, 224, 224, 3]);\n * model.predict(zeros).print();\n * ```\n * @param modelUrl The url or an `io.IOHandler` that loads the model.\n * @param options Options for the HTTP request, which allows to send\n *     credentials\n *    and custom headers.\n *\n * @doc {heading: 'Models', subheading: 'Loading'}\n */\nexport async function loadGraphModel(\n    modelUrl: string|io.IOHandler, options: io.LoadOptions = {},\n    tfio = io): Promise<GraphModel> {\n  if (modelUrl == null) {\n    throw new Error(\n        'modelUrl in loadGraphModel() cannot be null. Please provide a url ' +\n        'or an IOHandler that loads the model');\n  }\n  if (options == null) {\n    options = {};\n  }\n\n  if (options.fromTFHub && typeof modelUrl === 'string') {\n    modelUrl = getTFHubUrl(modelUrl);\n  }\n  const model = new GraphModel(modelUrl, options, tfio);\n  await model.load();\n  return model;\n}\n\n/**\n * Load a graph model given a synchronous IO handler with a 'load' method.\n *\n * @param modelSource The `io.IOHandlerSync` that loads the model, or the\n *     `io.ModelArtifacts` that encode the model, or a tuple of\n *     `[io.ModelJSON, ArrayBuffer]` of which the first element encodes the\n *      model and the second contains the weights.\n *\n * @doc {heading: 'Models', subheading: 'Loading'}\n */\nexport function loadGraphModelSync(\n    modelSource: io.IOHandlerSync|\n    io.ModelArtifacts|[io.ModelJSON, /* Weights */ ArrayBuffer]):\n    GraphModel<io.IOHandlerSync> {\n  if (modelSource == null) {\n    throw new Error(\n        'modelUrl in loadGraphModelSync() cannot be null. Please provide ' +\n        'model artifacts or an IOHandler that loads the model');\n  }\n\n  let ioHandler: io.IOHandlerSync;\n  if (modelSource instanceof Array) {\n    const [modelJSON, weights] = modelSource;\n    if (!modelJSON) {\n      throw new Error('modelJSON must be the first element of the array');\n    }\n    if (!weights || !(weights instanceof ArrayBuffer)) {\n      throw new Error(\n          'An ArrayBuffer of weights must be the second element of' +\n          ' the array');\n    }\n    if (!('modelTopology' in modelJSON)) {\n      throw new Error('Model JSON is missing \\'modelTopology\\'');\n    }\n    if (!('weightsManifest' in modelJSON)) {\n      throw new Error('Model JSON is missing \\'weightsManifest\\'');\n    }\n\n    const weightSpecs = io.getWeightSpecs(modelJSON.weightsManifest);\n    const modelArtifacts =\n        io.getModelArtifactsForJSONSync(modelJSON, weightSpecs, weights);\n    ioHandler = io.fromMemorySync(modelArtifacts);\n  } else if ('load' in modelSource) {\n    // Then modelSource is already an IOHandlerSync.\n    ioHandler = modelSource;\n  } else if (\n      'modelTopology' in modelSource && 'weightSpecs' in modelSource &&\n      'weightData' in modelSource) {\n    // modelSource is of type ModelArtifacts.\n    ioHandler = io.fromMemorySync(modelSource);\n  } else {\n    throw new Error('Unknown model format');\n  }\n\n  const model = new GraphModel(ioHandler);\n  model.load();\n  return model;\n}\n\nfunction getTFHubUrl(modelUrl: string): string {\n  if (!modelUrl.endsWith('/')) {\n    modelUrl = (modelUrl) + '/';\n  }\n  return `${modelUrl}${DEFAULT_MODEL_NAME}${TFHUB_SEARCH_PARAM}`;\n}\n"],"mappings":";;;;;AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQA,OAAO,IAAPA,QAAO,EAAkBC,EAAE,EAAsCC,MAAM,EAAEC,IAAI,QAAO,uBAAuB;AAInH,SAAQC,eAAe,QAAO,gCAAgC;AAE9D,SAAQC,aAAa,QAAO,kBAAkB;AAC9C,SAAQC,eAAe,QAAO,oBAAoB;AAElD,OAAO,IAAMC,kBAAkB,GAAG,mBAAmB;AACrD,OAAO,IAAMC,kBAAkB,GAAG,YAAY;AAI9C;;;;;;;;;;AAUA,WAAaC,UAAU;EAmDrB;;;;;;;;;EASA,SAAAA,WACYC,QAAkB,EACjB;IAAA,IAD2BC,WAAA,GAAAC,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAA8B,EAAE;IAAA,IACpEG,IAAI,GAAAH,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAGX,EAAE;IAAAe,eAAA,OAAAP,UAAA;IADD,KAAAC,QAAQ,GAARA,QAAQ;IAAoB,KAAAC,WAAW,GAAXA,WAAW;IA1D3C,KAAAM,OAAO,GAAG,KAAK;IA4DrB,IAAI,CAAChB,EAAE,GAAGc,IAAI;IACd,IAAIJ,WAAW,IAAI,IAAI,EAAE;MACvB,IAAI,CAACA,WAAW,GAAG,EAAE;;IAEvB,IAAI,CAACO,eAAe,GAAG,IAAIZ,eAAe,EAAE;EAC9C;EAACa,YAAA,CAAAV,UAAA;IAAAW,GAAA;IAAAC,GAAA;IAtDD;IACA,SAAAA,IAAA,EAAgB;MACd,OAAO,IAAI,CAACJ,OAAO;IACrB;EAAC;IAAAG,GAAA;IAAAC,GAAA,EAED,SAAAA,IAAA,EAAc;MACZ,OAAO,IAAI,CAACC,QAAQ,CAACC,UAAU;IACjC;EAAC;IAAAH,GAAA;IAAAC,GAAA,EAED,SAAAA,IAAA,EAAe;MACb,OAAO,IAAI,CAACC,QAAQ,CAACE,WAAW;IAClC;EAAC;IAAAJ,GAAA;IAAAC,GAAA,EAED,SAAAA,IAAA,EAAU;MACR,OAAO,IAAI,CAACC,QAAQ,CAACG,MAAM;IAC7B;EAAC;IAAAL,GAAA;IAAAC,GAAA,EAED,SAAAA,IAAA,EAAW;MACT,OAAO,IAAI,CAACC,QAAQ,CAACI,OAAO;IAC9B;EAAC;IAAAN,GAAA;IAAAC,GAAA,EAED,SAAAA,IAAA,EAAW;MACT,OAAO,IAAI,CAACC,QAAQ,CAACK,SAAS;IAChC;EAAC;IAAAP,GAAA;IAAAC,GAAA,EAED,SAAAA,IAAA,EAAY;MACV,OAAO,IAAI,CAACO,SAAS,CAACC,mBAAmB;IAC3C;EAAC;IAAAT,GAAA;IAAAC,GAAA,EAED,SAAAA,IAAA,EAAkB;MAChB,OAAO,IAAI,CAACS,SAAS;IACvB;EAAC;IAAAV,GAAA;IAAAC,GAAA,EAED,SAAAA,IAAA,EAA6B;MAC3B,OAAO,IAAI,CAACU,oBAAoB;IAClC;EAAC;IAAAX,GAAA;IAAAY,KAAA,EAqBO,SAAAC,cAAA,EAAa;MAEnB,IAAMC,IAAI,GAAG,IAAI,CAACxB,QAAQ;MAC1B,IAAKwB,IAAqB,CAACC,IAAI,IAAI,IAAI,EAAE;QACvC;QACA,IAAI,CAACC,OAAO,GAAGF,IAAiB;OACjC,MAAM,IAAI,IAAI,CAACvB,WAAW,CAAC0B,WAAW,IAAI,IAAI,EAAE;QAC/C,IAAI,CAACD,OAAO,GAAG,IAAI,CAACnC,EAAE,CAACqC,kBAAkB,CACtBJ,IAAc,EAAE,IAAI,CAACvB,WAAW,CAAc;OAClE,MAAM;QACL,IAAM4B,QAAQ,GACV,IAAI,CAACtC,EAAE,CAACuC,eAAe,CAACN,IAAc,EAAE,IAAI,CAACvB,WAAW,CAAC;QAC7D,IAAI4B,QAAQ,CAAC1B,MAAM,KAAK,CAAC,EAAE;UACzB;UACA;UACA0B,QAAQ,CAACE,IAAI,CACT,IAAI,CAACxC,EAAE,CAACqC,kBAAkB,CAACJ,IAAc,EAAE,IAAI,CAACvB,WAAW,CAAC,CAAC;SAClE,MAAM,IAAI4B,QAAQ,CAAC1B,MAAM,GAAG,CAAC,EAAE;UAC9B,MAAM,IAAI6B,KAAK,CACX,wBAAAC,MAAA,CAAwBJ,QAAQ,CAAC1B,MAAM,oCAAA8B,MAAA,CAC/B,CAACT,IAAI,CAAC,MAAG,CAAC;;QAExB,IAAI,CAACE,OAAO,GAAGG,QAAQ,CAAC,CAAC,CAAc;;IAE3C;IAEA;;;;EAAA;IAAAnB,GAAA;IAAAY,KAAA,EAIA,SAAAG,KAAA,EAAI;MAAA,IAAAS,KAAA;MAGF,IAAI,CAACX,aAAa,EAAE;MACpB,IAAI,IAAI,CAACG,OAAO,CAACD,IAAI,IAAI,IAAI,EAAE;QAC7B,MAAM,IAAIO,KAAK,CACX,mEAAmE,GACnE,8CAA8C,CAAC;;MAMrD,IAAMG,UAAU,GAAG,IAAI,CAACT,OAAO,CAACD,IAAI,EAAmC;MACvE,IAAIhC,IAAI,CAAC2C,SAAS,CAACD,UAAU,CAAC,EAAE;QAC9B,OAAOA,UAAU,CAACE,IAAI,CAAC,UAAAnB,SAAS;UAAA,OAAIgB,KAAI,CAACI,QAAQ,CAACpB,SAAS,CAAC;QAAA,EAAW;;MAGzE,OAAO,IAAI,CAACoB,QAAQ,CAACH,UAAU,CAAW;IAC5C;IAEA;;;;;;EAAA;IAAAzB,GAAA;IAAAY,KAAA,EAMA,SAAAgB,SAASpB,SAA4B;MACnC,IAAI,CAACA,SAAS,GAAGA,SAAS;MAC1B,IAAMqB,KAAK,GAAG,IAAI,CAACrB,SAAS,CAACsB,aAAqC;MAElE,IAAIpB,SAAS,GAAG,IAAI,CAACF,SAAS,CAACE,SAAS;MACxC,IAAI,IAAI,CAACF,SAAS,CAACC,mBAAmB,IAAI,IAAI,EAAE;QAC9C,IAAMsB,QAAQ,GAAG,IAAI,CAACvB,SAAS,CAACC,mBAAmB;QACnD,IAAIsB,QAAQ,CAACrB,SAAS,IAAI,IAAI,EAAE;UAC9BA,SAAS,GAAGqB,QAAQ,CAACrB,SAAS;;QAGhC,IAAIqB,QAAQ,CAACpB,oBAAoB,IAAI,IAAI,EAAE;UACzC,IAAI,CAACA,oBAAoB,GAAGoB,QAAQ,CAACpB,oBAAgC;;;MAGzE,IAAI,CAACD,SAAS,GAAGA,SAAS;MAE1B,IAAI,CAACb,OAAO,MAAA0B,MAAA,CAAMM,KAAK,CAACG,QAAQ,CAACC,QAAQ,OAAAV,MAAA,CAAIM,KAAK,CAACG,QAAQ,CAACE,WAAW,CAAE;MACzE,IAAM3B,SAAS,GAAG,IAAI,CAAC1B,EAAE,CAACsD,aAAa,CACnC,IAAI,CAAC3B,SAAS,CAAC4B,UAAU,EAAE,IAAI,CAAC5B,SAAS,CAAC6B,WAAW,CAAC;MAC1D,IAAI,CAACnC,QAAQ,GAAG,IAAIjB,aAAa,CAC7BD,eAAe,CAACsD,QAAQ,CAACC,cAAc,CAACV,KAAK,EAAE,IAAI,CAACnB,SAAS,CAAC,CAAC;MACnE,IAAI,CAACR,QAAQ,CAACK,SAAS,GAAG,IAAI,CAACiC,4BAA4B,CAACjC,SAAS,CAAC;MACtE;MACA;MACA,IAAI,CAACL,QAAQ,CAACJ,eAAe,GAAG,IAAI,CAACA,eAAe;MAEpD,IAAIU,SAAS,CAACiC,gBAAgB,IAAI,IAAI,IACjCjC,SAAS,CAACiC,gBAAyC,CAACC,IAAI,IAAI,IAAI,EAAE;QACrE,IAAMC,WAAW,GACb3D,eAAe,CAACsD,QAAQ,CAACC,cAAc,CAAC/B,SAAS,CAACiC,gBAAgB,CAAC;QACvE,IAAI,CAACE,WAAW,GAAG,IAAI1D,aAAa,CAAC0D,WAAW,CAAC;QACjD,IAAI,CAACA,WAAW,CAACpC,SAAS,GAAG,IAAI,CAACL,QAAQ,CAACK,SAAS;QACpD;QACA;QACA;QACA,IAAI,CAACoC,WAAW,CAAC7C,eAAe,GAAG,IAAI,CAACA,eAAe;QACvD,IAAI,CAAC8C,oBAAoB,GAAGpC,SAAS,CAACoC,oBAAoB;;MAG5D,OAAO,IAAI;IACb;IAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;EAAA;IAAA5C,GAAA;IAAAY,KAAA;MAAA,IAAAiC,KAAA,GAAAC,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CA4CA,SAAAC,QAAWC,YAAiC,EAAEC,MAAsB;QAAA,IAAAhC,QAAA;QAAA,OAAA4B,mBAAA,GAAAK,IAAA,UAAAC,SAAAC,QAAA;UAAA,kBAAAA,QAAA,CAAAC,IAAA,GAAAD,QAAA,CAAAE,IAAA;YAAA;cAAA,MAE9D,OAAON,YAAY,KAAK,QAAQ;gBAAAI,QAAA,CAAAE,IAAA;gBAAA;cAAA;cAC5BrC,QAAQ,GAAG,IAAI,CAACtC,EAAE,CAAC4E,eAAe,CAACP,YAAY,CAAC;cAAA,MAClD/B,QAAQ,CAAC1B,MAAM,KAAK,CAAC;gBAAA6D,QAAA,CAAAE,IAAA;gBAAA;cAAA;cAAA,MACjB,IAAIlC,KAAK,2CAAAC,MAAA,CAC+B2B,YAAY,OAAI;YAAA;cAAA,MACrD/B,QAAQ,CAAC1B,MAAM,GAAG,CAAC;gBAAA6D,QAAA,CAAAE,IAAA;gBAAA;cAAA;cAAA,MACtB,IAAIlC,KAAK,CACX,wBAAAC,MAAA,CAAwBJ,QAAQ,CAAC1B,MAAM,oCAAA8B,MAAA,CAC/B2B,YAAY,MAAG,CAAC;YAAA;cAE9BA,YAAY,GAAG/B,QAAQ,CAAC,CAAC,CAAC;YAAC;cAAA,MAEzB+B,YAAY,CAACQ,IAAI,IAAI,IAAI;gBAAAJ,QAAA,CAAAE,IAAA;gBAAA;cAAA;cAAA,MACrB,IAAIlC,KAAK,CACX,yDAAyD,GACzD,sDAAsD,CAAC;YAAA;cAAA,OAAAgC,QAAA,CAAAK,MAAA,WAGtDT,YAAY,CAACQ,IAAI,CAAC,IAAI,CAAClD,SAAS,CAAC;YAAA;YAAA;cAAA,OAAA8C,QAAA,CAAAM,IAAA;UAAA;QAAA,GAAAX,OAAA;MAAA,CACzC;MAAA,SAAAS,KAAAG,EAAA,EAAAC,GAAA;QAAA,OAAAjB,KAAA,CAAAkB,KAAA,OAAAvE,SAAA;MAAA;MAAA,OAAAkE,IAAA;IAAA;EAAA;IAAA1D,GAAA;IAAAY,KAAA,EAEO,SAAAoD,yBAAyBC,aAA8B;MAAA,IAAAC,MAAA;MAC7D,IAAI,IAAI,CAACvD,oBAAoB,EAAE;QAC7B,IAAMwD,kBAAkB,GACpBF,aAAa,YAAYnF,MAAM,GAAG,CAACmF,aAAa,CAAC,GAAGA,aAAa;QACrE,IAAMG,eAAe,GAAmB,EAAE;QAE1CD,kBAAkB,CAACE,OAAO,CACtB,UAACC,YAAY,EAAEC,CAAC;UAAA,OAAKH,eAAe,CAACF,MAAI,CAACvD,oBAAoB,CAAC4D,CAAC,CAAC,CAAC,GAC9DD,YAAY;QAAA,EAAC;QAErB,OAAOF,eAAe;;MAExB,OAAOH,aAAa;IACtB;IAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;EAAA;IAAAjE,GAAA;IAAAY,KAAA,EAuCA,SAAA4D,QAAQnE,MAAsC,EAAE8C,MAA2B;MAEzE,IAAMc,aAAa,GAAG,IAAI,CAACQ,OAAO,CAACpE,MAAM,EAAE,IAAI,CAACD,WAAW,CAAC;MAC5D,OAAO,IAAI,CAAC4D,wBAAwB,CAACC,aAAa,CAAC;IACrD;IAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;EAAA;IAAAjE,GAAA;IAAAY,KAAA;MAAA,IAAA8D,aAAA,GAAA5B,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CAwCA,SAAA2B,SACItE,MAAsC,EACtC8C,MAA2B;QAAA,IAAAc,aAAA;QAAA,OAAAlB,mBAAA,GAAAK,IAAA,UAAAwB,UAAAC,SAAA;UAAA,kBAAAA,SAAA,CAAAtB,IAAA,GAAAsB,SAAA,CAAArB,IAAA;YAAA;cAAAqB,SAAA,CAAArB,IAAA;cAAA,OACD,IAAI,CAACsB,YAAY,CAACzE,MAAM,EAAE,IAAI,CAACD,WAAW,CAAC;YAAA;cAAjE6D,aAAa,GAAAY,SAAA,CAAAE,IAAA;cAAA,OAAAF,SAAA,CAAAlB,MAAA,WACZ,IAAI,CAACK,wBAAwB,CAACC,aAAa,CAAC;YAAA;YAAA;cAAA,OAAAY,SAAA,CAAAjB,IAAA;UAAA;QAAA,GAAAe,QAAA;MAAA,CACpD;MAAA,SAAAK,aAAAC,GAAA,EAAAC,GAAA;QAAA,OAAAR,aAAA,CAAAX,KAAA,OAAAvE,SAAA;MAAA;MAAA,OAAAwF,YAAA;IAAA;EAAA;IAAAhF,GAAA;IAAAY,KAAA,EAEO,SAAAuE,gBAAgB9E,MACc;MAAA,IAAA+E,MAAA;;MACpC,IAAI,EAAE/E,MAAM,YAAYvB,MAAM,CAAC,IAAI,CAACuG,KAAK,CAACC,OAAO,CAACjF,MAAM,CAAC,EAAE;QACzD;QACA,IAAMkF,eAAe,GAAG,CAAAC,EAAA,OAAI,CAAC9E,SAAS,cAAA8E,EAAA,uBAAAA,EAAA,CAAEnF,MAAM;QAC9C,IAAIkF,eAAe,IAAI,IAAI,EAAE;UAC3B,KAAK,IAAME,KAAK,IAAIF,eAAe,EAAE;YACnC,IAAMG,MAAM,GAAGH,eAAe,CAACE,KAAK,CAAC;YACrC,IAAIC,MAAM,CAACC,UAAU,IAAI,IAAI,EAAE;cAC7BtF,MAAM,CAACoF,KAAK,CAAC,GAAG,IAAI,CAACG,yBAAyB,CAACF,MAAM,CAACC,UAAU,CAAC;;;;QAIvE,OAAOtF,MAAM;;MAEfA,MAAM,GAAGgF,KAAK,CAACC,OAAO,CAACjF,MAAM,CAAC,GAAGA,MAAM,GAAG,CAACA,MAAM,CAAC;MAElD,IAAMwF,iBAAiB,GACnBC,MAAM,CAACC,IAAI,CAAC,IAAI,CAACH,yBAAyB,CAAC,CAACnG,MAAM;MACtD,IAAIY,MAAM,CAACZ,MAAM,GAAGoG,iBAAiB,KAAK,IAAI,CAAC1F,UAAU,CAACV,MAAM,EAAE;QAChE,MAAM,IAAI6B,KAAK,qDAAAC,MAAA,CACX,IAAI,CAACpB,UAAU,CAACV,MAAM,GACtBoG,iBAAiB,kDAAAtE,MAAA,CACjBlB,MAAM,CAACZ,MAAM,8BAA2B;;MAG9C,IAAIuG,UAAU,GAAG,CAAC;MAClB,OAAO,IAAI,CAAC7F,UAAU,CAAC8F,MAAM,CAAC,UAACC,GAAG,EAAEC,SAAS,EAAI;;QAC/C,IAAMR,UAAU,GAAG,CAAAS,EAAA,IAAAC,EAAA,IAAAb,EAAA,GAAAJ,MAAI,CAAC1E,SAAS,cAAA8E,EAAA,uBAAAA,EAAA,CAAEnF,MAAM,cAAAgG,EAAA,uBAAAA,EAAA,CAAGF,SAAS,CAAC,cAAAC,EAAA,uBAAAA,EAAA,CAAET,UAAU;QAClE,IAAIA,UAAU,IAAI,IAAI,EAAE;UACtBO,GAAG,CAACC,SAAS,CAAC,GAAGf,MAAI,CAACQ,yBAAyB,CAACD,UAAU,CAAC;SAC5D,MAAM;UACLO,GAAG,CAACC,SAAS,CAAC,GAAI9F,MAAmB,CAAC2F,UAAU,EAAE,CAAC;;QAErD,OAAOE,GAAG;MACZ,CAAC,EAAE,EAAoB,CAAC;IAC1B;EAAC;IAAAlG,GAAA;IAAAY,KAAA,EAEO,SAAA0F,iBAAiBhG,OAAwB;MAC/CA,OAAO,GAAGA,OAAO,IAAI,IAAI,CAACF,WAAW;MACrC,OAAO,CAACiF,KAAK,CAACC,OAAO,CAAChF,OAAO,CAAC,GAAG,CAACA,OAAO,CAAC,GAAGA,OAAO;IACtD;EAAC;IAAAN,GAAA;IAAAY,KAAA,EAEO,SAAA2F,wBAAA,EAAuB;MAC7B,IAAI,IAAI,CAAC5D,WAAW,IAAI,IAAI,EAAE;QAC5B,OAAO,EAAE;;MAEX,IAAI,IAAI,CAACC,oBAAoB,IAAI,IAAI,EAAE;QACrC,OAAO,IAAI,CAACD,WAAW,CAAC8B,OAAO,CAAC,EAAE,EAAE,EAAE,CAAC;OACxC,MAAM;QACL,OAAO,IAAI,CAAC9B,WAAW,CAAC8B,OAAO,CAC3B,EAAE,EAAEqB,MAAM,CAACC,IAAI,CAAC,IAAI,CAACnD,oBAAoB,CAACtC,OAAO,CAAC,CAAC;;IAE3D;EAAC;IAAAN,GAAA;IAAAY,KAAA;MAAA,IAAA4F,6BAAA,GAAA1D,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CAEO,SAAAyD,SAAA;QAAA,OAAA1D,mBAAA,GAAAK,IAAA,UAAAsD,UAAAC,SAAA;UAAA,kBAAAA,SAAA,CAAApD,IAAA,GAAAoD,SAAA,CAAAnD,IAAA;YAAA;cAAA,MACF,IAAI,CAACb,WAAW,IAAI,IAAI;gBAAAgE,SAAA,CAAAnD,IAAA;gBAAA;cAAA;cAAA,OAAAmD,SAAA,CAAAhD,MAAA,WACnB,EAAE;YAAA;cAAA,MAEP,IAAI,CAACf,oBAAoB,IAAI,IAAI;gBAAA+D,SAAA,CAAAnD,IAAA;gBAAA;cAAA;cAAA,OAAAmD,SAAA,CAAAhD,MAAA,WAC5B,IAAI,CAAChB,WAAW,CAACmC,YAAY,CAAC,EAAE,EAAE,EAAE,CAAC;YAAA;cAAA,OAAA6B,SAAA,CAAAhD,MAAA,WAErC,IAAI,CAAChB,WAAW,CAACmC,YAAY,CAChC,EAAE,EAAEgB,MAAM,CAACC,IAAI,CAAC,IAAI,CAACnD,oBAAoB,CAACtC,OAAO,CAAC,CAAC;YAAA;YAAA;cAAA,OAAAqG,SAAA,CAAA/C,IAAA;UAAA;QAAA,GAAA6C,QAAA;MAAA,CAE1D;MAAA,SAAAG,6BAAA;QAAA,OAAAJ,6BAAA,CAAAzC,KAAA,OAAAvE,SAAA;MAAA;MAAA,OAAAoH,4BAAA;IAAA;EAAA;IAAA5G,GAAA;IAAAY,KAAA,EAEO,SAAAiG,6BAA6BvG,OAAiB;MACpD,IAAI,CAACsF,yBAAyB,GAAG,EAAE;MAEnC,IAAI,IAAI,CAAChD,oBAAoB,EAAE;QAC7B,IAAMkE,gBAAgB,GAAG,IAAI,CAAClE,oBAAoB,CAACtC,OAAO;QAC1D,IAAMyG,WAAW,GAAGjB,MAAM,CAACC,IAAI,CAACe,gBAAgB,CAAC;QACjD,KAAK,IAAIvC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGwC,WAAW,CAACtH,MAAM,EAAE8E,CAAC,EAAE,EAAE;UAC3C,IAAMyC,UAAU,GAAGD,WAAW,CAACxC,CAAC,CAAC;UACjC,IAAM0C,UAAU,GAAGH,gBAAgB,CAACE,UAAU,CAAC;UAC/C,IAAI,CAACpB,yBAAyB,CAACqB,UAAU,CAACtB,UAAU,CAAC,GAAGrF,OAAO,CAACiE,CAAC,CAAC;;;IAGxE;IAEA;;;;;;;;;;;;;;;;EAAA;IAAAvE,GAAA;IAAAY,KAAA,EAgBA,SAAA6D,QAAQpE,MAAsC,EAAEC,OAAyB;MAEvE,IAAI,IAAI,CAACsF,yBAAyB,IAAI,IAAI,EAAE;QAC1C,IAAI,CAACiB,4BAA4B,CAAC,IAAI,CAACN,uBAAuB,EAAE,CAAC;;MAEnElG,MAAM,GAAG,IAAI,CAAC8E,eAAe,CAAC9E,MAAM,CAAC;MACrCC,OAAO,GAAG,IAAI,CAACgG,gBAAgB,CAAChG,OAAO,CAAC;MACxC,IAAM4G,MAAM,GAAG,IAAI,CAAChH,QAAQ,CAACuE,OAAO,CAACpE,MAAM,EAAEC,OAAO,CAAC;MACrD,OAAO4G,MAAM,CAACzH,MAAM,GAAG,CAAC,GAAGyH,MAAM,GAAGA,MAAM,CAAC,CAAC,CAAC;IAC/C;IAEA;;;;;;;;;;;;;;;;EAAA;IAAAlH,GAAA;IAAAY,KAAA;MAAA,IAAAuG,aAAA,GAAArE,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CAgBA,SAAAoE,SACI/G,MAAsC,EACtCC,OAAyB;QAAA,IAAA4G,MAAA;QAAA,OAAAnE,mBAAA,GAAAK,IAAA,UAAAiE,UAAAC,SAAA;UAAA,kBAAAA,SAAA,CAAA/D,IAAA,GAAA+D,SAAA,CAAA9D,IAAA;YAAA;cAAA,MACvB,IAAI,CAACoC,yBAAyB,IAAI,IAAI;gBAAA0B,SAAA,CAAA9D,IAAA;gBAAA;cAAA;cAAA8D,SAAA,CAAAC,EAAA,GACxC,IAAI;cAAAD,SAAA,CAAA9D,IAAA;cAAA,OACM,IAAI,CAACoD,4BAA4B,EAAE;YAAA;cAAAU,SAAA,CAAAE,EAAA,GAAAF,SAAA,CAAAvC,IAAA;cAAAuC,SAAA,CAAAC,EAAA,CADxCV,4BAA4B,CAAAY,IAAA,CAAAH,SAAA,CAAAC,EAAA,EAAAD,SAAA,CAAAE,EAAA;YAAA;cAGnCnH,MAAM,GAAG,IAAI,CAAC8E,eAAe,CAAC9E,MAAM,CAAC;cACrCC,OAAO,GAAG,IAAI,CAACgG,gBAAgB,CAAChG,OAAO,CAAC;cAACgH,SAAA,CAAA9D,IAAA;cAAA,OACpB,IAAI,CAACtD,QAAQ,CAAC4E,YAAY,CAACzE,MAAM,EAAEC,OAAO,CAAC;YAAA;cAA1D4G,MAAM,GAAAI,SAAA,CAAAvC,IAAA;cAAA,OAAAuC,SAAA,CAAA3D,MAAA,WACLuD,MAAM,CAACzH,MAAM,GAAG,CAAC,GAAGyH,MAAM,GAAGA,MAAM,CAAC,CAAC,CAAC;YAAA;YAAA;cAAA,OAAAI,SAAA,CAAA1D,IAAA;UAAA;QAAA,GAAAwD,QAAA;MAAA,CAC9C;MAAA,SAAAtC,aAAA4C,GAAA,EAAAC,GAAA;QAAA,OAAAR,aAAA,CAAApD,KAAA,OAAAvE,SAAA;MAAA;MAAA,OAAAsF,YAAA;IAAA;IAED;;;;;;EAAA;IAAA9E,GAAA;IAAAY,KAAA,EAMA,SAAAgH,uBAAA,EAAsB;MACpB,OAAO,IAAI,CAAC1H,QAAQ,CAAC0H,sBAAsB,EAAE;IAC/C;IAEA;;;;;;EAAA;IAAA5H,GAAA;IAAAY,KAAA,EAMA,SAAAiH,2BAAA,EAA0B;MACxB,IAAI,CAAC3H,QAAQ,CAAC2H,0BAA0B,EAAE;IAC5C;EAAC;IAAA7H,GAAA;IAAAY,KAAA,EAEO,SAAA4B,6BAA6B0D,GAAmB;MACtD,OAAOJ,MAAM,CAACC,IAAI,CAACG,GAAG,CAAC,CAACD,MAAM,CAAC,UAAC6B,MAAuB,EAAE9H,GAAG,EAAI;QAC9D8H,MAAM,CAAC9H,GAAG,CAAC,GAAG,CAACkG,GAAG,CAAClG,GAAG,CAAC,CAAC;QACxB,OAAO8H,MAAM;MACf,CAAC,EAAE,EAAE,CAAC;IACR;IAEA;;;;;EAAA;IAAA9H,GAAA;IAAAY,KAAA,EAKA,SAAAhC,QAAA,EAAO;MACL,IAAI,CAACsB,QAAQ,CAACtB,OAAO,EAAE;MAEvB,IAAI,IAAI,CAAC+D,WAAW,EAAE;QACpB,IAAI,CAACA,WAAW,CAAC/D,OAAO,EAAE;QAC1B,IAAI,IAAI,CAACgH,yBAAyB,EAAE;UAClChH,QAAO,CAAC,IAAI,CAACgH,yBAAyB,CAAC;;;MAI3C,IAAI,CAAC9F,eAAe,CAAClB,OAAO,EAAE;IAChC;EAAC;EAAA,OAAAS,UAAA;AAAA;AAGH;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA+BA,gBAAsB0I,cAAcA,CAAAC,GAAA;EAAA,OAAAC,eAAA,CAAAlE,KAAA,OAAAvE,SAAA;AAAA;AAoBpC;;;;;;;;;;AAAA,SAAAyI,gBAAA;EAAAA,eAAA,GAAAnF,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CApBO,SAAAkF,SACH5I,QAA6B;IAAA,IAAA6I,OAAA;MAAAxI,IAAA;MAAAyI,KAAA;MAAAC,MAAA,GAAA7I,SAAA;IAAA,OAAAuD,mBAAA,GAAAK,IAAA,UAAAkF,UAAAC,SAAA;MAAA,kBAAAA,SAAA,CAAAhF,IAAA,GAAAgF,SAAA,CAAA/E,IAAA;QAAA;UAAE2E,OAAA,GAAAE,MAAA,CAAA5I,MAAA,QAAA4I,MAAA,QAAA3I,SAAA,GAAA2I,MAAA,MAA0B,EAAE;UAC3D1I,IAAI,GAAA0I,MAAA,CAAA5I,MAAA,QAAA4I,MAAA,QAAA3I,SAAA,GAAA2I,MAAA,MAAGxJ,EAAE;UAAA,MACPS,QAAQ,IAAI,IAAI;YAAAiJ,SAAA,CAAA/E,IAAA;YAAA;UAAA;UAAA,MACZ,IAAIlC,KAAK,CACX,oEAAoE,GACpE,sCAAsC,CAAC;QAAA;UAE7C,IAAI6G,OAAO,IAAI,IAAI,EAAE;YACnBA,OAAO,GAAG,EAAE;;UAGd,IAAIA,OAAO,CAACK,SAAS,IAAI,OAAOlJ,QAAQ,KAAK,QAAQ,EAAE;YACrDA,QAAQ,GAAGmJ,WAAW,CAACnJ,QAAQ,CAAC;;UAE5B8I,KAAK,GAAG,IAAI/I,UAAU,CAACC,QAAQ,EAAE6I,OAAO,EAAExI,IAAI,CAAC;UAAA4I,SAAA,CAAA/E,IAAA;UAAA,OAC/C4E,KAAK,CAACrH,IAAI,EAAE;QAAA;UAAA,OAAAwH,SAAA,CAAA5E,MAAA,WACXyE,KAAK;QAAA;QAAA;UAAA,OAAAG,SAAA,CAAA3E,IAAA;MAAA;IAAA,GAAAsE,QAAA;EAAA,CACb;EAAA,OAAAD,eAAA,CAAAlE,KAAA,OAAAvE,SAAA;AAAA;AAYD,OAAM,SAAUkJ,kBAAkBA,CAC9BC,WAC2D;EAE7D,IAAIA,WAAW,IAAI,IAAI,EAAE;IACvB,MAAM,IAAIrH,KAAK,CACX,kEAAkE,GAClE,sDAAsD,CAAC;;EAG7D,IAAIsH,SAA2B;EAC/B,IAAID,WAAW,YAAYtD,KAAK,EAAE;IAChC,IAAAwD,YAAA,GAAAC,cAAA,CAA6BH,WAAW;MAAjCI,SAAS,GAAAF,YAAA;MAAEG,OAAO,GAAAH,YAAA;IACzB,IAAI,CAACE,SAAS,EAAE;MACd,MAAM,IAAIzH,KAAK,CAAC,kDAAkD,CAAC;;IAErE,IAAI,CAAC0H,OAAO,IAAI,EAAEA,OAAO,YAAYC,WAAW,CAAC,EAAE;MACjD,MAAM,IAAI3H,KAAK,CACX,yDAAyD,GACzD,YAAY,CAAC;;IAEnB,IAAI,EAAE,eAAe,IAAIyH,SAAS,CAAC,EAAE;MACnC,MAAM,IAAIzH,KAAK,CAAC,yCAAyC,CAAC;;IAE5D,IAAI,EAAE,iBAAiB,IAAIyH,SAAS,CAAC,EAAE;MACrC,MAAM,IAAIzH,KAAK,CAAC,2CAA2C,CAAC;;IAG9D,IAAMe,WAAW,GAAGxD,EAAE,CAACqK,cAAc,CAACH,SAAS,CAACI,eAAe,CAAC;IAChE,IAAMC,cAAc,GAChBvK,EAAE,CAACwK,4BAA4B,CAACN,SAAS,EAAE1G,WAAW,EAAE2G,OAAO,CAAC;IACpEJ,SAAS,GAAG/J,EAAE,CAACyK,cAAc,CAACF,cAAc,CAAC;GAC9C,MAAM,IAAI,MAAM,IAAIT,WAAW,EAAE;IAChC;IACAC,SAAS,GAAGD,WAAW;GACxB,MAAM,IACH,eAAe,IAAIA,WAAW,IAAI,aAAa,IAAIA,WAAW,IAC9D,YAAY,IAAIA,WAAW,EAAE;IAC/B;IACAC,SAAS,GAAG/J,EAAE,CAACyK,cAAc,CAACX,WAAW,CAAC;GAC3C,MAAM;IACL,MAAM,IAAIrH,KAAK,CAAC,sBAAsB,CAAC;;EAGzC,IAAM8G,KAAK,GAAG,IAAI/I,UAAU,CAACuJ,SAAS,CAAC;EACvCR,KAAK,CAACrH,IAAI,EAAE;EACZ,OAAOqH,KAAK;AACd;AAEA,SAASK,WAAWA,CAACnJ,QAAgB;EACnC,IAAI,CAACA,QAAQ,CAACiK,QAAQ,CAAC,GAAG,CAAC,EAAE;IAC3BjK,QAAQ,GAAIA,QAAQ,GAAI,GAAG;;EAE7B,UAAAiC,MAAA,CAAUjC,QAAQ,EAAAiC,MAAA,CAAGnC,kBAAkB,EAAAmC,MAAA,CAAGpC,kBAAkB;AAC9D"},"metadata":{},"sourceType":"module","externalDependencies":[]}