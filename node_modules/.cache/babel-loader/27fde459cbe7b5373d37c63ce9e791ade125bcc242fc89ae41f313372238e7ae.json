{"ast":null,"code":"import _regeneratorRuntime from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/regeneratorRuntime.js\";\nimport _asyncToGenerator from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/asyncToGenerator.js\";\nimport _classCallCheck from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/classCallCheck.js\";\nimport _createClass from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/createClass.js\";\nimport _inherits from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/inherits.js\";\nimport _createSuper from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/createSuper.js\";\n/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { dispose as _dispose, tidy } from '../globals';\nimport { add } from '../ops/add';\nimport { mul } from '../ops/mul';\nimport { scalar } from '../ops/scalar';\nimport { zerosLike } from '../ops/zeros_like';\nimport { SGDOptimizer } from './sgd_optimizer';\n/** @doclink Optimizer */\nexport var MomentumOptimizer = /*#__PURE__*/function (_SGDOptimizer) {\n  _inherits(MomentumOptimizer, _SGDOptimizer);\n  var _super = _createSuper(MomentumOptimizer);\n  function MomentumOptimizer(learningRate, momentum) {\n    var _this;\n    var useNesterov = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : false;\n    _classCallCheck(this, MomentumOptimizer);\n    _this = _super.call(this, learningRate);\n    _this.learningRate = learningRate;\n    _this.momentum = momentum;\n    _this.useNesterov = useNesterov;\n    _this.accumulations = [];\n    _this.m = scalar(_this.momentum);\n    return _this;\n  }\n  _createClass(MomentumOptimizer, [{\n    key: \"applyGradients\",\n    value: function applyGradients(variableGradients) {\n      var _this2 = this;\n      var variableNames = Array.isArray(variableGradients) ? variableGradients.map(function (item) {\n        return item.name;\n      }) : Object.keys(variableGradients);\n      variableNames.forEach(function (name, i) {\n        var value = ENGINE.registeredVariables[name];\n        if (_this2.accumulations[i] == null) {\n          var trainable = false;\n          _this2.accumulations[i] = {\n            originalName: \"\".concat(name, \"/momentum\"),\n            variable: tidy(function () {\n              return zerosLike(value).variable(trainable);\n            })\n          };\n        }\n        var accumulation = _this2.accumulations[i].variable;\n        var gradient = Array.isArray(variableGradients) ? variableGradients[i].tensor : variableGradients[name];\n        if (gradient == null) {\n          return;\n        }\n        tidy(function () {\n          var newValue;\n          var newAccumulation = add(mul(_this2.m, accumulation), gradient);\n          if (_this2.useNesterov) {\n            newValue = add(mul(_this2.c, add(gradient, mul(newAccumulation, _this2.m))), value);\n          } else {\n            newValue = add(mul(_this2.c, newAccumulation), value);\n          }\n          accumulation.assign(newAccumulation);\n          value.assign(newValue);\n        });\n      });\n      this.incrementIterations();\n    }\n  }, {\n    key: \"dispose\",\n    value: function dispose() {\n      this.m.dispose();\n      if (this.accumulations != null) {\n        _dispose(this.accumulations.map(function (v) {\n          return v.variable;\n        }));\n      }\n    }\n    /**\n     * Sets the momentum of the optimizer.\n     *\n     * @param momentum\n     */\n  }, {\n    key: \"setMomentum\",\n    value: function setMomentum(momentum) {\n      this.momentum = momentum;\n    }\n  }, {\n    key: \"getWeights\",\n    value: function () {\n      var _getWeights = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee() {\n        return _regeneratorRuntime().wrap(function _callee$(_context) {\n          while (1) switch (_context.prev = _context.next) {\n            case 0:\n              _context.next = 2;\n              return this.saveIterations();\n            case 2:\n              _context.t0 = _context.sent;\n              return _context.abrupt(\"return\", [_context.t0].concat(this.accumulations.map(function (v) {\n                return {\n                  name: v.originalName,\n                  tensor: v.variable\n                };\n              })));\n            case 4:\n            case \"end\":\n              return _context.stop();\n          }\n        }, _callee, this);\n      }));\n      function getWeights() {\n        return _getWeights.apply(this, arguments);\n      }\n      return getWeights;\n    }()\n  }, {\n    key: \"setWeights\",\n    value: function () {\n      var _setWeights = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee2(weightValues) {\n        var trainable;\n        return _regeneratorRuntime().wrap(function _callee2$(_context2) {\n          while (1) switch (_context2.prev = _context2.next) {\n            case 0:\n              _context2.next = 2;\n              return this.extractIterations(weightValues);\n            case 2:\n              weightValues = _context2.sent;\n              trainable = false;\n              this.accumulations = weightValues.map(function (v) {\n                return {\n                  originalName: v.name,\n                  variable: v.tensor.variable(trainable)\n                };\n              });\n            case 5:\n            case \"end\":\n              return _context2.stop();\n          }\n        }, _callee2, this);\n      }));\n      function setWeights(_x) {\n        return _setWeights.apply(this, arguments);\n      }\n      return setWeights;\n    }()\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      return {\n        'learningRate': this.learningRate,\n        'momentum': this.momentum,\n        'useNesterov': this.useNesterov\n      };\n    }\n    /** @nocollapse */\n  }], [{\n    key: \"className\",\n    get: /** @nocollapse */\n    // Name matters for Python compatibility.\n    function get() {\n      // Name matters for Python compatibility.\n      // This is a getter instead of a property because when it's a property, it\n      // prevents the entire class from being tree-shaken.\n      return 'Momentum';\n    }\n  }, {\n    key: \"fromConfig\",\n    value: function fromConfig(cls, config) {\n      return new cls(config['learningRate'], config['momentum'], config['useNesterov']);\n    }\n  }]);\n  return MomentumOptimizer;\n}(SGDOptimizer);","map":{"version":3,"names":["ENGINE","dispose","tidy","add","mul","scalar","zerosLike","SGDOptimizer","MomentumOptimizer","_SGDOptimizer","_inherits","_super","_createSuper","learningRate","momentum","_this","useNesterov","arguments","length","undefined","_classCallCheck","call","accumulations","m","_createClass","key","value","applyGradients","variableGradients","_this2","variableNames","Array","isArray","map","item","name","Object","keys","forEach","i","registeredVariables","trainable","originalName","concat","variable","accumulation","gradient","tensor","newValue","newAccumulation","c","assign","incrementIterations","v","setMomentum","_getWeights","_asyncToGenerator","_regeneratorRuntime","mark","_callee","wrap","_callee$","_context","prev","next","saveIterations","t0","sent","abrupt","stop","getWeights","apply","_setWeights","_callee2","weightValues","_callee2$","_context2","extractIterations","setWeights","_x","getConfig","get","fromConfig","cls","config"],"sources":["C:\\Users\\vince\\OneDrive\\Documents\\GitHub\\tfjs-core\\src\\optimizers\\momentum_optimizer.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from '../engine';\nimport {dispose, tidy} from '../globals';\nimport {add} from '../ops/add';\nimport {mul} from '../ops/mul';\nimport {scalar} from '../ops/scalar';\nimport {zerosLike} from '../ops/zeros_like';\nimport {ConfigDict, Serializable, SerializableConstructor} from '../serialization';\nimport {Scalar, Tensor} from '../tensor';\nimport {NamedTensor, NamedVariableMap} from '../tensor_types';\n\nimport {OptimizerVariable} from './optimizer';\nimport {SGDOptimizer} from './sgd_optimizer';\n\n/** @doclink Optimizer */\nexport class MomentumOptimizer extends SGDOptimizer {\n  /** @nocollapse */\n  // Name matters for Python compatibility.\n  static override get className() {\n    // Name matters for Python compatibility.\n    // This is a getter instead of a property because when it's a property, it\n    // prevents the entire class from being tree-shaken.\n    return 'Momentum';\n  }\n  private m: Scalar;\n  private accumulations: OptimizerVariable[] = [];\n\n  constructor(\n      protected override learningRate: number, private momentum: number,\n      private useNesterov = false) {\n    super(learningRate);\n    this.m = scalar(this.momentum);\n  }\n\n  override applyGradients(variableGradients: NamedVariableMap|NamedTensor[]) {\n    const variableNames = Array.isArray(variableGradients) ?\n        variableGradients.map(item => item.name) :\n        Object.keys(variableGradients);\n\n    variableNames.forEach((name, i) => {\n      const value = ENGINE.registeredVariables[name];\n      if (this.accumulations[i] == null) {\n        const trainable = false;\n        this.accumulations[i] = {\n          originalName: `${name}/momentum`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n\n      const accumulation = this.accumulations[i].variable;\n      const gradient = Array.isArray(variableGradients) ?\n          variableGradients[i].tensor :\n          variableGradients[name];\n      if (gradient == null) {\n        return;\n      }\n\n      tidy(() => {\n        let newValue: Tensor;\n        const newAccumulation = add(mul(this.m, accumulation), gradient);\n        if (this.useNesterov) {\n          newValue = add(\n              mul(this.c, add(gradient, mul(newAccumulation, this.m))), value);\n        } else {\n          newValue = add(mul(this.c, newAccumulation), value);\n        }\n        accumulation.assign(newAccumulation);\n        value.assign(newValue);\n      });\n    });\n    this.incrementIterations();\n  }\n\n  override dispose(): void {\n    this.m.dispose();\n    if (this.accumulations != null) {\n      dispose(this.accumulations.map(v => v.variable));\n    }\n  }\n\n  /**\n   * Sets the momentum of the optimizer.\n   *\n   * @param momentum\n   */\n  setMomentum(momentum: number) {\n    this.momentum = momentum;\n  }\n\n  override async getWeights(): Promise<NamedTensor[]> {\n    // Order matters for Python compatibility.\n    return [await this.saveIterations()].concat(this.accumulations.map(\n        v => ({name: v.originalName, tensor: v.variable})));\n  }\n\n  override async setWeights(weightValues: NamedTensor[]): Promise<void> {\n    weightValues = await this.extractIterations(weightValues);\n    const trainable = false;\n    this.accumulations = weightValues.map(\n        v => ({originalName: v.name, variable: v.tensor.variable(trainable)}));\n  }\n\n  override getConfig(): ConfigDict {\n    return {\n      'learningRate': this.learningRate,\n      'momentum': this.momentum,\n      'useNesterov': this.useNesterov\n    };\n  }\n\n  /** @nocollapse */\n  static override fromConfig<T extends Serializable>(\n      cls: SerializableConstructor<T>, config: ConfigDict): T {\n    return new cls(\n        config['learningRate'], config['momentum'], config['useNesterov']);\n  }\n}\n"],"mappings":";;;;;;AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQA,MAAM,QAAO,WAAW;AAChC,SAAQC,OAAO,IAAPA,QAAO,EAAEC,IAAI,QAAO,YAAY;AACxC,SAAQC,GAAG,QAAO,YAAY;AAC9B,SAAQC,GAAG,QAAO,YAAY;AAC9B,SAAQC,MAAM,QAAO,eAAe;AACpC,SAAQC,SAAS,QAAO,mBAAmB;AAM3C,SAAQC,YAAY,QAAO,iBAAiB;AAE5C;AACA,WAAaC,iBAAkB,0BAAAC,aAAA;EAAAC,SAAA,CAAAF,iBAAA,EAAAC,aAAA;EAAA,IAAAE,MAAA,GAAAC,YAAA,CAAAJ,iBAAA;EAY7B,SAAAA,kBACuBK,YAAoB,EAAUC,QAAgB,EACtC;IAAA,IAAAC,KAAA;IAAA,IAAnBC,WAAA,GAAAC,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAc,KAAK;IAAAG,eAAA,OAAAZ,iBAAA;IAC7BO,KAAA,GAAAJ,MAAA,CAAAU,IAAA,OAAMR,YAAY;IAFGE,KAAA,CAAAF,YAAY,GAAZA,YAAY;IAAkBE,KAAA,CAAAD,QAAQ,GAARA,QAAQ;IACjDC,KAAA,CAAAC,WAAW,GAAXA,WAAW;IAJfD,KAAA,CAAAO,aAAa,GAAwB,EAAE;IAM7CP,KAAA,CAAKQ,CAAC,GAAGlB,MAAM,CAACU,KAAA,CAAKD,QAAQ,CAAC;IAAC,OAAAC,KAAA;EACjC;EAACS,YAAA,CAAAhB,iBAAA;IAAAiB,GAAA;IAAAC,KAAA,EAEQ,SAAAC,eAAeC,iBAAiD;MAAA,IAAAC,MAAA;MACvE,IAAMC,aAAa,GAAGC,KAAK,CAACC,OAAO,CAACJ,iBAAiB,CAAC,GAClDA,iBAAiB,CAACK,GAAG,CAAC,UAAAC,IAAI;QAAA,OAAIA,IAAI,CAACC,IAAI;MAAA,EAAC,GACxCC,MAAM,CAACC,IAAI,CAACT,iBAAiB,CAAC;MAElCE,aAAa,CAACQ,OAAO,CAAC,UAACH,IAAI,EAAEI,CAAC,EAAI;QAChC,IAAMb,KAAK,GAAG1B,MAAM,CAACwC,mBAAmB,CAACL,IAAI,CAAC;QAC9C,IAAIN,MAAI,CAACP,aAAa,CAACiB,CAAC,CAAC,IAAI,IAAI,EAAE;UACjC,IAAME,SAAS,GAAG,KAAK;UACvBZ,MAAI,CAACP,aAAa,CAACiB,CAAC,CAAC,GAAG;YACtBG,YAAY,KAAAC,MAAA,CAAKR,IAAI,cAAW;YAChCS,QAAQ,EAAE1C,IAAI,CAAC;cAAA,OAAMI,SAAS,CAACoB,KAAK,CAAC,CAACkB,QAAQ,CAACH,SAAS,CAAC;YAAA;WAC1D;;QAGH,IAAMI,YAAY,GAAGhB,MAAI,CAACP,aAAa,CAACiB,CAAC,CAAC,CAACK,QAAQ;QACnD,IAAME,QAAQ,GAAGf,KAAK,CAACC,OAAO,CAACJ,iBAAiB,CAAC,GAC7CA,iBAAiB,CAACW,CAAC,CAAC,CAACQ,MAAM,GAC3BnB,iBAAiB,CAACO,IAAI,CAAC;QAC3B,IAAIW,QAAQ,IAAI,IAAI,EAAE;UACpB;;QAGF5C,IAAI,CAAC,YAAK;UACR,IAAI8C,QAAgB;UACpB,IAAMC,eAAe,GAAG9C,GAAG,CAACC,GAAG,CAACyB,MAAI,CAACN,CAAC,EAAEsB,YAAY,CAAC,EAAEC,QAAQ,CAAC;UAChE,IAAIjB,MAAI,CAACb,WAAW,EAAE;YACpBgC,QAAQ,GAAG7C,GAAG,CACVC,GAAG,CAACyB,MAAI,CAACqB,CAAC,EAAE/C,GAAG,CAAC2C,QAAQ,EAAE1C,GAAG,CAAC6C,eAAe,EAAEpB,MAAI,CAACN,CAAC,CAAC,CAAC,CAAC,EAAEG,KAAK,CAAC;WACrE,MAAM;YACLsB,QAAQ,GAAG7C,GAAG,CAACC,GAAG,CAACyB,MAAI,CAACqB,CAAC,EAAED,eAAe,CAAC,EAAEvB,KAAK,CAAC;;UAErDmB,YAAY,CAACM,MAAM,CAACF,eAAe,CAAC;UACpCvB,KAAK,CAACyB,MAAM,CAACH,QAAQ,CAAC;QACxB,CAAC,CAAC;MACJ,CAAC,CAAC;MACF,IAAI,CAACI,mBAAmB,EAAE;IAC5B;EAAC;IAAA3B,GAAA;IAAAC,KAAA,EAEQ,SAAAzB,QAAA,EAAO;MACd,IAAI,CAACsB,CAAC,CAACtB,OAAO,EAAE;MAChB,IAAI,IAAI,CAACqB,aAAa,IAAI,IAAI,EAAE;QAC9BrB,QAAO,CAAC,IAAI,CAACqB,aAAa,CAACW,GAAG,CAAC,UAAAoB,CAAC;UAAA,OAAIA,CAAC,CAACT,QAAQ;QAAA,EAAC,CAAC;;IAEpD;IAEA;;;;;EAAA;IAAAnB,GAAA;IAAAC,KAAA,EAKA,SAAA4B,YAAYxC,QAAgB;MAC1B,IAAI,CAACA,QAAQ,GAAGA,QAAQ;IAC1B;EAAC;IAAAW,GAAA;IAAAC,KAAA;MAAA,IAAA6B,WAAA,GAAAC,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CAEQ,SAAAC,QAAA;QAAA,OAAAF,mBAAA,GAAAG,IAAA,UAAAC,SAAAC,QAAA;UAAA,kBAAAA,QAAA,CAAAC,IAAA,GAAAD,QAAA,CAAAE,IAAA;YAAA;cAAAF,QAAA,CAAAE,IAAA;cAAA,OAEO,IAAI,CAACC,cAAc,EAAE;YAAA;cAAAH,QAAA,CAAAI,EAAA,GAAAJ,QAAA,CAAAK,IAAA;cAAA,OAAAL,QAAA,CAAAM,MAAA,YAAAN,QAAA,CAAAI,EAAA,EAAEvB,MAAM,CAAC,IAAI,CAACrB,aAAa,CAACW,GAAG,CAC9D,UAAAoB,CAAC;gBAAA,OAAK;kBAAClB,IAAI,EAAEkB,CAAC,CAACX,YAAY;kBAAEK,MAAM,EAAEM,CAAC,CAACT;gBAAQ,CAAC;cAAA,CAAC,CAAC;YAAA;YAAA;cAAA,OAAAkB,QAAA,CAAAO,IAAA;UAAA;QAAA,GAAAV,OAAA;MAAA,CACvD;MAAA,SAAAW,WAAA;QAAA,OAAAf,WAAA,CAAAgB,KAAA,OAAAtD,SAAA;MAAA;MAAA,OAAAqD,UAAA;IAAA;EAAA;IAAA7C,GAAA;IAAAC,KAAA;MAAA,IAAA8C,WAAA,GAAAhB,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CAEQ,SAAAe,SAAiBC,YAA2B;QAAA,IAAAjC,SAAA;QAAA,OAAAgB,mBAAA,GAAAG,IAAA,UAAAe,UAAAC,SAAA;UAAA,kBAAAA,SAAA,CAAAb,IAAA,GAAAa,SAAA,CAAAZ,IAAA;YAAA;cAAAY,SAAA,CAAAZ,IAAA;cAAA,OAC9B,IAAI,CAACa,iBAAiB,CAACH,YAAY,CAAC;YAAA;cAAzDA,YAAY,GAAAE,SAAA,CAAAT,IAAA;cACN1B,SAAS,GAAG,KAAK;cACvB,IAAI,CAACnB,aAAa,GAAGoD,YAAY,CAACzC,GAAG,CACjC,UAAAoB,CAAC;gBAAA,OAAK;kBAACX,YAAY,EAAEW,CAAC,CAAClB,IAAI;kBAAES,QAAQ,EAAES,CAAC,CAACN,MAAM,CAACH,QAAQ,CAACH,SAAS;gBAAC,CAAC;cAAA,CAAC,CAAC;YAAC;YAAA;cAAA,OAAAmC,SAAA,CAAAP,IAAA;UAAA;QAAA,GAAAI,QAAA;MAAA,CAC5E;MAAA,SAAAK,WAAAC,EAAA;QAAA,OAAAP,WAAA,CAAAD,KAAA,OAAAtD,SAAA;MAAA;MAAA,OAAA6D,UAAA;IAAA;EAAA;IAAArD,GAAA;IAAAC,KAAA,EAEQ,SAAAsD,UAAA,EAAS;MAChB,OAAO;QACL,cAAc,EAAE,IAAI,CAACnE,YAAY;QACjC,UAAU,EAAE,IAAI,CAACC,QAAQ;QACzB,aAAa,EAAE,IAAI,CAACE;OACrB;IACH;IAEA;EAAA;IAAAS,GAAA;IAAAwD,GAAA,EA9FA;IACA;IACA,SAAAA,IAAA,EAA6B;MAC3B;MACA;MACA;MACA,OAAO,UAAU;IACnB;EAAC;IAAAxD,GAAA;IAAAC,KAAA,EAwFD,SAAAwD,WACIC,GAA+B,EAAEC,MAAkB;MACrD,OAAO,IAAID,GAAG,CACVC,MAAM,CAAC,cAAc,CAAC,EAAEA,MAAM,CAAC,UAAU,CAAC,EAAEA,MAAM,CAAC,aAAa,CAAC,CAAC;IACxE;EAAC;EAAA,OAAA5E,iBAAA;AAAA,EApGoCD,YAAY"},"metadata":{},"sourceType":"module","externalDependencies":[]}