{"ast":null,"code":"import _regeneratorRuntime from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/regeneratorRuntime.js\";\nimport _asyncToGenerator from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/asyncToGenerator.js\";\nimport _createForOfIteratorHelper from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/createForOfIteratorHelper.js\";\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * Interfaces and methods for training models using TensorFlow.js datasets.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { scalar } from '@tensorflow/tfjs-core';\nimport { configureCallbacks, standardizeCallbacks } from '../base_callbacks';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { disposeTensorsInLogs } from '../logs';\nimport { singletonOrArray, toList } from '../utils/generic_utils';\nimport { standardizeClassWeights, standardizeWeights } from './training_utils';\n// Default batch size used during tensor-based validation.\nvar DEFAULT_VALIDATION_BATCH_SIZE = 32;\n/**\n * Standardize the output of a dataset iterator for use by\n * LayersModel.fitDataset().\n *\n * @param model: A `tf.LayersModel` object.\n * @param iteratorOut The output of a dataset iterator. It is required to be\n *   an object of the form `{xs: TensorOrArrayOrMap, ys:\n * TensorOrArrayOrMap}`, where `TensorOrArrayOrMap` is a single `tf.Tensor`,\n * a `tf.Tensor[]`, or a flat map from string names to `tf.Tensor`s.\n * @returns A flat array of `tf.Tensor` objects: the input `tf.Tensor`s\n *   followed by the target `tf.Tensor`s.  When `tf.Tensor`s are provided\n *   as a map, the order in the resulting array is taken from the `inputNames`\n *   and `outputNames` of the model.\n */\nfunction standardizeDataIteratorOutput(\n// Type `model` as `any` here to avoid circular dependency w/\n// training.ts.\n// tslint:disable-next-line:no-any\nmodel, iteratorOut) {\n  var xs;\n  var ys;\n  var iteratorOutObj = iteratorOut;\n  xs = iteratorOutObj['xs'];\n  ys = iteratorOutObj['ys'];\n  tfc.util.assert(xs != null && ys != null, function () {\n    return 'A Dataset iterator for fitDataset() is expected to generate ' + 'objects of the form `{xs: xVal, ys: yVal}`, where the two ' + 'values may be `tf.Tensor`, an array of Tensors, or a map of ' + 'string to Tensor.  The provided Dataset instead generates ' + \"\".concat(iteratorOut);\n  });\n  var flattenedXs = flattenTensorOrArrayOrMap('input', model.inputNames, xs);\n  var flattenedYs = flattenTensorOrArrayOrMap('output', model.outputNames, ys);\n  var batchSize = flattenedXs[0].shape[0];\n  tfc.util.assert(flattenedXs.length === model.inputs.length, function () {\n    return \"LayersModel has \".concat(model.inputs.length, \" inputs, but the dataset \") + \"provides \".concat(flattenedXs.length, \" inputs.  (Expected input keys: \") + \"\".concat(JSON.stringify(model.inputNames), \")\");\n  });\n  tfc.util.assert(flattenedYs.length === model.outputs.length, function () {\n    return \"LayersModel has \".concat(model.outputs.length, \" outputs, but the dataset \") + \"provides \".concat(flattenedYs.length, \" outputs.  (Expected output keys: \") + \"\".concat(JSON.stringify(model.outputNames), \")\");\n  });\n  var _loop = function _loop(xIndex) {\n    tfc.util.assert(flattenedXs[xIndex].shape[0] === batchSize, function () {\n      return \"Batch size mismatch: input \" + \"\".concat(model.inputNames[xIndex], \" has \").concat(flattenedXs[xIndex].shape[0], \"; \") + \"expected  \".concat(batchSize, \" based on input \").concat(model.inputNames[0], \".\");\n    });\n  };\n  for (var xIndex = 0; xIndex < flattenedXs.length; xIndex++) {\n    _loop(xIndex);\n  }\n  var _loop2 = function _loop2(yIndex) {\n    tfc.util.assert(flattenedYs[yIndex].shape[0] === batchSize, function () {\n      return \"Batch size mismatch: output \" + \"\".concat(model.outputNames[yIndex], \" has \").concat(flattenedYs[yIndex].shape[0], \"; \") + \"expected  \".concat(batchSize, \" based on input \").concat(model.inputNames[0], \".\");\n    });\n  };\n  for (var yIndex = 0; yIndex < flattenedYs.length; yIndex++) {\n    _loop2(yIndex);\n  }\n  return {\n    xs: flattenedXs,\n    ys: flattenedYs\n  };\n}\nfunction flattenTensorOrArrayOrMap(inputOrOutput, names, values) {\n  if (values instanceof tfc.Tensor) {\n    return [values];\n  } else if (Array.isArray(values)) {\n    tfc.util.assert(values.length === names.length, function () {\n      return \"Received an array of \".concat(values.length, \" Tensors, but expected \").concat(names.length, \" to match the \").concat(inputOrOutput, \" keys \").concat(names, \".\");\n    });\n    return values;\n  } else {\n    var result = [];\n    // Check that all the required keys are available.\n    var _iterator = _createForOfIteratorHelper(names),\n      _step;\n    try {\n      for (_iterator.s(); !(_step = _iterator.n()).done;) {\n        var name = _step.value;\n        if (values[name] == null) {\n          throw new ValueError(\"The feature data generated by the dataset lacks the required \" + \"\".concat(inputOrOutput, \" key '\").concat(name, \"'.\"));\n        }\n        result.push(values[name]);\n      }\n    } catch (err) {\n      _iterator.e(err);\n    } finally {\n      _iterator.f();\n    }\n    return result;\n  }\n}\nfunction standardizeTensorValidationData(data) {\n  if (data.length === 3) {\n    throw new NotImplementedError('Validation with sample weights is not implemented yet.');\n  }\n  return {\n    xs: data[0],\n    ys: data[1]\n  };\n}\nexport function fitDataset(_x, _x2, _x3) {\n  return _fitDataset.apply(this, arguments);\n}\n/** Helper function that determines number of steps (batches) per epoch. */\nfunction _fitDataset() {\n  _fitDataset = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee(\n  // Type `model` as `any` here to avoid circular dependency w/\n  // training.ts.\n  // tslint:disable-next-line:no-any\n  model, dataset, args) {\n    var hasBatchesPerEpoch, doValidation, valXs, valYs, validationData, trainFunction, outLabels, callbackMetrics, callbacks, verbose, _configureCallbacks, callbackList, history, epoch, dataIterator, epochLogs, stepsDone, batchIndex, iteratorOut, _standardizeDataItera, xs, ys, batchLogs, sampleWeights, standardClassWeights, i, ins, outs, _i, label, out, valOuts, _i2;\n    return _regeneratorRuntime().wrap(function _callee$(_context) {\n      while (1) switch (_context.prev = _context.next) {\n        case 0:\n          hasBatchesPerEpoch = args.batchesPerEpoch != null;\n          tfc.util.assert(model.optimizer != null, function () {\n            return 'You must compile a model before training/testing. Use ' + 'LayersModel.compile(modelCompileConfig).';\n          });\n          tfc.util.assert(args != null, function () {\n            return \"For fitDataset(), the 2nd argument (config) is required, \" + \"but it is not provided in this call.\";\n          });\n          tfc.util.assert(args.epochs != null && args.epochs > 0 && Number.isInteger(args.epochs), function () {\n            return \"For fitDataset(), config.epochs is expected to be a positive \" + \"integer, but got \".concat(args.epochs);\n          });\n          tfc.util.assert(!hasBatchesPerEpoch || args.batchesPerEpoch > 0 && Number.isInteger(args.batchesPerEpoch), function () {\n            return \"For fitDataset(), config.batchesPerEpoch is expected to be a \" + \"positive integer if specified, but got \".concat(args.batchesPerEpoch);\n          });\n          tfc.util.assert(\n          // tslint:disable-next-line:no-any\n          args['validationSplit'] == null, function () {\n            return '`validationSplit` is not supported by `fitDataset()`. ' + 'Use validationData instead.';\n          });\n          if (!model.isTraining) {\n            _context.next = 8;\n            break;\n          }\n          throw new Error('Cannot start training because another fit() call is ongoing.');\n        case 8:\n          model.isTraining = true;\n          _context.prev = 9;\n          doValidation = args.validationData != null;\n          if (doValidation) {\n            if (isDatasetObject(args.validationData)) {\n              tfc.util.assert(args.validationBatches == null || args.validationBatches > 0 && Number.isInteger(args.validationBatches), function () {\n                return \"For fitDataset() with dataset-based validation, \" + \"config.validationBatches is expected not to be provided, \" + \"or to be a positive integer, \" + \"but got \".concat(args.validationBatches);\n              });\n            } else {\n              validationData = standardizeTensorValidationData(args.validationData);\n              valXs = validationData.xs;\n              valYs = validationData.ys;\n            }\n          }\n          trainFunction = model.makeTrainFunction();\n          outLabels = model.getDedupedMetricsNames();\n          if (doValidation) {\n            callbackMetrics = outLabels.slice().concat(outLabels.map(function (n) {\n              return 'val_' + n;\n            }));\n          } else {\n            callbackMetrics = outLabels.slice();\n          }\n          callbacks = standardizeCallbacks(args.callbacks, args.yieldEvery);\n          verbose = args.verbose == null ? 1 : args.verbose;\n          _configureCallbacks = configureCallbacks(callbacks, verbose, args.epochs, null, null, getStepsPerEpoch(dataset, args), null,\n          // Batch size determined by the dataset itself.\n          doValidation, callbackMetrics), callbackList = _configureCallbacks.callbackList, history = _configureCallbacks.history;\n          callbackList.setModel(model);\n          model.history = history;\n          _context.next = 22;\n          return callbackList.onTrainBegin();\n        case 22:\n          model.stopTraining_ = false;\n          epoch = args.initialEpoch == null ? 0 : args.initialEpoch;\n          _context.next = 26;\n          return dataset.iterator();\n        case 26:\n          dataIterator = _context.sent;\n        case 27:\n          if (!(epoch < args.epochs)) {\n            _context.next = 98;\n            break;\n          }\n          epochLogs = {};\n          _context.next = 31;\n          return callbackList.onEpochBegin(epoch);\n        case 31:\n          stepsDone = 0;\n          batchIndex = 0;\n          if (hasBatchesPerEpoch) {\n            _context.next = 37;\n            break;\n          }\n          _context.next = 36;\n          return dataset.iterator();\n        case 36:\n          dataIterator = _context.sent;\n        case 37:\n          if (!(hasBatchesPerEpoch ? stepsDone < args.batchesPerEpoch : true)) {\n            _context.next = 91;\n            break;\n          }\n          _context.next = 40;\n          return dataIterator.next();\n        case 40:\n          iteratorOut = _context.sent;\n          if (!(hasBatchesPerEpoch && iteratorOut.done)) {\n            _context.next = 44;\n            break;\n          }\n          console.warn('You provided `batchesPerEpoch` as ' + \"\".concat(args.batchesPerEpoch, \", \") + 'but your dataset iterator ran out of data after ' + \"\".concat(stepsDone, \" batches; \") + 'interrupting training. Make sure that your ' + 'dataset can generate at least `batchesPerEpoch * epochs` ' + 'batches (in this case, ' + \"\".concat(args.batchesPerEpoch * args.epochs, \" batches). \") + 'You may need to use the repeat() function when building ' + 'your dataset.');\n          return _context.abrupt(\"break\", 91);\n        case 44:\n          if (!(iteratorOut.value != null)) {\n            _context.next = 73;\n            break;\n          }\n          _standardizeDataItera = standardizeDataIteratorOutput(model, iteratorOut.value), xs = _standardizeDataItera.xs, ys = _standardizeDataItera.ys;\n          batchLogs = {};\n          batchLogs['batch'] = batchIndex;\n          batchLogs['size'] = xs[0].shape[0];\n          _context.next = 51;\n          return callbackList.onBatchBegin(batchIndex, batchLogs);\n        case 51:\n          sampleWeights = [];\n          if (!(args.classWeight != null)) {\n            _context.next = 64;\n            break;\n          }\n          standardClassWeights = standardizeClassWeights(args.classWeight, model.outputNames);\n          i = 0;\n        case 55:\n          if (!(i < standardClassWeights.length)) {\n            _context.next = 64;\n            break;\n          }\n          _context.t0 = sampleWeights;\n          _context.next = 59;\n          return standardizeWeights(ys[i], null, standardClassWeights[i]);\n        case 59:\n          _context.t1 = _context.sent;\n          _context.t0.push.call(_context.t0, _context.t1);\n        case 61:\n          ++i;\n          _context.next = 55;\n          break;\n        case 64:\n          // Train on batch.\n          ins = xs.concat(ys).concat(sampleWeights);\n          outs = trainFunction(ins);\n          tfc.dispose(ins);\n          for (_i = 0; _i < outLabels.length; ++_i) {\n            label = outLabels[_i];\n            out = outs[_i];\n            batchLogs[label] = out;\n            tfc.keep(out);\n          }\n          _context.next = 70;\n          return callbackList.onBatchEnd(batchIndex, batchLogs);\n        case 70:\n          disposeTensorsInLogs(batchLogs);\n          batchIndex++;\n          stepsDone++;\n        case 73:\n          if (!(hasBatchesPerEpoch ? stepsDone >= args.batchesPerEpoch : iteratorOut.done)) {\n            _context.next = 87;\n            break;\n          }\n          if (!doValidation) {\n            _context.next = 86;\n            break;\n          }\n          valOuts = void 0;\n          if (!isDatasetObject(args.validationData)) {\n            _context.next = 84;\n            break;\n          }\n          _context.t2 = toList;\n          _context.next = 80;\n          return model.evaluateDataset(args.validationData, {\n            batches: args.validationBatches\n          });\n        case 80:\n          _context.t3 = _context.sent;\n          valOuts = (0, _context.t2)(_context.t3);\n          _context.next = 85;\n          break;\n        case 84:\n          valOuts = toList(model.evaluate(valXs, valYs, {\n            batchSize: args.validationBatchSize == null ? DEFAULT_VALIDATION_BATCH_SIZE : args.validationBatchSize,\n            verbose: 0\n          }));\n        case 85:\n          for (_i2 = 0; _i2 < model.metricsNames.length; ++_i2) {\n            epochLogs[\"val_\".concat(model.metricsNames[_i2])] = valOuts[_i2];\n          }\n        case 86:\n          return _context.abrupt(\"break\", 91);\n        case 87:\n          if (!model.stopTraining_) {\n            _context.next = 89;\n            break;\n          }\n          return _context.abrupt(\"break\", 91);\n        case 89:\n          _context.next = 37;\n          break;\n        case 91:\n          _context.next = 93;\n          return callbackList.onEpochEnd(epoch, epochLogs);\n        case 93:\n          epoch++;\n          if (!model.stopTraining_) {\n            _context.next = 96;\n            break;\n          }\n          return _context.abrupt(\"break\", 98);\n        case 96:\n          _context.next = 27;\n          break;\n        case 98:\n          _context.next = 100;\n          return callbackList.onTrainEnd();\n        case 100:\n          _context.next = 102;\n          return model.history.syncData();\n        case 102:\n          return _context.abrupt(\"return\", model.history);\n        case 103:\n          _context.prev = 103;\n          model.isTraining = false;\n          return _context.finish(103);\n        case 106:\n        case \"end\":\n          return _context.stop();\n      }\n    }, _callee, null, [[9,, 103, 106]]);\n  }));\n  return _fitDataset.apply(this, arguments);\n}\nfunction getStepsPerEpoch(dataset, args) {\n  // Attempt to determine # of batches in an epoch.\n  var stepsPerEpoch = null;\n  if (args.batchesPerEpoch != null) {\n    stepsPerEpoch = args.batchesPerEpoch;\n  } else if (Number.isFinite(dataset.size)) {\n    stepsPerEpoch = dataset.size;\n  }\n  return stepsPerEpoch;\n}\n// Check if provided object is a Dataset object by checking its .iterator\n// element.\nfunction isDatasetObject(dataset) {\n  return typeof dataset.iterator === 'function';\n}\n// Check if provided object is a LazyIterator object by checking it's .next\n// element.\nfunction isLazyIteratorObject(iterator) {\n  return typeof iterator.next === 'function';\n}\nexport function evaluateDataset(_x4, _x5, _x6) {\n  return _evaluateDataset.apply(this, arguments);\n}\nfunction _evaluateDataset() {\n  _evaluateDataset = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee2(\n  // Type `model` as `any` here to avoid circular dependency w/\n  // training.ts.\n  // tslint:disable-next-line:no-any\n  model, dataset, args) {\n    var hasBatches, f, outs, dataIterator, numExamples, batch, _loop3, _ret, i, oldScalar;\n    return _regeneratorRuntime().wrap(function _callee2$(_context3) {\n      while (1) switch (_context3.prev = _context3.next) {\n        case 0:\n          args = args || {};\n          hasBatches = args.batches != null;\n          f = model.testFunction;\n          outs = [];\n          if (!(args.verbose > 0)) {\n            _context3.next = 6;\n            break;\n          }\n          throw new NotImplementedError('Verbose mode is not implemented yet.');\n        case 6:\n          tfc.util.assert(!hasBatches || args.batches > 0 && Number.isInteger(args.batches), function () {\n            return 'Test loop expects `batches` to be a positive integer, but ' + \"received \".concat(JSON.stringify(args.batches));\n          });\n          if (!isLazyIteratorObject(dataset)) {\n            _context3.next = 11;\n            break;\n          }\n          _context3.t0 = dataset;\n          _context3.next = 14;\n          break;\n        case 11:\n          _context3.next = 13;\n          return dataset.iterator();\n        case 13:\n          _context3.t0 = _context3.sent;\n        case 14:\n          dataIterator = _context3.t0;\n          // Keeps track of number of examples used in this evaluation.\n          numExamples = 0;\n          batch = 0;\n          _loop3 = /*#__PURE__*/_regeneratorRuntime().mark(function _loop3() {\n            var iteratorOut;\n            return _regeneratorRuntime().wrap(function _loop3$(_context2) {\n              while (1) switch (_context2.prev = _context2.next) {\n                case 0:\n                  _context2.next = 2;\n                  return dataIterator.next();\n                case 2:\n                  iteratorOut = _context2.sent;\n                  outs = tfc.tidy(function () {\n                    if (iteratorOut.value) {\n                      // TODO(cais): Once real dataset is available, use\n                      //   `map(x => standardizeDataIteratorOutput(model, x).map(f)`.\n                      var _standardizeDataItera2 = standardizeDataIteratorOutput(model, iteratorOut.value),\n                        xs = _standardizeDataItera2.xs,\n                        ys = _standardizeDataItera2.ys;\n                      var xsAndYs = xs.concat(ys);\n                      var batchOuts = tfc.tidy(function () {\n                        return f(xsAndYs);\n                      });\n                      tfc.dispose(xsAndYs);\n                      if (batch === 0) {\n                        for (var _i3 = 0; _i3 < batchOuts.length; ++_i3) {\n                          outs.push(scalar(0));\n                        }\n                      }\n                      var batchSize = xsAndYs[0].shape[0];\n                      var _loop4 = function _loop4(_i4) {\n                        var batchOut = batchOuts[_i4];\n                        var oldScalar = outs[_i4];\n                        outs[_i4] = tfc.tidy(function () {\n                          return tfc.add(outs[_i4], tfc.mul(batchSize, batchOut));\n                        });\n                        if (batch > 0) {\n                          tfc.dispose(oldScalar);\n                        }\n                      };\n                      for (var _i4 = 0; _i4 < batchOuts.length; ++_i4) {\n                        _loop4(_i4);\n                      }\n                      tfc.dispose(batchOuts);\n                      numExamples += batchSize;\n                      ++batch;\n                    }\n                    return outs;\n                  });\n                  if (!iteratorOut.done) {\n                    _context2.next = 7;\n                    break;\n                  }\n                  if (hasBatches) {\n                    console.warn('Your dataset iterator ran out of data during evaluateDataset(). ' + 'Interrupting evalution. Make sure that your ' + 'dataset can generate at least `batches` ' + \"batches (in this case, \".concat(args.batches, \" batches). \") + 'You may need to use the repeat() function when building ' + 'your dataset.');\n                  }\n                  return _context2.abrupt(\"return\", \"break\");\n                case 7:\n                case \"end\":\n                  return _context2.stop();\n              }\n            }, _loop3);\n          });\n        case 18:\n          if (!(hasBatches ? batch < args.batches : true)) {\n            _context3.next = 25;\n            break;\n          }\n          return _context3.delegateYield(_loop3(), \"t1\", 20);\n        case 20:\n          _ret = _context3.t1;\n          if (!(_ret === \"break\")) {\n            _context3.next = 23;\n            break;\n          }\n          return _context3.abrupt(\"break\", 25);\n        case 23:\n          _context3.next = 18;\n          break;\n        case 25:\n          for (i = 0; i < outs.length; ++i) {\n            oldScalar = outs[i];\n            outs[i] = tfc.div(outs[i], numExamples);\n            tfc.dispose(oldScalar);\n          }\n          return _context3.abrupt(\"return\", singletonOrArray(outs));\n        case 27:\n        case \"end\":\n          return _context3.stop();\n      }\n    }, _callee2);\n  }));\n  return _evaluateDataset.apply(this, arguments);\n}","map":{"version":3,"names":["tfc","scalar","configureCallbacks","standardizeCallbacks","NotImplementedError","ValueError","disposeTensorsInLogs","singletonOrArray","toList","standardizeClassWeights","standardizeWeights","DEFAULT_VALIDATION_BATCH_SIZE","standardizeDataIteratorOutput","model","iteratorOut","xs","ys","iteratorOutObj","util","assert","concat","flattenedXs","flattenTensorOrArrayOrMap","inputNames","flattenedYs","outputNames","batchSize","shape","length","inputs","JSON","stringify","outputs","_loop","xIndex","_loop2","yIndex","inputOrOutput","names","values","Tensor","Array","isArray","result","_iterator","_createForOfIteratorHelper","_step","s","n","done","name","value","push","err","e","f","standardizeTensorValidationData","data","fitDataset","_x","_x2","_x3","_fitDataset","apply","arguments","_asyncToGenerator","_regeneratorRuntime","mark","_callee","dataset","args","hasBatchesPerEpoch","doValidation","valXs","valYs","validationData","trainFunction","outLabels","callbackMetrics","callbacks","verbose","_configureCallbacks","callbackList","history","epoch","dataIterator","epochLogs","stepsDone","batchIndex","_standardizeDataItera","batchLogs","sampleWeights","standardClassWeights","i","ins","outs","_i","label","out","valOuts","_i2","wrap","_callee$","_context","prev","next","batchesPerEpoch","optimizer","epochs","Number","isInteger","isTraining","Error","isDatasetObject","validationBatches","makeTrainFunction","getDedupedMetricsNames","slice","map","yieldEvery","getStepsPerEpoch","setModel","onTrainBegin","stopTraining_","initialEpoch","iterator","sent","onEpochBegin","console","warn","abrupt","onBatchBegin","classWeight","t0","t1","call","dispose","keep","onBatchEnd","t2","evaluateDataset","batches","t3","evaluate","validationBatchSize","metricsNames","onEpochEnd","onTrainEnd","syncData","finish","stop","stepsPerEpoch","isFinite","size","isLazyIteratorObject","_x4","_x5","_x6","_evaluateDataset","_callee2","hasBatches","numExamples","batch","_loop3","_ret","oldScalar","_callee2$","_context3","testFunction","_loop3$","_context2","tidy","_standardizeDataItera2","xsAndYs","batchOuts","_loop4","_i4","batchOut","add","mul","delegateYield","div"],"sources":["C:\\Users\\vince\\OneDrive\\Documents\\GitHub\\tfjs-layers\\src\\engine\\training_dataset.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Interfaces and methods for training models using TensorFlow.js datasets.\n */\n\nimport * as tfc from '@tensorflow/tfjs-core';\nimport {scalar} from '@tensorflow/tfjs-core';\nimport {BaseCallback, configureCallbacks, CustomCallbackArgs, History, ModelLoggingVerbosity, standardizeCallbacks, YieldEveryOptions} from '../base_callbacks';\nimport {NotImplementedError, ValueError} from '../errors';\nimport {disposeTensorsInLogs, UnresolvedLogs} from '../logs';\nimport {TensorOrArrayOrMap} from '../types';\nimport {singletonOrArray, toList} from '../utils/generic_utils';\n\nimport {Dataset, LazyIterator} from './dataset_stub';\nimport {ClassWeight, ClassWeightMap, standardizeClassWeights, standardizeWeights} from './training_utils';\n\n/**\n * Interface for configuring model training based on a dataset object.\n */\nexport interface ModelFitDatasetArgs<T> {\n  /**\n   * (Optional) Total number of steps (batches of samples) before\n   * declaring one epoch finished and starting the next epoch. It should\n   * typically be equal to the number of samples of your dataset divided by\n   * the batch size, so that `fitDataset`() call can utilize the entire dataset.\n   * If it is not provided, use `done` return value in `iterator.next()` as\n   * signal to finish an epoch.\n   */\n  batchesPerEpoch?: number;\n\n  /**\n   * Integer number of times to iterate over the training dataset.\n   */\n  epochs: number;\n\n  /**\n   * Verbosity level.\n   *\n   * Expected to be 0, 1, or 2. Default: 1.\n   *\n   * 0 - No printed message during fit() call.\n   * 1 - In Node.js (tfjs-node), prints the progress bar, together with\n   *     real-time updates of loss and metric values and training speed.\n   *     In the browser: no action. This is the default.\n   * 2 - Not implemented yet.\n   */\n  verbose?: ModelLoggingVerbosity;\n\n  /**\n   * List of callbacks to be called during training.\n   * Can have one or more of the following callbacks:\n   *   - `onTrainBegin(logs)`: called when training starts.\n   *   - `onTrainEnd(logs)`: called when training ends.\n   *   - `onEpochBegin(epoch, logs)`: called at the start of every epoch.\n   *   - `onEpochEnd(epoch, logs)`: called at the end of every epoch.\n   *   - `onBatchBegin(batch, logs)`: called at the start of every batch.\n   *   - `onBatchEnd(batch, logs)`: called at the end of every batch.\n   *   - `onYield(epoch, batch, logs)`: called every `yieldEvery` milliseconds\n   *      with the current epoch, batch and logs. The logs are the same\n   *      as in `onBatchEnd()`. Note that `onYield` can skip batches or\n   *      epochs. See also docs for `yieldEvery` below.\n   */\n  callbacks?: BaseCallback[]|CustomCallbackArgs|CustomCallbackArgs[];\n\n  /**\n   * Data on which to evaluate the loss and any model\n   * metrics at the end of each epoch. The model will not be trained on this\n   * data. This could be any of the following:\n   *\n   *   - An array `[xVal, yVal]`, where the two values may be `tf.Tensor`,\n   *     an array of Tensors, or a map of string to Tensor.\n   *   - Similarly, an array ` [xVal, yVal, valSampleWeights]`\n   *     (not implemented yet).\n   *   - a `Dataset` object with elements of the form `{xs: xVal, ys: yVal}`,\n   *     where `xs` and `ys` are the feature and label tensors, respectively.\n   *\n   * If `validationData` is an Array of Tensor objects, each `tf.Tensor` will be\n   * sliced into batches during validation, using the parameter\n   * `validationBatchSize` (which defaults to 32). The entirety of the\n   * `tf.Tensor` objects will be used in the validation.\n   *\n   * If `validationData` is a dataset object, and the `validationBatches`\n   * parameter is specified, the validation will use `validationBatches` batches\n   * drawn from the dataset object. If `validationBatches` parameter is not\n   * specified, the validation will stop when the dataset is exhausted.\n   *\n   * The model will not be trained on this data.\n   */\n  validationData?: [\n    TensorOrArrayOrMap, TensorOrArrayOrMap\n  ]|[TensorOrArrayOrMap, TensorOrArrayOrMap, TensorOrArrayOrMap]|Dataset<T>;\n\n  /**\n   * Optional batch size for validation.\n   *\n   * Used only if `validationData` is an array of `tf.Tensor` objects, i.e., not\n   * a dataset object.\n   *\n   * If not specified, its value defaults to 32.\n   */\n  validationBatchSize?: number;\n\n  /**\n   * (Optional) Only relevant if `validationData` is specified and is a dataset\n   * object.\n   *\n   * Total number of batches of samples to draw from `validationData` for\n   * validation purpose before stopping at the end of every epoch. If not\n   * specified, `evaluateDataset` will use `iterator.next().done` as signal to\n   * stop validation.\n   */\n  validationBatches?: number;\n\n  /**\n   * Configures the frequency of yielding the main thread to other tasks.\n   *\n   * In the browser environment, yielding the main thread can improve the\n   * responsiveness of the page during training. In the Node.js environment,\n   * it can ensure tasks queued in the event loop can be handled in a timely\n   * manner.\n   *\n   * The value can be one of the following:\n   *   - `'auto'`: The yielding happens at a certain frame rate (currently set\n   *               at 125ms). This is the default.\n   *   - `'batch'`: yield every batch.\n   *   - `'epoch'`: yield every epoch.\n   *   - a `number`: Will yield every `number` milliseconds.\n   *   - `'never'`: never yield. (But yielding can still happen through `await\n   *      nextFrame()` calls in custom callbacks.)\n   */\n  yieldEvery?: YieldEveryOptions;\n\n  /**\n   * Epoch at which to start training (useful for resuming a previous training\n   * run). When this is used, `epochs` is the index of the \"final epoch\".\n   * The model is not trained for a number of iterations given by `epochs`,\n   * but merely until the epoch of index `epochs` is reached.\n   */\n  initialEpoch?: number;\n\n  /**\n   * Optional object mapping class indices (integers) to\n   * a weight (float) to apply to the model's loss for the samples from this\n   * class during training. This can be useful to tell the model to \"pay more\n   * attention\" to samples from an under-represented class.\n   *\n   * If the model has multiple outputs, a class weight can be specified for\n   * each of the outputs by setting this field an array of weight object\n   * or an object that maps model output names (e.g., `model.outputNames[0]`)\n   * to weight objects.\n   */\n  classWeight?: ClassWeight|ClassWeight[]|ClassWeightMap;\n}\n\nexport interface FitDatasetElement {\n  xs: TensorOrArrayOrMap;\n  ys: TensorOrArrayOrMap;\n}\n\n/**\n * Interface for configuring model evaluation based on a dataset object.\n */\nexport interface ModelEvaluateDatasetArgs {\n  /**\n   * Number of batches to draw from the dataset object before ending the\n   * evaluation.\n   */\n  batches?: number;\n\n  /**\n   * Verbosity mode.\n   */\n  verbose?: ModelLoggingVerbosity;\n}\n\n// Default batch size used during tensor-based validation.\nconst DEFAULT_VALIDATION_BATCH_SIZE = 32;\n\n/**\n * Standardize the output of a dataset iterator for use by\n * LayersModel.fitDataset().\n *\n * @param model: A `tf.LayersModel` object.\n * @param iteratorOut The output of a dataset iterator. It is required to be\n *   an object of the form `{xs: TensorOrArrayOrMap, ys:\n * TensorOrArrayOrMap}`, where `TensorOrArrayOrMap` is a single `tf.Tensor`,\n * a `tf.Tensor[]`, or a flat map from string names to `tf.Tensor`s.\n * @returns A flat array of `tf.Tensor` objects: the input `tf.Tensor`s\n *   followed by the target `tf.Tensor`s.  When `tf.Tensor`s are provided\n *   as a map, the order in the resulting array is taken from the `inputNames`\n *   and `outputNames` of the model.\n */\nfunction standardizeDataIteratorOutput(\n    // Type `model` as `any` here to avoid circular dependency w/\n    // training.ts.\n    // tslint:disable-next-line:no-any\n    model: any, iteratorOut: {}): {xs: tfc.Tensor[], ys: tfc.Tensor[]} {\n  let xs: TensorOrArrayOrMap;\n  let ys: TensorOrArrayOrMap;\n\n  const iteratorOutObj = iteratorOut as FitDatasetElement;\n  xs = iteratorOutObj['xs'];\n  ys = iteratorOutObj['ys'];\n  tfc.util.assert(\n      xs != null && ys != null,\n      () => 'A Dataset iterator for fitDataset() is expected to generate ' +\n          'objects of the form `{xs: xVal, ys: yVal}`, where the two ' +\n          'values may be `tf.Tensor`, an array of Tensors, or a map of ' +\n          'string to Tensor.  The provided Dataset instead generates ' +\n          `${iteratorOut}`);\n\n  const flattenedXs: tfc.Tensor[] =\n      flattenTensorOrArrayOrMap('input', model.inputNames, xs);\n  const flattenedYs: tfc.Tensor[] =\n      flattenTensorOrArrayOrMap('output', model.outputNames, ys);\n\n  const batchSize: number = flattenedXs[0].shape[0];\n\n  tfc.util.assert(\n      flattenedXs.length === model.inputs.length,\n      () => `LayersModel has ${model.inputs.length} inputs, but the dataset ` +\n          `provides ${flattenedXs.length} inputs.  (Expected input keys: ` +\n          `${JSON.stringify(model.inputNames)})`);\n\n  tfc.util.assert(\n      flattenedYs.length === model.outputs.length,\n      () =>\n          `LayersModel has ${model.outputs.length} outputs, but the dataset ` +\n          `provides ${flattenedYs.length} outputs.  (Expected output keys: ` +\n          `${JSON.stringify(model.outputNames)})`);\n\n  for (let xIndex = 0; xIndex < flattenedXs.length; xIndex++) {\n    tfc.util.assert(\n        flattenedXs[xIndex].shape[0] === batchSize,\n        () => `Batch size mismatch: input ` +\n            `${model.inputNames[xIndex]} has ${\n                  flattenedXs[xIndex].shape[0]}; ` +\n            `expected  ${batchSize} based on input ${model.inputNames[0]}.`);\n  }\n\n  for (let yIndex = 0; yIndex < flattenedYs.length; yIndex++) {\n    tfc.util.assert(\n        flattenedYs[yIndex].shape[0] === batchSize,\n        () => `Batch size mismatch: output ` +\n            `${model.outputNames[yIndex]} has ${\n                  flattenedYs[yIndex].shape[0]}; ` +\n            `expected  ${batchSize} based on input ${model.inputNames[0]}.`);\n  }\n\n  return {xs: flattenedXs, ys: flattenedYs};\n}\n\nfunction flattenTensorOrArrayOrMap(\n    inputOrOutput: string, names: string[], values: TensorOrArrayOrMap) {\n  if (values instanceof tfc.Tensor) {\n    return [values];\n  } else if (Array.isArray(values)) {\n    tfc.util.assert(\n        values.length === names.length,\n        () => `Received an array of ${values.length} Tensors, but expected ${\n            names.length} to match the ${inputOrOutput} keys ${names}.`);\n    return values;\n  } else {\n    const result: tfc.Tensor[] = [];\n    // Check that all the required keys are available.\n    for (const name of names) {\n      if (values[name] == null) {\n        throw new ValueError(\n            `The feature data generated by the dataset lacks the required ` +\n            `${inputOrOutput} key '${name}'.`);\n      }\n      result.push(values[name]);\n    }\n    return result;\n  }\n}\n\nfunction standardizeTensorValidationData<T>(\n    data:\n        [\n          tfc.Tensor|tfc.Tensor[], tfc.Tensor|tfc.Tensor[]\n        ]|[tfc.Tensor | tfc.Tensor[], tfc.Tensor | tfc.Tensor[],\n           tfc.Tensor | tfc.Tensor[]]):\n    {xs: tfc.Tensor|tfc.Tensor[], ys: tfc.Tensor|tfc.Tensor[]} {\n  if (data.length === 3) {\n    throw new NotImplementedError(\n        'Validation with sample weights is not implemented yet.');\n  }\n  return {xs: data[0], ys: data[1]};\n}\n\nexport async function fitDataset<T>(\n    // Type `model` as `any` here to avoid circular dependency w/\n    // training.ts.\n    // tslint:disable-next-line:no-any\n    model: any, dataset: Dataset<T>,\n    args: ModelFitDatasetArgs<T>): Promise<History> {\n  const hasBatchesPerEpoch = args.batchesPerEpoch != null;\n  tfc.util.assert(\n      model.optimizer != null,\n      () => 'You must compile a model before training/testing. Use ' +\n          'LayersModel.compile(modelCompileConfig).');\n\n  tfc.util.assert(\n      args != null,\n      () => `For fitDataset(), the 2nd argument (config) is required, ` +\n          `but it is not provided in this call.`);\n  tfc.util.assert(\n      args.epochs != null && args.epochs > 0 && Number.isInteger(args.epochs),\n      () => `For fitDataset(), config.epochs is expected to be a positive ` +\n          `integer, but got ${args.epochs}`);\n  tfc.util.assert(\n      !hasBatchesPerEpoch ||\n          (args.batchesPerEpoch > 0 && Number.isInteger(args.batchesPerEpoch)),\n      () => `For fitDataset(), config.batchesPerEpoch is expected to be a ` +\n          `positive integer if specified, but got ${args.batchesPerEpoch}`);\n  tfc.util.assert(\n      // tslint:disable-next-line:no-any\n      (args as any)['validationSplit'] == null,\n      () => '`validationSplit` is not supported by `fitDataset()`. ' +\n          'Use validationData instead.');\n\n  if (model.isTraining) {\n    throw new Error(\n        'Cannot start training because another fit() call is ongoing.');\n  }\n  model.isTraining = true;\n\n  try {\n    const doValidation = args.validationData != null;\n    let valXs: tfc.Tensor|tfc.Tensor[];\n    let valYs: tfc.Tensor|tfc.Tensor[];\n    if (doValidation) {\n      if (isDatasetObject(args.validationData)) {\n        tfc.util.assert(\n            args.validationBatches == null ||\n                (args.validationBatches > 0 &&\n                 Number.isInteger(args.validationBatches)),\n            () => `For fitDataset() with dataset-based validation, ` +\n                `config.validationBatches is expected not to be provided, ` +\n                `or to be a positive integer, ` +\n                `but got ${args.validationBatches}`);\n      } else {\n        const validationData = standardizeTensorValidationData(\n            args.validationData as\n                    [tfc.Tensor | tfc.Tensor[], tfc.Tensor | tfc.Tensor[]] |\n            [\n              tfc.Tensor | tfc.Tensor[], tfc.Tensor | tfc.Tensor[],\n              tfc.Tensor | tfc.Tensor[]\n            ]);\n        valXs = validationData.xs;\n        valYs = validationData.ys;\n      }\n    }\n\n    const trainFunction = model.makeTrainFunction();\n    const outLabels = model.getDedupedMetricsNames() as string[];\n\n    let callbackMetrics: string[];\n    if (doValidation) {\n      callbackMetrics =\n          outLabels.slice().concat(outLabels.map(n => 'val_' + n));\n    } else {\n      callbackMetrics = outLabels.slice();\n    }\n\n    const callbacks = standardizeCallbacks(args.callbacks, args.yieldEvery);\n    const verbose = args.verbose == null ? 1 : args.verbose;\n    const {callbackList, history} = configureCallbacks(\n        callbacks, verbose, args.epochs, null, null,\n        getStepsPerEpoch(dataset, args),\n        null,  // Batch size determined by the dataset itself.\n        doValidation, callbackMetrics);\n    callbackList.setModel(model);\n    model.history = history;\n\n    await callbackList.onTrainBegin();\n    model.stopTraining_ = false;\n    let epoch = args.initialEpoch == null ? 0 : args.initialEpoch;\n\n    let dataIterator = await dataset.iterator();\n    while (epoch < args.epochs) {\n      const epochLogs: UnresolvedLogs = {};\n      await callbackList.onEpochBegin(epoch);\n      let stepsDone = 0;\n      let batchIndex = 0;\n      if (!hasBatchesPerEpoch) {\n        dataIterator = await dataset.iterator();\n      }\n      while (hasBatchesPerEpoch ? stepsDone < args.batchesPerEpoch : true) {\n        const iteratorOut = await dataIterator.next();\n\n        // If `batchesPerEpoch` is specified, the dataset should not be\n        // exhausted until all epoches are done.\n        if (hasBatchesPerEpoch && iteratorOut.done) {\n          console.warn(\n              'You provided `batchesPerEpoch` as ' +\n              `${args.batchesPerEpoch}, ` +\n              'but your dataset iterator ran out of data after ' +\n              `${stepsDone} batches; ` +\n              'interrupting training. Make sure that your ' +\n              'dataset can generate at least `batchesPerEpoch * epochs` ' +\n              'batches (in this case, ' +\n              `${args.batchesPerEpoch * args.epochs} batches). ` +\n              'You may need to use the repeat() function when building ' +\n              'your dataset.');\n          break;\n        }\n\n        if (iteratorOut.value != null) {\n          const {xs, ys} =\n              standardizeDataIteratorOutput(model, iteratorOut.value);\n          const batchLogs: UnresolvedLogs = {};\n          batchLogs['batch'] = batchIndex;\n          batchLogs['size'] = xs[0].shape[0];\n\n          await callbackList.onBatchBegin(batchIndex, batchLogs);\n\n          const sampleWeights: tfc.Tensor[] = [];\n          if (args.classWeight != null) {\n            const standardClassWeights =\n                standardizeClassWeights(args.classWeight, model.outputNames);\n            for (let i = 0; i < standardClassWeights.length; ++i) {\n              sampleWeights.push(await standardizeWeights(\n                  ys[i], null, standardClassWeights[i]));\n            }\n          }\n\n          // Train on batch.\n          const ins = xs.concat(ys).concat(sampleWeights);\n          const outs = trainFunction(ins);\n          tfc.dispose(ins);\n          for (let i = 0; i < outLabels.length; ++i) {\n            const label = outLabels[i];\n            const out = outs[i];\n            batchLogs[label] = out;\n            tfc.keep(out);\n          }\n\n          await callbackList.onBatchEnd(batchIndex, batchLogs);\n          disposeTensorsInLogs(batchLogs);\n\n          batchIndex++;\n          stepsDone++;\n        }\n\n        if (hasBatchesPerEpoch ? stepsDone >= args.batchesPerEpoch :\n                                 iteratorOut.done) {\n          // Epoch finished. Perform validation.\n          if (doValidation) {\n            let valOuts: tfc.Scalar[];\n            if (isDatasetObject(args.validationData)) {\n              valOuts = toList(await model.evaluateDataset(\n                  args.validationData, {batches: args.validationBatches}));\n            } else {\n              valOuts = toList(model.evaluate(valXs, valYs, {\n                batchSize: args.validationBatchSize == null ?\n                    DEFAULT_VALIDATION_BATCH_SIZE :\n                    args.validationBatchSize,\n                verbose: 0\n              }));\n            }\n            for (let i = 0; i < model.metricsNames.length; ++i) {\n              epochLogs[`val_${model.metricsNames[i]}`] = valOuts[i];\n            }\n          }\n          // Call `break` to exit one epoch lopp after validation is done. If\n          // config.batchesPerEpoch is specified, an epoch while loop will\n          // stop when `stepsDone >= config.batchesPerEpoch`. When\n          // config.batchesPerEpoch is not provided, the following `break` is\n          // required to exit the while lopp after dataset is exhausted.\n          break;\n        }\n\n        if (model.stopTraining_) {\n          break;\n        }\n      }\n      await callbackList.onEpochEnd(epoch, epochLogs);\n      epoch++;\n      if (model.stopTraining_) {\n        break;\n      }\n    }\n    await callbackList.onTrainEnd();\n    await model.history.syncData();\n    return model.history;\n  } finally {\n    model.isTraining = false;\n  }\n}\n\n/** Helper function that determines number of steps (batches) per epoch. */\nfunction getStepsPerEpoch<T>(\n    dataset: Dataset<T>, args: ModelFitDatasetArgs<T>): number {\n  // Attempt to determine # of batches in an epoch.\n  let stepsPerEpoch: number = null;\n  if (args.batchesPerEpoch != null) {\n    stepsPerEpoch = args.batchesPerEpoch;\n  } else if (Number.isFinite(dataset.size)) {\n    stepsPerEpoch = dataset.size;\n  }\n  return stepsPerEpoch;\n}\n\n// Check if provided object is a Dataset object by checking its .iterator\n// element.\nfunction isDatasetObject<T>(\n    dataset:\n        [\n          TensorOrArrayOrMap, TensorOrArrayOrMap\n        ]|[TensorOrArrayOrMap, TensorOrArrayOrMap, TensorOrArrayOrMap]|\n    Dataset<T>): boolean {\n  return (typeof (dataset as Dataset<T>).iterator === 'function');\n}\n\n// Check if provided object is a LazyIterator object by checking it's .next\n// element.\nfunction isLazyIteratorObject<T>(iterator: Dataset<T>|\n                                 LazyIterator<T>): boolean {\n  return (typeof (iterator as LazyIterator<T>).next === 'function');\n}\n\nexport async function evaluateDataset<T>(\n    // Type `model` as `any` here to avoid circular dependency w/\n    // training.ts.\n    // tslint:disable-next-line:no-any\n    model: any, dataset: Dataset<T>|LazyIterator<T>,\n    args: ModelEvaluateDatasetArgs): Promise<tfc.Scalar|tfc.Scalar[]> {\n  args = args || {};\n  const hasBatches = args.batches != null;\n  const f = model.testFunction;\n  let outs: tfc.Scalar[] = [];\n  if (args.verbose > 0) {\n    throw new NotImplementedError('Verbose mode is not implemented yet.');\n  }\n\n  tfc.util.assert(\n      !hasBatches || (args.batches > 0 && Number.isInteger(args.batches)),\n      () => 'Test loop expects `batches` to be a positive integer, but ' +\n          `received ${JSON.stringify(args.batches)}`);\n  const dataIterator = isLazyIteratorObject(dataset) ?\n      dataset as LazyIterator<T>:\n      await (dataset as Dataset<T>).iterator();\n  // Keeps track of number of examples used in this evaluation.\n  let numExamples = 0;\n  let batch = 0;\n\n  while (hasBatches ? batch < args.batches : true) {\n    const iteratorOut = await dataIterator.next();\n    outs = tfc.tidy(() => {\n      if (iteratorOut.value) {\n        // TODO(cais): Once real dataset is available, use\n        //   `map(x => standardizeDataIteratorOutput(model, x).map(f)`.\n        const {xs, ys} =\n            standardizeDataIteratorOutput(model, iteratorOut.value);\n        const xsAndYs = xs.concat(ys);\n        const batchOuts = tfc.tidy(() => f(xsAndYs));\n        tfc.dispose(xsAndYs);\n\n        if (batch === 0) {\n          for (let i = 0; i < batchOuts.length; ++i) {\n            outs.push(scalar(0));\n          }\n        }\n\n        const batchSize = xsAndYs[0].shape[0];\n        for (let i = 0; i < batchOuts.length; ++i) {\n          const batchOut = batchOuts[i];\n          const oldScalar = outs[i];\n          outs[i] =\n              tfc.tidy(() => tfc.add(outs[i], tfc.mul(batchSize, batchOut)));\n          if (batch > 0) {\n            tfc.dispose(oldScalar);\n          }\n        }\n        tfc.dispose(batchOuts);\n        numExamples += batchSize;\n\n        ++batch;\n      }\n      return outs;\n    });\n\n    if (iteratorOut.done) {\n      if (hasBatches) {\n        console.warn(\n            'Your dataset iterator ran out of data during evaluateDataset(). ' +\n            'Interrupting evalution. Make sure that your ' +\n            'dataset can generate at least `batches` ' +\n            `batches (in this case, ${args.batches} batches). ` +\n            'You may need to use the repeat() function when building ' +\n            'your dataset.');\n      }\n      break;\n    }\n  }\n\n  for (let i = 0; i < outs.length; ++i) {\n    const oldScalar = outs[i];\n    outs[i] = tfc.div(outs[i], numExamples);\n    tfc.dispose(oldScalar);\n  }\n\n  return singletonOrArray(outs);\n}\n"],"mappings":";;;AAAA;;;;;;;;;AAUA;;;AAIA,OAAO,KAAKA,GAAG,MAAM,uBAAuB;AAC5C,SAAQC,MAAM,QAAO,uBAAuB;AAC5C,SAAsBC,kBAAkB,EAAsDC,oBAAoB,QAA0B,mBAAmB;AAC/J,SAAQC,mBAAmB,EAAEC,UAAU,QAAO,WAAW;AACzD,SAAQC,oBAAoB,QAAuB,SAAS;AAE5D,SAAQC,gBAAgB,EAAEC,MAAM,QAAO,wBAAwB;AAG/D,SAAqCC,uBAAuB,EAAEC,kBAAkB,QAAO,kBAAkB;AAiKzG;AACA,IAAMC,6BAA6B,GAAG,EAAE;AAExC;;;;;;;;;;;;;;AAcA,SAASC,6BAA6BA;AAClC;AACA;AACA;AACAC,KAAU,EAAEC,WAAe;EAC7B,IAAIC,EAAsB;EAC1B,IAAIC,EAAsB;EAE1B,IAAMC,cAAc,GAAGH,WAAgC;EACvDC,EAAE,GAAGE,cAAc,CAAC,IAAI,CAAC;EACzBD,EAAE,GAAGC,cAAc,CAAC,IAAI,CAAC;EACzBjB,GAAG,CAACkB,IAAI,CAACC,MAAM,CACXJ,EAAE,IAAI,IAAI,IAAIC,EAAE,IAAI,IAAI,EACxB;IAAA,OAAM,8DAA8D,GAChE,4DAA4D,GAC5D,8DAA8D,GAC9D,4DAA4D,MAAAI,MAAA,CACzDN,WAAW,CAAE;EAAA,EAAC;EAEzB,IAAMO,WAAW,GACbC,yBAAyB,CAAC,OAAO,EAAET,KAAK,CAACU,UAAU,EAAER,EAAE,CAAC;EAC5D,IAAMS,WAAW,GACbF,yBAAyB,CAAC,QAAQ,EAAET,KAAK,CAACY,WAAW,EAAET,EAAE,CAAC;EAE9D,IAAMU,SAAS,GAAWL,WAAW,CAAC,CAAC,CAAC,CAACM,KAAK,CAAC,CAAC,CAAC;EAEjD3B,GAAG,CAACkB,IAAI,CAACC,MAAM,CACXE,WAAW,CAACO,MAAM,KAAKf,KAAK,CAACgB,MAAM,CAACD,MAAM,EAC1C;IAAA,OAAM,mBAAAR,MAAA,CAAmBP,KAAK,CAACgB,MAAM,CAACD,MAAM,6CAAAR,MAAA,CAC5BC,WAAW,CAACO,MAAM,qCAAkC,MAAAR,MAAA,CAC7DU,IAAI,CAACC,SAAS,CAAClB,KAAK,CAACU,UAAU,CAAC,MAAG;EAAA,EAAC;EAE/CvB,GAAG,CAACkB,IAAI,CAACC,MAAM,CACXK,WAAW,CAACI,MAAM,KAAKf,KAAK,CAACmB,OAAO,CAACJ,MAAM,EAC3C;IAAA,OACI,mBAAAR,MAAA,CAAmBP,KAAK,CAACmB,OAAO,CAACJ,MAAM,8CAAAR,MAAA,CAC3BI,WAAW,CAACI,MAAM,uCAAoC,MAAAR,MAAA,CAC/DU,IAAI,CAACC,SAAS,CAAClB,KAAK,CAACY,WAAW,CAAC,MAAG;EAAA,EAAC;EAAC,IAAAQ,KAAA,YAAAA,MAAAC,MAAA,EAEW;IAC1DlC,GAAG,CAACkB,IAAI,CAACC,MAAM,CACXE,WAAW,CAACa,MAAM,CAAC,CAACP,KAAK,CAAC,CAAC,CAAC,KAAKD,SAAS,EAC1C;MAAA,OAAM,mCAAAN,MAAA,CACCP,KAAK,CAACU,UAAU,CAACW,MAAM,CAAC,WAAAd,MAAA,CACrBC,WAAW,CAACa,MAAM,CAAC,CAACP,KAAK,CAAC,CAAC,CAAC,OAAI,gBAAAP,MAAA,CACzBM,SAAS,sBAAAN,MAAA,CAAmBP,KAAK,CAACU,UAAU,CAAC,CAAC,CAAC,MAAG;IAAA,EAAC;GACzE;EAPD,KAAK,IAAIW,MAAM,GAAG,CAAC,EAAEA,MAAM,GAAGb,WAAW,CAACO,MAAM,EAAEM,MAAM,EAAE;IAAAD,KAAA,CAAAC,MAAA;EAAA;EAOzD,IAAAC,MAAA,YAAAA,OAAAC,MAAA,EAE2D;IAC1DpC,GAAG,CAACkB,IAAI,CAACC,MAAM,CACXK,WAAW,CAACY,MAAM,CAAC,CAACT,KAAK,CAAC,CAAC,CAAC,KAAKD,SAAS,EAC1C;MAAA,OAAM,oCAAAN,MAAA,CACCP,KAAK,CAACY,WAAW,CAACW,MAAM,CAAC,WAAAhB,MAAA,CACtBI,WAAW,CAACY,MAAM,CAAC,CAACT,KAAK,CAAC,CAAC,CAAC,OAAI,gBAAAP,MAAA,CACzBM,SAAS,sBAAAN,MAAA,CAAmBP,KAAK,CAACU,UAAU,CAAC,CAAC,CAAC,MAAG;IAAA,EAAC;GACzE;EAPD,KAAK,IAAIa,MAAM,GAAG,CAAC,EAAEA,MAAM,GAAGZ,WAAW,CAACI,MAAM,EAAEQ,MAAM,EAAE;IAAAD,MAAA,CAAAC,MAAA;EAAA;EAS1D,OAAO;IAACrB,EAAE,EAAEM,WAAW;IAAEL,EAAE,EAAEQ;EAAW,CAAC;AAC3C;AAEA,SAASF,yBAAyBA,CAC9Be,aAAqB,EAAEC,KAAe,EAAEC,MAA0B;EACpE,IAAIA,MAAM,YAAYvC,GAAG,CAACwC,MAAM,EAAE;IAChC,OAAO,CAACD,MAAM,CAAC;GAChB,MAAM,IAAIE,KAAK,CAACC,OAAO,CAACH,MAAM,CAAC,EAAE;IAChCvC,GAAG,CAACkB,IAAI,CAACC,MAAM,CACXoB,MAAM,CAACX,MAAM,KAAKU,KAAK,CAACV,MAAM,EAC9B;MAAA,+BAAAR,MAAA,CAA8BmB,MAAM,CAACX,MAAM,6BAAAR,MAAA,CACvCkB,KAAK,CAACV,MAAM,oBAAAR,MAAA,CAAiBiB,aAAa,YAAAjB,MAAA,CAASkB,KAAK;IAAA,CAAG,CAAC;IACpE,OAAOC,MAAM;GACd,MAAM;IACL,IAAMI,MAAM,GAAiB,EAAE;IAC/B;IAAA,IAAAC,SAAA,GAAAC,0BAAA,CACmBP,KAAK;MAAAQ,KAAA;IAAA;MAAxB,KAAAF,SAAA,CAAAG,CAAA,MAAAD,KAAA,GAAAF,SAAA,CAAAI,CAAA,IAAAC,IAAA,GAA0B;QAAA,IAAfC,IAAI,GAAAJ,KAAA,CAAAK,KAAA;QACb,IAAIZ,MAAM,CAACW,IAAI,CAAC,IAAI,IAAI,EAAE;UACxB,MAAM,IAAI7C,UAAU,CAChB,qEAAAe,MAAA,CACGiB,aAAa,YAAAjB,MAAA,CAAS8B,IAAI,OAAI,CAAC;;QAExCP,MAAM,CAACS,IAAI,CAACb,MAAM,CAACW,IAAI,CAAC,CAAC;;IAC1B,SAAAG,GAAA;MAAAT,SAAA,CAAAU,CAAA,CAAAD,GAAA;IAAA;MAAAT,SAAA,CAAAW,CAAA;IAAA;IACD,OAAOZ,MAAM;;AAEjB;AAEA,SAASa,+BAA+BA,CACpCC,IAIiC;EAEnC,IAAIA,IAAI,CAAC7B,MAAM,KAAK,CAAC,EAAE;IACrB,MAAM,IAAIxB,mBAAmB,CACzB,wDAAwD,CAAC;;EAE/D,OAAO;IAACW,EAAE,EAAE0C,IAAI,CAAC,CAAC,CAAC;IAAEzC,EAAE,EAAEyC,IAAI,CAAC,CAAC;EAAC,CAAC;AACnC;AAEA,gBAAsBC,UAAUA,CAAAC,EAAA,EAAAC,GAAA,EAAAC,GAAA;EAAA,OAAAC,WAAA,CAAAC,KAAA,OAAAC,SAAA;AAAA;AAyMhC;AAAA,SAAAF,YAAA;EAAAA,WAAA,GAAAG,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CAzMO,SAAAC;EACH;EACA;EACA;EACAvD,KAAU,EAAEwD,OAAmB,EAC/BC,IAA4B;IAAA,IAAAC,kBAAA,EAAAC,YAAA,EAAAC,KAAA,EAAAC,KAAA,EAAAC,cAAA,EAAAC,aAAA,EAAAC,SAAA,EAAAC,eAAA,EAAAC,SAAA,EAAAC,OAAA,EAAAC,mBAAA,EAAAC,YAAA,EAAAC,OAAA,EAAAC,KAAA,EAAAC,YAAA,EAAAC,SAAA,EAAAC,SAAA,EAAAC,UAAA,EAAA1E,WAAA,EAAA2E,qBAAA,EAAA1E,EAAA,EAAAC,EAAA,EAAA0E,SAAA,EAAAC,aAAA,EAAAC,oBAAA,EAAAC,CAAA,EAAAC,GAAA,EAAAC,IAAA,EAAAC,EAAA,EAAAC,KAAA,EAAAC,GAAA,EAAAC,OAAA,EAAAC,GAAA;IAAA,OAAAlC,mBAAA,GAAAmC,IAAA,UAAAC,SAAAC,QAAA;MAAA,kBAAAA,QAAA,CAAAC,IAAA,GAAAD,QAAA,CAAAE,IAAA;QAAA;UACxBlC,kBAAkB,GAAGD,IAAI,CAACoC,eAAe,IAAI,IAAI;UACvD1G,GAAG,CAACkB,IAAI,CAACC,MAAM,CACXN,KAAK,CAAC8F,SAAS,IAAI,IAAI,EACvB;YAAA,OAAM,wDAAwD,GAC1D,0CAA0C;UAAA,EAAC;UAEnD3G,GAAG,CAACkB,IAAI,CAACC,MAAM,CACXmD,IAAI,IAAI,IAAI,EACZ;YAAA,OAAM,oGACoC;UAAA,EAAC;UAC/CtE,GAAG,CAACkB,IAAI,CAACC,MAAM,CACXmD,IAAI,CAACsC,MAAM,IAAI,IAAI,IAAItC,IAAI,CAACsC,MAAM,GAAG,CAAC,IAAIC,MAAM,CAACC,SAAS,CAACxC,IAAI,CAACsC,MAAM,CAAC,EACvE;YAAA,OAAM,sFAAAxF,MAAA,CACkBkD,IAAI,CAACsC,MAAM,CAAE;UAAA,EAAC;UAC1C5G,GAAG,CAACkB,IAAI,CAACC,MAAM,CACX,CAACoD,kBAAkB,IACdD,IAAI,CAACoC,eAAe,GAAG,CAAC,IAAIG,MAAM,CAACC,SAAS,CAACxC,IAAI,CAACoC,eAAe,CAAE,EACxE;YAAA,OAAM,4GAAAtF,MAAA,CACwCkD,IAAI,CAACoC,eAAe,CAAE;UAAA,EAAC;UACzE1G,GAAG,CAACkB,IAAI,CAACC,MAAM;UACX;UACCmD,IAAY,CAAC,iBAAiB,CAAC,IAAI,IAAI,EACxC;YAAA,OAAM,wDAAwD,GAC1D,6BAA6B;UAAA,EAAC;UAAC,KAEnCzD,KAAK,CAACkG,UAAU;YAAAR,QAAA,CAAAE,IAAA;YAAA;UAAA;UAAA,MACZ,IAAIO,KAAK,CACX,8DAA8D,CAAC;QAAA;UAErEnG,KAAK,CAACkG,UAAU,GAAG,IAAI;UAACR,QAAA,CAAAC,IAAA;UAGhBhC,YAAY,GAAGF,IAAI,CAACK,cAAc,IAAI,IAAI;UAGhD,IAAIH,YAAY,EAAE;YAChB,IAAIyC,eAAe,CAAC3C,IAAI,CAACK,cAAc,CAAC,EAAE;cACxC3E,GAAG,CAACkB,IAAI,CAACC,MAAM,CACXmD,IAAI,CAAC4C,iBAAiB,IAAI,IAAI,IACzB5C,IAAI,CAAC4C,iBAAiB,GAAG,CAAC,IAC1BL,MAAM,CAACC,SAAS,CAACxC,IAAI,CAAC4C,iBAAiB,CAAE,EAC9C;gBAAA,OAAM,gHACyD,kCAC5B,cAAA9F,MAAA,CACpBkD,IAAI,CAAC4C,iBAAiB,CAAE;cAAA,EAAC;aAC7C,MAAM;cACCvC,cAAc,GAAGnB,+BAA+B,CAClDc,IAAI,CAACK,cAKJ,CAAC;cACNF,KAAK,GAAGE,cAAc,CAAC5D,EAAE;cACzB2D,KAAK,GAAGC,cAAc,CAAC3D,EAAE;;;UAIvB4D,aAAa,GAAG/D,KAAK,CAACsG,iBAAiB,EAAE;UACzCtC,SAAS,GAAGhE,KAAK,CAACuG,sBAAsB,EAAc;UAG5D,IAAI5C,YAAY,EAAE;YAChBM,eAAe,GACXD,SAAS,CAACwC,KAAK,EAAE,CAACjG,MAAM,CAACyD,SAAS,CAACyC,GAAG,CAAC,UAAAtE,CAAC;cAAA,OAAI,MAAM,GAAGA,CAAC;YAAA,EAAC,CAAC;WAC7D,MAAM;YACL8B,eAAe,GAAGD,SAAS,CAACwC,KAAK,EAAE;;UAG/BtC,SAAS,GAAG5E,oBAAoB,CAACmE,IAAI,CAACS,SAAS,EAAET,IAAI,CAACiD,UAAU,CAAC;UACjEvC,OAAO,GAAGV,IAAI,CAACU,OAAO,IAAI,IAAI,GAAG,CAAC,GAAGV,IAAI,CAACU,OAAO;UAAAC,mBAAA,GACvB/E,kBAAkB,CAC9C6E,SAAS,EAAEC,OAAO,EAAEV,IAAI,CAACsC,MAAM,EAAE,IAAI,EAAE,IAAI,EAC3CY,gBAAgB,CAACnD,OAAO,EAAEC,IAAI,CAAC,EAC/B,IAAI;UAAG;UACPE,YAAY,EAAEM,eAAe,CAAC,EAJ3BI,YAAY,GAAAD,mBAAA,CAAZC,YAAY,EAAEC,OAAO,GAAAF,mBAAA,CAAPE,OAAO;UAK5BD,YAAY,CAACuC,QAAQ,CAAC5G,KAAK,CAAC;UAC5BA,KAAK,CAACsE,OAAO,GAAGA,OAAO;UAACoB,QAAA,CAAAE,IAAA;UAAA,OAElBvB,YAAY,CAACwC,YAAY,EAAE;QAAA;UACjC7G,KAAK,CAAC8G,aAAa,GAAG,KAAK;UACvBvC,KAAK,GAAGd,IAAI,CAACsD,YAAY,IAAI,IAAI,GAAG,CAAC,GAAGtD,IAAI,CAACsD,YAAY;UAAArB,QAAA,CAAAE,IAAA;UAAA,OAEpCpC,OAAO,CAACwD,QAAQ,EAAE;QAAA;UAAvCxC,YAAY,GAAAkB,QAAA,CAAAuB,IAAA;QAAA;UAAA,MACT1C,KAAK,GAAGd,IAAI,CAACsC,MAAM;YAAAL,QAAA,CAAAE,IAAA;YAAA;UAAA;UAClBnB,SAAS,GAAmB,EAAE;UAAAiB,QAAA,CAAAE,IAAA;UAAA,OAC9BvB,YAAY,CAAC6C,YAAY,CAAC3C,KAAK,CAAC;QAAA;UAClCG,SAAS,GAAG,CAAC;UACbC,UAAU,GAAG,CAAC;UAAA,IACbjB,kBAAkB;YAAAgC,QAAA,CAAAE,IAAA;YAAA;UAAA;UAAAF,QAAA,CAAAE,IAAA;UAAA,OACApC,OAAO,CAACwD,QAAQ,EAAE;QAAA;UAAvCxC,YAAY,GAAAkB,QAAA,CAAAuB,IAAA;QAAA;UAAA,MAEPvD,kBAAkB,GAAGgB,SAAS,GAAGjB,IAAI,CAACoC,eAAe,GAAG,IAAI;YAAAH,QAAA,CAAAE,IAAA;YAAA;UAAA;UAAAF,QAAA,CAAAE,IAAA;UAAA,OACvCpB,YAAY,CAACoB,IAAI,EAAE;QAAA;UAAvC3F,WAAW,GAAAyF,QAAA,CAAAuB,IAAA;UAAA,MAIbvD,kBAAkB,IAAIzD,WAAW,CAACmC,IAAI;YAAAsD,QAAA,CAAAE,IAAA;YAAA;UAAA;UACxCuB,OAAO,CAACC,IAAI,CACR,oCAAoC,MAAA7G,MAAA,CACjCkD,IAAI,CAACoC,eAAe,OAAI,GAC3B,kDAAkD,MAAAtF,MAAA,CAC/CmE,SAAS,eAAY,GACxB,6CAA6C,GAC7C,2DAA2D,GAC3D,yBAAyB,MAAAnE,MAAA,CACtBkD,IAAI,CAACoC,eAAe,GAAGpC,IAAI,CAACsC,MAAM,gBAAa,GAClD,0DAA0D,GAC1D,eAAe,CAAC;UAAC,OAAAL,QAAA,CAAA2B,MAAA;QAAA;UAAA,MAInBpH,WAAW,CAACqC,KAAK,IAAI,IAAI;YAAAoD,QAAA,CAAAE,IAAA;YAAA;UAAA;UAAAhB,qBAAA,GAEvB7E,6BAA6B,CAACC,KAAK,EAAEC,WAAW,CAACqC,KAAK,CAAC,EADpDpC,EAAE,GAAA0E,qBAAA,CAAF1E,EAAE,EAAEC,EAAE,GAAAyE,qBAAA,CAAFzE,EAAE;UAEP0E,SAAS,GAAmB,EAAE;UACpCA,SAAS,CAAC,OAAO,CAAC,GAAGF,UAAU;UAC/BE,SAAS,CAAC,MAAM,CAAC,GAAG3E,EAAE,CAAC,CAAC,CAAC,CAACY,KAAK,CAAC,CAAC,CAAC;UAAC4E,QAAA,CAAAE,IAAA;UAAA,OAE7BvB,YAAY,CAACiD,YAAY,CAAC3C,UAAU,EAAEE,SAAS,CAAC;QAAA;UAEhDC,aAAa,GAAiB,EAAE;UAAA,MAClCrB,IAAI,CAAC8D,WAAW,IAAI,IAAI;YAAA7B,QAAA,CAAAE,IAAA;YAAA;UAAA;UACpBb,oBAAoB,GACtBnF,uBAAuB,CAAC6D,IAAI,CAAC8D,WAAW,EAAEvH,KAAK,CAACY,WAAW,CAAC;UACvDoE,CAAC,GAAG,CAAC;QAAA;UAAA,MAAEA,CAAC,GAAGD,oBAAoB,CAAChE,MAAM;YAAA2E,QAAA,CAAAE,IAAA;YAAA;UAAA;UAAAF,QAAA,CAAA8B,EAAA,GAC7C1C,aAAa;UAAAY,QAAA,CAAAE,IAAA;UAAA,OAAY/F,kBAAkB,CACvCM,EAAE,CAAC6E,CAAC,CAAC,EAAE,IAAI,EAAED,oBAAoB,CAACC,CAAC,CAAC,CAAC;QAAA;UAAAU,QAAA,CAAA+B,EAAA,GAAA/B,QAAA,CAAAuB,IAAA;UAAAvB,QAAA,CAAA8B,EAAA,CAD3BjF,IAAI,CAAAmF,IAAA,CAAAhC,QAAA,CAAA8B,EAAA,EAAA9B,QAAA,CAAA+B,EAAA;QAAA;UAD6B,EAAEzC,CAAC;UAAAU,QAAA,CAAAE,IAAA;UAAA;QAAA;UAMtD;UACMX,GAAG,GAAG/E,EAAE,CAACK,MAAM,CAACJ,EAAE,CAAC,CAACI,MAAM,CAACuE,aAAa,CAAC;UACzCI,IAAI,GAAGnB,aAAa,CAACkB,GAAG,CAAC;UAC/B9F,GAAG,CAACwI,OAAO,CAAC1C,GAAG,CAAC;UAChB,KAASD,EAAC,GAAG,CAAC,EAAEA,EAAC,GAAGhB,SAAS,CAACjD,MAAM,EAAE,EAAEiE,EAAC,EAAE;YACnCI,KAAK,GAAGpB,SAAS,CAACgB,EAAC,CAAC;YACpBK,GAAG,GAAGH,IAAI,CAACF,EAAC,CAAC;YACnBH,SAAS,CAACO,KAAK,CAAC,GAAGC,GAAG;YACtBlG,GAAG,CAACyI,IAAI,CAACvC,GAAG,CAAC;;UACdK,QAAA,CAAAE,IAAA;UAAA,OAEKvB,YAAY,CAACwD,UAAU,CAAClD,UAAU,EAAEE,SAAS,CAAC;QAAA;UACpDpF,oBAAoB,CAACoF,SAAS,CAAC;UAE/BF,UAAU,EAAE;UACZD,SAAS,EAAE;QAAC;UAAA,MAGVhB,kBAAkB,GAAGgB,SAAS,IAAIjB,IAAI,CAACoC,eAAe,GACjC5F,WAAW,CAACmC,IAAI;YAAAsD,QAAA,CAAAE,IAAA;YAAA;UAAA;UAAA,KAEnCjC,YAAY;YAAA+B,QAAA,CAAAE,IAAA;YAAA;UAAA;UACVN,OAAqB;UAAA,KACrBc,eAAe,CAAC3C,IAAI,CAACK,cAAc,CAAC;YAAA4B,QAAA,CAAAE,IAAA;YAAA;UAAA;UAAAF,QAAA,CAAAoC,EAAA,GAC5BnI,MAAM;UAAA+F,QAAA,CAAAE,IAAA;UAAA,OAAO5F,KAAK,CAAC+H,eAAe,CACxCtE,IAAI,CAACK,cAAc,EAAE;YAACkE,OAAO,EAAEvE,IAAI,CAAC4C;UAAiB,CAAC,CAAC;QAAA;UAAAX,QAAA,CAAAuC,EAAA,GAAAvC,QAAA,CAAAuB,IAAA;UAD3D3B,OAAO,OAAAI,QAAA,CAAAoC,EAAA,EAAApC,QAAA,CAAAuC,EAAA;UAAAvC,QAAA,CAAAE,IAAA;UAAA;QAAA;UAGPN,OAAO,GAAG3F,MAAM,CAACK,KAAK,CAACkI,QAAQ,CAACtE,KAAK,EAAEC,KAAK,EAAE;YAC5ChD,SAAS,EAAE4C,IAAI,CAAC0E,mBAAmB,IAAI,IAAI,GACvCrI,6BAA6B,GAC7B2D,IAAI,CAAC0E,mBAAmB;YAC5BhE,OAAO,EAAE;WACV,CAAC,CAAC;QAAC;UAEN,KAASa,GAAC,GAAG,CAAC,EAAEA,GAAC,GAAGhF,KAAK,CAACoI,YAAY,CAACrH,MAAM,EAAE,EAAEiE,GAAC,EAAE;YAClDP,SAAS,QAAAlE,MAAA,CAAQP,KAAK,CAACoI,YAAY,CAACpD,GAAC,CAAC,EAAG,GAAGM,OAAO,CAACN,GAAC,CAAC;;QACvD;UAAA,OAAAU,QAAA,CAAA2B,MAAA;QAAA;UAAA,KAUDrH,KAAK,CAAC8G,aAAa;YAAApB,QAAA,CAAAE,IAAA;YAAA;UAAA;UAAA,OAAAF,QAAA,CAAA2B,MAAA;QAAA;UAAA3B,QAAA,CAAAE,IAAA;UAAA;QAAA;UAAAF,QAAA,CAAAE,IAAA;UAAA,OAInBvB,YAAY,CAACgE,UAAU,CAAC9D,KAAK,EAAEE,SAAS,CAAC;QAAA;UAC/CF,KAAK,EAAE;UAAC,KACJvE,KAAK,CAAC8G,aAAa;YAAApB,QAAA,CAAAE,IAAA;YAAA;UAAA;UAAA,OAAAF,QAAA,CAAA2B,MAAA;QAAA;UAAA3B,QAAA,CAAAE,IAAA;UAAA;QAAA;UAAAF,QAAA,CAAAE,IAAA;UAAA,OAInBvB,YAAY,CAACiE,UAAU,EAAE;QAAA;UAAA5C,QAAA,CAAAE,IAAA;UAAA,OACzB5F,KAAK,CAACsE,OAAO,CAACiE,QAAQ,EAAE;QAAA;UAAA,OAAA7C,QAAA,CAAA2B,MAAA,WACvBrH,KAAK,CAACsE,OAAO;QAAA;UAAAoB,QAAA,CAAAC,IAAA;UAEpB3F,KAAK,CAACkG,UAAU,GAAG,KAAK;UAAC,OAAAR,QAAA,CAAA8C,MAAA;QAAA;QAAA;UAAA,OAAA9C,QAAA,CAAA+C,IAAA;MAAA;IAAA,GAAAlF,OAAA;EAAA,CAE5B;EAAA,OAAAN,WAAA,CAAAC,KAAA,OAAAC,SAAA;AAAA;AAGD,SAASwD,gBAAgBA,CACrBnD,OAAmB,EAAEC,IAA4B;EACnD;EACA,IAAIiF,aAAa,GAAW,IAAI;EAChC,IAAIjF,IAAI,CAACoC,eAAe,IAAI,IAAI,EAAE;IAChC6C,aAAa,GAAGjF,IAAI,CAACoC,eAAe;GACrC,MAAM,IAAIG,MAAM,CAAC2C,QAAQ,CAACnF,OAAO,CAACoF,IAAI,CAAC,EAAE;IACxCF,aAAa,GAAGlF,OAAO,CAACoF,IAAI;;EAE9B,OAAOF,aAAa;AACtB;AAEA;AACA;AACA,SAAStC,eAAeA,CACpB5C,OAIU;EACZ,OAAQ,OAAQA,OAAsB,CAACwD,QAAQ,KAAK,UAAU;AAChE;AAEA;AACA;AACA,SAAS6B,oBAAoBA,CAAI7B,QACe;EAC9C,OAAQ,OAAQA,QAA4B,CAACpB,IAAI,KAAK,UAAU;AAClE;AAEA,gBAAsBmC,eAAeA,CAAAe,GAAA,EAAAC,GAAA,EAAAC,GAAA;EAAA,OAAAC,gBAAA,CAAA/F,KAAA,OAAAC,SAAA;AAAA;AAkFpC,SAAA8F,iBAAA;EAAAA,gBAAA,GAAA7F,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CAlFM,SAAA4F;EACH;EACA;EACA;EACAlJ,KAAU,EAAEwD,OAAmC,EAC/CC,IAA8B;IAAA,IAAA0F,UAAA,EAAAzG,CAAA,EAAAwC,IAAA,EAAAV,YAAA,EAAA4E,WAAA,EAAAC,KAAA,EAAAC,MAAA,EAAAC,IAAA,EAAAvE,CAAA,EAAAwE,SAAA;IAAA,OAAAnG,mBAAA,GAAAmC,IAAA,UAAAiE,UAAAC,SAAA;MAAA,kBAAAA,SAAA,CAAA/D,IAAA,GAAA+D,SAAA,CAAA9D,IAAA;QAAA;UAChCnC,IAAI,GAAGA,IAAI,IAAI,EAAE;UACX0F,UAAU,GAAG1F,IAAI,CAACuE,OAAO,IAAI,IAAI;UACjCtF,CAAC,GAAG1C,KAAK,CAAC2J,YAAY;UACxBzE,IAAI,GAAiB,EAAE;UAAA,MACvBzB,IAAI,CAACU,OAAO,GAAG,CAAC;YAAAuF,SAAA,CAAA9D,IAAA;YAAA;UAAA;UAAA,MACZ,IAAIrG,mBAAmB,CAAC,sCAAsC,CAAC;QAAA;UAGvEJ,GAAG,CAACkB,IAAI,CAACC,MAAM,CACX,CAAC6I,UAAU,IAAK1F,IAAI,CAACuE,OAAO,GAAG,CAAC,IAAIhC,MAAM,CAACC,SAAS,CAACxC,IAAI,CAACuE,OAAO,CAAE,EACnE;YAAA,OAAM,4DAA4D,eAAAzH,MAAA,CAClDU,IAAI,CAACC,SAAS,CAACuC,IAAI,CAACuE,OAAO,CAAC,CAAE;UAAA,EAAC;UAAC,KAC/Ba,oBAAoB,CAACrF,OAAO,CAAC;YAAAkG,SAAA,CAAA9D,IAAA;YAAA;UAAA;UAAA8D,SAAA,CAAAlC,EAAA,GAC9ChE,OAA0B;UAAAkG,SAAA,CAAA9D,IAAA;UAAA;QAAA;UAAA8D,SAAA,CAAA9D,IAAA;UAAA,OACnBpC,OAAsB,CAACwD,QAAQ,EAAE;QAAA;UAAA0C,SAAA,CAAAlC,EAAA,GAAAkC,SAAA,CAAAzC,IAAA;QAAA;UAFtCzC,YAAY,GAAAkF,SAAA,CAAAlC,EAAA;UAGlB;UACI4B,WAAW,GAAG,CAAC;UACfC,KAAK,GAAG,CAAC;UAAAC,MAAA,gBAAAjG,mBAAA,GAAAC,IAAA,UAAAgG,OAAA;YAAA,IAAArJ,WAAA;YAAA,OAAAoD,mBAAA,GAAAmC,IAAA,UAAAoE,QAAAC,SAAA;cAAA,kBAAAA,SAAA,CAAAlE,IAAA,GAAAkE,SAAA,CAAAjE,IAAA;gBAAA;kBAAAiE,SAAA,CAAAjE,IAAA;kBAAA,OAGepB,YAAY,CAACoB,IAAI,EAAE;gBAAA;kBAAvC3F,WAAW,GAAA4J,SAAA,CAAA5C,IAAA;kBACjB/B,IAAI,GAAG/F,GAAG,CAAC2K,IAAI,CAAC,YAAK;oBACnB,IAAI7J,WAAW,CAACqC,KAAK,EAAE;sBACrB;sBACA;sBACA,IAAAyH,sBAAA,GACIhK,6BAA6B,CAACC,KAAK,EAAEC,WAAW,CAACqC,KAAK,CAAC;wBADpDpC,EAAE,GAAA6J,sBAAA,CAAF7J,EAAE;wBAAEC,EAAE,GAAA4J,sBAAA,CAAF5J,EAAE;sBAEb,IAAM6J,OAAO,GAAG9J,EAAE,CAACK,MAAM,CAACJ,EAAE,CAAC;sBAC7B,IAAM8J,SAAS,GAAG9K,GAAG,CAAC2K,IAAI,CAAC;wBAAA,OAAMpH,CAAC,CAACsH,OAAO,CAAC;sBAAA,EAAC;sBAC5C7K,GAAG,CAACwI,OAAO,CAACqC,OAAO,CAAC;sBAEpB,IAAIX,KAAK,KAAK,CAAC,EAAE;wBACf,KAAK,IAAIrE,GAAC,GAAG,CAAC,EAAEA,GAAC,GAAGiF,SAAS,CAAClJ,MAAM,EAAE,EAAEiE,GAAC,EAAE;0BACzCE,IAAI,CAAC3C,IAAI,CAACnD,MAAM,CAAC,CAAC,CAAC,CAAC;;;sBAIxB,IAAMyB,SAAS,GAAGmJ,OAAO,CAAC,CAAC,CAAC,CAAClJ,KAAK,CAAC,CAAC,CAAC;sBAAC,IAAAoJ,MAAA,YAAAA,OAAAC,GAAA,EACK;wBACzC,IAAMC,QAAQ,GAAGH,SAAS,CAACjF,GAAC,CAAC;wBAC7B,IAAMwE,SAAS,GAAGtE,IAAI,CAACF,GAAC,CAAC;wBACzBE,IAAI,CAACF,GAAC,CAAC,GACH7F,GAAG,CAAC2K,IAAI,CAAC;0BAAA,OAAM3K,GAAG,CAACkL,GAAG,CAACnF,IAAI,CAACF,GAAC,CAAC,EAAE7F,GAAG,CAACmL,GAAG,CAACzJ,SAAS,EAAEuJ,QAAQ,CAAC,CAAC;wBAAA,EAAC;wBAClE,IAAIf,KAAK,GAAG,CAAC,EAAE;0BACblK,GAAG,CAACwI,OAAO,CAAC6B,SAAS,CAAC;;uBAEzB;sBARD,KAAK,IAAIxE,GAAC,GAAG,CAAC,EAAEA,GAAC,GAAGiF,SAAS,CAAClJ,MAAM,EAAE,EAAEiE,GAAC;wBAAAkF,MAAA,CAAAC,GAAA;sBAAA;sBASzChL,GAAG,CAACwI,OAAO,CAACsC,SAAS,CAAC;sBACtBb,WAAW,IAAIvI,SAAS;sBAExB,EAAEwI,KAAK;;oBAET,OAAOnE,IAAI;kBACb,CAAC,CAAC;kBAAC,KAECjF,WAAW,CAACmC,IAAI;oBAAAyH,SAAA,CAAAjE,IAAA;oBAAA;kBAAA;kBAClB,IAAIuD,UAAU,EAAE;oBACdhC,OAAO,CAACC,IAAI,CACR,kEAAkE,GAClE,8CAA8C,GAC9C,0CAA0C,6BAAA7G,MAAA,CAChBkD,IAAI,CAACuE,OAAO,gBAAa,GACnD,0DAA0D,GAC1D,eAAe,CAAC;;kBACrB,OAAA6B,SAAA,CAAAxC,MAAA;gBAAA;gBAAA;kBAAA,OAAAwC,SAAA,CAAApB,IAAA;cAAA;YAAA,GAAAa,MAAA;UAAA;QAAA;UAAA,MA7CEH,UAAU,GAAGE,KAAK,GAAG5F,IAAI,CAACuE,OAAO,GAAG,IAAI;YAAA0B,SAAA,CAAA9D,IAAA;YAAA;UAAA;UAAA,OAAA8D,SAAA,CAAAa,aAAA,CAAAjB,MAAA;QAAA;UAAAC,IAAA,GAAAG,SAAA,CAAAjC,EAAA;UAAA,MAAA8B,IAAA;YAAAG,SAAA,CAAA9D,IAAA;YAAA;UAAA;UAAA,OAAA8D,SAAA,CAAArC,MAAA;QAAA;UAAAqC,SAAA,CAAA9D,IAAA;UAAA;QAAA;UAkD/C,KAASZ,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGE,IAAI,CAACnE,MAAM,EAAE,EAAEiE,CAAC,EAAE;YAC9BwE,SAAS,GAAGtE,IAAI,CAACF,CAAC,CAAC;YACzBE,IAAI,CAACF,CAAC,CAAC,GAAG7F,GAAG,CAACqL,GAAG,CAACtF,IAAI,CAACF,CAAC,CAAC,EAAEoE,WAAW,CAAC;YACvCjK,GAAG,CAACwI,OAAO,CAAC6B,SAAS,CAAC;;UACvB,OAAAE,SAAA,CAAArC,MAAA,WAEM3H,gBAAgB,CAACwF,IAAI,CAAC;QAAA;QAAA;UAAA,OAAAwE,SAAA,CAAAjB,IAAA;MAAA;IAAA,GAAAS,QAAA;EAAA,CAC9B;EAAA,OAAAD,gBAAA,CAAA/F,KAAA,OAAAC,SAAA;AAAA"},"metadata":{},"sourceType":"module","externalDependencies":[]}