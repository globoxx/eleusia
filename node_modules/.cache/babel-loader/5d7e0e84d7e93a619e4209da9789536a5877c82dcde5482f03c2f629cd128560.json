{"ast":null,"code":"import _slicedToArray from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/slicedToArray.js\";\nimport _defineProperty from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/defineProperty.js\";\nimport _classCallCheck from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/classCallCheck.js\";\nimport _createClass from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/createClass.js\";\nimport _get from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/get.js\";\nimport _getPrototypeOf from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/getPrototypeOf.js\";\nimport _inherits from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/inherits.js\";\nimport _createSuper from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/createSuper.js\";\nimport _createForOfIteratorHelper from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/createForOfIteratorHelper.js\";\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * Normalization layers.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { moments, reshape, serialization, tidy, util } from '@tensorflow/tfjs-core';\nimport { getConstraint, serializeConstraint } from '../constraints';\nimport { InputSpec, Layer } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { getInitializer, serializeInitializer } from '../initializers';\nimport { getRegularizer, serializeRegularizer } from '../regularizers';\nimport * as generic_utils from '../utils/generic_utils';\nimport * as math_utils from '../utils/math_utils';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\n/**\n * Applies batch normalization on x given mean, var, beta and gamma.\n *\n * I.e. returns:\n *   `output = (x - mean) / (sqrt(var) + epsilon) * gamma + beta`\n *\n * @param x Input tensor.\n * @param mean Mean of batch.\n * @param variance Variance of batch.\n * @param beta Tensor with which to center the input.\n * @param gamma Tensor by which to scale the input.\n * @param epsilon Fuzz factor.\n * @returns The result of the batch normalization.\n */\nexport function batchNormalization(x, mean, variance, beta, gamma) {\n  var epsilon = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : 1e-3;\n  var out;\n  if (x.rank === 2) {\n    out = tfc.batchNorm2d(x, mean, variance, beta, gamma, epsilon);\n  } else if (x.rank === 3) {\n    // TODO(cais): Check rank; give proper error message.\n    out = tfc.batchNorm3d(x, mean, variance, beta, gamma, epsilon);\n  } else if (x.rank === 4) {\n    out = tfc.batchNorm4d(x, mean, variance, beta, gamma, epsilon);\n  } else {\n    throw new NotImplementedError(\"batchNormalization is not implemented for array of rank \".concat(x.rank, \" \") + \"yet\");\n  }\n  return out;\n}\n/**\n * Non-broadcasting batch normalization for use in training (not inference).\n *\n * The input is normalized to zero mean and unit variance along the\n * `reductionAxes`, followed by scaling with `gamma` and shifted by `beta`.\n * The result of that is returned as the first element\n * of the returned `Array`. The other two elements are the mean and variance,\n * respectively.\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nfunction regularNormalizeBatchInTraining(x, gamma, beta, reductionAxes) {\n  var epsilon = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : 1e-3;\n  return tidy(function () {\n    var meanAndVariance = tfc.moments(x, reductionAxes);\n    var mean = meanAndVariance.mean;\n    var variance = meanAndVariance.variance;\n    var normed = batchNormalization(x, mean, variance, beta, gamma, epsilon);\n    return [normed, mean, variance];\n  });\n}\n/**\n * Broadcasting batch normalization for use in training (not inference).\n *\n * The input is normalized to zero mean and unit variance along the\n * `reductionAxes`, followed by scaling with `gamma` and shifted by `beta`.\n * The result of that is returned as the first element\n * of the returned `Array`. The other two elements are the mean and variance,\n * respectively.\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nfunction broadcastNormalizeBatchInTraining(x, gamma, beta, reductionAxes) {\n  var epsilon = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : 1e-3;\n  return tidy(function () {\n    var meanAndVariance = tfc.moments(x, reductionAxes);\n    var mean = meanAndVariance.mean;\n    var variance = meanAndVariance.variance;\n    var targetShape = [];\n    var _iterator = _createForOfIteratorHelper(math_utils.range(0, x.rank)),\n      _step;\n    try {\n      for (_iterator.s(); !(_step = _iterator.n()).done;) {\n        var axis = _step.value;\n        if (reductionAxes.indexOf(axis) !== -1) {\n          targetShape.push(1);\n        } else {\n          targetShape.push(x.shape[axis]);\n        }\n      }\n    } catch (err) {\n      _iterator.e(err);\n    } finally {\n      _iterator.f();\n    }\n    var broadcastMean = reshape(mean, targetShape);\n    var broadcastVariance = reshape(variance, targetShape);\n    var broadcastGamma = gamma == null ? null : reshape(gamma, targetShape);\n    var broadcastBeta = beta == null ? null : reshape(beta, targetShape);\n    var normed = batchNormalization(x, broadcastMean, broadcastVariance, broadcastBeta, broadcastGamma, epsilon);\n    return [normed, mean, variance];\n  });\n}\n/**\n * Batch normalization for use in training (not inference).\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nexport function normalizeBatchInTraining(x, gamma, beta, reductionAxes) {\n  var epsilon = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : 1e-3;\n  if (util.arraysEqual(reductionAxes.slice().sort(), math_utils.range(0, x.rank - 1))) {\n    return regularNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon);\n  } else {\n    return broadcastNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon);\n  }\n}\nexport var BatchNormalization = /*#__PURE__*/function (_Layer) {\n  _inherits(BatchNormalization, _Layer);\n  var _super = _createSuper(BatchNormalization);\n  function BatchNormalization(args) {\n    var _this;\n    _classCallCheck(this, BatchNormalization);\n    if (args == null) {\n      args = {};\n    }\n    _this = _super.call(this, args);\n    _this.supportsMasking = true;\n    _this.axis = args.axis == null ? -1 : args.axis;\n    _this.momentum = args.momentum == null ? 0.99 : args.momentum;\n    _this.epsilon = args.epsilon == null ? 1e-3 : args.epsilon;\n    _this.center = args.center == null ? true : args.center;\n    _this.scale = args.scale == null ? true : args.scale;\n    _this.betaInitializer = getInitializer(args.betaInitializer || 'zeros');\n    _this.gammaInitializer = getInitializer(args.gammaInitializer || 'ones');\n    _this.movingMeanInitializer = getInitializer(args.movingMeanInitializer || 'zeros');\n    _this.movingVarianceInitializer = getInitializer(args.movingVarianceInitializer || 'ones');\n    _this.betaConstraint = getConstraint(args.betaConstraint);\n    _this.gammaConstraint = getConstraint(args.gammaConstraint);\n    _this.betaRegularizer = getRegularizer(args.betaRegularizer);\n    _this.gammaRegularizer = getRegularizer(args.gammaRegularizer);\n    return _this;\n  }\n  _createClass(BatchNormalization, [{\n    key: \"build\",\n    value: function build(inputShape) {\n      inputShape = getExactlyOneShape(inputShape);\n      var axis = this.axis >= 0 ? this.axis : this.axis + inputShape.length;\n      var dim = inputShape[axis];\n      if (dim == null) {\n        throw new ValueError(\"Axis \".concat(axis, \" of input tensor should have a defined dimension but \") + \"the layer received an input with shape \" + \"\".concat(JSON.stringify(inputShape), \".\"));\n      }\n      this.inputSpec = [new InputSpec({\n        ndim: inputShape.length,\n        axes: _defineProperty({}, axis, dim)\n      })];\n      var shape = [dim];\n      if (this.scale) {\n        this.gamma = this.addWeight('gamma', shape, null, this.gammaInitializer, this.gammaRegularizer, true, this.gammaConstraint);\n      }\n      if (this.center) {\n        this.beta = this.addWeight('beta', shape, null, this.betaInitializer, this.betaRegularizer, true, this.betaConstraint);\n      }\n      this.movingMean = this.addWeight('moving_mean', shape, null, this.movingMeanInitializer, null, false);\n      this.movingVariance = this.addWeight('moving_variance', shape, null, this.movingVarianceInitializer, null, false);\n      this.built = true;\n    }\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this2 = this;\n      return tidy(function () {\n        var training = kwargs['training'] == null ? false : kwargs['training'];\n        var input = getExactlyOneTensor(inputs);\n        var inputShape = input.shape;\n        var ndim = inputShape.length;\n        var reductionAxes = math_utils.range(0, ndim);\n        var axis = _this2.axis >= 0 ? _this2.axis : _this2.axis + ndim;\n        reductionAxes.splice(axis, 1);\n        var broadcastShape = generic_utils.pyListRepeat(1, ndim);\n        broadcastShape[axis] = inputShape[axis];\n        var sortedReductionAxes = reductionAxes.slice();\n        sortedReductionAxes.sort();\n        var needsBroadcasting = !util.arraysEqual(sortedReductionAxes, math_utils.range(0, ndim).slice(0, ndim - 1));\n        var normalizeInference = function normalizeInference() {\n          if (needsBroadcasting) {\n            var broadcastMovingMean = reshape(_this2.movingMean.read(), broadcastShape);\n            var broadcastMovingVariance = reshape(_this2.movingVariance.read(), broadcastShape);\n            var broadcastBeta = _this2.center ? reshape(_this2.beta.read(), broadcastShape) : null;\n            var broadcastGamma = _this2.scale ? reshape(_this2.gamma.read(), broadcastShape) : null;\n            return batchNormalization(input, broadcastMovingMean, broadcastMovingVariance, broadcastBeta, broadcastGamma, _this2.epsilon);\n          } else {\n            return batchNormalization(input, _this2.movingMean.read(), _this2.movingVariance.read(), _this2.beta == null ? null : _this2.beta.read(), _this2.gamma == null ? null : _this2.gamma.read(), _this2.epsilon);\n          }\n        };\n        if (!training) {\n          return normalizeInference();\n        }\n        var _normalizeBatchInTrai = normalizeBatchInTraining(input, _this2.gamma.read(), _this2.beta.read(), reductionAxes, _this2.epsilon),\n          _normalizeBatchInTrai2 = _slicedToArray(_normalizeBatchInTrai, 3),\n          normedTraining = _normalizeBatchInTrai2[0],\n          mean = _normalizeBatchInTrai2[1],\n          variance = _normalizeBatchInTrai2[2];\n        var doMovingAverage = function doMovingAverage(variable, value, momentum) {\n          tfc.tidy(function () {\n            var decay = 1 - momentum;\n            var origValue = variable.read();\n            var updateDelta = tfc.mul(tfc.sub(origValue, value), decay);\n            variable.write(tfc.sub(origValue, updateDelta));\n          });\n        };\n        // Perform updates to moving mean and moving variance for training.\n        // Porting Note: In PyKeras, these updates to `movingMean` and\n        //   `movingAverage` are done as a deferred Graph, added to the `Layer`'s\n        //   `update`s using the `add_update()` method. Here we do it imperatively\n        //   and encapsulate the updates in a function that is invoked\n        //   immediately.\n        var updateMovingMeanAndVariance = function updateMovingMeanAndVariance() {\n          doMovingAverage(_this2.movingMean, mean, _this2.momentum);\n          doMovingAverage(_this2.movingVariance, variance, _this2.momentum);\n        };\n        updateMovingMeanAndVariance();\n        return normedTraining;\n      });\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        axis: this.axis,\n        momentum: this.momentum,\n        epsilon: this.epsilon,\n        center: this.center,\n        scale: this.scale,\n        betaInitializer: serializeInitializer(this.betaInitializer),\n        gammaInitializer: serializeInitializer(this.gammaInitializer),\n        movingMeanInitializer: serializeInitializer(this.movingMeanInitializer),\n        movingVarianceInitializer: serializeInitializer(this.movingVarianceInitializer),\n        betaRegularizer: serializeRegularizer(this.betaRegularizer),\n        gammaRegularizer: serializeRegularizer(this.gammaRegularizer),\n        betaConstraint: serializeConstraint(this.betaConstraint),\n        gammaConstraint: serializeConstraint(this.gammaConstraint)\n      };\n      var baseConfig = _get(_getPrototypeOf(BatchNormalization.prototype), \"getConfig\", this).call(this);\n      Object.assign(config, baseConfig);\n      return config;\n    }\n  }]);\n  return BatchNormalization;\n}(Layer);\n/** @nocollapse */\nBatchNormalization.className = 'BatchNormalization';\nserialization.registerClass(BatchNormalization);\nexport var LayerNormalization = /*#__PURE__*/function (_Layer2) {\n  _inherits(LayerNormalization, _Layer2);\n  var _super2 = _createSuper(LayerNormalization);\n  function LayerNormalization(args) {\n    var _this3;\n    _classCallCheck(this, LayerNormalization);\n    if (args == null) {\n      args = {};\n    }\n    _this3 = _super2.call(this, args);\n    _this3.axis = args.axis == null ? -1 : args.axis;\n    if (typeof _this3.axis === 'number') {\n      if (!Number.isInteger(_this3.axis)) {\n        throw new Error(\"Expected axis to be an integer, but received \".concat(_this3.axis));\n      }\n    } else if (Array.isArray(_this3.axis)) {\n      var _iterator2 = _createForOfIteratorHelper(_this3.axis),\n        _step2;\n      try {\n        for (_iterator2.s(); !(_step2 = _iterator2.n()).done;) {\n          var axis = _step2.value;\n          if (!Number.isInteger(axis)) {\n            throw new Error(\"Expected axis to be an array of integers, \" + \"but received \".concat(JSON.stringify(_this3.axis)));\n          }\n        }\n      } catch (err) {\n        _iterator2.e(err);\n      } finally {\n        _iterator2.f();\n      }\n    } else {\n      throw new Error(\"Expected axis to be an integer or an array of integers, \" + \"but received \".concat(JSON.stringify(_this3.axis)));\n    }\n    _this3.epsilon = args.epsilon == null ? 1e-3 : args.epsilon;\n    _this3.center = args.center == null ? true : args.center;\n    _this3.scale = args.scale == null ? true : args.scale;\n    _this3.betaInitializer = getInitializer(args.betaInitializer || 'zeros');\n    _this3.gammaInitializer = getInitializer(args.gammaInitializer || 'ones');\n    _this3.betaRegularizer = getRegularizer(args.betaRegularizer);\n    _this3.gammaRegularizer = getRegularizer(args.gammaRegularizer);\n    _this3.supportsMasking = true;\n    return _this3;\n  }\n  _createClass(LayerNormalization, [{\n    key: \"build\",\n    value: function build(inputShape) {\n      inputShape = getExactlyOneShape(inputShape);\n      var nDims = inputShape.length;\n      // Convert axis to array and resolve negatives.\n      if (typeof this.axis === 'number') {\n        this.axis = [this.axis];\n      }\n      for (var i = 0; i < this.axis.length; ++i) {\n        if (this.axis[i] < 0) {\n          this.axis[i] += nDims;\n        }\n      }\n      // Further validate axes.\n      var _iterator3 = _createForOfIteratorHelper(this.axis),\n        _step3;\n      try {\n        for (_iterator3.s(); !(_step3 = _iterator3.n()).done;) {\n          var axis = _step3.value;\n          if (axis < 0 || axis >= nDims) {\n            throw new Error(\"Invalid axis: \".concat(axis));\n          }\n        }\n      } catch (err) {\n        _iterator3.e(err);\n      } finally {\n        _iterator3.f();\n      }\n      if (this.axis.length !== generic_utils.unique(this.axis).length) {\n        throw new Error(\"Found duplicate axes in: \".concat(this.axis));\n      }\n      var paramShape = this.axis.map(function (axis) {\n        return inputShape[axis];\n      });\n      var trainable = true;\n      if (this.scale) {\n        this.gamma = this.addWeight('gamma', paramShape, 'float32', this.gammaInitializer, this.gammaRegularizer, trainable);\n      } else {\n        this.gamma = null;\n      }\n      if (this.center) {\n        this.beta = this.addWeight('beta', paramShape, 'float32', this.betaInitializer, this.betaRegularizer, trainable);\n      } else {\n        this.beta = null;\n      }\n      this.built = true;\n    }\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this4 = this;\n      var input = getExactlyOneTensor(inputs);\n      var inputShape = input.shape;\n      var nDims = inputShape.length;\n      return tidy(function () {\n        var keepDims = true;\n        var _moments = moments(input, _this4.axis, keepDims),\n          mean = _moments.mean,\n          variance = _moments.variance;\n        var broadcastShape = generic_utils.pyListRepeat(1, nDims);\n        var _iterator4 = _createForOfIteratorHelper(_this4.axis),\n          _step4;\n        try {\n          for (_iterator4.s(); !(_step4 = _iterator4.n()).done;) {\n            var dim = _step4.value;\n            broadcastShape[dim] = inputShape[dim];\n          }\n        } catch (err) {\n          _iterator4.e(err);\n        } finally {\n          _iterator4.f();\n        }\n        var broadcast = function broadcast(v) {\n          if (v != null && v.shape.length !== nDims) {\n            return tfc.reshape(v, broadcastShape);\n          } else {\n            return v;\n          }\n        };\n        var scale = _this4.scale ? broadcast(_this4.gamma.read()) : null;\n        var offset = _this4.center ? broadcast(_this4.beta.read()) : null;\n        // TODO(https://github.com/tensorflow/tfjs/issues/2120): The tiling below\n        // is a workaround for the limitation of core's batchNormalization?d don't\n        // support broadcasting in their gradients. In addition, the tiling is\n        // necessary to ensure correctness on the browser CPU backend regardless\n        // of forward or backward computation. Remove this workaround once the\n        // limitation is addressed. See .\n        var momentsTiling = [];\n        var scaleOffsetTiling = [];\n        for (var i = 0; i < nDims; ++i) {\n          if (_this4.axis.indexOf(i) !== -1) {\n            momentsTiling.push(inputShape[i]);\n            scaleOffsetTiling.push(1);\n          } else {\n            momentsTiling.push(1);\n            scaleOffsetTiling.push(inputShape[i]);\n          }\n        }\n        mean = tfc.tile(mean, momentsTiling);\n        variance = tfc.tile(variance, momentsTiling);\n        if (scale != null) {\n          scale = tfc.tile(scale, scaleOffsetTiling);\n        }\n        if (offset != null) {\n          offset = tfc.tile(offset, scaleOffsetTiling);\n        }\n        return batchNormalization(input, mean, variance, offset, scale, _this4.epsilon);\n      });\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        axis: this.axis,\n        epsilon: this.epsilon,\n        center: this.center,\n        scale: this.scale,\n        betaInitializer: serializeInitializer(this.betaInitializer),\n        gammaInitializer: serializeInitializer(this.gammaInitializer),\n        betaRegularizer: serializeRegularizer(this.betaRegularizer),\n        gammaRegularizer: serializeRegularizer(this.gammaRegularizer)\n      };\n      var baseConfig = _get(_getPrototypeOf(LayerNormalization.prototype), \"getConfig\", this).call(this);\n      Object.assign(config, baseConfig);\n      return config;\n    }\n  }]);\n  return LayerNormalization;\n}(Layer);\n/** @nocollapse */\nLayerNormalization.className = 'LayerNormalization';\nserialization.registerClass(LayerNormalization);","map":{"version":3,"names":["tfc","moments","reshape","serialization","tidy","util","getConstraint","serializeConstraint","InputSpec","Layer","NotImplementedError","ValueError","getInitializer","serializeInitializer","getRegularizer","serializeRegularizer","generic_utils","math_utils","getExactlyOneShape","getExactlyOneTensor","batchNormalization","x","mean","variance","beta","gamma","epsilon","arguments","length","undefined","out","rank","batchNorm2d","batchNorm3d","batchNorm4d","concat","regularNormalizeBatchInTraining","reductionAxes","meanAndVariance","normed","broadcastNormalizeBatchInTraining","targetShape","_iterator","_createForOfIteratorHelper","range","_step","s","n","done","axis","value","indexOf","push","shape","err","e","f","broadcastMean","broadcastVariance","broadcastGamma","broadcastBeta","normalizeBatchInTraining","arraysEqual","slice","sort","BatchNormalization","_Layer","_inherits","_super","_createSuper","args","_this","_classCallCheck","call","supportsMasking","momentum","center","scale","betaInitializer","gammaInitializer","movingMeanInitializer","movingVarianceInitializer","betaConstraint","gammaConstraint","betaRegularizer","gammaRegularizer","_createClass","key","build","inputShape","dim","JSON","stringify","inputSpec","ndim","axes","_defineProperty","addWeight","movingMean","movingVariance","built","inputs","kwargs","_this2","training","input","splice","broadcastShape","pyListRepeat","sortedReductionAxes","needsBroadcasting","normalizeInference","broadcastMovingMean","read","broadcastMovingVariance","_normalizeBatchInTrai","_normalizeBatchInTrai2","_slicedToArray","normedTraining","doMovingAverage","variable","decay","origValue","updateDelta","mul","sub","write","updateMovingMeanAndVariance","getConfig","config","baseConfig","_get","_getPrototypeOf","prototype","Object","assign","className","registerClass","LayerNormalization","_Layer2","_super2","_this3","Number","isInteger","Error","Array","isArray","_iterator2","_step2","nDims","i","_iterator3","_step3","unique","paramShape","map","trainable","_this4","keepDims","_moments","_iterator4","_step4","broadcast","v","offset","momentsTiling","scaleOffsetTiling","tile"],"sources":["C:\\Users\\vince\\OneDrive\\Documents\\GitHub\\tfjs-layers\\src\\layers\\normalization.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Normalization layers.\n */\n\nimport * as tfc from '@tensorflow/tfjs-core';\nimport {moments, reshape, serialization, Tensor, Tensor1D, Tensor2D, Tensor3D, Tensor4D, tidy, util} from '@tensorflow/tfjs-core';\n\nimport {Constraint, ConstraintIdentifier, getConstraint, serializeConstraint} from '../constraints';\nimport {InputSpec, Layer, LayerArgs} from '../engine/topology';\nimport {NotImplementedError, ValueError} from '../errors';\nimport {getInitializer, Initializer, InitializerIdentifier, serializeInitializer} from '../initializers';\nimport {Shape} from '../keras_format/common';\nimport {getRegularizer, Regularizer, RegularizerIdentifier, serializeRegularizer} from '../regularizers';\nimport {Kwargs} from '../types';\nimport * as generic_utils from '../utils/generic_utils';\nimport * as math_utils from '../utils/math_utils';\nimport {getExactlyOneShape, getExactlyOneTensor} from '../utils/types_utils';\nimport {LayerVariable} from '../variables';\n\n/**\n * Applies batch normalization on x given mean, var, beta and gamma.\n *\n * I.e. returns:\n *   `output = (x - mean) / (sqrt(var) + epsilon) * gamma + beta`\n *\n * @param x Input tensor.\n * @param mean Mean of batch.\n * @param variance Variance of batch.\n * @param beta Tensor with which to center the input.\n * @param gamma Tensor by which to scale the input.\n * @param epsilon Fuzz factor.\n * @returns The result of the batch normalization.\n */\nexport function batchNormalization(\n    x: Tensor, mean: Tensor, variance: Tensor, beta?: Tensor, gamma?: Tensor,\n    epsilon = 1e-3): Tensor {\n  let out: Tensor;\n  if (x.rank === 2) {\n    out = tfc.batchNorm2d(\n        x as Tensor2D, mean as Tensor2D | Tensor1D,\n        variance as Tensor2D | Tensor1D, beta as Tensor2D | Tensor1D,\n        gamma as Tensor2D | Tensor1D, epsilon);\n  } else if (x.rank === 3) {\n    // TODO(cais): Check rank; give proper error message.\n    out = tfc.batchNorm3d(\n        x as Tensor3D, mean as Tensor3D | Tensor1D,\n        variance as Tensor3D | Tensor1D, beta as Tensor3D | Tensor1D,\n        gamma as Tensor3D | Tensor1D, epsilon);\n  } else if (x.rank === 4) {\n    out = tfc.batchNorm4d(\n        x as Tensor4D, mean as Tensor4D | Tensor1D,\n        variance as Tensor4D | Tensor1D, beta as Tensor4D | Tensor1D,\n        gamma as Tensor4D | Tensor1D, epsilon);\n  } else {\n    throw new NotImplementedError(\n        `batchNormalization is not implemented for array of rank ${x.rank} ` +\n        `yet`);\n  }\n  return out;\n}\n\n/**\n * Non-broadcasting batch normalization for use in training (not inference).\n *\n * The input is normalized to zero mean and unit variance along the\n * `reductionAxes`, followed by scaling with `gamma` and shifted by `beta`.\n * The result of that is returned as the first element\n * of the returned `Array`. The other two elements are the mean and variance,\n * respectively.\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nfunction regularNormalizeBatchInTraining(\n    x: Tensor, gamma: Tensor, beta: Tensor, reductionAxes: number[],\n    epsilon = 1e-3): [Tensor, Tensor, Tensor] {\n  return tidy(() => {\n           const meanAndVariance = tfc.moments(x, reductionAxes);\n           const mean = meanAndVariance.mean;\n           const variance = meanAndVariance.variance;\n           const normed =\n               batchNormalization(x, mean, variance, beta, gamma, epsilon);\n           return [normed, mean, variance];\n         }) as [Tensor, Tensor, Tensor];\n}\n\n/**\n * Broadcasting batch normalization for use in training (not inference).\n *\n * The input is normalized to zero mean and unit variance along the\n * `reductionAxes`, followed by scaling with `gamma` and shifted by `beta`.\n * The result of that is returned as the first element\n * of the returned `Array`. The other two elements are the mean and variance,\n * respectively.\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nfunction broadcastNormalizeBatchInTraining(\n    x: Tensor, gamma: Tensor, beta: Tensor, reductionAxes: number[],\n    epsilon = 1e-3): [Tensor, Tensor, Tensor] {\n  return tidy(() => {\n           const meanAndVariance = tfc.moments(x, reductionAxes);\n           const mean = meanAndVariance.mean;\n           const variance = meanAndVariance.variance;\n           const targetShape: number[] = [];\n           for (const axis of math_utils.range(0, x.rank)) {\n             if (reductionAxes.indexOf(axis) !== -1) {\n               targetShape.push(1);\n             } else {\n               targetShape.push(x.shape[axis]);\n             }\n           }\n           const broadcastMean = reshape(mean, targetShape);\n           const broadcastVariance = reshape(variance, targetShape);\n           const broadcastGamma =\n               gamma == null ? null : reshape(gamma, targetShape);\n           const broadcastBeta =\n               beta == null ? null : reshape(beta, targetShape);\n           const normed = batchNormalization(\n               x, broadcastMean, broadcastVariance, broadcastBeta,\n               broadcastGamma, epsilon);\n           return [normed, mean, variance];\n         }) as [Tensor, Tensor, Tensor];\n}\n\n/**\n * Batch normalization for use in training (not inference).\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nexport function normalizeBatchInTraining(\n    x: Tensor, gamma: Tensor, beta: Tensor, reductionAxes: number[],\n    epsilon = 1e-3): [Tensor, Tensor, Tensor] {\n  if (util.arraysEqual(\n          reductionAxes.slice().sort(), math_utils.range(0, x.rank - 1))) {\n    return regularNormalizeBatchInTraining(\n        x, gamma, beta, reductionAxes, epsilon);\n  } else {\n    return broadcastNormalizeBatchInTraining(\n        x, gamma, beta, reductionAxes, epsilon);\n  }\n}\n\nexport declare interface BatchNormalizationLayerArgs extends LayerArgs {\n  /**\n   * The integer axis that should be normalized (typically the features axis).\n   * Defaults to -1.\n   *\n   * For instance, after a `Conv2D` layer with `data_format=\"channels_first\"`,\n   * set `axis=1` in `batchNormalization`.\n   */\n  axis?: number;\n\n  /**\n   * Momentum of the moving average. Defaults to 0.99.\n   */\n  momentum?: number;\n\n  /**\n   * Small float added to the variance to avoid dividing by zero. Defaults to\n   * 1e-3.\n   */\n  epsilon?: number;\n\n  /**\n   * If `true`, add offset of `beta` to normalized tensor.\n   * If `false`, `beta` is ignored.\n   * Defaults to `true`.\n   */\n  center?: boolean;\n\n  /**\n   * If `true`, multiply by `gamma`.\n   * If `false`, `gamma` is not used.\n   * When the next layer is linear (also e.g. `nn.relu`),\n   * this can be disabled since the scaling will be done by the next layer.\n   * Defaults to `true`.\n   */\n  scale?: boolean;\n\n  /**\n   * Initializer for the beta weight.\n   *  Defaults to 'zeros'.\n   */\n  betaInitializer?: InitializerIdentifier|Initializer;\n\n  /**\n   * Initializer for the gamma weight.\n   *  Defaults to `ones`.\n   */\n  gammaInitializer?: InitializerIdentifier|Initializer;\n\n  /**\n   * Initializer for the moving mean.\n   * Defaults to `zeros`\n   */\n  movingMeanInitializer?: InitializerIdentifier|Initializer;\n\n  /**\n   * Initializer for the moving variance.\n   *  Defaults to 'Ones'.\n   */\n  movingVarianceInitializer?: InitializerIdentifier|Initializer;\n\n  /**\n   * Constraint for the beta weight.\n   */\n  betaConstraint?: ConstraintIdentifier|Constraint;\n\n  /**\n   * Constraint for gamma weight.\n   */\n  gammaConstraint?: ConstraintIdentifier|Constraint;\n\n  /**\n   * Regularizer for the beta weight.\n   */\n  betaRegularizer?: RegularizerIdentifier|Regularizer;\n\n  /**\n   * Regularizer for the gamma weight.\n   */\n  gammaRegularizer?: RegularizerIdentifier|Regularizer;\n}\n\nexport class BatchNormalization extends Layer {\n  /** @nocollapse */\n  static className = 'BatchNormalization';\n  private readonly axis: number;\n  private readonly momentum: number;\n  private readonly epsilon: number;\n  private readonly center: boolean;\n  private readonly scale: boolean;\n  private readonly betaInitializer: Initializer;\n  private readonly gammaInitializer: Initializer;\n  private readonly movingMeanInitializer: Initializer;\n  private readonly movingVarianceInitializer: Initializer;\n  private readonly betaConstraint: Constraint;\n  private readonly gammaConstraint: Constraint;\n  private readonly betaRegularizer: Regularizer;\n  private readonly gammaRegularizer: Regularizer;\n  private gamma: LayerVariable;\n  private beta: LayerVariable;\n  private movingMean: LayerVariable;\n  private movingVariance: LayerVariable;\n\n  constructor(args?: BatchNormalizationLayerArgs) {\n    if (args == null) {\n      args = {};\n    }\n    super(args);\n\n    this.supportsMasking = true;\n    this.axis = args.axis == null ? -1 : args.axis;\n    this.momentum = args.momentum == null ? 0.99 : args.momentum;\n    this.epsilon = args.epsilon == null ? 1e-3 : args.epsilon;\n    this.center = args.center == null ? true : args.center;\n    this.scale = args.scale == null ? true : args.scale;\n    this.betaInitializer = getInitializer(args.betaInitializer || 'zeros');\n    this.gammaInitializer = getInitializer(args.gammaInitializer || 'ones');\n    this.movingMeanInitializer =\n        getInitializer(args.movingMeanInitializer || 'zeros');\n    this.movingVarianceInitializer =\n        getInitializer(args.movingVarianceInitializer || 'ones');\n    this.betaConstraint = getConstraint(args.betaConstraint);\n    this.gammaConstraint = getConstraint(args.gammaConstraint);\n    this.betaRegularizer = getRegularizer(args.betaRegularizer);\n    this.gammaRegularizer = getRegularizer(args.gammaRegularizer);\n  }\n\n  public override build(inputShape: Shape|Shape[]): void {\n    inputShape = getExactlyOneShape(inputShape);\n    const axis = this.axis >= 0 ? this.axis : (this.axis + inputShape.length);\n    const dim = inputShape[axis];\n    if (dim == null) {\n      throw new ValueError(\n          `Axis ${axis} of input tensor should have a defined dimension but ` +\n          `the layer received an input with shape ` +\n          `${JSON.stringify(inputShape)}.`);\n    }\n    this.inputSpec =\n        [new InputSpec({ndim: inputShape.length, axes: {[axis]: dim}})];\n    const shape = [dim];\n    if (this.scale) {\n      this.gamma = this.addWeight(\n          'gamma', shape, null, this.gammaInitializer, this.gammaRegularizer,\n          true, this.gammaConstraint);\n    }\n    if (this.center) {\n      this.beta = this.addWeight(\n          'beta', shape, null, this.betaInitializer, this.betaRegularizer, true,\n          this.betaConstraint);\n    }\n    this.movingMean = this.addWeight(\n        'moving_mean', shape, null, this.movingMeanInitializer, null, false);\n    this.movingVariance = this.addWeight(\n        'moving_variance', shape, null, this.movingVarianceInitializer, null,\n        false);\n    this.built = true;\n  }\n\n  override call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      const training = kwargs['training'] == null ? false : kwargs['training'];\n      const input = getExactlyOneTensor(inputs);\n      const inputShape = input.shape;\n      const ndim = inputShape.length;\n      const reductionAxes = math_utils.range(0, ndim);\n      const axis = this.axis >= 0 ? this.axis : (this.axis + ndim);\n      reductionAxes.splice(axis, 1);\n      const broadcastShape = generic_utils.pyListRepeat(1, ndim);\n      broadcastShape[axis] = inputShape[axis];\n\n      const sortedReductionAxes = reductionAxes.slice();\n      sortedReductionAxes.sort();\n      const needsBroadcasting = !util.arraysEqual(\n          sortedReductionAxes, math_utils.range(0, ndim).slice(0, ndim - 1));\n\n      const normalizeInference: () => Tensor = () => {\n        if (needsBroadcasting) {\n          const broadcastMovingMean =\n              reshape(this.movingMean.read(), broadcastShape);\n          const broadcastMovingVariance =\n              reshape(this.movingVariance.read(), broadcastShape);\n          const broadcastBeta =\n              this.center ? reshape(this.beta.read(), broadcastShape) : null;\n          const broadcastGamma =\n              this.scale ? reshape(this.gamma.read(), broadcastShape) : null;\n          return batchNormalization(\n              input, broadcastMovingMean, broadcastMovingVariance,\n              broadcastBeta, broadcastGamma, this.epsilon);\n        } else {\n          return batchNormalization(\n              input, this.movingMean.read(), this.movingVariance.read(),\n              this.beta == null ? null : this.beta.read(),\n              this.gamma == null ? null : this.gamma.read(), this.epsilon);\n        }\n      };\n\n      if (!training) {\n        return normalizeInference();\n      }\n\n      const [normedTraining, mean, variance] = normalizeBatchInTraining(\n          input, this.gamma.read(), this.beta.read(), reductionAxes,\n          this.epsilon);\n\n      const doMovingAverage =\n          (variable: LayerVariable, value: Tensor, momentum: number): void => {\n            tfc.tidy(() => {\n              const decay = 1 - momentum;\n              const origValue = variable.read();\n              const updateDelta = tfc.mul(tfc.sub(origValue, value), decay);\n              variable.write(tfc.sub(origValue, updateDelta));\n            });\n          };\n\n      // Perform updates to moving mean and moving variance for training.\n      // Porting Note: In PyKeras, these updates to `movingMean` and\n      //   `movingAverage` are done as a deferred Graph, added to the `Layer`'s\n      //   `update`s using the `add_update()` method. Here we do it imperatively\n      //   and encapsulate the updates in a function that is invoked\n      //   immediately.\n      const updateMovingMeanAndVariance = () => {\n        doMovingAverage(this.movingMean, mean, this.momentum);\n        doMovingAverage(this.movingVariance, variance, this.momentum);\n      };\n      updateMovingMeanAndVariance();\n\n      return normedTraining;\n    });\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {\n      axis: this.axis,\n      momentum: this.momentum,\n      epsilon: this.epsilon,\n      center: this.center,\n      scale: this.scale,\n      betaInitializer: serializeInitializer(this.betaInitializer),\n      gammaInitializer: serializeInitializer(this.gammaInitializer),\n      movingMeanInitializer: serializeInitializer(this.movingMeanInitializer),\n      movingVarianceInitializer:\n          serializeInitializer(this.movingVarianceInitializer),\n      betaRegularizer: serializeRegularizer(this.betaRegularizer),\n      gammaRegularizer: serializeRegularizer(this.gammaRegularizer),\n      betaConstraint: serializeConstraint(this.betaConstraint),\n      gammaConstraint: serializeConstraint(this.gammaConstraint)\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(BatchNormalization);\n\nexport interface LayerNormalizationLayerArgs extends LayerArgs {\n  /**\n   * The axis or axes that should be normalized (typically, the feature axis).\n   * Defaults to -1 (the last axis).\n   */\n  axis?: number|number[];\n\n  /**\n   * A small positive float added to variance to avoid divison by zero.\n   * Defaults to 1e-3.\n   */\n  epsilon?: number;\n\n  /**\n   * If `true`, add offset of `beta` to normalized tensor.\n   * If `false`, `beta` is ignored.\n   * Default: `true`.\n   */\n  center?: boolean;\n\n  /**\n   * If `true`, multiply output by `gamma`.\n   * If `false`, `gamma` is not used.\n   * When the next layer is linear, this can be disabled since scaling will\n   * be done by the next layer.\n   * Default: `true`.\n   */\n  scale?: boolean;\n\n  /**\n   * Initializer for the beta weight.\n   * Default: `'zeros'`.\n   */\n  betaInitializer?: InitializerIdentifier|Initializer;\n\n  /**\n   * Initializer for the gamma weight.\n   * Default: `'ones'`.\n   */\n  gammaInitializer?: InitializerIdentifier|Initializer;\n\n  /** Regularizer for the beta weight. */\n  betaRegularizer?: RegularizerIdentifier|Regularizer;\n\n  /** Regularizer for the gamma weight. */\n  gammaRegularizer?: RegularizerIdentifier|Regularizer;\n}\n\nexport class LayerNormalization extends Layer {\n  /** @nocollapse */\n  static className = 'LayerNormalization';\n\n  private axis: number|number[];\n  readonly epsilon: number;\n  readonly center: boolean;\n  readonly scale: boolean;\n  readonly betaInitializer: Initializer;\n  readonly gammaInitializer: Initializer;\n  readonly betaRegularizer: Regularizer;\n  readonly gammaRegularizer: Regularizer;\n\n  private gamma: LayerVariable;\n  private beta: LayerVariable;\n\n  constructor(args?: LayerNormalizationLayerArgs) {\n    if (args == null) {\n      args = {};\n    }\n    super(args);\n\n    this.axis = args.axis == null ? -1 : args.axis;\n    if (typeof this.axis === 'number') {\n      if (!Number.isInteger(this.axis)) {\n        throw new Error(\n            `Expected axis to be an integer, but received ${this.axis}`);\n      }\n    } else if (Array.isArray(this.axis)) {\n      for (const axis of this.axis) {\n        if (!Number.isInteger(axis)) {\n          throw new Error(\n              `Expected axis to be an array of integers, ` +\n              `but received ${JSON.stringify(this.axis)}`);\n        }\n      }\n    } else {\n      throw new Error(\n          `Expected axis to be an integer or an array of integers, ` +\n          `but received ${JSON.stringify(this.axis)}`);\n    }\n\n    this.epsilon = args.epsilon == null ? 1e-3 : args.epsilon;\n    this.center = args.center == null ? true : args.center;\n    this.scale = args.scale == null ? true : args.scale;\n    this.betaInitializer = getInitializer(args.betaInitializer || 'zeros');\n    this.gammaInitializer = getInitializer(args.gammaInitializer || 'ones');\n    this.betaRegularizer = getRegularizer(args.betaRegularizer);\n    this.gammaRegularizer = getRegularizer(args.gammaRegularizer);\n\n    this.supportsMasking = true;\n  }\n\n  public override build(inputShape: Shape|Shape[]): void {\n    inputShape = getExactlyOneShape(inputShape);\n    const nDims = inputShape.length;\n\n    // Convert axis to array and resolve negatives.\n    if (typeof this.axis === 'number') {\n      this.axis = [this.axis];\n    }\n    for (let i = 0; i < this.axis.length; ++i) {\n      if (this.axis[i] < 0) {\n        this.axis[i] += nDims;\n      }\n    }\n\n    // Further validate axes.\n    for (const axis of this.axis) {\n      if (axis < 0 || axis >= nDims) {\n        throw new Error(`Invalid axis: ${axis}`);\n      }\n    }\n    if (this.axis.length !== generic_utils.unique(this.axis).length) {\n      throw new Error(`Found duplicate axes in: ${this.axis}`);\n    }\n\n    const paramShape = this.axis.map(axis => inputShape[axis]) as number[];\n\n    const trainable = true;\n    if (this.scale) {\n      this.gamma = this.addWeight(\n          'gamma', paramShape, 'float32', this.gammaInitializer,\n          this.gammaRegularizer, trainable);\n    } else {\n      this.gamma = null;\n    }\n    if (this.center) {\n      this.beta = this.addWeight(\n          'beta', paramShape, 'float32', this.betaInitializer,\n          this.betaRegularizer, trainable);\n    } else {\n      this.beta = null;\n    }\n\n    this.built = true;\n  }\n\n  override call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    const input = getExactlyOneTensor(inputs);\n    const inputShape = input.shape;\n    const nDims = inputShape.length;\n\n    return tidy(() => {\n      const keepDims = true;\n      let {mean, variance} = moments(input, this.axis, keepDims);\n      const broadcastShape = generic_utils.pyListRepeat(1, nDims);\n      for (const dim of this.axis as number[]) {\n        broadcastShape[dim] = inputShape[dim];\n      }\n\n      const broadcast = (v: Tensor) => {\n        if (v != null && v.shape.length !== nDims) {\n          return tfc.reshape(v, broadcastShape);\n        } else {\n          return v;\n        }\n      };\n\n      let scale = this.scale ? broadcast(this.gamma.read()) : null;\n      let offset = this.center ? broadcast(this.beta.read()) : null;\n\n      // TODO(https://github.com/tensorflow/tfjs/issues/2120): The tiling below\n      // is a workaround for the limitation of core's batchNormalization?d don't\n      // support broadcasting in their gradients. In addition, the tiling is\n      // necessary to ensure correctness on the browser CPU backend regardless\n      // of forward or backward computation. Remove this workaround once the\n      // limitation is addressed. See .\n      const momentsTiling: number[] = [];\n      const scaleOffsetTiling: number[] = [];\n      for (let i = 0; i < nDims; ++i) {\n        if ((this.axis as number[]).indexOf(i) !== -1) {\n          momentsTiling.push(inputShape[i]);\n          scaleOffsetTiling.push(1);\n        } else {\n          momentsTiling.push(1);\n          scaleOffsetTiling.push(inputShape[i]);\n        }\n      }\n      mean = tfc.tile(mean, momentsTiling);\n      variance = tfc.tile(variance, momentsTiling);\n      if (scale != null) {\n        scale = tfc.tile(scale, scaleOffsetTiling);\n      }\n      if (offset != null) {\n        offset = tfc.tile(offset, scaleOffsetTiling);\n      }\n\n      return batchNormalization(\n          input, mean, variance, offset, scale, this.epsilon);\n    });\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {\n      axis: this.axis,\n      epsilon: this.epsilon,\n      center: this.center,\n      scale: this.scale,\n      betaInitializer: serializeInitializer(this.betaInitializer),\n      gammaInitializer: serializeInitializer(this.gammaInitializer),\n      betaRegularizer: serializeRegularizer(this.betaRegularizer),\n      gammaRegularizer: serializeRegularizer(this.gammaRegularizer)\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(LayerNormalization);\n"],"mappings":";;;;;;;;;AAAA;;;;;;;;;AAUA;;;AAIA,OAAO,KAAKA,GAAG,MAAM,uBAAuB;AAC5C,SAAQC,OAAO,EAAEC,OAAO,EAAEC,aAAa,EAAkDC,IAAI,EAAEC,IAAI,QAAO,uBAAuB;AAEjI,SAA0CC,aAAa,EAAEC,mBAAmB,QAAO,gBAAgB;AACnG,SAAQC,SAAS,EAAEC,KAAK,QAAkB,oBAAoB;AAC9D,SAAQC,mBAAmB,EAAEC,UAAU,QAAO,WAAW;AACzD,SAAQC,cAAc,EAAsCC,oBAAoB,QAAO,iBAAiB;AAExG,SAAQC,cAAc,EAAsCC,oBAAoB,QAAO,iBAAiB;AAExG,OAAO,KAAKC,aAAa,MAAM,wBAAwB;AACvD,OAAO,KAAKC,UAAU,MAAM,qBAAqB;AACjD,SAAQC,kBAAkB,EAAEC,mBAAmB,QAAO,sBAAsB;AAG5E;;;;;;;;;;;;;;AAcA,OAAM,SAAUC,kBAAkBA,CAC9BC,CAAS,EAAEC,IAAY,EAAEC,QAAgB,EAAEC,IAAa,EAAEC,KAAc,EAC1D;EAAA,IAAdC,OAAO,GAAAC,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,IAAI;EAChB,IAAIG,GAAW;EACf,IAAIT,CAAC,CAACU,IAAI,KAAK,CAAC,EAAE;IAChBD,GAAG,GAAG9B,GAAG,CAACgC,WAAW,CACjBX,CAAa,EAAEC,IAA2B,EAC1CC,QAA+B,EAAEC,IAA2B,EAC5DC,KAA4B,EAAEC,OAAO,CAAC;GAC3C,MAAM,IAAIL,CAAC,CAACU,IAAI,KAAK,CAAC,EAAE;IACvB;IACAD,GAAG,GAAG9B,GAAG,CAACiC,WAAW,CACjBZ,CAAa,EAAEC,IAA2B,EAC1CC,QAA+B,EAAEC,IAA2B,EAC5DC,KAA4B,EAAEC,OAAO,CAAC;GAC3C,MAAM,IAAIL,CAAC,CAACU,IAAI,KAAK,CAAC,EAAE;IACvBD,GAAG,GAAG9B,GAAG,CAACkC,WAAW,CACjBb,CAAa,EAAEC,IAA2B,EAC1CC,QAA+B,EAAEC,IAA2B,EAC5DC,KAA4B,EAAEC,OAAO,CAAC;GAC3C,MAAM;IACL,MAAM,IAAIhB,mBAAmB,CACzB,2DAAAyB,MAAA,CAA2Dd,CAAC,CAACU,IAAI,cAC5D,CAAC;;EAEZ,OAAOD,GAAG;AACZ;AAEA;;;;;;;;;;;;;;;;;AAiBA,SAASM,+BAA+BA,CACpCf,CAAS,EAAEI,KAAa,EAAED,IAAY,EAAEa,aAAuB,EACjD;EAAA,IAAdX,OAAO,GAAAC,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,IAAI;EAChB,OAAOvB,IAAI,CAAC,YAAK;IACR,IAAMkC,eAAe,GAAGtC,GAAG,CAACC,OAAO,CAACoB,CAAC,EAAEgB,aAAa,CAAC;IACrD,IAAMf,IAAI,GAAGgB,eAAe,CAAChB,IAAI;IACjC,IAAMC,QAAQ,GAAGe,eAAe,CAACf,QAAQ;IACzC,IAAMgB,MAAM,GACRnB,kBAAkB,CAACC,CAAC,EAAEC,IAAI,EAAEC,QAAQ,EAAEC,IAAI,EAAEC,KAAK,EAAEC,OAAO,CAAC;IAC/D,OAAO,CAACa,MAAM,EAAEjB,IAAI,EAAEC,QAAQ,CAAC;EACjC,CAAC,CAA6B;AACvC;AAEA;;;;;;;;;;;;;;;;;AAiBA,SAASiB,iCAAiCA,CACtCnB,CAAS,EAAEI,KAAa,EAAED,IAAY,EAAEa,aAAuB,EACjD;EAAA,IAAdX,OAAO,GAAAC,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,IAAI;EAChB,OAAOvB,IAAI,CAAC,YAAK;IACR,IAAMkC,eAAe,GAAGtC,GAAG,CAACC,OAAO,CAACoB,CAAC,EAAEgB,aAAa,CAAC;IACrD,IAAMf,IAAI,GAAGgB,eAAe,CAAChB,IAAI;IACjC,IAAMC,QAAQ,GAAGe,eAAe,CAACf,QAAQ;IACzC,IAAMkB,WAAW,GAAa,EAAE;IAAC,IAAAC,SAAA,GAAAC,0BAAA,CACd1B,UAAU,CAAC2B,KAAK,CAAC,CAAC,EAAEvB,CAAC,CAACU,IAAI,CAAC;MAAAc,KAAA;IAAA;MAA9C,KAAAH,SAAA,CAAAI,CAAA,MAAAD,KAAA,GAAAH,SAAA,CAAAK,CAAA,IAAAC,IAAA,GAAgD;QAAA,IAArCC,IAAI,GAAAJ,KAAA,CAAAK,KAAA;QACb,IAAIb,aAAa,CAACc,OAAO,CAACF,IAAI,CAAC,KAAK,CAAC,CAAC,EAAE;UACtCR,WAAW,CAACW,IAAI,CAAC,CAAC,CAAC;SACpB,MAAM;UACLX,WAAW,CAACW,IAAI,CAAC/B,CAAC,CAACgC,KAAK,CAACJ,IAAI,CAAC,CAAC;;;IAElC,SAAAK,GAAA;MAAAZ,SAAA,CAAAa,CAAA,CAAAD,GAAA;IAAA;MAAAZ,SAAA,CAAAc,CAAA;IAAA;IACD,IAAMC,aAAa,GAAGvD,OAAO,CAACoB,IAAI,EAAEmB,WAAW,CAAC;IAChD,IAAMiB,iBAAiB,GAAGxD,OAAO,CAACqB,QAAQ,EAAEkB,WAAW,CAAC;IACxD,IAAMkB,cAAc,GAChBlC,KAAK,IAAI,IAAI,GAAG,IAAI,GAAGvB,OAAO,CAACuB,KAAK,EAAEgB,WAAW,CAAC;IACtD,IAAMmB,aAAa,GACfpC,IAAI,IAAI,IAAI,GAAG,IAAI,GAAGtB,OAAO,CAACsB,IAAI,EAAEiB,WAAW,CAAC;IACpD,IAAMF,MAAM,GAAGnB,kBAAkB,CAC7BC,CAAC,EAAEoC,aAAa,EAAEC,iBAAiB,EAAEE,aAAa,EAClDD,cAAc,EAAEjC,OAAO,CAAC;IAC5B,OAAO,CAACa,MAAM,EAAEjB,IAAI,EAAEC,QAAQ,CAAC;EACjC,CAAC,CAA6B;AACvC;AAEA;;;;;;;;;;;AAWA,OAAM,SAAUsC,wBAAwBA,CACpCxC,CAAS,EAAEI,KAAa,EAAED,IAAY,EAAEa,aAAuB,EACjD;EAAA,IAAdX,OAAO,GAAAC,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,IAAI;EAChB,IAAItB,IAAI,CAACyD,WAAW,CACZzB,aAAa,CAAC0B,KAAK,EAAE,CAACC,IAAI,EAAE,EAAE/C,UAAU,CAAC2B,KAAK,CAAC,CAAC,EAAEvB,CAAC,CAACU,IAAI,GAAG,CAAC,CAAC,CAAC,EAAE;IACtE,OAAOK,+BAA+B,CAClCf,CAAC,EAAEI,KAAK,EAAED,IAAI,EAAEa,aAAa,EAAEX,OAAO,CAAC;GAC5C,MAAM;IACL,OAAOc,iCAAiC,CACpCnB,CAAC,EAAEI,KAAK,EAAED,IAAI,EAAEa,aAAa,EAAEX,OAAO,CAAC;;AAE/C;AAoFA,WAAauC,kBAAmB,0BAAAC,MAAA;EAAAC,SAAA,CAAAF,kBAAA,EAAAC,MAAA;EAAA,IAAAE,MAAA,GAAAC,YAAA,CAAAJ,kBAAA;EAqB9B,SAAAA,mBAAYK,IAAkC;IAAA,IAAAC,KAAA;IAAAC,eAAA,OAAAP,kBAAA;IAC5C,IAAIK,IAAI,IAAI,IAAI,EAAE;MAChBA,IAAI,GAAG,EAAE;;IAEXC,KAAA,GAAAH,MAAA,CAAAK,IAAA,OAAMH,IAAI;IAEVC,KAAA,CAAKG,eAAe,GAAG,IAAI;IAC3BH,KAAA,CAAKtB,IAAI,GAAGqB,IAAI,CAACrB,IAAI,IAAI,IAAI,GAAG,CAAC,CAAC,GAAGqB,IAAI,CAACrB,IAAI;IAC9CsB,KAAA,CAAKI,QAAQ,GAAGL,IAAI,CAACK,QAAQ,IAAI,IAAI,GAAG,IAAI,GAAGL,IAAI,CAACK,QAAQ;IAC5DJ,KAAA,CAAK7C,OAAO,GAAG4C,IAAI,CAAC5C,OAAO,IAAI,IAAI,GAAG,IAAI,GAAG4C,IAAI,CAAC5C,OAAO;IACzD6C,KAAA,CAAKK,MAAM,GAAGN,IAAI,CAACM,MAAM,IAAI,IAAI,GAAG,IAAI,GAAGN,IAAI,CAACM,MAAM;IACtDL,KAAA,CAAKM,KAAK,GAAGP,IAAI,CAACO,KAAK,IAAI,IAAI,GAAG,IAAI,GAAGP,IAAI,CAACO,KAAK;IACnDN,KAAA,CAAKO,eAAe,GAAGlE,cAAc,CAAC0D,IAAI,CAACQ,eAAe,IAAI,OAAO,CAAC;IACtEP,KAAA,CAAKQ,gBAAgB,GAAGnE,cAAc,CAAC0D,IAAI,CAACS,gBAAgB,IAAI,MAAM,CAAC;IACvER,KAAA,CAAKS,qBAAqB,GACtBpE,cAAc,CAAC0D,IAAI,CAACU,qBAAqB,IAAI,OAAO,CAAC;IACzDT,KAAA,CAAKU,yBAAyB,GAC1BrE,cAAc,CAAC0D,IAAI,CAACW,yBAAyB,IAAI,MAAM,CAAC;IAC5DV,KAAA,CAAKW,cAAc,GAAG5E,aAAa,CAACgE,IAAI,CAACY,cAAc,CAAC;IACxDX,KAAA,CAAKY,eAAe,GAAG7E,aAAa,CAACgE,IAAI,CAACa,eAAe,CAAC;IAC1DZ,KAAA,CAAKa,eAAe,GAAGtE,cAAc,CAACwD,IAAI,CAACc,eAAe,CAAC;IAC3Db,KAAA,CAAKc,gBAAgB,GAAGvE,cAAc,CAACwD,IAAI,CAACe,gBAAgB,CAAC;IAAC,OAAAd,KAAA;EAChE;EAACe,YAAA,CAAArB,kBAAA;IAAAsB,GAAA;IAAArC,KAAA,EAEe,SAAAsC,MAAMC,UAAyB;MAC7CA,UAAU,GAAGvE,kBAAkB,CAACuE,UAAU,CAAC;MAC3C,IAAMxC,IAAI,GAAG,IAAI,CAACA,IAAI,IAAI,CAAC,GAAG,IAAI,CAACA,IAAI,GAAI,IAAI,CAACA,IAAI,GAAGwC,UAAU,CAAC7D,MAAO;MACzE,IAAM8D,GAAG,GAAGD,UAAU,CAACxC,IAAI,CAAC;MAC5B,IAAIyC,GAAG,IAAI,IAAI,EAAE;QACf,MAAM,IAAI/E,UAAU,CAChB,QAAAwB,MAAA,CAAQc,IAAI,sGAC6B,MAAAd,MAAA,CACtCwD,IAAI,CAACC,SAAS,CAACH,UAAU,CAAC,MAAG,CAAC;;MAEvC,IAAI,CAACI,SAAS,GACV,CAAC,IAAIrF,SAAS,CAAC;QAACsF,IAAI,EAAEL,UAAU,CAAC7D,MAAM;QAAEmE,IAAI,EAAAC,eAAA,KAAI/C,IAAI,EAAGyC,GAAG;MAAC,CAAC,CAAC,CAAC;MACnE,IAAMrC,KAAK,GAAG,CAACqC,GAAG,CAAC;MACnB,IAAI,IAAI,CAACb,KAAK,EAAE;QACd,IAAI,CAACpD,KAAK,GAAG,IAAI,CAACwE,SAAS,CACvB,OAAO,EAAE5C,KAAK,EAAE,IAAI,EAAE,IAAI,CAAC0B,gBAAgB,EAAE,IAAI,CAACM,gBAAgB,EAClE,IAAI,EAAE,IAAI,CAACF,eAAe,CAAC;;MAEjC,IAAI,IAAI,CAACP,MAAM,EAAE;QACf,IAAI,CAACpD,IAAI,GAAG,IAAI,CAACyE,SAAS,CACtB,MAAM,EAAE5C,KAAK,EAAE,IAAI,EAAE,IAAI,CAACyB,eAAe,EAAE,IAAI,CAACM,eAAe,EAAE,IAAI,EACrE,IAAI,CAACF,cAAc,CAAC;;MAE1B,IAAI,CAACgB,UAAU,GAAG,IAAI,CAACD,SAAS,CAC5B,aAAa,EAAE5C,KAAK,EAAE,IAAI,EAAE,IAAI,CAAC2B,qBAAqB,EAAE,IAAI,EAAE,KAAK,CAAC;MACxE,IAAI,CAACmB,cAAc,GAAG,IAAI,CAACF,SAAS,CAChC,iBAAiB,EAAE5C,KAAK,EAAE,IAAI,EAAE,IAAI,CAAC4B,yBAAyB,EAAE,IAAI,EACpE,KAAK,CAAC;MACV,IAAI,CAACmB,KAAK,GAAG,IAAI;IACnB;EAAC;IAAAb,GAAA;IAAArC,KAAA,EAEQ,SAAAuB,KAAK4B,MAAuB,EAAEC,MAAc;MAAA,IAAAC,MAAA;MACnD,OAAOnG,IAAI,CAAC,YAAK;QACf,IAAMoG,QAAQ,GAAGF,MAAM,CAAC,UAAU,CAAC,IAAI,IAAI,GAAG,KAAK,GAAGA,MAAM,CAAC,UAAU,CAAC;QACxE,IAAMG,KAAK,GAAGtF,mBAAmB,CAACkF,MAAM,CAAC;QACzC,IAAMZ,UAAU,GAAGgB,KAAK,CAACpD,KAAK;QAC9B,IAAMyC,IAAI,GAAGL,UAAU,CAAC7D,MAAM;QAC9B,IAAMS,aAAa,GAAGpB,UAAU,CAAC2B,KAAK,CAAC,CAAC,EAAEkD,IAAI,CAAC;QAC/C,IAAM7C,IAAI,GAAGsD,MAAI,CAACtD,IAAI,IAAI,CAAC,GAAGsD,MAAI,CAACtD,IAAI,GAAIsD,MAAI,CAACtD,IAAI,GAAG6C,IAAK;QAC5DzD,aAAa,CAACqE,MAAM,CAACzD,IAAI,EAAE,CAAC,CAAC;QAC7B,IAAM0D,cAAc,GAAG3F,aAAa,CAAC4F,YAAY,CAAC,CAAC,EAAEd,IAAI,CAAC;QAC1Da,cAAc,CAAC1D,IAAI,CAAC,GAAGwC,UAAU,CAACxC,IAAI,CAAC;QAEvC,IAAM4D,mBAAmB,GAAGxE,aAAa,CAAC0B,KAAK,EAAE;QACjD8C,mBAAmB,CAAC7C,IAAI,EAAE;QAC1B,IAAM8C,iBAAiB,GAAG,CAACzG,IAAI,CAACyD,WAAW,CACvC+C,mBAAmB,EAAE5F,UAAU,CAAC2B,KAAK,CAAC,CAAC,EAAEkD,IAAI,CAAC,CAAC/B,KAAK,CAAC,CAAC,EAAE+B,IAAI,GAAG,CAAC,CAAC,CAAC;QAEtE,IAAMiB,kBAAkB,GAAiB,SAAnCA,kBAAkBA,CAAA,EAAsB;UAC5C,IAAID,iBAAiB,EAAE;YACrB,IAAME,mBAAmB,GACrB9G,OAAO,CAACqG,MAAI,CAACL,UAAU,CAACe,IAAI,EAAE,EAAEN,cAAc,CAAC;YACnD,IAAMO,uBAAuB,GACzBhH,OAAO,CAACqG,MAAI,CAACJ,cAAc,CAACc,IAAI,EAAE,EAAEN,cAAc,CAAC;YACvD,IAAM/C,aAAa,GACf2C,MAAI,CAAC3B,MAAM,GAAG1E,OAAO,CAACqG,MAAI,CAAC/E,IAAI,CAACyF,IAAI,EAAE,EAAEN,cAAc,CAAC,GAAG,IAAI;YAClE,IAAMhD,cAAc,GAChB4C,MAAI,CAAC1B,KAAK,GAAG3E,OAAO,CAACqG,MAAI,CAAC9E,KAAK,CAACwF,IAAI,EAAE,EAAEN,cAAc,CAAC,GAAG,IAAI;YAClE,OAAOvF,kBAAkB,CACrBqF,KAAK,EAAEO,mBAAmB,EAAEE,uBAAuB,EACnDtD,aAAa,EAAED,cAAc,EAAE4C,MAAI,CAAC7E,OAAO,CAAC;WACjD,MAAM;YACL,OAAON,kBAAkB,CACrBqF,KAAK,EAAEF,MAAI,CAACL,UAAU,CAACe,IAAI,EAAE,EAAEV,MAAI,CAACJ,cAAc,CAACc,IAAI,EAAE,EACzDV,MAAI,CAAC/E,IAAI,IAAI,IAAI,GAAG,IAAI,GAAG+E,MAAI,CAAC/E,IAAI,CAACyF,IAAI,EAAE,EAC3CV,MAAI,CAAC9E,KAAK,IAAI,IAAI,GAAG,IAAI,GAAG8E,MAAI,CAAC9E,KAAK,CAACwF,IAAI,EAAE,EAAEV,MAAI,CAAC7E,OAAO,CAAC;;QAEpE,CAAC;QAED,IAAI,CAAC8E,QAAQ,EAAE;UACb,OAAOO,kBAAkB,EAAE;;QAG7B,IAAAI,qBAAA,GAAyCtD,wBAAwB,CAC7D4C,KAAK,EAAEF,MAAI,CAAC9E,KAAK,CAACwF,IAAI,EAAE,EAAEV,MAAI,CAAC/E,IAAI,CAACyF,IAAI,EAAE,EAAE5E,aAAa,EACzDkE,MAAI,CAAC7E,OAAO,CAAC;UAAA0F,sBAAA,GAAAC,cAAA,CAAAF,qBAAA;UAFVG,cAAc,GAAAF,sBAAA;UAAE9F,IAAI,GAAA8F,sBAAA;UAAE7F,QAAQ,GAAA6F,sBAAA;QAIrC,IAAMG,eAAe,GACjB,SADEA,eAAeA,CAChBC,QAAuB,EAAEtE,KAAa,EAAEyB,QAAgB,EAAU;UACjE3E,GAAG,CAACI,IAAI,CAAC,YAAK;YACZ,IAAMqH,KAAK,GAAG,CAAC,GAAG9C,QAAQ;YAC1B,IAAM+C,SAAS,GAAGF,QAAQ,CAACP,IAAI,EAAE;YACjC,IAAMU,WAAW,GAAG3H,GAAG,CAAC4H,GAAG,CAAC5H,GAAG,CAAC6H,GAAG,CAACH,SAAS,EAAExE,KAAK,CAAC,EAAEuE,KAAK,CAAC;YAC7DD,QAAQ,CAACM,KAAK,CAAC9H,GAAG,CAAC6H,GAAG,CAACH,SAAS,EAAEC,WAAW,CAAC,CAAC;UACjD,CAAC,CAAC;QACJ,CAAC;QAEL;QACA;QACA;QACA;QACA;QACA;QACA,IAAMI,2BAA2B,GAAG,SAA9BA,2BAA2BA,CAAA,EAAQ;UACvCR,eAAe,CAAChB,MAAI,CAACL,UAAU,EAAE5E,IAAI,EAAEiF,MAAI,CAAC5B,QAAQ,CAAC;UACrD4C,eAAe,CAAChB,MAAI,CAACJ,cAAc,EAAE5E,QAAQ,EAAEgF,MAAI,CAAC5B,QAAQ,CAAC;QAC/D,CAAC;QACDoD,2BAA2B,EAAE;QAE7B,OAAOT,cAAc;MACvB,CAAC,CAAC;IACJ;EAAC;IAAA/B,GAAA;IAAArC,KAAA,EAEQ,SAAA8E,UAAA,EAAS;MAChB,IAAMC,MAAM,GAA6B;QACvChF,IAAI,EAAE,IAAI,CAACA,IAAI;QACf0B,QAAQ,EAAE,IAAI,CAACA,QAAQ;QACvBjD,OAAO,EAAE,IAAI,CAACA,OAAO;QACrBkD,MAAM,EAAE,IAAI,CAACA,MAAM;QACnBC,KAAK,EAAE,IAAI,CAACA,KAAK;QACjBC,eAAe,EAAEjE,oBAAoB,CAAC,IAAI,CAACiE,eAAe,CAAC;QAC3DC,gBAAgB,EAAElE,oBAAoB,CAAC,IAAI,CAACkE,gBAAgB,CAAC;QAC7DC,qBAAqB,EAAEnE,oBAAoB,CAAC,IAAI,CAACmE,qBAAqB,CAAC;QACvEC,yBAAyB,EACrBpE,oBAAoB,CAAC,IAAI,CAACoE,yBAAyB,CAAC;QACxDG,eAAe,EAAErE,oBAAoB,CAAC,IAAI,CAACqE,eAAe,CAAC;QAC3DC,gBAAgB,EAAEtE,oBAAoB,CAAC,IAAI,CAACsE,gBAAgB,CAAC;QAC7DH,cAAc,EAAE3E,mBAAmB,CAAC,IAAI,CAAC2E,cAAc,CAAC;QACxDC,eAAe,EAAE5E,mBAAmB,CAAC,IAAI,CAAC4E,eAAe;OAC1D;MACD,IAAM+C,UAAU,GAAAC,IAAA,CAAAC,eAAA,CAAAnE,kBAAA,CAAAoE,SAAA,sBAAA5D,IAAA,MAAoB;MACpC6D,MAAM,CAACC,MAAM,CAACN,MAAM,EAAEC,UAAU,CAAC;MACjC,OAAOD,MAAM;IACf;EAAC;EAAA,OAAAhE,kBAAA;AAAA,EAxKqCxD,KAAK;AAC3C;AACOwD,kBAAA,CAAAuE,SAAS,GAAG,oBAAoB;AAwKzCrI,aAAa,CAACsI,aAAa,CAACxE,kBAAkB,CAAC;AAkD/C,WAAayE,kBAAmB,0BAAAC,OAAA;EAAAxE,SAAA,CAAAuE,kBAAA,EAAAC,OAAA;EAAA,IAAAC,OAAA,GAAAvE,YAAA,CAAAqE,kBAAA;EAgB9B,SAAAA,mBAAYpE,IAAkC;IAAA,IAAAuE,MAAA;IAAArE,eAAA,OAAAkE,kBAAA;IAC5C,IAAIpE,IAAI,IAAI,IAAI,EAAE;MAChBA,IAAI,GAAG,EAAE;;IAEXuE,MAAA,GAAAD,OAAA,CAAAnE,IAAA,OAAMH,IAAI;IAEVuE,MAAA,CAAK5F,IAAI,GAAGqB,IAAI,CAACrB,IAAI,IAAI,IAAI,GAAG,CAAC,CAAC,GAAGqB,IAAI,CAACrB,IAAI;IAC9C,IAAI,OAAO4F,MAAA,CAAK5F,IAAI,KAAK,QAAQ,EAAE;MACjC,IAAI,CAAC6F,MAAM,CAACC,SAAS,CAACF,MAAA,CAAK5F,IAAI,CAAC,EAAE;QAChC,MAAM,IAAI+F,KAAK,iDAAA7G,MAAA,CACqC0G,MAAA,CAAK5F,IAAI,EAAG;;KAEnE,MAAM,IAAIgG,KAAK,CAACC,OAAO,CAACL,MAAA,CAAK5F,IAAI,CAAC,EAAE;MAAA,IAAAkG,UAAA,GAAAxG,0BAAA,CAChBkG,MAAA,CAAK5F,IAAI;QAAAmG,MAAA;MAAA;QAA5B,KAAAD,UAAA,CAAArG,CAAA,MAAAsG,MAAA,GAAAD,UAAA,CAAApG,CAAA,IAAAC,IAAA,GAA8B;UAAA,IAAnBC,IAAI,GAAAmG,MAAA,CAAAlG,KAAA;UACb,IAAI,CAAC4F,MAAM,CAACC,SAAS,CAAC9F,IAAI,CAAC,EAAE;YAC3B,MAAM,IAAI+F,KAAK,CACX,+DAAA7G,MAAA,CACgBwD,IAAI,CAACC,SAAS,CAACiD,MAAA,CAAK5F,IAAI,CAAC,CAAE,CAAC;;;MAEnD,SAAAK,GAAA;QAAA6F,UAAA,CAAA5F,CAAA,CAAAD,GAAA;MAAA;QAAA6F,UAAA,CAAA3F,CAAA;MAAA;KACF,MAAM;MACL,MAAM,IAAIwF,KAAK,CACX,6EAAA7G,MAAA,CACgBwD,IAAI,CAACC,SAAS,CAACiD,MAAA,CAAK5F,IAAI,CAAC,CAAE,CAAC;;IAGlD4F,MAAA,CAAKnH,OAAO,GAAG4C,IAAI,CAAC5C,OAAO,IAAI,IAAI,GAAG,IAAI,GAAG4C,IAAI,CAAC5C,OAAO;IACzDmH,MAAA,CAAKjE,MAAM,GAAGN,IAAI,CAACM,MAAM,IAAI,IAAI,GAAG,IAAI,GAAGN,IAAI,CAACM,MAAM;IACtDiE,MAAA,CAAKhE,KAAK,GAAGP,IAAI,CAACO,KAAK,IAAI,IAAI,GAAG,IAAI,GAAGP,IAAI,CAACO,KAAK;IACnDgE,MAAA,CAAK/D,eAAe,GAAGlE,cAAc,CAAC0D,IAAI,CAACQ,eAAe,IAAI,OAAO,CAAC;IACtE+D,MAAA,CAAK9D,gBAAgB,GAAGnE,cAAc,CAAC0D,IAAI,CAACS,gBAAgB,IAAI,MAAM,CAAC;IACvE8D,MAAA,CAAKzD,eAAe,GAAGtE,cAAc,CAACwD,IAAI,CAACc,eAAe,CAAC;IAC3DyD,MAAA,CAAKxD,gBAAgB,GAAGvE,cAAc,CAACwD,IAAI,CAACe,gBAAgB,CAAC;IAE7DwD,MAAA,CAAKnE,eAAe,GAAG,IAAI;IAAC,OAAAmE,MAAA;EAC9B;EAACvD,YAAA,CAAAoD,kBAAA;IAAAnD,GAAA;IAAArC,KAAA,EAEe,SAAAsC,MAAMC,UAAyB;MAC7CA,UAAU,GAAGvE,kBAAkB,CAACuE,UAAU,CAAC;MAC3C,IAAM4D,KAAK,GAAG5D,UAAU,CAAC7D,MAAM;MAE/B;MACA,IAAI,OAAO,IAAI,CAACqB,IAAI,KAAK,QAAQ,EAAE;QACjC,IAAI,CAACA,IAAI,GAAG,CAAC,IAAI,CAACA,IAAI,CAAC;;MAEzB,KAAK,IAAIqG,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG,IAAI,CAACrG,IAAI,CAACrB,MAAM,EAAE,EAAE0H,CAAC,EAAE;QACzC,IAAI,IAAI,CAACrG,IAAI,CAACqG,CAAC,CAAC,GAAG,CAAC,EAAE;UACpB,IAAI,CAACrG,IAAI,CAACqG,CAAC,CAAC,IAAID,KAAK;;;MAIzB;MAAA,IAAAE,UAAA,GAAA5G,0BAAA,CACmB,IAAI,CAACM,IAAI;QAAAuG,MAAA;MAAA;QAA5B,KAAAD,UAAA,CAAAzG,CAAA,MAAA0G,MAAA,GAAAD,UAAA,CAAAxG,CAAA,IAAAC,IAAA,GAA8B;UAAA,IAAnBC,IAAI,GAAAuG,MAAA,CAAAtG,KAAA;UACb,IAAID,IAAI,GAAG,CAAC,IAAIA,IAAI,IAAIoG,KAAK,EAAE;YAC7B,MAAM,IAAIL,KAAK,kBAAA7G,MAAA,CAAkBc,IAAI,EAAG;;;MAE3C,SAAAK,GAAA;QAAAiG,UAAA,CAAAhG,CAAA,CAAAD,GAAA;MAAA;QAAAiG,UAAA,CAAA/F,CAAA;MAAA;MACD,IAAI,IAAI,CAACP,IAAI,CAACrB,MAAM,KAAKZ,aAAa,CAACyI,MAAM,CAAC,IAAI,CAACxG,IAAI,CAAC,CAACrB,MAAM,EAAE;QAC/D,MAAM,IAAIoH,KAAK,6BAAA7G,MAAA,CAA6B,IAAI,CAACc,IAAI,EAAG;;MAG1D,IAAMyG,UAAU,GAAG,IAAI,CAACzG,IAAI,CAAC0G,GAAG,CAAC,UAAA1G,IAAI;QAAA,OAAIwC,UAAU,CAACxC,IAAI,CAAC;MAAA,EAAa;MAEtE,IAAM2G,SAAS,GAAG,IAAI;MACtB,IAAI,IAAI,CAAC/E,KAAK,EAAE;QACd,IAAI,CAACpD,KAAK,GAAG,IAAI,CAACwE,SAAS,CACvB,OAAO,EAAEyD,UAAU,EAAE,SAAS,EAAE,IAAI,CAAC3E,gBAAgB,EACrD,IAAI,CAACM,gBAAgB,EAAEuE,SAAS,CAAC;OACtC,MAAM;QACL,IAAI,CAACnI,KAAK,GAAG,IAAI;;MAEnB,IAAI,IAAI,CAACmD,MAAM,EAAE;QACf,IAAI,CAACpD,IAAI,GAAG,IAAI,CAACyE,SAAS,CACtB,MAAM,EAAEyD,UAAU,EAAE,SAAS,EAAE,IAAI,CAAC5E,eAAe,EACnD,IAAI,CAACM,eAAe,EAAEwE,SAAS,CAAC;OACrC,MAAM;QACL,IAAI,CAACpI,IAAI,GAAG,IAAI;;MAGlB,IAAI,CAAC4E,KAAK,GAAG,IAAI;IACnB;EAAC;IAAAb,GAAA;IAAArC,KAAA,EAEQ,SAAAuB,KAAK4B,MAAuB,EAAEC,MAAc;MAAA,IAAAuD,MAAA;MACnD,IAAMpD,KAAK,GAAGtF,mBAAmB,CAACkF,MAAM,CAAC;MACzC,IAAMZ,UAAU,GAAGgB,KAAK,CAACpD,KAAK;MAC9B,IAAMgG,KAAK,GAAG5D,UAAU,CAAC7D,MAAM;MAE/B,OAAOxB,IAAI,CAAC,YAAK;QACf,IAAM0J,QAAQ,GAAG,IAAI;QACrB,IAAAC,QAAA,GAAuB9J,OAAO,CAACwG,KAAK,EAAEoD,MAAI,CAAC5G,IAAI,EAAE6G,QAAQ,CAAC;UAArDxI,IAAI,GAAAyI,QAAA,CAAJzI,IAAI;UAAEC,QAAQ,GAAAwI,QAAA,CAARxI,QAAQ;QACnB,IAAMoF,cAAc,GAAG3F,aAAa,CAAC4F,YAAY,CAAC,CAAC,EAAEyC,KAAK,CAAC;QAAC,IAAAW,UAAA,GAAArH,0BAAA,CAC1CkH,MAAI,CAAC5G,IAAgB;UAAAgH,MAAA;QAAA;UAAvC,KAAAD,UAAA,CAAAlH,CAAA,MAAAmH,MAAA,GAAAD,UAAA,CAAAjH,CAAA,IAAAC,IAAA,GAAyC;YAAA,IAA9B0C,GAAG,GAAAuE,MAAA,CAAA/G,KAAA;YACZyD,cAAc,CAACjB,GAAG,CAAC,GAAGD,UAAU,CAACC,GAAG,CAAC;;QACtC,SAAApC,GAAA;UAAA0G,UAAA,CAAAzG,CAAA,CAAAD,GAAA;QAAA;UAAA0G,UAAA,CAAAxG,CAAA;QAAA;QAED,IAAM0G,SAAS,GAAG,SAAZA,SAASA,CAAIC,CAAS,EAAI;UAC9B,IAAIA,CAAC,IAAI,IAAI,IAAIA,CAAC,CAAC9G,KAAK,CAACzB,MAAM,KAAKyH,KAAK,EAAE;YACzC,OAAOrJ,GAAG,CAACE,OAAO,CAACiK,CAAC,EAAExD,cAAc,CAAC;WACtC,MAAM;YACL,OAAOwD,CAAC;;QAEZ,CAAC;QAED,IAAItF,KAAK,GAAGgF,MAAI,CAAChF,KAAK,GAAGqF,SAAS,CAACL,MAAI,CAACpI,KAAK,CAACwF,IAAI,EAAE,CAAC,GAAG,IAAI;QAC5D,IAAImD,MAAM,GAAGP,MAAI,CAACjF,MAAM,GAAGsF,SAAS,CAACL,MAAI,CAACrI,IAAI,CAACyF,IAAI,EAAE,CAAC,GAAG,IAAI;QAE7D;QACA;QACA;QACA;QACA;QACA;QACA,IAAMoD,aAAa,GAAa,EAAE;QAClC,IAAMC,iBAAiB,GAAa,EAAE;QACtC,KAAK,IAAIhB,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGD,KAAK,EAAE,EAAEC,CAAC,EAAE;UAC9B,IAAKO,MAAI,CAAC5G,IAAiB,CAACE,OAAO,CAACmG,CAAC,CAAC,KAAK,CAAC,CAAC,EAAE;YAC7Ce,aAAa,CAACjH,IAAI,CAACqC,UAAU,CAAC6D,CAAC,CAAC,CAAC;YACjCgB,iBAAiB,CAAClH,IAAI,CAAC,CAAC,CAAC;WAC1B,MAAM;YACLiH,aAAa,CAACjH,IAAI,CAAC,CAAC,CAAC;YACrBkH,iBAAiB,CAAClH,IAAI,CAACqC,UAAU,CAAC6D,CAAC,CAAC,CAAC;;;QAGzChI,IAAI,GAAGtB,GAAG,CAACuK,IAAI,CAACjJ,IAAI,EAAE+I,aAAa,CAAC;QACpC9I,QAAQ,GAAGvB,GAAG,CAACuK,IAAI,CAAChJ,QAAQ,EAAE8I,aAAa,CAAC;QAC5C,IAAIxF,KAAK,IAAI,IAAI,EAAE;UACjBA,KAAK,GAAG7E,GAAG,CAACuK,IAAI,CAAC1F,KAAK,EAAEyF,iBAAiB,CAAC;;QAE5C,IAAIF,MAAM,IAAI,IAAI,EAAE;UAClBA,MAAM,GAAGpK,GAAG,CAACuK,IAAI,CAACH,MAAM,EAAEE,iBAAiB,CAAC;;QAG9C,OAAOlJ,kBAAkB,CACrBqF,KAAK,EAAEnF,IAAI,EAAEC,QAAQ,EAAE6I,MAAM,EAAEvF,KAAK,EAAEgF,MAAI,CAACnI,OAAO,CAAC;MACzD,CAAC,CAAC;IACJ;EAAC;IAAA6D,GAAA;IAAArC,KAAA,EAEQ,SAAA8E,UAAA,EAAS;MAChB,IAAMC,MAAM,GAA6B;QACvChF,IAAI,EAAE,IAAI,CAACA,IAAI;QACfvB,OAAO,EAAE,IAAI,CAACA,OAAO;QACrBkD,MAAM,EAAE,IAAI,CAACA,MAAM;QACnBC,KAAK,EAAE,IAAI,CAACA,KAAK;QACjBC,eAAe,EAAEjE,oBAAoB,CAAC,IAAI,CAACiE,eAAe,CAAC;QAC3DC,gBAAgB,EAAElE,oBAAoB,CAAC,IAAI,CAACkE,gBAAgB,CAAC;QAC7DK,eAAe,EAAErE,oBAAoB,CAAC,IAAI,CAACqE,eAAe,CAAC;QAC3DC,gBAAgB,EAAEtE,oBAAoB,CAAC,IAAI,CAACsE,gBAAgB;OAC7D;MACD,IAAM6C,UAAU,GAAAC,IAAA,CAAAC,eAAA,CAAAM,kBAAA,CAAAL,SAAA,sBAAA5D,IAAA,MAAoB;MACpC6D,MAAM,CAACC,MAAM,CAACN,MAAM,EAAEC,UAAU,CAAC;MACjC,OAAOD,MAAM;IACf;EAAC;EAAA,OAAAS,kBAAA;AAAA,EAvKqCjI,KAAK;AAC3C;AACOiI,kBAAA,CAAAF,SAAS,GAAG,oBAAoB;AAuKzCrI,aAAa,CAACsI,aAAa,CAACC,kBAAkB,CAAC"},"metadata":{},"sourceType":"module","externalDependencies":[]}