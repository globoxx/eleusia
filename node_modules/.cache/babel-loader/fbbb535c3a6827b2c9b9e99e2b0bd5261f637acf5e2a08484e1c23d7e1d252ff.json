{"ast":null,"code":"/**\r\n * @license\r\n * Copyright 2020 Google LLC. All Rights Reserved.\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n * http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n * =============================================================================\r\n */\nimport { backend_util, Dilation2DBackpropInput, util } from '@tensorflow/tfjs-core';\nexport var dilation2DBackpropInputConfig = {\n  kernelName: Dilation2DBackpropInput,\n  backendName: 'cpu',\n  kernelFunc: function kernelFunc(_ref) {\n    var inputs = _ref.inputs,\n      backend = _ref.backend,\n      attrs = _ref.attrs;\n    var x = inputs.x,\n      filter = inputs.filter,\n      dy = inputs.dy;\n    var strides = attrs.strides,\n      pad = attrs.pad,\n      dilations = attrs.dilations;\n    var cpuBackend = backend;\n    var $x = util.toNestedArray(x.shape, cpuBackend.data.get(x.dataId).values);\n    var $filter = util.toNestedArray(filter.shape, cpuBackend.data.get(filter.dataId).values);\n    var _backend_util$compute = backend_util.computeDilation2DInfo(x.shape, filter.shape, strides, pad, 'NHWC' /* dataFormat */, dilations),\n      batchSize = _backend_util$compute.batchSize,\n      inHeight = _backend_util$compute.inHeight,\n      inWidth = _backend_util$compute.inWidth,\n      inChannels = _backend_util$compute.inChannels,\n      outHeight = _backend_util$compute.outHeight,\n      outWidth = _backend_util$compute.outWidth,\n      padInfo = _backend_util$compute.padInfo,\n      strideHeight = _backend_util$compute.strideHeight,\n      strideWidth = _backend_util$compute.strideWidth,\n      filterHeight = _backend_util$compute.filterHeight,\n      filterWidth = _backend_util$compute.filterWidth,\n      dilationHeight = _backend_util$compute.dilationHeight,\n      dilationWidth = _backend_util$compute.dilationWidth,\n      outShape = _backend_util$compute.outShape;\n    util.assert(dy.rank === outShape.length, function () {\n      return \"Error in \".concat(Dilation2DBackpropInput, \", dy \") + \"must have the same rank as output \".concat(outShape.length, \", but got \") + \"\".concat(dy.rank);\n    });\n    var $dy = util.toNestedArray(outShape, cpuBackend.data.get(dy.dataId).values);\n    // The computed gradients has the same dimensions as the input:\n    // [batch, inputHeight, inputCols, inChannel]\n    var gradients = util.makeZerosNestedTypedArray(x.shape, x.dtype);\n    // In the case of multiple argmax branches, we only back-propagate along the\n    // last branch, i.e., the one with largest value of `h * filter_cols + w`,\n    // similarly to the max-pooling backward routines.\n    // This implementation follows the TF c++ implementation:\n    // https://github.com/tensorflow/tensorflow/blob/d9a3a849edc198e90172bc58eb293de457f9d986/tensorflow/core/kernels/dilation_ops.cc\n    for (var b = 0; b < batchSize; ++b) {\n      for (var hOut = 0; hOut < outHeight; ++hOut) {\n        var hBeg = hOut * strideHeight - padInfo.top;\n        for (var wOut = 0; wOut < outWidth; ++wOut) {\n          var wBeg = wOut * strideWidth - padInfo.left;\n          for (var d = 0; d < inChannels; ++d) {\n            var curVal = Number.MIN_SAFE_INTEGER;\n            var hInMax = hBeg < 0 ? 0 : hBeg;\n            var wInMax = wBeg < 0 ? 0 : wBeg;\n            for (var h = 0; h < filterHeight; ++h) {\n              var hIn = hBeg + h * dilationHeight;\n              if (hIn >= 0 && hIn < inHeight) {\n                for (var w = 0; w < filterWidth; ++w) {\n                  var wIn = wBeg + w * dilationWidth;\n                  if (wIn >= 0 && wIn < inWidth) {\n                    var val = $x[b][hIn][wIn][d] + $filter[h][w][d];\n                    if (val > curVal) {\n                      curVal = val;\n                      hInMax = hIn;\n                      wInMax = wIn;\n                    }\n                  }\n                }\n              }\n            }\n            gradients[b][hInMax][wInMax][d] += $dy[b][hOut][wOut][d];\n          }\n        }\n      }\n    }\n    var dataId = cpuBackend.write(util.toTypedArray(gradients, x.dtype), x.shape, x.dtype);\n    return {\n      dataId: dataId,\n      shape: x.shape,\n      dtype: x.dtype\n    };\n  }\n};","map":{"version":3,"names":["backend_util","Dilation2DBackpropInput","util","dilation2DBackpropInputConfig","kernelName","backendName","kernelFunc","_ref","inputs","backend","attrs","x","filter","dy","strides","pad","dilations","cpuBackend","$x","toNestedArray","shape","data","get","dataId","values","$filter","_backend_util$compute","computeDilation2DInfo","batchSize","inHeight","inWidth","inChannels","outHeight","outWidth","padInfo","strideHeight","strideWidth","filterHeight","filterWidth","dilationHeight","dilationWidth","outShape","assert","rank","length","concat","$dy","gradients","makeZerosNestedTypedArray","dtype","b","hOut","hBeg","top","wOut","wBeg","left","d","curVal","Number","MIN_SAFE_INTEGER","hInMax","wInMax","h","hIn","w","wIn","val","write","toTypedArray"],"sources":["C:\\Users\\vince\\OneDrive\\Documents\\GitHub\\tfjs-backend-cpu\\src\\kernels\\Dilation2DBackpropInput.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, Dilation2DAttrs, Dilation2DBackpropInput, Tensor3D, Tensor4D, TypedArray, util} from '@tensorflow/tfjs-core';\nimport {KernelConfig} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\n\nexport const dilation2DBackpropInputConfig: KernelConfig = {\n  kernelName: Dilation2DBackpropInput,\n  backendName: 'cpu',\n  kernelFunc: ({inputs, backend, attrs}) => {\n    const {x, filter, dy} =\n        inputs as {x: Tensor4D, filter: Tensor3D, dy: Tensor4D};\n    const {strides, pad, dilations} = attrs as unknown as Dilation2DAttrs;\n    const cpuBackend = backend as MathBackendCPU;\n\n    const $x =\n        util.toNestedArray(\n            x.shape, cpuBackend.data.get(x.dataId).values as TypedArray) as\n        number[][][][];\n\n    const $filter = util.toNestedArray(\n                        filter.shape,\n                        cpuBackend.data.get(filter.dataId).values as\n                            TypedArray) as number[][][];\n\n    const {\n      batchSize,\n      inHeight,\n      inWidth,\n      inChannels,\n      outHeight,\n      outWidth,\n      padInfo,\n      strideHeight,\n      strideWidth,\n      filterHeight,\n      filterWidth,\n      dilationHeight,\n      dilationWidth,\n      outShape\n    } =\n        backend_util.computeDilation2DInfo(\n            x.shape as [number, number, number, number],\n            filter.shape as [number, number, number], strides, pad,\n            'NHWC' /* dataFormat */, dilations);\n\n    util.assert(\n        dy.rank === outShape.length,\n        () => `Error in ${Dilation2DBackpropInput}, dy ` +\n            `must have the same rank as output ${outShape.length}, but got ` +\n            `${dy.rank}`);\n\n    const $dy =\n        util.toNestedArray(\n            outShape, cpuBackend.data.get(dy.dataId).values as TypedArray) as\n        number[][][][];\n\n    // The computed gradients has the same dimensions as the input:\n    // [batch, inputHeight, inputCols, inChannel]\n    const gradients =\n        util.makeZerosNestedTypedArray(x.shape, x.dtype) as number[][][][];\n\n    // In the case of multiple argmax branches, we only back-propagate along the\n    // last branch, i.e., the one with largest value of `h * filter_cols + w`,\n    // similarly to the max-pooling backward routines.\n    // This implementation follows the TF c++ implementation:\n    // https://github.com/tensorflow/tensorflow/blob/d9a3a849edc198e90172bc58eb293de457f9d986/tensorflow/core/kernels/dilation_ops.cc\n    for (let b = 0; b < batchSize; ++b) {\n      for (let hOut = 0; hOut < outHeight; ++hOut) {\n        const hBeg = hOut * strideHeight - padInfo.top;\n        for (let wOut = 0; wOut < outWidth; ++wOut) {\n          const wBeg = wOut * strideWidth - padInfo.left;\n          for (let d = 0; d < inChannels; ++d) {\n            let curVal = Number.MIN_SAFE_INTEGER;\n            let hInMax = (hBeg < 0) ? 0 : hBeg;\n            let wInMax = (wBeg < 0) ? 0 : wBeg;\n            for (let h = 0; h < filterHeight; ++h) {\n              const hIn = hBeg + h * dilationHeight;\n              if (hIn >= 0 && hIn < inHeight) {\n                for (let w = 0; w < filterWidth; ++w) {\n                  const wIn = wBeg + w * dilationWidth;\n                  if (wIn >= 0 && wIn < inWidth) {\n                    const val = $x[b][hIn][wIn][d] + $filter[h][w][d];\n                    if (val > curVal) {\n                      curVal = val;\n                      hInMax = hIn;\n                      wInMax = wIn;\n                    }\n                  }\n                }\n              }\n            }\n            gradients[b][hInMax][wInMax][d] += $dy[b][hOut][wOut][d];\n          }\n        }\n      }\n    }\n\n    const dataId = cpuBackend.write(\n        util.toTypedArray(gradients, x.dtype), x.shape, x.dtype);\n\n    return {dataId, shape: x.shape, dtype: x.dtype};\n  }\n};\n"],"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQA,YAAY,EAAmBC,uBAAuB,EAAkCC,IAAI,QAAO,uBAAuB;AAKlI,OAAO,IAAMC,6BAA6B,GAAiB;EACzDC,UAAU,EAAEH,uBAAuB;EACnCI,WAAW,EAAE,KAAK;EAClBC,UAAU,EAAE,SAAAA,WAAAC,IAAA,EAA6B;IAAA,IAA3BC,MAAM,GAAAD,IAAA,CAANC,MAAM;MAAEC,OAAO,GAAAF,IAAA,CAAPE,OAAO;MAAEC,KAAK,GAAAH,IAAA,CAALG,KAAK;IAClC,IAAOC,CAAC,GACJH,MAAuD,CADpDG,CAAC;MAAEC,MAAM,GACZJ,MAAuD,CADjDI,MAAM;MAAEC,EAAE,GAChBL,MAAuD,CADzCK,EAAE;IAEpB,IAAOC,OAAO,GAAoBJ,KAAmC,CAA9DI,OAAO;MAAEC,GAAG,GAAeL,KAAmC,CAArDK,GAAG;MAAEC,SAAS,GAAIN,KAAmC,CAAhDM,SAAS;IAC9B,IAAMC,UAAU,GAAGR,OAAyB;IAE5C,IAAMS,EAAE,GACJhB,IAAI,CAACiB,aAAa,CACdR,CAAC,CAACS,KAAK,EAAEH,UAAU,CAACI,IAAI,CAACC,GAAG,CAACX,CAAC,CAACY,MAAM,CAAC,CAACC,MAAoB,CACjD;IAElB,IAAMC,OAAO,GAAGvB,IAAI,CAACiB,aAAa,CACdP,MAAM,CAACQ,KAAK,EACZH,UAAU,CAACI,IAAI,CAACC,GAAG,CAACV,MAAM,CAACW,MAAM,CAAC,CAACC,MACrB,CAAiB;IAEnD,IAAAE,qBAAA,GAgBI1B,YAAY,CAAC2B,qBAAqB,CAC9BhB,CAAC,CAACS,KAAyC,EAC3CR,MAAM,CAACQ,KAAiC,EAAEN,OAAO,EAAEC,GAAG,EACtD,MAAM,CAAC,kBAAkBC,SAAS,CAAC;MAlBzCY,SAAS,GAAAF,qBAAA,CAATE,SAAS;MACTC,QAAQ,GAAAH,qBAAA,CAARG,QAAQ;MACRC,OAAO,GAAAJ,qBAAA,CAAPI,OAAO;MACPC,UAAU,GAAAL,qBAAA,CAAVK,UAAU;MACVC,SAAS,GAAAN,qBAAA,CAATM,SAAS;MACTC,QAAQ,GAAAP,qBAAA,CAARO,QAAQ;MACRC,OAAO,GAAAR,qBAAA,CAAPQ,OAAO;MACPC,YAAY,GAAAT,qBAAA,CAAZS,YAAY;MACZC,WAAW,GAAAV,qBAAA,CAAXU,WAAW;MACXC,YAAY,GAAAX,qBAAA,CAAZW,YAAY;MACZC,WAAW,GAAAZ,qBAAA,CAAXY,WAAW;MACXC,cAAc,GAAAb,qBAAA,CAAda,cAAc;MACdC,aAAa,GAAAd,qBAAA,CAAbc,aAAa;MACbC,QAAQ,GAAAf,qBAAA,CAARe,QAAQ;IAOVvC,IAAI,CAACwC,MAAM,CACP7B,EAAE,CAAC8B,IAAI,KAAKF,QAAQ,CAACG,MAAM,EAC3B;MAAA,OAAM,YAAAC,MAAA,CAAY5C,uBAAuB,kDAAA4C,MAAA,CACAJ,QAAQ,CAACG,MAAM,eAAY,MAAAC,MAAA,CAC7DhC,EAAE,CAAC8B,IAAI,CAAE;IAAA,EAAC;IAErB,IAAMG,GAAG,GACL5C,IAAI,CAACiB,aAAa,CACdsB,QAAQ,EAAExB,UAAU,CAACI,IAAI,CAACC,GAAG,CAACT,EAAE,CAACU,MAAM,CAAC,CAACC,MAAoB,CACnD;IAElB;IACA;IACA,IAAMuB,SAAS,GACX7C,IAAI,CAAC8C,yBAAyB,CAACrC,CAAC,CAACS,KAAK,EAAET,CAAC,CAACsC,KAAK,CAAmB;IAEtE;IACA;IACA;IACA;IACA;IACA,KAAK,IAAIC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGtB,SAAS,EAAE,EAAEsB,CAAC,EAAE;MAClC,KAAK,IAAIC,IAAI,GAAG,CAAC,EAAEA,IAAI,GAAGnB,SAAS,EAAE,EAAEmB,IAAI,EAAE;QAC3C,IAAMC,IAAI,GAAGD,IAAI,GAAGhB,YAAY,GAAGD,OAAO,CAACmB,GAAG;QAC9C,KAAK,IAAIC,IAAI,GAAG,CAAC,EAAEA,IAAI,GAAGrB,QAAQ,EAAE,EAAEqB,IAAI,EAAE;UAC1C,IAAMC,IAAI,GAAGD,IAAI,GAAGlB,WAAW,GAAGF,OAAO,CAACsB,IAAI;UAC9C,KAAK,IAAIC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG1B,UAAU,EAAE,EAAE0B,CAAC,EAAE;YACnC,IAAIC,MAAM,GAAGC,MAAM,CAACC,gBAAgB;YACpC,IAAIC,MAAM,GAAIT,IAAI,GAAG,CAAC,GAAI,CAAC,GAAGA,IAAI;YAClC,IAAIU,MAAM,GAAIP,IAAI,GAAG,CAAC,GAAI,CAAC,GAAGA,IAAI;YAClC,KAAK,IAAIQ,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG1B,YAAY,EAAE,EAAE0B,CAAC,EAAE;cACrC,IAAMC,GAAG,GAAGZ,IAAI,GAAGW,CAAC,GAAGxB,cAAc;cACrC,IAAIyB,GAAG,IAAI,CAAC,IAAIA,GAAG,GAAGnC,QAAQ,EAAE;gBAC9B,KAAK,IAAIoC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG3B,WAAW,EAAE,EAAE2B,CAAC,EAAE;kBACpC,IAAMC,GAAG,GAAGX,IAAI,GAAGU,CAAC,GAAGzB,aAAa;kBACpC,IAAI0B,GAAG,IAAI,CAAC,IAAIA,GAAG,GAAGpC,OAAO,EAAE;oBAC7B,IAAMqC,GAAG,GAAGjD,EAAE,CAACgC,CAAC,CAAC,CAACc,GAAG,CAAC,CAACE,GAAG,CAAC,CAACT,CAAC,CAAC,GAAGhC,OAAO,CAACsC,CAAC,CAAC,CAACE,CAAC,CAAC,CAACR,CAAC,CAAC;oBACjD,IAAIU,GAAG,GAAGT,MAAM,EAAE;sBAChBA,MAAM,GAAGS,GAAG;sBACZN,MAAM,GAAGG,GAAG;sBACZF,MAAM,GAAGI,GAAG;;;;;;YAMtBnB,SAAS,CAACG,CAAC,CAAC,CAACW,MAAM,CAAC,CAACC,MAAM,CAAC,CAACL,CAAC,CAAC,IAAIX,GAAG,CAACI,CAAC,CAAC,CAACC,IAAI,CAAC,CAACG,IAAI,CAAC,CAACG,CAAC,CAAC;;;;;IAMhE,IAAMlC,MAAM,GAAGN,UAAU,CAACmD,KAAK,CAC3BlE,IAAI,CAACmE,YAAY,CAACtB,SAAS,EAAEpC,CAAC,CAACsC,KAAK,CAAC,EAAEtC,CAAC,CAACS,KAAK,EAAET,CAAC,CAACsC,KAAK,CAAC;IAE5D,OAAO;MAAC1B,MAAM,EAANA,MAAM;MAAEH,KAAK,EAAET,CAAC,CAACS,KAAK;MAAE6B,KAAK,EAAEtC,CAAC,CAACsC;IAAK,CAAC;EACjD;CACD"},"metadata":{},"sourceType":"module","externalDependencies":[]}