{"ast":null,"code":"import _toConsumableArray from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/toConsumableArray.js\";\nimport _slicedToArray from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/slicedToArray.js\";\nimport _regeneratorRuntime from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/regeneratorRuntime.js\";\nimport _asyncToGenerator from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/asyncToGenerator.js\";\nimport _classCallCheck from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/classCallCheck.js\";\nimport _createClass from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/createClass.js\";\nimport _get from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/get.js\";\nimport _getPrototypeOf from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/getPrototypeOf.js\";\nimport _inherits from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/inherits.js\";\nimport _createSuper from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/createSuper.js\";\nimport _createForOfIteratorHelper from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/createForOfIteratorHelper.js\";\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/* Original Source: engine/training.py */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { io, Optimizer, scalar, serialization, Tensor, tensor1d, util } from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport { configureCallbacks, standardizeCallbacks } from '../base_callbacks';\nimport { nameScope } from '../common';\nimport { NotImplementedError, RuntimeError, ValueError } from '../errors';\nimport { deserialize } from '../layers/serialization';\nimport { disposeTensorsInLogs } from '../logs';\nimport * as losses from '../losses';\nimport * as Metrics from '../metrics';\nimport * as optimizers from '../optimizers';\nimport { checkUserDefinedMetadata } from '../user_defined_metadata';\nimport { count, pyListRepeat, singletonOrArray, toCamelCase, toSnakeCase, unique } from '../utils/generic_utils';\nimport { printSummary } from '../utils/layer_utils';\nimport { range } from '../utils/math_utils';\nimport { convertPythonicToTs } from '../utils/serialization_utils';\nimport { version } from '../version';\nimport { Container } from './container';\nimport { execute as _execute, FeedDict } from './executor';\nimport { evaluateDataset as _evaluateDataset, fitDataset as _fitDataset } from './training_dataset';\nimport { checkBatchSize, disposeNewTensors, ensureTensorsRank2OrHigher, makeBatches, sliceArrays, sliceArraysByIndices } from './training_tensors';\nimport { computeWeightedLoss, standardizeClassWeights, standardizeWeights } from './training_utils';\n/**\n * Helper function for polymorphic input data: 1. singleton Tensor.\n */\nexport function isDataTensor(x) {\n  return x instanceof Tensor;\n}\n/**\n * Helper function for polymorphic input data: 2. Array of Tensor.\n */\nexport function isDataArray(x) {\n  return Array.isArray(x);\n}\n/**\n * Helper function for polymorphic input data: 3. \"dict\" of Tensor.\n */\nexport function isDataDict(x) {\n  return !isDataTensor(x) && !isDataArray(x);\n}\n/**\n * Normalizes inputs and targets provided by users.\n * @param data User-provided input data (polymorphic).\n * @param names An Array of expected Tensor names.\n * @param shapes Optional Array of expected Tensor shapes.\n * @param checkBatchAxis Whether to check that the batch axis of the arrays\n *   match  the expected value found in `shapes`.\n * @param exceptionPrefix String prefix used for exception formatting.\n * @returns List of standardized input Tensors (one Tensor per model input).\n * @throws ValueError: in case of improperly formatted user data.\n */\nexport function standardizeInputData(data, names, shapes) {\n  var checkBatchAxis = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : true;\n  var exceptionPrefix = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : '';\n  if (names == null || names.length === 0) {\n    // Check for the case where the model expected no data, but some data got\n    // sent.\n    if (data != null) {\n      var gotUnexpectedData = false;\n      if (isDataArray(data) && data.length > 0) {\n        gotUnexpectedData = true;\n      } else if (isDataDict(data)) {\n        for (var key in data) {\n          if (data.hasOwnProperty(key)) {\n            gotUnexpectedData = true;\n            break;\n          }\n        }\n      } else {\n        // `data` is a singleton Tensor in this case.\n        gotUnexpectedData = true;\n      }\n      if (gotUnexpectedData) {\n        throw new ValueError(\"Error when checking model \".concat(exceptionPrefix, \" expected no data, \") + \"but got \".concat(data));\n      }\n    }\n    return [];\n  }\n  if (data == null) {\n    return names.map(function (name) {\n      return null;\n    });\n  }\n  var arrays;\n  if (isDataDict(data)) {\n    data = data;\n    arrays = [];\n    var _iterator = _createForOfIteratorHelper(names),\n      _step;\n    try {\n      for (_iterator.s(); !(_step = _iterator.n()).done;) {\n        var name = _step.value;\n        if (data[name] == null) {\n          throw new ValueError(\"No data provided for \\\"\".concat(name, \"\\\". Need data for each key in: \") + \"\".concat(names));\n        }\n        arrays.push(data[name]);\n      }\n    } catch (err) {\n      _iterator.e(err);\n    } finally {\n      _iterator.f();\n    }\n  } else if (isDataArray(data)) {\n    data = data;\n    if (data.length !== names.length) {\n      throw new ValueError(\"Error when checking model \".concat(exceptionPrefix, \": the Array of \") + \"Tensors that you are passing to your model is not the size the \" + \"model expected. Expected to see \".concat(names.length, \" Tensor(s), but \") + \"instead got the following list of Tensor(s): \".concat(data));\n    }\n    arrays = data;\n  } else {\n    data = data;\n    if (names.length > 1) {\n      throw new ValueError(\"The model \".concat(exceptionPrefix, \" expects \").concat(names.length, \" Tensor(s), \") + \"but only received one Tensor. Found: Tensor with shape \".concat(data.shape));\n    }\n    arrays = [data];\n  }\n  arrays = ensureTensorsRank2OrHigher(arrays);\n  // Check shape compatibility.\n  if (shapes != null) {\n    for (var i = 0; i < names.length; ++i) {\n      if (shapes[i] == null) {\n        continue;\n      }\n      var array = arrays[i];\n      if (array.shape.length !== shapes[i].length) {\n        throw new ValueError(\"Error when checking \".concat(exceptionPrefix, \": expected \").concat(names[i], \" \") + \"to have \".concat(shapes[i].length, \" dimension(s). but got array with \") + \"shape \".concat(array.shape));\n      }\n      for (var j = 0; j < shapes[i].length; ++j) {\n        if (j === 0 && !checkBatchAxis) {\n          // Skip the first (batch) axis.\n          continue;\n        }\n        var dim = array.shape[j];\n        var refDim = shapes[i][j];\n        if (refDim != null && refDim >= 0 && dim !== refDim) {\n          throw new ValueError(\"\".concat(exceptionPrefix, \" expected a batch of elements where each \") + \"example has shape [\".concat(shapes[i].slice(1, shapes[i].length), \"] \") + \"(i.e.,tensor shape [*,\".concat(shapes[i].slice(1, shapes[i].length), \"])\") + \" but the \".concat(exceptionPrefix, \" received an input with \").concat(array.shape[0]) + \" examples, each with shape [\".concat(array.shape.slice(1, array.shape.length), \"]\") + \" (tensor shape [\".concat(array.shape, \"])\"));\n        }\n      }\n    }\n  }\n  return arrays;\n}\n/**\n * User input validation for Tensors.\n * @param inputs `Array` of `tf.Tensor`s for inputs.\n * @param targets `Array` of `tf.Tensor`s for targets.\n * @param weights Optional `Array` of `tf.Tensor`s for sample weights.\n * @throws ValueError: in case of incorrectly formatted data.\n */\nexport function checkArrayLengths(inputs, targets, weights) {\n  var setX = unique(inputs.map(function (input) {\n    return input.shape[0];\n  }));\n  setX.sort();\n  var setY = unique(targets.map(function (target) {\n    return target.shape[0];\n  }));\n  setY.sort();\n  // TODO(cais): Check `weights` as well.\n  if (setX.length > 1) {\n    throw new ValueError(\"All input Tensors (x) should have the same number of samples. \" + \"Got array shapes: \" + \"\".concat(JSON.stringify(inputs.map(function (input) {\n      return input.shape;\n    }))));\n  }\n  if (setY.length > 1) {\n    throw new ValueError(\"All target Tensors (y) should have the same number of samples. \" + \"Got array shapes: \" + \"\".concat(JSON.stringify(targets.map(function (target) {\n      return target.shape;\n    }))));\n  }\n  if (setX.length > 0 && setY.length > 0 && !util.arraysEqual(setX, setY)) {\n    throw new ValueError(\"Input Tensors should have the same number of samples as target \" + \"Tensors. Found \".concat(setX[0], \" input sample(s) and \").concat(setY[0], \" target \") + \"sample(s).\");\n  }\n}\n/**\n * Validation on the compatibility of targes and loss functions.\n *\n * This helps prevent users from using loss functions incorrectly.\n *\n * @param targets `Array` of `tf.Tensor`s of targets.\n * @param lossFns `Array` of loss functions.\n * @param outputShapes `Array` of shapes of model outputs.\n */\nfunction checkLossAndTargetCompatibility(targets, lossFns, outputShapes) {\n  // TODO(cais): Dedicated test coverage?\n  var keyLosses = [losses.meanSquaredError, losses.binaryCrossentropy, losses.categoricalCrossentropy];\n  for (var i = 0; i < targets.length; ++i) {\n    var y = targets[i];\n    var loss = lossFns[i];\n    var shape = outputShapes[i];\n    if (loss == null) {\n      continue;\n    }\n    if (loss === losses.categoricalCrossentropy) {\n      if (y.shape[y.shape.length - 1] === 1) {\n        throw new ValueError(\"You are passing a target array of shape \".concat(y.shape, \" while using \") + \"a loss 'categorical_crossentropy'. 'categorical_crossentropy'\" + \"expects targets to be binary matrices (1s and 0s) of shape \" + \"[samples, classes].\");\n        // TODO(cais): Example code in error message.\n      }\n    }\n\n    if (keyLosses.indexOf(loss) !== -1) {\n      var slicedYShape = y.shape.slice(1);\n      var slicedShape = shape.slice(1);\n      for (var j = 0; j < slicedYShape.length; ++j) {\n        var targetDim = slicedYShape[j];\n        var outDim = slicedShape[j];\n        if (outDim != null && targetDim !== outDim) {\n          throw new ValueError(\"A target Tensor with shape \".concat(y.shape, \" was passed for an \") + \"output of shape \".concat(shape, \", while using a loss function that \") + \"expects targets to have the same shape as the output.\");\n        }\n      }\n    }\n  }\n}\n/**\n * Check inputs provided by the user.\n *\n * Porting Note: This corresponds to _standardize_input_data() in Python\n *   Keras. Because of the strong typing in TF.js, we do not need to convert\n *   the data. Specifically:\n *   1) in PyKeras, `data` can be `DataFrame` instances from pandas, for\n *      example. We don't need to worry about that here because there is no\n *      widely popular javascript/typesdcript equivalent of pandas (so far).\n *      If one becomes available in the future, we can add support.\n *   2) in PyKeras, inputs can be Python dict. But here we are stipulating\n * that the data is either a single `tf.Tensor` or an Array of `tf.Tensor`s. We\n * may add support for `Object` data inputs in the future when the need\n * arises.\n *\n * Instead, we perform basic checks for number of parameters and shapes.\n *\n * @param data: The input data.\n * @param names: Name for the inputs, from the model.\n * @param shapes: Expected shapes for the input data, from the model.\n * @param checkBatchAxis: Whether the size along the batch axis (i.e., the\n *   first dimension) will be checked for matching.\n * @param exceptionPrefix: Execption prefix message, used in generating error\n *   messages.\n * @throws ValueError: on incorrect number of inputs or mismatches in shapes.\n */\nfunction checkInputData(data, names, shapes) {\n  var checkBatchAxis = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : true;\n  var exceptionPrefix = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : '';\n  var arrays;\n  if (Array.isArray(data)) {\n    if (data.length !== names.length) {\n      throw new ValueError(\"Error when checking model \".concat(exceptionPrefix, \": the Array of \") + \"Tensors that you are passing to your model is not the size the \" + \"the model expected. Expected to see \".concat(names.length, \" Tensor(s),\") + \" but instead got \".concat(data.length, \" Tensors(s).\"));\n    }\n    arrays = data;\n  } else {\n    if (names.length > 1) {\n      throw new ValueError(\"The model expects \".concat(names.length, \" \").concat(exceptionPrefix, \" Tensors, \") + \"but only received one Tensor. Found: array with shape \" + \"\".concat(JSON.stringify(data.shape), \".\"));\n    }\n    arrays = [data];\n  }\n  if (shapes != null) {\n    for (var i = 0; i < names.length; ++i) {\n      if (shapes[i] == null) {\n        continue;\n      }\n      var array = arrays[i];\n      if (array.shape.length !== shapes[i].length) {\n        throw new ValueError(\"Error when checking \".concat(exceptionPrefix, \": expected \").concat(names[i], \" \") + \"to have \".concat(shapes[i].length, \" dimension(s), but got array with \") + \"shape \".concat(JSON.stringify(array.shape)));\n      }\n      for (var j = 0; j < shapes[i].length; ++j) {\n        if (j === 0 && !checkBatchAxis) {\n          continue;\n        }\n        var dim = array.shape[j];\n        var refDim = shapes[i][j];\n        if (refDim != null) {\n          if (refDim !== dim) {\n            throw new ValueError(\"Error when checking \".concat(exceptionPrefix, \": expected \") + \"\".concat(names[i], \" to have shape \").concat(JSON.stringify(shapes[i]), \" but \") + \"got array with shape \".concat(JSON.stringify(array.shape), \".\"));\n          }\n        }\n      }\n    }\n  }\n}\n/**\n * Maps metric functions to model outputs.\n * @param metrics An shortcut strings name, metric function, `Array` or dict\n *   (`Object`) of metric functions.\n * @param outputNames An `Array` of the names of model outputs.\n * @returns An `Array` (one entry per model output) of `Array` of metric\n *   functions. For instance, if the model has 2 outputs, and for the first\n *   output we want to compute `binaryAccuracy` and `binaryCrossentropy`,\n *   and just `binaryAccuracy` for the second output, the `Array` would look\n *   like:\n *     `[[binaryAccuracy, binaryCrossentropy],  [binaryAccuracy]]`\n * @throws TypeError: incompatible metrics format.\n */\nexport function collectMetrics(metrics, outputNames) {\n  if (metrics == null || Array.isArray(metrics) && metrics.length === 0) {\n    return outputNames.map(function (name) {\n      return [];\n    });\n  }\n  var wrappedMetrics;\n  if (typeof metrics === 'string' || typeof metrics === 'function') {\n    wrappedMetrics = [metrics];\n  } else if (Array.isArray(metrics) || typeof metrics === 'object') {\n    wrappedMetrics = metrics;\n  } else {\n    throw new TypeError('Type of metrics argument not understood. Expected an string,' + \"function, Array, or Object, found: \".concat(metrics));\n  }\n  if (Array.isArray(wrappedMetrics)) {\n    // We then apply all metrics to all outputs.\n    return outputNames.map(function (name) {\n      return wrappedMetrics;\n    });\n  } else {\n    // In this case, metrics is a dict.\n    var nestedMetrics = [];\n    var _iterator2 = _createForOfIteratorHelper(outputNames),\n      _step2;\n    try {\n      for (_iterator2.s(); !(_step2 = _iterator2.n()).done;) {\n        var name = _step2.value;\n        var outputMetrics = wrappedMetrics.hasOwnProperty(name) ? wrappedMetrics[name] : [];\n        if (!Array.isArray(outputMetrics)) {\n          outputMetrics = [outputMetrics];\n        }\n        nestedMetrics.push(outputMetrics);\n      }\n    } catch (err) {\n      _iterator2.e(err);\n    } finally {\n      _iterator2.f();\n    }\n    return nestedMetrics;\n  }\n}\nvar LAYERS_MODEL_FORMAT_NAME = 'layers-model';\n/**\n * A `tf.LayersModel` is a directed, acyclic graph of `tf.Layer`s plus methods\n * for training, evaluation, prediction and saving.\n *\n * `tf.LayersModel` is the basic unit of training, inference and evaluation in\n * TensorFlow.js. To create a `tf.LayersModel`, use `tf.LayersModel`.\n *\n * See also:\n *   `tf.Sequential`, `tf.loadLayersModel`.\n *\n * @doc {heading: 'Models', subheading: 'Classes'}\n */\nexport var LayersModel = /*#__PURE__*/function (_Container) {\n  _inherits(LayersModel, _Container);\n  var _super = _createSuper(LayersModel);\n  function LayersModel(args) {\n    var _this;\n    _classCallCheck(this, LayersModel);\n    _this = _super.call(this, args);\n    _this.isTraining = false;\n    return _this;\n  }\n  /**\n   * Print a text summary of the model's layers.\n   *\n   * The summary includes\n   * - Name and type of all layers that comprise the model.\n   * - Output shape(s) of the layers\n   * - Number of weight parameters of each layer\n   * - If the model has non-sequential-like topology, the inputs each layer\n   *   receives\n   * - The total number of trainable and non-trainable parameters of the model.\n   *\n   * ```js\n   * const input1 = tf.input({shape: [10]});\n   * const input2 = tf.input({shape: [20]});\n   * const dense1 = tf.layers.dense({units: 4}).apply(input1);\n   * const dense2 = tf.layers.dense({units: 8}).apply(input2);\n   * const concat = tf.layers.concatenate().apply([dense1, dense2]);\n   * const output =\n   *     tf.layers.dense({units: 3, activation: 'softmax'}).apply(concat);\n   *\n   * const model = tf.model({inputs: [input1, input2], outputs: output});\n   * model.summary();\n   * ```\n   *\n   * @param lineLength Custom line length, in number of characters.\n   * @param positions Custom widths of each of the columns, as either\n   *   fractions of `lineLength` (e.g., `[0.5, 0.75, 1]`) or absolute number\n   *   of characters (e.g., `[30, 50, 65]`). Each number corresponds to\n   *   right-most (i.e., ending) position of a column.\n   * @param printFn Custom print function. Can be used to replace the default\n   *   `console.log`. For example, you can use `x => {}` to mute the printed\n   *   messages in the console.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  _createClass(LayersModel, [{\n    key: \"summary\",\n    value: function summary(lineLength, positions) {\n      var printFn = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : console.log;\n      if (!this.built) {\n        throw new ValueError(\"This model has never been called, thus its weights have not been \" + \"created yet. So no summary can be displayed. Build the model \" + \"first (e.g., by calling it on some test data).\");\n      }\n      printSummary(this, lineLength, positions, printFn);\n    }\n    /**\n     * Configures and prepares the model for training and evaluation.  Compiling\n     * outfits the model with an optimizer, loss, and/or metrics.  Calling `fit`\n     * or `evaluate` on an un-compiled model will throw an error.\n     *\n     * @param args a `ModelCompileArgs` specifying the loss, optimizer, and\n     * metrics to be used for fitting and evaluating this model.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes'}\n     */\n  }, {\n    key: \"compile\",\n    value: function compile(args) {\n      var _this2 = this;\n      if (args.loss == null) {\n        args.loss = [];\n      }\n      this.loss = args.loss;\n      if (typeof args.optimizer === 'string') {\n        this.optimizer_ = optimizers.getOptimizer(args.optimizer);\n        this.isOptimizerOwned = true;\n      } else {\n        if (!(args.optimizer instanceof Optimizer)) {\n          throw new ValueError(\"User-defined optimizer must be an instance of tf.Optimizer.\");\n        }\n        this.optimizer_ = args.optimizer;\n        this.isOptimizerOwned = false;\n      }\n      // TODO(cais): Add lossWeights.\n      // TODO(cais): Add sampleWeightMode.\n      // Prepare loss functions.\n      var lossFunctions = [];\n      if (!Array.isArray(args.loss) && typeof args.loss !== 'string' && typeof args.loss !== 'function') {\n        args.loss = args.loss;\n        for (var name in args.loss) {\n          if (this.outputNames.indexOf(name) === -1) {\n            throw new ValueError(\"Unknown entry in loss dictionary: \\\"\".concat(name, \"\\\". \") + \"Only expected the following keys: \".concat(this.outputNames));\n          }\n        }\n        var _iterator3 = _createForOfIteratorHelper(this.outputNames),\n          _step3;\n        try {\n          for (_iterator3.s(); !(_step3 = _iterator3.n()).done;) {\n            var _name = _step3.value;\n            if (args.loss[_name] == null) {\n              console.warn(\"Output \\\"\".concat(_name, \"\\\" is missing from loss dictionary. We assume \") + \"this was done on purpose, and we will not be expecting data \" + \"to be passed to \".concat(_name, \" during training\"));\n            }\n            lossFunctions.push(losses.get(args.loss[_name]));\n          }\n        } catch (err) {\n          _iterator3.e(err);\n        } finally {\n          _iterator3.f();\n        }\n      } else if (Array.isArray(args.loss)) {\n        if (args.loss.length !== this.outputs.length) {\n          throw new ValueError(\"When passing an Array as loss, it should have one entry per \" + \"model output. The model has \".concat(this.outputs.length, \" output(s), \") + \"but you passed loss=\".concat(args.loss, \".\"));\n        }\n        var theLosses = args.loss;\n        lossFunctions = theLosses.map(function (l) {\n          return losses.get(l);\n        });\n      } else {\n        var lossFunction = losses.get(args.loss);\n        this.outputs.forEach(function (_) {\n          lossFunctions.push(lossFunction);\n        });\n      }\n      this.lossFunctions = lossFunctions;\n      this.feedOutputNames = [];\n      this.feedOutputShapes = [];\n      this.feedLossFns = [];\n      for (var i = 0; i < this.outputs.length; ++i) {\n        // TODO(cais): Logic for skipping target(s).\n        var shape = this.internalOutputShapes[i];\n        var _name2 = this.outputNames[i];\n        this.feedOutputNames.push(_name2);\n        this.feedOutputShapes.push(shape);\n        this.feedLossFns.push(this.lossFunctions[i]);\n      }\n      // TODO(cais): Add logic for output masks.\n      // TODO(cais): Add logic for sample weights.\n      var skipTargetIndices = [];\n      // Prepare metrics.\n      this.metrics = args.metrics;\n      // TODO(cais): Add weightedMetrics.\n      this.metricsNames = ['loss'];\n      this.metricsTensors = [];\n      // Compute total loss.\n      // Porting Note: In PyKeras, metrics_tensors are symbolic tensor objects.\n      //   Here, metricsTensors are TypeScript functions. This difference is due\n      //   to the difference in symbolic/imperative property of the backends.\n      nameScope('loss', function () {\n        for (var _i = 0; _i < _this2.outputs.length; ++_i) {\n          if (skipTargetIndices.indexOf(_i) !== -1) {\n            continue;\n          }\n          // TODO(cais): Add weightedLoss, sampleWeight and mask.\n          //   The following line should be weightedLoss\n          var weightedLoss = _this2.lossFunctions[_i];\n          if (_this2.outputs.length > 1) {\n            _this2.metricsTensors.push([weightedLoss, _i]);\n            _this2.metricsNames.push(_this2.outputNames[_i] + '_loss');\n          }\n        }\n        // Porting Note: Due to the imperative nature of the backend, we calculate\n        //   the regularizer penalties in the totalLossFunction, instead of here.\n      });\n\n      var nestedMetrics = collectMetrics(args.metrics, this.outputNames);\n      // TODO(cais): Add nestedWeightedMetrics.\n      /**\n       * Helper function used in loop below.\n       */\n      var appendMetric = function appendMetric(outputIndex, metricName, metricTensor) {\n        if (_this2.outputNames.length > 1) {\n          metricName = _this2.outputNames[outputIndex] + '_' + metricName;\n        }\n        _this2.metricsNames.push(metricName);\n        _this2.metricsTensors.push([metricTensor, outputIndex]);\n      };\n      nameScope('metric', function () {\n        var _loop = function _loop(_i2) {\n          if (skipTargetIndices.indexOf(_i2) !== -1) {\n            return \"continue\";\n          }\n          var outputMetrics = nestedMetrics[_i2];\n          // TODO(cais): Add weights and outputWeightedMetrics.\n          // TODO(cais): Add optional arg `weights` to the following function.\n          var handleMetrics = function handleMetrics(metrics) {\n            var metricNamePrefix = '';\n            var metricName;\n            var accFn;\n            var weightedMetricFn;\n            //  TODO(cais): Use 'weights_' for weighted metrics.\n            var _iterator4 = _createForOfIteratorHelper(metrics),\n              _step4;\n            try {\n              var _loop2 = function _loop2() {\n                var metric = _step4.value;\n                if (typeof metric === 'string' && ['accuracy', 'acc', 'crossentropy', 'ce'].indexOf(metric) !== -1) {\n                  var outputShape = _this2.internalOutputShapes[_i2];\n                  if (outputShape[outputShape.length - 1] === 1 || _this2.lossFunctions[_i2] === losses.binaryCrossentropy) {\n                    // case: binary accuracy/crossentropy.\n                    if (['accuracy', 'acc'].indexOf(metric) !== -1) {\n                      accFn = Metrics.binaryAccuracy;\n                    } else if (['crossentropy', 'ce'].indexOf(metric) !== -1) {\n                      accFn = Metrics.binaryCrossentropy;\n                    }\n                  } else if (_this2.lossFunctions[_i2] === losses.sparseCategoricalCrossentropy) {\n                    // case: categorical accuracy / crossentropy with sparse\n                    // targets.\n                    if (['accuracy', 'acc'].indexOf(metric) !== -1) {\n                      accFn = Metrics.sparseCategoricalAccuracy;\n                    } else if (['crossentropy', 'ce'].indexOf(metric) !== -1) {\n                      accFn = Metrics.sparseCategoricalCrossentropy;\n                    }\n                  } else {\n                    // case: categorical accuracy / crossentropy.\n                    if (['accuracy', 'acc'].indexOf(metric) !== -1) {\n                      accFn = Metrics.categoricalAccuracy;\n                    } else if (['crossentropy', 'ce'].indexOf(metric) !== -1) {\n                      accFn = Metrics.categoricalCrossentropy;\n                    }\n                  }\n                  var suffix;\n                  if (['accuracy', 'acc'].indexOf(metric) !== -1) {\n                    suffix = 'acc';\n                  } else if (['crossentropy', 'ce'].indexOf(metric) !== -1) {\n                    suffix = 'ce';\n                  }\n                  // TODO(cais): Add weighting actually.\n                  weightedMetricFn = accFn;\n                  metricName = metricNamePrefix + suffix;\n                } else {\n                  var metricFn = Metrics.get(metric);\n                  // TODO(cais): Add weighting actually.\n                  weightedMetricFn = metricFn;\n                  metricName = metricNamePrefix + Metrics.getLossOrMetricName(metric);\n                }\n                // TODO(cais): Add weighting and masking to metricResult.\n                var metricResult;\n                nameScope(metricName, function () {\n                  metricResult = weightedMetricFn;\n                });\n                appendMetric(_i2, metricName, metricResult);\n              };\n              for (_iterator4.s(); !(_step4 = _iterator4.n()).done;) {\n                _loop2();\n              }\n            } catch (err) {\n              _iterator4.e(err);\n            } finally {\n              _iterator4.f();\n            }\n          };\n          handleMetrics(outputMetrics);\n          // TODO(cais): Call handleMetrics with weights.\n        };\n        for (var _i2 = 0; _i2 < _this2.outputs.length; ++_i2) {\n          var _ret = _loop(_i2);\n          if (_ret === \"continue\") continue;\n        }\n      });\n      // Porting Notes: Given the imperative backend of tfjs-core,\n      //   there is no need for constructing the symbolic graph and placeholders.\n      this.collectedTrainableWeights = this.trainableWeights;\n    }\n    /**\n     * Check trainable weights count consistency.\n     *\n     * This will raise a warning if `this.trainableWeights` and\n     * `this.collectedTrainableWeights` are inconsistent (i.e., have different\n     * numbers of parameters).\n     * Inconsistency will typically arise when one modifies `model.trainable`\n     * without calling `model.compile()` again.\n     */\n  }, {\n    key: \"checkTrainableWeightsConsistency\",\n    value: function checkTrainableWeightsConsistency() {\n      if (this.collectedTrainableWeights == null) {\n        return;\n      }\n      if (this.trainableWeights.length !== this.collectedTrainableWeights.length) {\n        console.warn('Discrepancy between trainableweights and collected trainable ' + 'weights. Did you set `model.trainable` without calling ' + '`model.compile()` afterwards?');\n      }\n    }\n    /**\n     * Returns the loss value & metrics values for the model in test mode.\n     *\n     * Loss and metrics are specified during `compile()`, which needs to happen\n     * before calls to `evaluate()`.\n     *\n     * Computation is done in batches.\n     *\n     * ```js\n     * const model = tf.sequential({\n     *   layers: [tf.layers.dense({units: 1, inputShape: [10]})]\n     * });\n     * model.compile({optimizer: 'sgd', loss: 'meanSquaredError'});\n     * const result = model.evaluate(\n     *     tf.ones([8, 10]), tf.ones([8, 1]), {batchSize: 4});\n     * result.print();\n     * ```\n     *\n     * @param x `tf.Tensor` of test data, or an `Array` of `tf.Tensor`s if the\n     * model has multiple inputs.\n     * @param y `tf.Tensor` of target data, or an `Array` of `tf.Tensor`s if the\n     * model has multiple outputs.\n     * @param args A `ModelEvaluateArgs`, containing optional fields.\n     *\n     * @return `Scalar` test loss (if the model has a single output and no\n     *   metrics) or `Array` of `Scalar`s (if the model has multiple outputs\n     *   and/or metrics). The attribute `model.metricsNames`\n     *   will give you the display labels for the scalar outputs.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes'}\n     */\n  }, {\n    key: \"evaluate\",\n    value: function evaluate(x, y) {\n      var args = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n      var batchSize = args.batchSize == null ? 32 : args.batchSize;\n      checkBatchSize(batchSize);\n      // TODO(cais): Standardize `config.sampleWeights` as well.\n      // Validate user data.\n      var checkBatchAxis = true;\n      var standardizedOuts = this.standardizeUserDataXY(x, y, checkBatchAxis, batchSize);\n      try {\n        // TODO(cais): If uses `useLearningPhase`, set the corresponding element\n        // of the input to 0.\n        var ins = standardizedOuts[0].concat(standardizedOuts[1]);\n        this.makeTestFunction();\n        var f = this.testFunction;\n        var testOuts = this.testLoop(f, ins, batchSize, args.verbose, args.steps);\n        return singletonOrArray(testOuts);\n      } finally {\n        disposeNewTensors(standardizedOuts[0], x);\n        disposeNewTensors(standardizedOuts[1], y);\n      }\n    }\n    // TODO(cais): Add code snippet below once real dataset objects are\n    //   available.\n    /**\n     * Evaluate model using a dataset object.\n     *\n     * Note: Unlike `evaluate()`, this method is asynchronous (`async`).\n     *\n     * @param dataset A dataset object. Its `iterator()` method is expected\n     *   to generate a dataset iterator object, the `next()` method of which\n     *   is expected to produce data batches for evaluation. The return value\n     *   of the `next()` call ought to contain a boolean `done` field and a\n     *   `value` field. The `value` field is expected to be an array of two\n     *   `tf.Tensor`s or an array of two nested `tf.Tensor` structures. The former\n     *   case is for models with exactly one input and one output (e.g.\n     *   a sequential model). The latter case is for models with multiple\n     *   inputs and/or multiple outputs. Of the two items in the array, the\n     *   first is the input feature(s) and the second is the output target(s).\n     * @param args A configuration object for the dataset-based evaluation.\n     * @returns Loss and metric values as an Array of `Scalar` objects.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes'}\n     */\n  }, {\n    key: \"evaluateDataset\",\n    value: function () {\n      var _evaluateDataset2 = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee(dataset, args) {\n        return _regeneratorRuntime().wrap(function _callee$(_context) {\n          while (1) switch (_context.prev = _context.next) {\n            case 0:\n              this.makeTestFunction();\n              return _context.abrupt(\"return\", _evaluateDataset(this, dataset, args));\n            case 2:\n            case \"end\":\n              return _context.stop();\n          }\n        }, _callee, this);\n      }));\n      function evaluateDataset(_x, _x2) {\n        return _evaluateDataset2.apply(this, arguments);\n      }\n      return evaluateDataset;\n    }()\n    /**\n     * Get number of samples provided for training, evaluation or prediction.\n     *\n     * @param ins Input `tf.Tensor`.\n     * @param batchSize Integer batch size, optional.\n     * @param steps Total number of steps (batches of samples) before\n     * declaring loop finished. Optional.\n     * @param stepsName The public API's parameter name for `steps`.\n     * @returns Number of samples provided.\n     */\n  }, {\n    key: \"checkNumSamples\",\n    value: function checkNumSamples(ins, batchSize, steps) {\n      var stepsName = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : 'steps';\n      var numSamples;\n      if (steps != null) {\n        numSamples = null;\n        if (batchSize != null) {\n          throw new ValueError(\"If \".concat(stepsName, \" is set, batchSize must be null or undefined.\") + \"Got batchSize = \".concat(batchSize));\n        }\n      } else if (ins != null) {\n        if (Array.isArray(ins)) {\n          numSamples = ins[0].shape[0];\n        } else {\n          numSamples = ins.shape[0];\n        }\n      } else {\n        throw new ValueError(\"Either the input data should have a defined shape, or \" + \"\".concat(stepsName, \" shoud be specified.\"));\n      }\n      return numSamples;\n    }\n    /**\n     * Execute internal tensors of the model with input data feed.\n     * @param inputs Input data feed. Must match the inputs of the model.\n     * @param outputs Names of the output tensors to be fetched. Must match\n     *   names of the SymbolicTensors that belong to the graph.\n     * @returns Fetched values for `outputs`.\n     */\n  }, {\n    key: \"execute\",\n    value: function execute(inputs, outputs) {\n      if (Array.isArray(outputs) && outputs.length === 0) {\n        throw new ValueError('`outputs` is an empty Array, which is not allowed.');\n      }\n      var outputsIsArray = Array.isArray(outputs);\n      var outputNames = outputsIsArray ? outputs : [outputs];\n      var outputSymbolicTensors = this.retrieveSymbolicTensors(outputNames);\n      // Format the input into a FeedDict.\n      var feedDict = new FeedDict();\n      if (inputs instanceof Tensor) {\n        inputs = [inputs];\n      }\n      if (Array.isArray(inputs)) {\n        if (inputs.length !== this.inputs.length) {\n          throw new ValueError(\"The number of inputs provided (\".concat(inputs.length, \") \") + \"does not match the number of inputs of this model \" + \"(\".concat(this.inputs.length, \").\"));\n        }\n        for (var i = 0; i < this.inputs.length; ++i) {\n          feedDict.add(this.inputs[i], inputs[i]);\n        }\n      } else {\n        var _iterator5 = _createForOfIteratorHelper(this.inputs),\n          _step5;\n        try {\n          for (_iterator5.s(); !(_step5 = _iterator5.n()).done;) {\n            var input = _step5.value;\n            var tensorValue = inputs[input.name];\n            if (tensorValue == null) {\n              throw new ValueError(\"No value is provided for the model's input \".concat(input.name));\n            }\n            feedDict.add(input, tensorValue);\n          }\n        } catch (err) {\n          _iterator5.e(err);\n        } finally {\n          _iterator5.f();\n        }\n      }\n      // Run execution.\n      var executeOutputs = _execute(outputSymbolicTensors, feedDict);\n      return outputsIsArray ? executeOutputs : executeOutputs[0];\n    }\n    /**\n     * Retrieve the model's internal symbolic tensors from symbolic-tensor names.\n     */\n  }, {\n    key: \"retrieveSymbolicTensors\",\n    value: function retrieveSymbolicTensors(symbolicTensorNames) {\n      var outputSymbolicTensors = pyListRepeat(null, symbolicTensorNames.length);\n      var outputsRemaining = symbolicTensorNames.length;\n      var _iterator6 = _createForOfIteratorHelper(this.layers),\n        _step6;\n      try {\n        for (_iterator6.s(); !(_step6 = _iterator6.n()).done;) {\n          var layer = _step6.value;\n          var layerOutputs = Array.isArray(layer.output) ? layer.output : [layer.output];\n          var layerOutputNames = layerOutputs.map(function (output) {\n            return output.name;\n          });\n          for (var i = 0; i < symbolicTensorNames.length; ++i) {\n            var index = layerOutputNames.indexOf(symbolicTensorNames[i]);\n            if (index !== -1) {\n              outputSymbolicTensors[i] = layerOutputs[index];\n              outputsRemaining--;\n            }\n            if (outputsRemaining === 0) {\n              break;\n            }\n          }\n          if (outputsRemaining === 0) {\n            break;\n          }\n        }\n      } catch (err) {\n        _iterator6.e(err);\n      } finally {\n        _iterator6.f();\n      }\n      if (outputsRemaining > 0) {\n        var remainingNames = [];\n        outputSymbolicTensors.forEach(function (tensor, i) {\n          if (tensor == null) {\n            remainingNames.push(symbolicTensorNames[i]);\n          }\n        });\n        throw new ValueError(\"Cannot find SymbolicTensors for output name(s): \" + \"\".concat(JSON.stringify(remainingNames)));\n      }\n      return outputSymbolicTensors;\n    }\n    /**\n     * Helper method to loop over some data in batches.\n     *\n     * Porting Note: Not using the functional approach in the Python equivalent\n     *   due to the imperative backend.\n     * Porting Note: Does not support step mode currently.\n     *\n     * @param ins: input data\n     * @param batchSize: integer batch size.\n     * @param verbose: verbosity model\n     * @returns: Predictions as `tf.Tensor` (if a single output) or an `Array` of\n     *   `tf.Tensor` (if multipe outputs).\n     */\n  }, {\n    key: \"predictLoop\",\n    value: function predictLoop(ins) {\n      var _this3 = this;\n      var batchSize = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 32;\n      var verbose = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : false;\n      return tfc.tidy(function () {\n        var numSamples = _this3.checkNumSamples(ins);\n        if (verbose) {\n          throw new NotImplementedError('Verbose predictLoop() is not implemented yet.');\n        }\n        // Sample-based predictions.\n        // Porting Note: Tensor currently does not support sliced assignments as\n        //   in numpy, e.g., x[1:3] = y. Therefore we use concatenation while\n        //   iterating over the batches.\n        var batches = makeBatches(numSamples, batchSize);\n        var outsBatches = _this3.outputs.map(function (output) {\n          return [];\n        });\n        // TODO(cais): Can the scope() be pushed down inside the for loop?\n        var _loop3 = function _loop3(batchIndex) {\n          var batchOuts = tfc.tidy(function () {\n            var batchStart = batches[batchIndex][0];\n            var batchEnd = batches[batchIndex][1];\n            // TODO(cais): Take care of the case of the last element is a flag for\n            //   training/test.\n            var insBatch = sliceArrays(ins, batchStart, batchEnd);\n            // Construct the feeds for execute();\n            var feeds = [];\n            if (Array.isArray(insBatch)) {\n              for (var i = 0; i < insBatch.length; ++i) {\n                feeds.push({\n                  key: _this3.inputs[i],\n                  value: insBatch[i]\n                });\n              }\n            } else {\n              feeds.push({\n                key: _this3.inputs[0],\n                value: insBatch\n              });\n            }\n            var feedDict = new FeedDict(feeds);\n            return _execute(_this3.outputs, feedDict);\n          });\n          batchOuts.forEach(function (batchOut, i) {\n            return outsBatches[i].push(batchOut);\n          });\n        };\n        for (var batchIndex = 0; batchIndex < batches.length; ++batchIndex) {\n          _loop3(batchIndex);\n        }\n        return singletonOrArray(outsBatches.map(function (batches) {\n          return tfc.concat(batches, 0);\n        }));\n      });\n    }\n    /**\n     * Generates output predictions for the input samples.\n     *\n     * Computation is done in batches.\n     *\n     * Note: the \"step\" mode of predict() is currently not supported.\n     *   This is because the TensorFlow.js core backend is imperative only.\n     *\n     * ```js\n     * const model = tf.sequential({\n     *   layers: [tf.layers.dense({units: 1, inputShape: [10]})]\n     * });\n     * model.predict(tf.ones([8, 10]), {batchSize: 4}).print();\n     * ```\n     *\n     * @param x The input data, as a Tensor, or an `Array` of `tf.Tensor`s if\n     *   the model has multiple inputs.\n     * @param args A `ModelPredictArgs` object containing optional fields.\n     *\n     * @return Prediction results as a `tf.Tensor`(s).\n     *\n     * @exception ValueError In case of mismatch between the provided input data\n     *   and the model's expectations, or in case a stateful model receives a\n     *   number of samples that is not a multiple of the batch size.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes'}\n     */\n  }, {\n    key: \"predict\",\n    value: function predict(x) {\n      var args = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n      var xsRank2OrHigher = ensureTensorsRank2OrHigher(x);\n      checkInputData(xsRank2OrHigher, this.inputNames, this.feedInputShapes, false);\n      try {\n        // TODO(cais): Take care of stateful models.\n        //   if (this.stateful) ...\n        // TODO(cais): Take care of the learning_phase boolean flag.\n        //   if (this.useLearningPhase) ...\n        var batchSize = args.batchSize == null ? 32 : args.batchSize;\n        checkBatchSize(batchSize);\n        return this.predictLoop(xsRank2OrHigher, batchSize);\n      } finally {\n        disposeNewTensors(xsRank2OrHigher, x);\n      }\n    }\n    /**\n     * Returns predictions for a single batch of samples.\n     *\n     * ```js\n     * const model = tf.sequential({\n     *   layers: [tf.layers.dense({units: 1, inputShape: [10]})]\n     * });\n     * model.predictOnBatch(tf.ones([8, 10])).print();\n     * ```\n     * @param x: Input samples, as a Tensor (for models with exactly one\n     *   input) or an array of Tensors (for models with more than one input).\n     * @return Tensor(s) of predictions\n     *\n     * @doc {heading: 'Models', subheading: 'Classes'}\n     */\n  }, {\n    key: \"predictOnBatch\",\n    value: function predictOnBatch(x) {\n      checkInputData(x, this.inputNames, this.feedInputShapes, true);\n      // TODO(cais): Take care of the learning_phase boolean flag.\n      //   if (this.useLearningPhase) ...\n      var batchSize = (Array.isArray(x) ? x[0] : x).shape[0];\n      return this.predictLoop(x, batchSize);\n    }\n  }, {\n    key: \"standardizeUserDataXY\",\n    value: function standardizeUserDataXY(x, y) {\n      var checkBatchAxis = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : true;\n      var batchSize = arguments.length > 3 ? arguments[3] : undefined;\n      // TODO(cais): Add sampleWeight, classWeight\n      if (this.optimizer_ == null) {\n        throw new RuntimeError('You must compile a model before training/testing. Use ' + 'LayersModel.compile(modelCompileArgs).');\n      }\n      var outputShapes = [];\n      for (var i = 0; i < this.feedOutputShapes.length; ++i) {\n        var outputShape = this.feedOutputShapes[i];\n        var lossFn = this.feedLossFns[i];\n        if (lossFn === losses.sparseCategoricalCrossentropy) {\n          outputShapes.push(outputShape.slice(0, outputShape.length - 1).concat([1]));\n        } else {\n          // Porting Note: Because of strong typing `lossFn` must be a function.\n          outputShapes.push(outputShape);\n        }\n      }\n      x = standardizeInputData(x, this.feedInputNames, this.feedInputShapes, false, 'input');\n      y = standardizeInputData(y, this.feedOutputNames, outputShapes, false, 'target');\n      // TODO(cais): Standardize sampleWeights & classWeights.\n      checkArrayLengths(x, y, null);\n      // TODO(cais): Check sampleWeights as well.\n      checkLossAndTargetCompatibility(y, this.feedLossFns, this.feedOutputShapes);\n      if (this.stateful && batchSize != null && batchSize > 0) {\n        if (x[0].shape[0] % batchSize !== 0) {\n          throw new ValueError(\"In a stateful network, you should only pass inputs with a \" + \"number of samples that is divisible by the batch size \" + \"\".concat(batchSize, \". Found: \").concat(x[0].shape[0], \" sample(s).\"));\n        }\n      }\n      return [x, y];\n    }\n  }, {\n    key: \"standardizeUserData\",\n    value: function () {\n      var _standardizeUserData = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee2(x, y, sampleWeight, classWeight) {\n        var checkBatchAxis,\n          batchSize,\n          _this$standardizeUser,\n          _this$standardizeUser2,\n          standardXs,\n          standardYs,\n          standardSampleWeights,\n          classWeights,\n          i,\n          _args2 = arguments;\n        return _regeneratorRuntime().wrap(function _callee2$(_context2) {\n          while (1) switch (_context2.prev = _context2.next) {\n            case 0:\n              checkBatchAxis = _args2.length > 4 && _args2[4] !== undefined ? _args2[4] : true;\n              batchSize = _args2.length > 5 ? _args2[5] : undefined;\n              _this$standardizeUser = this.standardizeUserDataXY(x, y, checkBatchAxis, batchSize), _this$standardizeUser2 = _slicedToArray(_this$standardizeUser, 2), standardXs = _this$standardizeUser2[0], standardYs = _this$standardizeUser2[1]; // TODO(cais): Handle sampleWeights.\n              if (!(sampleWeight != null)) {\n                _context2.next = 5;\n                break;\n              }\n              throw new Error('sample weight is not supported yet.');\n            case 5:\n              standardSampleWeights = null;\n              if (!(classWeight != null)) {\n                _context2.next = 19;\n                break;\n              }\n              classWeights = standardizeClassWeights(classWeight, this.outputNames);\n              standardSampleWeights = [];\n              i = 0;\n            case 10:\n              if (!(i < classWeights.length)) {\n                _context2.next = 19;\n                break;\n              }\n              _context2.t0 = standardSampleWeights;\n              _context2.next = 14;\n              return standardizeWeights(standardYs[i], null, classWeights[i]);\n            case 14:\n              _context2.t1 = _context2.sent;\n              _context2.t0.push.call(_context2.t0, _context2.t1);\n            case 16:\n              ++i;\n              _context2.next = 10;\n              break;\n            case 19:\n              return _context2.abrupt(\"return\", [standardXs, standardYs, standardSampleWeights]);\n            case 20:\n            case \"end\":\n              return _context2.stop();\n          }\n        }, _callee2, this);\n      }));\n      function standardizeUserData(_x3, _x4, _x5, _x6) {\n        return _standardizeUserData.apply(this, arguments);\n      }\n      return standardizeUserData;\n    }()\n    /**\n     * Loop over some test data in batches.\n     * @param f A Function returning a list of tensors.\n     * @param ins Array of tensors to be fed to `f`.\n     * @param batchSize Integer batch size or `null` / `undefined`.\n     * @param verbose verbosity mode.\n     * @param steps Total number of steps (batches of samples) before\n     * declaring test finished. Ignored with the default value of `null` /\n     * `undefined`.\n     * @returns Array of Scalars.\n     */\n  }, {\n    key: \"testLoop\",\n    value: function testLoop(f, ins, batchSize) {\n      var _this4 = this;\n      var verbose = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : 0;\n      var steps = arguments.length > 4 ? arguments[4] : undefined;\n      return tfc.tidy(function () {\n        var numSamples = _this4.checkNumSamples(ins, batchSize, steps, 'steps');\n        var outs = [];\n        if (verbose > 0) {\n          throw new NotImplementedError('Verbose mode is not implemented yet.');\n        }\n        // TODO(cais): Use `indicesForConversionToDense' to prevent slow down.\n        if (steps != null) {\n          throw new NotImplementedError('steps mode in testLoop() is not implemented yet');\n        } else {\n          var batches = makeBatches(numSamples, batchSize);\n          var indexArray = tensor1d(range(0, numSamples));\n          for (var batchIndex = 0; batchIndex < batches.length; ++batchIndex) {\n            var batchStart = batches[batchIndex][0];\n            var batchEnd = batches[batchIndex][1];\n            var batchIds = K.sliceAlongFirstAxis(indexArray, batchStart, batchEnd - batchStart);\n            // TODO(cais): In ins, train flag can be a number, instead of an\n            //   Tensor? Do we need to handle this in tfjs-layers?\n            var insBatch = sliceArraysByIndices(ins, batchIds);\n            var batchOuts = f(insBatch);\n            if (batchIndex === 0) {\n              for (var i = 0; i < batchOuts.length; ++i) {\n                outs.push(scalar(0));\n              }\n            }\n            for (var _i3 = 0; _i3 < batchOuts.length; ++_i3) {\n              var batchOut = batchOuts[_i3];\n              outs[_i3] = tfc.add(outs[_i3], tfc.mul(batchEnd - batchStart, batchOut));\n            }\n          }\n          for (var _i4 = 0; _i4 < outs.length; ++_i4) {\n            outs[_i4] = tfc.div(outs[_i4], numSamples);\n          }\n        }\n        return outs;\n      });\n    }\n  }, {\n    key: \"getDedupedMetricsNames\",\n    value: function getDedupedMetricsNames() {\n      var outLabels = this.metricsNames;\n      // Rename duplicated metrics names (can happen with an output layer\n      // shared among multiple dataflows).\n      var dedupedOutLabels = [];\n      for (var i = 0; i < outLabels.length; ++i) {\n        var label = outLabels[i];\n        var newLabel = label;\n        if (count(outLabels, label) > 1) {\n          var dupIndex = count(outLabels.slice(0, i), label);\n          newLabel += \"_\".concat(dupIndex);\n        }\n        dedupedOutLabels.push(newLabel);\n      }\n      return dedupedOutLabels;\n    }\n    /**\n     * Creates a function that performs the following actions:\n     *\n     * 1. computes the losses\n     * 2. sums them to get the total loss\n     * 3. call the optimizer computes the gradients of the LayersModel's\n     *    trainable weights w.r.t. the total loss and update the variables\n     * 4. calculates the metrics\n     * 5. returns the values of the losses and metrics.\n     */\n  }, {\n    key: \"makeTrainFunction\",\n    value: function makeTrainFunction() {\n      var _this5 = this;\n      return function (data) {\n        var lossValues = [];\n        var inputs = data.slice(0, _this5.inputs.length);\n        var targets = data.slice(_this5.inputs.length, _this5.inputs.length + _this5.outputs.length);\n        var sampleWeights = data.slice(_this5.inputs.length + _this5.outputs.length, _this5.inputs.length + _this5.outputs.length * 2);\n        var metricsValues = [];\n        // Create a function that computes the total loss based on the\n        // inputs. This function is used for obtaining gradients through\n        // backprop.\n        var totalLossFunction = function totalLossFunction() {\n          var feeds = [];\n          for (var i = 0; i < _this5.inputs.length; ++i) {\n            feeds.push({\n              key: _this5.inputs[i],\n              value: inputs[i]\n            });\n          }\n          var feedDict = new FeedDict(feeds);\n          var outputs = _execute(_this5.outputs, feedDict, {\n            'training': true\n          });\n          // TODO(cais): Take care of the case of multiple outputs from a\n          //   single layer?\n          var totalLoss;\n          for (var _i5 = 0; _i5 < _this5.lossFunctions.length; ++_i5) {\n            var lossFunction = _this5.lossFunctions[_i5];\n            var loss = lossFunction(targets[_i5], outputs[_i5]);\n            if (sampleWeights[_i5] != null) {\n              loss = computeWeightedLoss(loss, sampleWeights[_i5]);\n            }\n            // TODO(cais): push Scalar instead.\n            var meanLoss = tfc.mean(loss);\n            // TODO(cais): Use a scope() instead, to avoid ownership.\n            lossValues.push(meanLoss);\n            if (_i5 === 0) {\n              totalLoss = loss;\n            } else {\n              totalLoss = tfc.add(totalLoss, loss);\n            }\n          }\n          // Compute the metrics.\n          // TODO(cais): These should probably be calculated outside\n          //   totalLossFunction to benefit speed?\n          for (var _i6 = 0; _i6 < _this5.metricsTensors.length; ++_i6) {\n            var weightedMetric = void 0;\n            if (_this5.outputs.length > 1 && _i6 < _this5.outputs.length) {\n              weightedMetric = lossValues[_i6];\n            } else {\n              var metric = _this5.metricsTensors[_i6][0];\n              var outputIndex = _this5.metricsTensors[_i6][1];\n              weightedMetric = tfc.mean(metric(targets[outputIndex], outputs[outputIndex]));\n            }\n            tfc.keep(weightedMetric);\n            // TODO(cais): Use a scope() instead, to avoid ownership.\n            metricsValues.push(weightedMetric);\n          }\n          totalLoss = tfc.mean(totalLoss);\n          // Add regularizer penalties.\n          _this5.calculateLosses().forEach(function (regularizerLoss) {\n            totalLoss = tfc.add(totalLoss, regularizerLoss);\n          });\n          return totalLoss;\n        };\n        var variables = _this5.collectedTrainableWeights.map(function (param) {\n          return param.read();\n        });\n        var returnCost = true;\n        var totalLossValue = _this5.optimizer_.minimize(totalLossFunction, returnCost, variables);\n        return [totalLossValue].concat(metricsValues);\n      };\n    }\n    /**\n     * Create a function which, when invoked with an array of `tf.Tensor`s as a\n     * batch of inputs, returns the prespecified loss and metrics of the model\n     * under the batch of input data.\n     */\n  }, {\n    key: \"makeTestFunction\",\n    value: function makeTestFunction() {\n      var _this6 = this;\n      this.testFunction = function (data) {\n        return tfc.tidy(function () {\n          var valOutputs = [];\n          var totalLoss;\n          var inputs = data.slice(0, _this6.inputs.length);\n          var targets = data.slice(_this6.inputs.length, _this6.inputs.length + _this6.outputs.length);\n          var feeds = [];\n          for (var i = 0; i < _this6.inputs.length; ++i) {\n            feeds.push({\n              key: _this6.inputs[i],\n              value: inputs[i]\n            });\n          }\n          var feedDict = new FeedDict(feeds);\n          var outputs = _execute(_this6.outputs, feedDict);\n          // Compute total loss.\n          for (var _i7 = 0; _i7 < _this6.lossFunctions.length; ++_i7) {\n            var lossFunction = _this6.lossFunctions[_i7];\n            // TODO(cais): Add sample weighting and replace the simple\n            // averaging.\n            var loss = tfc.mean(lossFunction(targets[_i7], outputs[_i7]));\n            if (_i7 === 0) {\n              totalLoss = loss;\n            } else {\n              totalLoss = tfc.add(totalLoss, loss);\n            }\n            valOutputs.push(totalLoss);\n          }\n          // Compute the metrics.\n          for (var _i8 = 0; _i8 < _this6.metricsTensors.length; ++_i8) {\n            var metric = _this6.metricsTensors[_i8][0];\n            var outputIndex = _this6.metricsTensors[_i8][1];\n            // TODO(cais): Replace K.mean() with a proper weighting function.\n            var meanMetric = tfc.mean(metric(targets[outputIndex], outputs[outputIndex]));\n            valOutputs.push(meanMetric);\n          }\n          return valOutputs;\n        });\n      };\n    }\n    /**\n     * Trains the model for a fixed number of epochs (iterations on a\n     * dataset).\n     *\n     * ```js\n     * const model = tf.sequential({\n     *     layers: [tf.layers.dense({units: 1, inputShape: [10]})]\n     * });\n     * model.compile({optimizer: 'sgd', loss: 'meanSquaredError'});\n     * for (let i = 1; i < 5 ; ++i) {\n     *   const h = await model.fit(tf.ones([8, 10]), tf.ones([8, 1]), {\n     *       batchSize: 4,\n     *       epochs: 3\n     *   });\n     *   console.log(\"Loss after Epoch \" + i + \" : \" + h.history.loss[0]);\n     * }\n     * ```\n     *\n     * @param x `tf.Tensor` of training data, or an array of `tf.Tensor`s if the\n     * model has multiple inputs. If all inputs in the model are named, you\n     * can also pass a dictionary mapping input names to `tf.Tensor`s.\n     * @param y `tf.Tensor` of target (label) data, or an array of `tf.Tensor`s if\n     * the model has multiple outputs. If all outputs in the model are named,\n     * you can also pass a dictionary mapping output names to `tf.Tensor`s.\n     * @param args A `ModelFitArgs`, containing optional fields.\n     *\n     * @return A `History` instance. Its `history` attribute contains all\n     *   information collected during training.\n     *\n     * @exception ValueError In case of mismatch between the provided input\n     * data and what the model expects.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes'}\n     */\n  }, {\n    key: \"fit\",\n    value: function () {\n      var _fit = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee3(x, y) {\n        var args,\n          inputs,\n          targets,\n          originalInputs,\n          originalTargets,\n          inputValX,\n          inputValY,\n          valX,\n          valY,\n          sampleWeights,\n          batchSize,\n          checkBatchAxis,\n          standardizedOuts,\n          doValidation,\n          valIns,\n          _checkBatchAxis,\n          valStandardized,\n          splitAt,\n          originalBatchSize,\n          ins,\n          trainFunction,\n          outLabels,\n          valFunction,\n          callbackMetrics,\n          callbacks,\n          out,\n          _args3 = arguments;\n        return _regeneratorRuntime().wrap(function _callee3$(_context3) {\n          while (1) switch (_context3.prev = _context3.next) {\n            case 0:\n              args = _args3.length > 2 && _args3[2] !== undefined ? _args3[2] : {};\n              if (!this.isTraining) {\n                _context3.next = 3;\n                break;\n              }\n              throw new Error('Cannot start training because another fit() call is ongoing.');\n            case 3:\n              this.isTraining = true;\n              _context3.prev = 4;\n              batchSize = args.batchSize == null ? 32 : args.batchSize;\n              checkBatchSize(batchSize);\n              // Validate user data.\n              // TODO(cais): Support sampleWeight.\n              checkBatchAxis = false;\n              _context3.next = 10;\n              return this.standardizeUserData(x, y, args.sampleWeight, args.classWeight, checkBatchAxis, batchSize);\n            case 10:\n              standardizedOuts = _context3.sent;\n              inputs = standardizedOuts[0];\n              targets = standardizedOuts[1];\n              sampleWeights = standardizedOuts[2];\n              // Prepare validation data.\n              doValidation = false;\n              if (!(args.validationData != null && args.validationData.length > 0)) {\n                _context3.next = 36;\n                break;\n              }\n              doValidation = true;\n              if (!(args.validationData.length === 2)) {\n                _context3.next = 22;\n                break;\n              }\n              // config.validationData consists of valX and valY.\n              inputValX = args.validationData[0];\n              inputValY = args.validationData[1];\n              _context3.next = 27;\n              break;\n            case 22:\n              if (!(args.validationData.length === 3)) {\n                _context3.next = 26;\n                break;\n              }\n              throw new NotImplementedError('validationData including sample weights is not supported yet.');\n            case 26:\n              throw new ValueError(\"When passing validation data, it must contain 2 (valX, valY) \" + \"or 3 (valX, valY, valSampleWeight) items; \" + \"\".concat(args.validationData, \" is invalid.\"));\n            case 27:\n              _checkBatchAxis = true;\n              _context3.next = 30;\n              return this.standardizeUserData(inputValX, inputValY, null, /** Unused sample weights. */null, /** Unused class weights. */_checkBatchAxis, batchSize);\n            case 30:\n              valStandardized = _context3.sent;\n              valX = valStandardized[0];\n              valY = valStandardized[1];\n              valIns = valX.concat(valY);\n              // TODO(cais): Add useLearningPhase data properly.\n              _context3.next = 37;\n              break;\n            case 36:\n              if (args.validationSplit != null && args.validationSplit > 0 && args.validationSplit < 1) {\n                doValidation = true;\n                // Porting Note: In tfjs-layers, inputs[0] is always a Tensor.\n                splitAt = Math.floor(inputs[0].shape[0] * (1 - args.validationSplit));\n                originalBatchSize = inputs[0].shape[0];\n                valX = sliceArrays(inputs, splitAt, originalBatchSize);\n                originalInputs = inputs;\n                inputs = sliceArrays(inputs, 0, splitAt);\n                valY = sliceArrays(targets, splitAt, originalBatchSize);\n                originalTargets = targets;\n                targets = sliceArrays(targets, 0, splitAt);\n                // TODO(cais): Once sampleWeights becomes available, slice it to get\n                //   valSampleWeights.\n                valIns = valX.concat(valY);\n                // TODO(cais): Add useLearningPhase data properly.\n              } else if (args.validationSteps != null) {\n                doValidation = true;\n                // TODO(cais): Add useLearningPhase.\n              }\n            case 37:\n              ins = inputs.concat(targets).concat(sampleWeights);\n              this.checkTrainableWeightsConsistency();\n              // TODO(cais): Handle use_learning_phase and learning_phase?\n              // Porting Note: Here we see a key deviation of tfjs-layers from\n              // Keras.\n              //  Due to the imperative nature of tfjs-layers' backend (tfjs-core),\n              //  we do not construct symbolic computation graphs to embody the\n              //  training process. Instead, we define a function that performs the\n              //  training action. In PyKeras, the data (inputs and targets) are fed\n              //  through graph placeholders. In tfjs-layers, the data are fed as\n              //  function arguments. Since the function are defined below in the\n              //  scope, we don't have equivalents of PyKeras's\n              //  `_make_train_funciton`.\n              trainFunction = this.makeTrainFunction();\n              outLabels = this.getDedupedMetricsNames();\n              if (doValidation) {\n                this.makeTestFunction();\n                valFunction = this.testFunction;\n                callbackMetrics = outLabels.slice().concat(outLabels.map(function (n) {\n                  return 'val_' + n;\n                }));\n              } else {\n                valFunction = null;\n                valIns = [];\n                callbackMetrics = outLabels.slice();\n              }\n              callbacks = standardizeCallbacks(args.callbacks, args.yieldEvery);\n              _context3.next = 45;\n              return this.fitLoop(trainFunction, ins, outLabels, batchSize, args.epochs, args.verbose, callbacks, valFunction, valIns, args.shuffle, callbackMetrics, args.initialEpoch, null, null);\n            case 45:\n              out = _context3.sent;\n              return _context3.abrupt(\"return\", out);\n            case 47:\n              _context3.prev = 47;\n              this.isTraining = false;\n              // Memory clean up.\n              disposeNewTensors(inputs, x);\n              disposeNewTensors(targets, y);\n              disposeNewTensors(originalInputs, x);\n              disposeNewTensors(originalTargets, y);\n              disposeNewTensors(valX, inputValX);\n              disposeNewTensors(valY, inputValY);\n              if (sampleWeights != null) {\n                tfc.dispose(sampleWeights);\n              }\n              return _context3.finish(47);\n            case 57:\n            case \"end\":\n              return _context3.stop();\n          }\n        }, _callee3, this, [[4,, 47, 57]]);\n      }));\n      function fit(_x7, _x8) {\n        return _fit.apply(this, arguments);\n      }\n      return fit;\n    }()\n    /**\n     * Abstract fit function for `f(ins)`.\n     * @param f A Function returning a list of tensors. For training, this\n     *   function is expected to perform the updates to the variables.\n     * @param ins List of tensors to be fed to `f`.\n     * @param outLabels List of strings, display names of the outputs of `f`.\n     * @param batchSize Integer batch size or `== null` if unknown. Default : 32.\n     * @param epochs Number of times to iterate over the data. Default : 1.\n     * @param verbose Verbosity mode: 0, 1, or 2. Default: 1.\n     * @param callbacks List of callbacks to be called during training.\n     * @param valF Function to call for validation.\n     * @param valIns List of tensors to be fed to `valF`.\n     * @param shuffle Whether to shuffle the data at the beginning of every\n     * epoch. Default : true.\n     * @param callbackMetrics List of strings, the display names of the metrics\n     *   passed to the callbacks. They should be the concatenation of the\n     *   display names of the outputs of `f` and the list of display names\n     *   of the outputs of `valF`.\n     * @param initialEpoch Epoch at which to start training (useful for\n     *   resuming a previous training run). Default : 0.\n     * @param stepsPerEpoch Total number of steps (batches on samples) before\n     *   declaring one epoch finished and starting the next epoch. Ignored with\n     *   the default value of `undefined` or `null`.\n     * @param validationSteps Number of steps to run validation for (only if\n     *   doing validation from data tensors). Not applicable for tfjs-layers.\n     * @returns A `History` object.\n     */\n  }, {\n    key: \"fitLoop\",\n    value: function () {\n      var _fitLoop = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee4(f, ins, outLabels, batchSize, epochs, verbose, callbacks, valF, valIns, shuffle, callbackMetrics, initialEpoch, stepsPerEpoch, validationSteps) {\n        var _this7 = this;\n        var doValidation, numTrainSamples, indexArray, _configureCallbacks, callbackList, history, _loop4, epoch, _ret2;\n        return _regeneratorRuntime().wrap(function _callee4$(_context6) {\n          while (1) switch (_context6.prev = _context6.next) {\n            case 0:\n              if (batchSize == null) {\n                batchSize = 32;\n              }\n              if (epochs == null) {\n                epochs = 1;\n              }\n              if (shuffle == null) {\n                shuffle = true;\n              }\n              if (initialEpoch == null) {\n                initialEpoch = 0;\n              }\n              // TODO(cais): Change const to let below when implementing validation.\n              doValidation = false;\n              if (valF != null && valIns != null) {\n                doValidation = true;\n                // TODO(cais): verbose message.\n              }\n              if (!(validationSteps != null)) {\n                _context6.next = 10;\n                break;\n              }\n              doValidation = true;\n              if (!(stepsPerEpoch == null)) {\n                _context6.next = 10;\n                break;\n              }\n              throw new ValueError('Can only use `validationSteps` when doing step-wise training, ' + 'i.e., `stepsPerEpoch` must be set.');\n            case 10:\n              numTrainSamples = this.checkNumSamples(ins, batchSize, stepsPerEpoch, 'steps_per_epoch');\n              if (numTrainSamples != null) {\n                indexArray = range(0, numTrainSamples);\n              }\n              if (verbose == null) {\n                verbose = 1;\n              }\n              _configureCallbacks = configureCallbacks(callbacks, verbose, epochs, initialEpoch, numTrainSamples, stepsPerEpoch, batchSize, doValidation, callbackMetrics), callbackList = _configureCallbacks.callbackList, history = _configureCallbacks.history;\n              callbackList.setModel(this);\n              this.history = history;\n              _context6.next = 18;\n              return callbackList.onTrainBegin();\n            case 18:\n              this.stopTraining_ = false;\n              // TODO(cais): Take care of callbacks.validation_data as in PyKeras.\n              // TODO(cais): Pre-convert feeds for performance as in PyKeras.\n              _loop4 = /*#__PURE__*/_regeneratorRuntime().mark(function _loop4() {\n                var epochLogs, epochIndexArray1D, batches, _loop5, batchIndex, _ret3;\n                return _regeneratorRuntime().wrap(function _loop4$(_context5) {\n                  while (1) switch (_context5.prev = _context5.next) {\n                    case 0:\n                      _context5.next = 2;\n                      return callbackList.onEpochBegin(epoch);\n                    case 2:\n                      epochLogs = {};\n                      if (!(stepsPerEpoch != null)) {\n                        _context5.next = 7;\n                        break;\n                      }\n                      throw new NotImplementedError('stepsPerEpoch mode is not implemented yet.');\n                    case 7:\n                      if (!(shuffle === 'batch')) {\n                        _context5.next = 11;\n                        break;\n                      }\n                      throw new NotImplementedError('batch shuffling is not implemneted' + ' yet');\n                    case 11:\n                      if (shuffle) {\n                        util.shuffle(indexArray);\n                      }\n                    case 12:\n                      // Convert the potentially shuffled indices to Tensor1D, to avoid the\n                      // cost of repeated creation of Array1Ds later on.\n                      epochIndexArray1D = tensor1d(indexArray);\n                      batches = makeBatches(numTrainSamples, batchSize);\n                      _loop5 = /*#__PURE__*/_regeneratorRuntime().mark(function _loop5(batchIndex) {\n                        var batchLogs;\n                        return _regeneratorRuntime().wrap(function _loop5$(_context4) {\n                          while (1) switch (_context4.prev = _context4.next) {\n                            case 0:\n                              batchLogs = {};\n                              _context4.next = 3;\n                              return callbackList.onBatchBegin(batchIndex, batchLogs);\n                            case 3:\n                              tfc.tidy(function () {\n                                var batchStart = batches[batchIndex][0];\n                                var batchEnd = batches[batchIndex][1];\n                                var batchIds = K.sliceAlongFirstAxis(epochIndexArray1D, batchStart, batchEnd - batchStart);\n                                batchLogs['batch'] = batchIndex;\n                                batchLogs['size'] = batchEnd - batchStart;\n                                // TODO(cais): In ins, train flag can be a number, instead of an\n                                //   Tensor? Do we need to handle this in tfjs-layers?\n                                var insBatch = sliceArraysByIndices(ins, batchIds);\n                                var outs = f(insBatch);\n                                for (var i = 0; i < outLabels.length; ++i) {\n                                  var label = outLabels[i];\n                                  var out = outs[i];\n                                  batchLogs[label] = out;\n                                  tfc.keep(out);\n                                  // TODO(cais): Use scope() to avoid ownership.\n                                }\n\n                                if (batchIndex === batches.length - 1) {\n                                  // Last batch.\n                                  if (doValidation) {\n                                    var valOuts = _this7.testLoop(valF, valIns, batchSize);\n                                    // Porting Notes: In tfjs-layers, valOuts is always an Array.\n                                    for (var _i9 = 0; _i9 < outLabels.length; ++_i9) {\n                                      var _label = outLabels[_i9];\n                                      var _out = valOuts[_i9];\n                                      tfc.keep(_out);\n                                      // TODO(cais): Use scope() to avoid ownership.\n                                      epochLogs['val_' + _label] = _out;\n                                    }\n                                  }\n                                }\n                              });\n                              _context4.next = 6;\n                              return callbackList.onBatchEnd(batchIndex, batchLogs);\n                            case 6:\n                              disposeTensorsInLogs(batchLogs);\n                              if (!_this7.stopTraining_) {\n                                _context4.next = 9;\n                                break;\n                              }\n                              return _context4.abrupt(\"return\", \"break\");\n                            case 9:\n                            case \"end\":\n                              return _context4.stop();\n                          }\n                        }, _loop5);\n                      });\n                      batchIndex = 0;\n                    case 16:\n                      if (!(batchIndex < batches.length)) {\n                        _context5.next = 24;\n                        break;\n                      }\n                      return _context5.delegateYield(_loop5(batchIndex), \"t0\", 18);\n                    case 18:\n                      _ret3 = _context5.t0;\n                      if (!(_ret3 === \"break\")) {\n                        _context5.next = 21;\n                        break;\n                      }\n                      return _context5.abrupt(\"break\", 24);\n                    case 21:\n                      ++batchIndex;\n                      _context5.next = 16;\n                      break;\n                    case 24:\n                      epochIndexArray1D.dispose();\n                    case 25:\n                      _context5.next = 27;\n                      return callbackList.onEpochEnd(epoch, epochLogs);\n                    case 27:\n                      if (!_this7.stopTraining_) {\n                        _context5.next = 29;\n                        break;\n                      }\n                      return _context5.abrupt(\"return\", \"break\");\n                    case 29:\n                    case \"end\":\n                      return _context5.stop();\n                  }\n                }, _loop4);\n              });\n              epoch = initialEpoch;\n            case 21:\n              if (!(epoch < epochs)) {\n                _context6.next = 29;\n                break;\n              }\n              return _context6.delegateYield(_loop4(), \"t0\", 23);\n            case 23:\n              _ret2 = _context6.t0;\n              if (!(_ret2 === \"break\")) {\n                _context6.next = 26;\n                break;\n              }\n              return _context6.abrupt(\"break\", 29);\n            case 26:\n              ++epoch;\n              _context6.next = 21;\n              break;\n            case 29:\n              _context6.next = 31;\n              return callbackList.onTrainEnd();\n            case 31:\n              _context6.next = 33;\n              return this.history.syncData();\n            case 33:\n              return _context6.abrupt(\"return\", this.history);\n            case 34:\n            case \"end\":\n              return _context6.stop();\n          }\n        }, _callee4, this);\n      }));\n      function fitLoop(_x9, _x10, _x11, _x12, _x13, _x14, _x15, _x16, _x17, _x18, _x19, _x20, _x21, _x22) {\n        return _fitLoop.apply(this, arguments);\n      }\n      return fitLoop;\n    }() // TODO(cais): Add code snippet below when it's possible to instantiate\n    //   actual dataset objects.\n    /**\n     * Trains the model using a dataset object.\n     *\n     * @param dataset A dataset object. Its `iterator()` method is expected\n     *   to generate a dataset iterator object, the `next()` method of which\n     *   is expected to produce data batches for training. The return value\n     *   of the `next()` call ought to contain a boolean `done` field and a\n     *   `value` field. The `value` field is expected to be an array of two\n     *   `tf.Tensor`s or an array of two nested `tf.Tensor` structures. The former\n     *   case is for models with exactly one input and one output (e.g.\n     *   a sequential model). The latter case is for models with multiple\n     *   inputs and/or multiple outputs.\n     *   Of the two items in the array, the first is the input feature(s) and\n     *   the second is the output target(s).\n     * @param args A `ModelFitDatasetArgs`, containing optional fields.\n     *\n     * @return A `History` instance. Its `history` attribute contains all\n     *   information collected during training.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes'}\n     */\n  }, {\n    key: \"fitDataset\",\n    value: function () {\n      var _fitDataset2 = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee5(dataset, args) {\n        return _regeneratorRuntime().wrap(function _callee5$(_context7) {\n          while (1) switch (_context7.prev = _context7.next) {\n            case 0:\n              return _context7.abrupt(\"return\", _fitDataset(this, dataset, args));\n            case 1:\n            case \"end\":\n              return _context7.stop();\n          }\n        }, _callee5, this);\n      }));\n      function fitDataset(_x23, _x24) {\n        return _fitDataset2.apply(this, arguments);\n      }\n      return fitDataset;\n    }()\n    /**\n     * Runs a single gradient update on a single batch of data.\n     *\n     * This method differs from `fit()` and `fitDataset()` in the following\n     * regards:\n     *   - It operates on exactly one batch of data.\n     *   - It returns only the loss and metric values, instead of\n     *     returning the batch-by-batch loss and metric values.\n     *   - It doesn't support fine-grained options such as verbosity and\n     *     callbacks.\n     *\n     * @param x Input data. It could be one of the following:\n     *   - A `tf.Tensor`, or an Array of `tf.Tensor`s (in case the model has\n     *     multiple inputs).\n     *   - An Object mapping input names to corresponding `tf.Tensor` (if the\n     *     model has named inputs).\n     * @param y Target data. It could be either a `tf.Tensor` or multiple\n     *   `tf.Tensor`s. It should be consistent with `x`.\n     * @returns Training loss or losses (in case the model has\n     *   multiple outputs), along with metrics (if any), as numbers.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes'}\n     */\n  }, {\n    key: \"trainOnBatch\",\n    value: function () {\n      var _trainOnBatch = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee6(x, y) {\n        var standardizeOut, inputs, targets, trainFunction, losses, lossValues, _iterator7, _step7, loss, v;\n        return _regeneratorRuntime().wrap(function _callee6$(_context8) {\n          while (1) switch (_context8.prev = _context8.next) {\n            case 0:\n              _context8.next = 2;\n              return this.standardizeUserData(x, y);\n            case 2:\n              standardizeOut = _context8.sent;\n              inputs = standardizeOut[0];\n              targets = standardizeOut[1];\n              trainFunction = this.makeTrainFunction();\n              losses = trainFunction(inputs.concat(targets));\n              lossValues = [];\n              _iterator7 = _createForOfIteratorHelper(losses);\n              _context8.prev = 9;\n              _iterator7.s();\n            case 11:\n              if ((_step7 = _iterator7.n()).done) {\n                _context8.next = 19;\n                break;\n              }\n              loss = _step7.value;\n              _context8.next = 15;\n              return loss.data();\n            case 15:\n              v = _context8.sent;\n              lossValues.push(v[0]);\n            case 17:\n              _context8.next = 11;\n              break;\n            case 19:\n              _context8.next = 24;\n              break;\n            case 21:\n              _context8.prev = 21;\n              _context8.t0 = _context8[\"catch\"](9);\n              _iterator7.e(_context8.t0);\n            case 24:\n              _context8.prev = 24;\n              _iterator7.f();\n              return _context8.finish(24);\n            case 27:\n              tfc.dispose(losses);\n              disposeNewTensors(standardizeOut[0], x);\n              disposeNewTensors(standardizeOut[1], y);\n              return _context8.abrupt(\"return\", singletonOrArray(lossValues));\n            case 31:\n            case \"end\":\n              return _context8.stop();\n          }\n        }, _callee6, this, [[9, 21, 24, 27]]);\n      }));\n      function trainOnBatch(_x25, _x26) {\n        return _trainOnBatch.apply(this, arguments);\n      }\n      return trainOnBatch;\n    }()\n    /**\n     * Extract weight values of the model.\n     *\n     * @param config: An instance of `io.SaveConfig`, which specifies\n     * model-saving options such as whether only trainable weights are to be\n     * saved.\n     * @returns A `NamedTensorMap` mapping original weight names (i.e.,\n     *   non-uniqueified weight names) to their values.\n     */\n  }, {\n    key: \"getNamedWeights\",\n    value: function getNamedWeights(config) {\n      var namedWeights = [];\n      var trainableOnly = config != null && config.trainableOnly;\n      var weights = trainableOnly ? this.trainableWeights : this.weights;\n      var weightValues = this.getWeights(trainableOnly);\n      for (var i = 0; i < weights.length; ++i) {\n        if (trainableOnly && !weights[i].trainable) {\n          // Optionally skip non-trainable weights.\n          continue;\n        }\n        namedWeights.push({\n          name: weights[i].originalName,\n          tensor: weightValues[i]\n        });\n      }\n      return namedWeights;\n    }\n    /**\n     * Setter used for force stopping of LayersModel.fit() (i.e., training).\n     *\n     * Example:\n     *\n     * ```js\n     * const input = tf.input({shape: [10]});\n     * const output = tf.layers.dense({units: 1}).apply(input);\n     * const model = tf.model({inputs: [input], outputs: [output]});\n     * model.compile({loss: 'meanSquaredError', optimizer: 'sgd'});\n     * const xs = tf.ones([8, 10]);\n     * const ys = tf.zeros([8, 1]);\n     *\n     * const history = await model.fit(xs, ys, {\n     *   epochs: 10,\n     *   callbacks: {\n     *     onEpochEnd: async (epoch, logs) => {\n     *       if (epoch === 2) {\n     *         model.stopTraining = true;\n     *       }\n     *     }\n     *   }\n     * });\n     *\n     * // There should be only 3 values in the loss array, instead of 10\n     * values,\n     * // due to the stopping after 3 epochs.\n     * console.log(history.history.loss);\n     * ```\n     */\n  }, {\n    key: \"stopTraining\",\n    get: function get() {\n      return this.stopTraining_;\n    },\n    set: function set(stop) {\n      this.stopTraining_ = stop;\n    }\n  }, {\n    key: \"optimizer\",\n    get: function get() {\n      return this.optimizer_;\n    },\n    set: function set(optimizer) {\n      if (this.optimizer_ !== optimizer) {\n        this.optimizer_ = optimizer;\n        this.isOptimizerOwned = false;\n      }\n    }\n  }, {\n    key: \"dispose\",\n    value: function dispose() {\n      var result = _get(_getPrototypeOf(LayersModel.prototype), \"dispose\", this).call(this);\n      if (result.refCountAfterDispose === 0 && this.optimizer != null && this.isOptimizerOwned) {\n        var numTensorsBeforeOptmizerDisposal = tfc.memory().numTensors;\n        this.optimizer_.dispose();\n        result.numDisposedVariables += numTensorsBeforeOptmizerDisposal - tfc.memory().numTensors;\n      }\n      return result;\n    }\n  }, {\n    key: \"getLossIdentifiers\",\n    value: function getLossIdentifiers() {\n      var lossNames;\n      if (typeof this.loss === 'string') {\n        lossNames = toSnakeCase(this.loss);\n      } else if (Array.isArray(this.loss)) {\n        var _iterator8 = _createForOfIteratorHelper(this.loss),\n          _step8;\n        try {\n          for (_iterator8.s(); !(_step8 = _iterator8.n()).done;) {\n            var loss = _step8.value;\n            if (typeof loss !== 'string') {\n              throw new Error('Serialization of non-string loss is not supported.');\n            }\n          }\n        } catch (err) {\n          _iterator8.e(err);\n        } finally {\n          _iterator8.f();\n        }\n        lossNames = this.loss.map(function (name) {\n          return toSnakeCase(name);\n        });\n      } else {\n        var outputNames = Object.keys(this.loss);\n        lossNames = {};\n        var _losses = this.loss;\n        for (var _i10 = 0, _outputNames = outputNames; _i10 < _outputNames.length; _i10++) {\n          var outputName = _outputNames[_i10];\n          if (typeof _losses[outputName] === 'string') {\n            lossNames[outputName] = toSnakeCase(_losses[outputName]);\n          } else {\n            throw new Error('Serialization of non-string loss is not supported.');\n          }\n        }\n      }\n      return lossNames;\n    }\n  }, {\n    key: \"getMetricIdentifiers\",\n    value: function getMetricIdentifiers() {\n      if (typeof this.metrics === 'string' || typeof this.metrics === 'function') {\n        return [toSnakeCase(Metrics.getLossOrMetricName(this.metrics))];\n      } else if (Array.isArray(this.metrics)) {\n        return this.metrics.map(function (metric) {\n          return toSnakeCase(Metrics.getLossOrMetricName(metric));\n        });\n      } else {\n        var metricsIdentifiers = {};\n        for (var key in this.metrics) {\n          metricsIdentifiers[key] = toSnakeCase(Metrics.getLossOrMetricName(this.metrics[key]));\n        }\n        return metricsIdentifiers;\n      }\n    }\n  }, {\n    key: \"getTrainingConfig\",\n    value: function getTrainingConfig() {\n      return {\n        loss: this.getLossIdentifiers(),\n        metrics: this.getMetricIdentifiers(),\n        optimizer_config: {\n          class_name: this.optimizer.getClassName(),\n          config: this.optimizer.getConfig()\n        }\n      };\n      // TODO(cais): Add weight_metrics when they are supported.\n      // TODO(cais): Add sample_weight_mode when it's supported.\n      // TODO(cais): Add loss_weights when it's supported.\n    }\n  }, {\n    key: \"loadTrainingConfig\",\n    value: function loadTrainingConfig(trainingConfig) {\n      if (trainingConfig.weighted_metrics != null) {\n        throw new Error('Loading weight_metrics is not supported yet.');\n      }\n      if (trainingConfig.loss_weights != null) {\n        throw new Error('Loading loss_weights is not supported yet.');\n      }\n      if (trainingConfig.sample_weight_mode != null) {\n        throw new Error('Loading sample_weight_mode is not supported yet.');\n      }\n      var tsConfig = convertPythonicToTs(trainingConfig.optimizer_config);\n      var optimizer = deserialize(tsConfig);\n      var loss;\n      if (typeof trainingConfig.loss === 'string') {\n        loss = toCamelCase(trainingConfig.loss);\n      } else if (Array.isArray(trainingConfig.loss)) {\n        loss = trainingConfig.loss.map(function (lossEntry) {\n          return toCamelCase(lossEntry);\n        });\n      } else if (trainingConfig.loss != null) {\n        loss = {};\n        for (var key in trainingConfig.loss) {\n          loss[key] = toCamelCase(trainingConfig.loss[key]);\n        }\n      }\n      var metrics;\n      if (Array.isArray(trainingConfig.metrics)) {\n        metrics = trainingConfig.metrics.map(function (metric) {\n          return toCamelCase(metric);\n        });\n      } else if (trainingConfig.metrics != null) {\n        metrics = {};\n        for (var _key in trainingConfig.metrics) {\n          metrics[_key] = toCamelCase(trainingConfig.metrics[_key]);\n        }\n      }\n      this.compile({\n        loss: loss,\n        metrics: metrics,\n        optimizer: optimizer\n      });\n    }\n    /**\n     * Save the configuration and/or weights of the LayersModel.\n     *\n     * An `IOHandler` is an object that has a `save` method of the proper\n     * signature defined. The `save` method manages the storing or\n     * transmission of serialized data (\"artifacts\") that represent the\n     * model's topology and weights onto or via a specific medium, such as\n     * file downloads, local storage, IndexedDB in the web browser and HTTP\n     * requests to a server. TensorFlow.js provides `IOHandler`\n     * implementations for a number of frequently used saving mediums, such as\n     * `tf.io.browserDownloads` and `tf.io.browserLocalStorage`. See `tf.io`\n     * for more details.\n     *\n     * This method also allows you to refer to certain types of `IOHandler`s\n     * as URL-like string shortcuts, such as 'localstorage://' and\n     * 'indexeddb://'.\n     *\n     * Example 1: Save `model`'s topology and weights to browser [local\n     * storage](https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage);\n     * then load it back.\n     *\n     * ```js\n     * const model = tf.sequential(\n     *     {layers: [tf.layers.dense({units: 1, inputShape: [3]})]});\n     * console.log('Prediction from original model:');\n     * model.predict(tf.ones([1, 3])).print();\n     *\n     * const saveResults = await model.save('localstorage://my-model-1');\n     *\n     * const loadedModel = await tf.loadLayersModel('localstorage://my-model-1');\n     * console.log('Prediction from loaded model:');\n     * loadedModel.predict(tf.ones([1, 3])).print();\n     * ```\n     *\n     * Example 2. Saving `model`'s topology and weights to browser\n     * [IndexedDB](https://developer.mozilla.org/en-US/docs/Web/API/IndexedDB_API);\n     * then load it back.\n     *\n     * ```js\n     * const model = tf.sequential(\n     *     {layers: [tf.layers.dense({units: 1, inputShape: [3]})]});\n     * console.log('Prediction from original model:');\n     * model.predict(tf.ones([1, 3])).print();\n     *\n     * const saveResults = await model.save('indexeddb://my-model-1');\n     *\n     * const loadedModel = await tf.loadLayersModel('indexeddb://my-model-1');\n     * console.log('Prediction from loaded model:');\n     * loadedModel.predict(tf.ones([1, 3])).print();\n     * ```\n     *\n     * Example 3. Saving `model`'s topology and weights as two files\n     * (`my-model-1.json` and `my-model-1.weights.bin`) downloaded from\n     * browser.\n     *\n     * ```js\n     * const model = tf.sequential(\n     *     {layers: [tf.layers.dense({units: 1, inputShape: [3]})]});\n     * const saveResults = await model.save('downloads://my-model-1');\n     * ```\n     *\n     * Example 4. Send  `model`'s topology and weights to an HTTP server.\n     * See the documentation of `tf.io.http` for more details\n     * including specifying request parameters and implementation of the\n     * server.\n     *\n     * ```js\n     * const model = tf.sequential(\n     *     {layers: [tf.layers.dense({units: 1, inputShape: [3]})]});\n     * const saveResults = await model.save('http://my-server/model/upload');\n     * ```\n     *\n     * @param handlerOrURL An instance of `IOHandler` or a URL-like,\n     * scheme-based string shortcut for `IOHandler`.\n     * @param config Options for saving the model.\n     * @returns A `Promise` of `SaveResult`, which summarizes the result of\n     * the saving, such as byte sizes of the saved artifacts for the model's\n     *   topology and weight values.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes', ignoreCI: true}\n     */\n  }, {\n    key: \"save\",\n    value: function () {\n      var _save = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee7(handlerOrURL, config) {\n        var handlers, weightDataAndSpecs, returnString, unusedArg, modelConfig, modelArtifacts, includeOptimizer, _weightDataAndSpecs$s, weightType, _yield$io$encodeWeigh, optimizerWeightData, optimizerWeightSpecs, checkSize;\n        return _regeneratorRuntime().wrap(function _callee7$(_context9) {\n          while (1) switch (_context9.prev = _context9.next) {\n            case 0:\n              if (!(typeof handlerOrURL === 'string')) {\n                _context9.next = 9;\n                break;\n              }\n              handlers = io.getSaveHandlers(handlerOrURL);\n              if (!(handlers.length === 0)) {\n                _context9.next = 6;\n                break;\n              }\n              throw new ValueError(\"Cannot find any save handlers for URL '\".concat(handlerOrURL, \"'\"));\n            case 6:\n              if (!(handlers.length > 1)) {\n                _context9.next = 8;\n                break;\n              }\n              throw new ValueError(\"Found more than one (\".concat(handlers.length, \") save handlers for \") + \"URL '\".concat(handlerOrURL, \"'\"));\n            case 8:\n              handlerOrURL = handlers[0];\n            case 9:\n              if (!(handlerOrURL.save == null)) {\n                _context9.next = 11;\n                break;\n              }\n              throw new ValueError('LayersModel.save() cannot proceed because the IOHandler ' + 'provided does not have the `save` attribute defined.');\n            case 11:\n              _context9.next = 13;\n              return io.encodeWeights(this.getNamedWeights(config));\n            case 13:\n              weightDataAndSpecs = _context9.sent;\n              returnString = false;\n              unusedArg = null;\n              modelConfig = this.toJSON(unusedArg, returnString);\n              modelArtifacts = {\n                modelTopology: modelConfig,\n                format: LAYERS_MODEL_FORMAT_NAME,\n                generatedBy: \"TensorFlow.js tfjs-layers v\".concat(version),\n                convertedBy: null\n              };\n              includeOptimizer = config == null ? false : config.includeOptimizer;\n              if (!(includeOptimizer && this.optimizer != null)) {\n                _context9.next = 34;\n                break;\n              }\n              modelArtifacts.trainingConfig = this.getTrainingConfig();\n              weightType = 'optimizer';\n              _context9.t0 = io;\n              _context9.next = 25;\n              return this.optimizer.getWeights();\n            case 25:\n              _context9.t1 = _context9.sent;\n              _context9.t2 = weightType;\n              _context9.next = 29;\n              return _context9.t0.encodeWeights.call(_context9.t0, _context9.t1, _context9.t2);\n            case 29:\n              _yield$io$encodeWeigh = _context9.sent;\n              optimizerWeightData = _yield$io$encodeWeigh.data;\n              optimizerWeightSpecs = _yield$io$encodeWeigh.specs;\n              (_weightDataAndSpecs$s = weightDataAndSpecs.specs).push.apply(_weightDataAndSpecs$s, _toConsumableArray(optimizerWeightSpecs));\n              weightDataAndSpecs.data = io.concatenateArrayBuffers([weightDataAndSpecs.data, optimizerWeightData]);\n            case 34:\n              if (this.userDefinedMetadata != null) {\n                // Check serialized size of user-defined metadata.\n                checkSize = true;\n                checkUserDefinedMetadata(this.userDefinedMetadata, this.name, checkSize);\n                modelArtifacts.userDefinedMetadata = this.userDefinedMetadata;\n              }\n              modelArtifacts.weightData = weightDataAndSpecs.data;\n              modelArtifacts.weightSpecs = weightDataAndSpecs.specs;\n              return _context9.abrupt(\"return\", handlerOrURL.save(modelArtifacts));\n            case 38:\n            case \"end\":\n              return _context9.stop();\n          }\n        }, _callee7, this);\n      }));\n      function save(_x27, _x28) {\n        return _save.apply(this, arguments);\n      }\n      return save;\n    }()\n    /**\n     * Set user-defined metadata.\n     *\n     * The set metadata will be serialized together with the topology\n     * and weights of the model during `save()` calls.\n     *\n     * @param setUserDefinedMetadata\n     */\n  }, {\n    key: \"setUserDefinedMetadata\",\n    value: function setUserDefinedMetadata(userDefinedMetadata) {\n      checkUserDefinedMetadata(userDefinedMetadata, this.name);\n      this.userDefinedMetadata = userDefinedMetadata;\n    }\n    /**\n     * Get user-defined metadata.\n     *\n     * The metadata is supplied via one of the two routes:\n     *   1. By calling `setUserDefinedMetadata()`.\n     *   2. Loaded during model loading (if the model is constructed\n     *      via `tf.loadLayersModel()`.)\n     *\n     * If no user-defined metadata is available from either of the\n     * two routes, this function will return `undefined`.\n     */\n  }, {\n    key: \"getUserDefinedMetadata\",\n    value: function getUserDefinedMetadata() {\n      return this.userDefinedMetadata;\n    }\n  }]);\n  return LayersModel;\n}(Container);\n// The class name is 'Model' rather than 'LayersModel' for backwards\n// compatibility since this class name shows up in the serialization format.\n/** @nocollapse */\nLayersModel.className = 'Model';\nserialization.registerClass(LayersModel);\n/**\n * A `tf.Functional` is an alias to `tf.LayersModel`.\n *\n * See also:\n *   `tf.LayersModel`, `tf.Sequential`, `tf.loadLayersModel`.\n */\n/** @doc {heading: 'Models', subheading: 'Classes'} */\nexport var Functional = /*#__PURE__*/function (_LayersModel) {\n  _inherits(Functional, _LayersModel);\n  var _super2 = _createSuper(Functional);\n  function Functional() {\n    _classCallCheck(this, Functional);\n    return _super2.apply(this, arguments);\n  }\n  return _createClass(Functional);\n}(LayersModel);\nFunctional.className = 'Functional';\nserialization.registerClass(Functional);","map":{"version":3,"names":["tfc","io","Optimizer","scalar","serialization","Tensor","tensor1d","util","K","configureCallbacks","standardizeCallbacks","nameScope","NotImplementedError","RuntimeError","ValueError","deserialize","disposeTensorsInLogs","losses","Metrics","optimizers","checkUserDefinedMetadata","count","pyListRepeat","singletonOrArray","toCamelCase","toSnakeCase","unique","printSummary","range","convertPythonicToTs","version","Container","execute","FeedDict","evaluateDataset","fitDataset","checkBatchSize","disposeNewTensors","ensureTensorsRank2OrHigher","makeBatches","sliceArrays","sliceArraysByIndices","computeWeightedLoss","standardizeClassWeights","standardizeWeights","isDataTensor","x","isDataArray","Array","isArray","isDataDict","standardizeInputData","data","names","shapes","checkBatchAxis","arguments","length","undefined","exceptionPrefix","gotUnexpectedData","key","hasOwnProperty","concat","map","name","arrays","_iterator","_createForOfIteratorHelper","_step","s","n","done","value","push","err","e","f","shape","i","array","j","dim","refDim","slice","checkArrayLengths","inputs","targets","weights","setX","input","sort","setY","target","JSON","stringify","arraysEqual","checkLossAndTargetCompatibility","lossFns","outputShapes","keyLosses","meanSquaredError","binaryCrossentropy","categoricalCrossentropy","y","loss","indexOf","slicedYShape","slicedShape","targetDim","outDim","checkInputData","collectMetrics","metrics","outputNames","wrappedMetrics","TypeError","nestedMetrics","_iterator2","_step2","outputMetrics","LAYERS_MODEL_FORMAT_NAME","LayersModel","_Container","_inherits","_super","_createSuper","args","_this","_classCallCheck","call","isTraining","_createClass","summary","lineLength","positions","printFn","console","log","built","compile","_this2","optimizer","optimizer_","getOptimizer","isOptimizerOwned","lossFunctions","_iterator3","_step3","warn","get","outputs","theLosses","l","lossFunction","forEach","_","feedOutputNames","feedOutputShapes","feedLossFns","internalOutputShapes","skipTargetIndices","metricsNames","metricsTensors","weightedLoss","appendMetric","outputIndex","metricName","metricTensor","_loop","_i2","handleMetrics","metricNamePrefix","accFn","weightedMetricFn","_iterator4","_step4","_loop2","metric","outputShape","binaryAccuracy","sparseCategoricalCrossentropy","sparseCategoricalAccuracy","categoricalAccuracy","suffix","metricFn","getLossOrMetricName","metricResult","_ret","collectedTrainableWeights","trainableWeights","checkTrainableWeightsConsistency","evaluate","batchSize","standardizedOuts","standardizeUserDataXY","ins","makeTestFunction","testFunction","testOuts","testLoop","verbose","steps","_evaluateDataset2","_asyncToGenerator","_regeneratorRuntime","mark","_callee","dataset","wrap","_callee$","_context","prev","next","abrupt","stop","_x","_x2","apply","checkNumSamples","stepsName","numSamples","outputsIsArray","outputSymbolicTensors","retrieveSymbolicTensors","feedDict","add","_iterator5","_step5","tensorValue","executeOutputs","symbolicTensorNames","outputsRemaining","_iterator6","layers","_step6","layer","layerOutputs","output","layerOutputNames","index","remainingNames","tensor","predictLoop","_this3","tidy","batches","outsBatches","_loop3","batchIndex","batchOuts","batchStart","batchEnd","insBatch","feeds","batchOut","predict","xsRank2OrHigher","inputNames","feedInputShapes","predictOnBatch","lossFn","feedInputNames","stateful","_standardizeUserData","_callee2","sampleWeight","classWeight","_this$standardizeUser","_this$standardizeUser2","standardXs","standardYs","standardSampleWeights","classWeights","_args2","_callee2$","_context2","_slicedToArray","Error","t0","t1","sent","standardizeUserData","_x3","_x4","_x5","_x6","_this4","outs","indexArray","batchIds","sliceAlongFirstAxis","mul","div","getDedupedMetricsNames","outLabels","dedupedOutLabels","label","newLabel","dupIndex","makeTrainFunction","_this5","lossValues","sampleWeights","metricsValues","totalLossFunction","totalLoss","meanLoss","mean","weightedMetric","keep","calculateLosses","regularizerLoss","variables","param","read","returnCost","totalLossValue","minimize","_this6","valOutputs","meanMetric","_fit","_callee3","originalInputs","originalTargets","inputValX","inputValY","valX","valY","doValidation","valIns","_checkBatchAxis","valStandardized","splitAt","originalBatchSize","trainFunction","valFunction","callbackMetrics","callbacks","out","_args3","_callee3$","_context3","validationData","validationSplit","Math","floor","validationSteps","yieldEvery","fitLoop","epochs","shuffle","initialEpoch","dispose","finish","fit","_x7","_x8","_fitLoop","_callee4","valF","stepsPerEpoch","_this7","numTrainSamples","_configureCallbacks","callbackList","history","_loop4","epoch","_ret2","_callee4$","_context6","setModel","onTrainBegin","stopTraining_","epochLogs","epochIndexArray1D","_loop5","_ret3","_loop4$","_context5","onEpochBegin","batchLogs","_loop5$","_context4","onBatchBegin","valOuts","onBatchEnd","delegateYield","onEpochEnd","onTrainEnd","syncData","_x9","_x10","_x11","_x12","_x13","_x14","_x15","_x16","_x17","_x18","_x19","_x20","_x21","_x22","_fitDataset2","_callee5","_callee5$","_context7","_x23","_x24","_trainOnBatch","_callee6","standardizeOut","_iterator7","_step7","v","_callee6$","_context8","trainOnBatch","_x25","_x26","getNamedWeights","config","namedWeights","trainableOnly","weightValues","getWeights","trainable","originalName","set","result","_get","_getPrototypeOf","prototype","refCountAfterDispose","numTensorsBeforeOptmizerDisposal","memory","numTensors","numDisposedVariables","getLossIdentifiers","lossNames","_iterator8","_step8","Object","keys","_i10","_outputNames","outputName","getMetricIdentifiers","metricsIdentifiers","getTrainingConfig","optimizer_config","class_name","getClassName","getConfig","loadTrainingConfig","trainingConfig","weighted_metrics","loss_weights","sample_weight_mode","tsConfig","lossEntry","_save","_callee7","handlerOrURL","handlers","weightDataAndSpecs","returnString","unusedArg","modelConfig","modelArtifacts","includeOptimizer","_weightDataAndSpecs$s","weightType","_yield$io$encodeWeigh","optimizerWeightData","optimizerWeightSpecs","checkSize","_callee7$","_context9","getSaveHandlers","save","encodeWeights","toJSON","modelTopology","format","generatedBy","convertedBy","t2","specs","_toConsumableArray","concatenateArrayBuffers","userDefinedMetadata","weightData","weightSpecs","_x27","_x28","setUserDefinedMetadata","getUserDefinedMetadata","className","registerClass","Functional","_LayersModel","_super2"],"sources":["C:\\Users\\vince\\OneDrive\\Documents\\GitHub\\tfjs-layers\\src\\engine\\training.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/* Original Source: engine/training.py */\n\nimport * as tfc from '@tensorflow/tfjs-core';\nimport {io, ModelPredictConfig as ModelPredictArgs, NamedTensorMap, Optimizer, Scalar, scalar, serialization, Tensor, Tensor1D, tensor1d, util} from '@tensorflow/tfjs-core';\n\nimport * as K from '../backend/tfjs_backend';\nimport {BaseCallback, configureCallbacks, History, ModelLoggingVerbosity, standardizeCallbacks} from '../base_callbacks';\nimport {nameScope} from '../common';\nimport {NotImplementedError, RuntimeError, ValueError} from '../errors';\nimport {Shape} from '../keras_format/common';\nimport {LossIdentifier} from '../keras_format/loss_config';\nimport {OptimizerSerialization} from '../keras_format/optimizer_config';\nimport {MetricsIdentifier, TrainingConfig} from '../keras_format/training_config';\nimport {deserialize} from '../layers/serialization';\nimport { disposeTensorsInLogs, UnresolvedLogs } from '../logs';\nimport * as losses from '../losses';\nimport * as Metrics from '../metrics';\nimport * as optimizers from '../optimizers';\nimport {LossOrMetricFn, NamedTensor} from '../types';\nimport {checkUserDefinedMetadata} from '../user_defined_metadata';\nimport {count, pyListRepeat, singletonOrArray, toCamelCase, toSnakeCase, unique} from '../utils/generic_utils';\nimport {printSummary} from '../utils/layer_utils';\nimport {range} from '../utils/math_utils';\nimport {convertPythonicToTs} from '../utils/serialization_utils';\nimport {LayerVariable} from '../variables';\nimport {version} from '../version';\n\nimport {Container, ContainerArgs} from './container';\nimport {Dataset} from './dataset_stub';\nimport {execute, FeedDict} from './executor';\nimport {DisposeResult, SymbolicTensor} from './topology';\nimport {evaluateDataset, fitDataset, ModelEvaluateDatasetArgs, ModelFitDatasetArgs} from './training_dataset';\nimport {checkBatchSize, disposeNewTensors, ensureTensorsRank2OrHigher, makeBatches, ModelFitArgs, sliceArrays, sliceArraysByIndices} from './training_tensors';\nimport {ClassWeight, ClassWeightMap, computeWeightedLoss, standardizeClassWeights, standardizeWeights} from './training_utils';\n\n/**\n * Helper function for polymorphic input data: 1. singleton Tensor.\n */\nexport function isDataTensor(x: Tensor|Tensor[]|{[inputName: string]: Tensor}|\n                             {[inputName: string]: Tensor[]}): boolean {\n  return x instanceof Tensor;\n}\n\n/**\n * Helper function for polymorphic input data: 2. Array of Tensor.\n */\nexport function isDataArray(x: Tensor|Tensor[]|\n                            {[inputName: string]: Tensor}): boolean {\n  return Array.isArray(x);\n}\n\n/**\n * Helper function for polymorphic input data: 3. \"dict\" of Tensor.\n */\nexport function isDataDict(x: Tensor|Tensor[]|\n                           {[inputName: string]: Tensor}): boolean {\n  return !isDataTensor(x) && !isDataArray(x);\n}\n\n/**\n * Normalizes inputs and targets provided by users.\n * @param data User-provided input data (polymorphic).\n * @param names An Array of expected Tensor names.\n * @param shapes Optional Array of expected Tensor shapes.\n * @param checkBatchAxis Whether to check that the batch axis of the arrays\n *   match  the expected value found in `shapes`.\n * @param exceptionPrefix String prefix used for exception formatting.\n * @returns List of standardized input Tensors (one Tensor per model input).\n * @throws ValueError: in case of improperly formatted user data.\n */\nexport function standardizeInputData(\n    data: Tensor|Tensor[]|{[inputName: string]: Tensor}, names: string[],\n    shapes?: Shape[], checkBatchAxis = true, exceptionPrefix = ''): Tensor[] {\n  if (names == null || names.length === 0) {\n    // Check for the case where the model expected no data, but some data got\n    // sent.\n    if (data != null) {\n      let gotUnexpectedData = false;\n      if (isDataArray(data) && (data as Tensor[]).length > 0) {\n        gotUnexpectedData = true;\n      } else if (isDataDict(data)) {\n        for (const key in data) {\n          if (data.hasOwnProperty(key)) {\n            gotUnexpectedData = true;\n            break;\n          }\n        }\n      } else {\n        // `data` is a singleton Tensor in this case.\n        gotUnexpectedData = true;\n      }\n      if (gotUnexpectedData) {\n        throw new ValueError(\n            `Error when checking model ${exceptionPrefix} expected no data, ` +\n            `but got ${data}`);\n      }\n    }\n    return [];\n  }\n  if (data == null) {\n    return names.map(name => null);\n  }\n\n  let arrays: Tensor[];\n  if (isDataDict(data)) {\n    data = data as {[inputName: string]: Tensor};\n    arrays = [];\n    for (const name of names) {\n      if (data[name] == null) {\n        throw new ValueError(\n            `No data provided for \"${name}\". Need data for each key in: ` +\n            `${names}`);\n      }\n      arrays.push(data[name]);\n    }\n  } else if (isDataArray(data)) {\n    data = data as Tensor[];\n    if (data.length !== names.length) {\n      throw new ValueError(\n          `Error when checking model ${exceptionPrefix}: the Array of ` +\n          `Tensors that you are passing to your model is not the size the ` +\n          `model expected. Expected to see ${names.length} Tensor(s), but ` +\n          `instead got the following list of Tensor(s): ${data}`);\n    }\n    arrays = data;\n  } else {\n    data = data as Tensor;\n    if (names.length > 1) {\n      throw new ValueError(\n          `The model ${exceptionPrefix} expects ${names.length} Tensor(s), ` +\n          `but only received one Tensor. Found: Tensor with shape ${\n              data.shape}`);\n    }\n    arrays = [data];\n  }\n\n  arrays = ensureTensorsRank2OrHigher(arrays);\n\n  // Check shape compatibility.\n  if (shapes != null) {\n    for (let i = 0; i < names.length; ++i) {\n      if (shapes[i] == null) {\n        continue;\n      }\n      const array = arrays[i];\n      if (array.shape.length !== shapes[i].length) {\n        throw new ValueError(\n            `Error when checking ${exceptionPrefix}: expected ${names[i]} ` +\n            `to have ${shapes[i].length} dimension(s). but got array with ` +\n            `shape ${array.shape}`);\n      }\n      for (let j = 0; j < shapes[i].length; ++j) {\n        if (j === 0 && !checkBatchAxis) {\n          // Skip the first (batch) axis.\n          continue;\n        }\n        const dim = array.shape[j];\n        const refDim = shapes[i][j];\n        if (refDim != null && refDim >= 0 && dim !== refDim) {\n          throw new ValueError(\n              `${exceptionPrefix} expected a batch of elements where each ` +\n              `example has shape [${shapes[i].slice(1, shapes[i].length)}] ` +\n              `(i.e.,tensor shape [*,${\n                  shapes[i].slice(1, shapes[i].length)}])` +\n              ` but the ${exceptionPrefix} received an input with ${\n                  array.shape[0]}` +\n              ` examples, each with shape [${\n                  array.shape.slice(1, array.shape.length)}]` +\n              ` (tensor shape [${array.shape}])`);\n        }\n      }\n    }\n  }\n  return arrays;\n}\n\n/**\n * User input validation for Tensors.\n * @param inputs `Array` of `tf.Tensor`s for inputs.\n * @param targets `Array` of `tf.Tensor`s for targets.\n * @param weights Optional `Array` of `tf.Tensor`s for sample weights.\n * @throws ValueError: in case of incorrectly formatted data.\n */\nexport function checkArrayLengths(\n    inputs: Tensor[], targets: Tensor[], weights?: Tensor[]) {\n  const setX = unique(inputs.map(input => input.shape[0]));\n  setX.sort();\n  const setY = unique(targets.map(target => target.shape[0]));\n  setY.sort();\n  // TODO(cais): Check `weights` as well.\n  if (setX.length > 1) {\n    throw new ValueError(\n        `All input Tensors (x) should have the same number of samples. ` +\n        `Got array shapes: ` +\n        `${JSON.stringify(inputs.map(input => input.shape))}`);\n  }\n  if (setY.length > 1) {\n    throw new ValueError(\n        `All target Tensors (y) should have the same number of samples. ` +\n        `Got array shapes: ` +\n        `${JSON.stringify(targets.map(target => target.shape))}`);\n  }\n  if (setX.length > 0 && setY.length > 0 && !util.arraysEqual(setX, setY)) {\n    throw new ValueError(\n        `Input Tensors should have the same number of samples as target ` +\n        `Tensors. Found ${setX[0]} input sample(s) and ${setY[0]} target ` +\n        `sample(s).`);\n  }\n}\n\n/**\n * Validation on the compatibility of targes and loss functions.\n *\n * This helps prevent users from using loss functions incorrectly.\n *\n * @param targets `Array` of `tf.Tensor`s of targets.\n * @param lossFns `Array` of loss functions.\n * @param outputShapes `Array` of shapes of model outputs.\n */\nfunction checkLossAndTargetCompatibility(\n    targets: Tensor[], lossFns: LossOrMetricFn[], outputShapes: Shape[]) {\n  // TODO(cais): Dedicated test coverage?\n  const keyLosses = [\n    losses.meanSquaredError, losses.binaryCrossentropy,\n    losses.categoricalCrossentropy\n  ];\n  for (let i = 0; i < targets.length; ++i) {\n    const y = targets[i];\n    const loss = lossFns[i];\n    const shape = outputShapes[i];\n    if (loss == null) {\n      continue;\n    }\n    if (loss === losses.categoricalCrossentropy) {\n      if (y.shape[y.shape.length - 1] === 1) {\n        throw new ValueError(\n            `You are passing a target array of shape ${y.shape} while using ` +\n            `a loss 'categorical_crossentropy'. 'categorical_crossentropy'` +\n            `expects targets to be binary matrices (1s and 0s) of shape ` +\n            `[samples, classes].`);\n        // TODO(cais): Example code in error message.\n      }\n    }\n    if (keyLosses.indexOf(loss) !== -1) {\n      const slicedYShape = y.shape.slice(1);\n      const slicedShape = shape.slice(1);\n      for (let j = 0; j < slicedYShape.length; ++j) {\n        const targetDim = slicedYShape[j];\n        const outDim = slicedShape[j];\n        if (outDim != null && targetDim !== outDim) {\n          throw new ValueError(\n              `A target Tensor with shape ${y.shape} was passed for an ` +\n              `output of shape ${shape}, while using a loss function that ` +\n              `expects targets to have the same shape as the output.`);\n        }\n      }\n    }\n  }\n}\n\n/**\n * Check inputs provided by the user.\n *\n * Porting Note: This corresponds to _standardize_input_data() in Python\n *   Keras. Because of the strong typing in TF.js, we do not need to convert\n *   the data. Specifically:\n *   1) in PyKeras, `data` can be `DataFrame` instances from pandas, for\n *      example. We don't need to worry about that here because there is no\n *      widely popular javascript/typesdcript equivalent of pandas (so far).\n *      If one becomes available in the future, we can add support.\n *   2) in PyKeras, inputs can be Python dict. But here we are stipulating\n * that the data is either a single `tf.Tensor` or an Array of `tf.Tensor`s. We\n * may add support for `Object` data inputs in the future when the need\n * arises.\n *\n * Instead, we perform basic checks for number of parameters and shapes.\n *\n * @param data: The input data.\n * @param names: Name for the inputs, from the model.\n * @param shapes: Expected shapes for the input data, from the model.\n * @param checkBatchAxis: Whether the size along the batch axis (i.e., the\n *   first dimension) will be checked for matching.\n * @param exceptionPrefix: Execption prefix message, used in generating error\n *   messages.\n * @throws ValueError: on incorrect number of inputs or mismatches in shapes.\n */\nfunction checkInputData(\n    data: Tensor|Tensor[], names: string[], shapes?: Shape[],\n    checkBatchAxis = true, exceptionPrefix = '') {\n  let arrays: Tensor[];\n  if (Array.isArray(data)) {\n    if (data.length !== names.length) {\n      throw new ValueError(\n          `Error when checking model ${exceptionPrefix}: the Array of ` +\n          `Tensors that you are passing to your model is not the size the ` +\n          `the model expected. Expected to see ${names.length} Tensor(s),` +\n          ` but instead got ${data.length} Tensors(s).`);\n    }\n    arrays = data;\n  } else {\n    if (names.length > 1) {\n      throw new ValueError(\n          `The model expects ${names.length} ${exceptionPrefix} Tensors, ` +\n          `but only received one Tensor. Found: array with shape ` +\n          `${JSON.stringify(data.shape)}.`);\n    }\n    arrays = [data];\n  }\n\n  if (shapes != null) {\n    for (let i = 0; i < names.length; ++i) {\n      if (shapes[i] == null) {\n        continue;\n      }\n      const array = arrays[i];\n      if (array.shape.length !== shapes[i].length) {\n        throw new ValueError(\n            `Error when checking ${exceptionPrefix}: expected ${names[i]} ` +\n            `to have ${shapes[i].length} dimension(s), but got array with ` +\n            `shape ${JSON.stringify(array.shape)}`);\n      }\n      for (let j = 0; j < shapes[i].length; ++j) {\n        if (j === 0 && !checkBatchAxis) {\n          continue;\n        }\n        const dim = array.shape[j];\n        const refDim = shapes[i][j];\n        if (refDim != null) {\n          if (refDim !== dim) {\n            throw new ValueError(\n                `Error when checking ${exceptionPrefix}: expected ` +\n                `${names[i]} to have shape ${JSON.stringify(shapes[i])} but ` +\n                `got array with shape ${JSON.stringify(array.shape)}.`);\n          }\n        }\n      }\n    }\n  }\n}\n\n/**\n * Maps metric functions to model outputs.\n * @param metrics An shortcut strings name, metric function, `Array` or dict\n *   (`Object`) of metric functions.\n * @param outputNames An `Array` of the names of model outputs.\n * @returns An `Array` (one entry per model output) of `Array` of metric\n *   functions. For instance, if the model has 2 outputs, and for the first\n *   output we want to compute `binaryAccuracy` and `binaryCrossentropy`,\n *   and just `binaryAccuracy` for the second output, the `Array` would look\n *   like:\n *     `[[binaryAccuracy, binaryCrossentropy],  [binaryAccuracy]]`\n * @throws TypeError: incompatible metrics format.\n */\nexport function collectMetrics(\n    metrics: string|LossOrMetricFn|Array<string|LossOrMetricFn>|\n    {[outputName: string]: string | LossOrMetricFn},\n    outputNames: string[]): Array<Array<string|LossOrMetricFn>> {\n  if (metrics == null || Array.isArray(metrics) && metrics.length === 0) {\n    return outputNames.map(name => []);\n  }\n\n  let wrappedMetrics: Array<string|LossOrMetricFn>|\n      {[outputName: string]: string | LossOrMetricFn};\n  if (typeof metrics === 'string' || typeof metrics === 'function') {\n    wrappedMetrics = [metrics];\n  } else if (Array.isArray(metrics) || typeof metrics === 'object') {\n    wrappedMetrics = metrics as Array<string|LossOrMetricFn>|\n        {[outputName: string]: string} | {[outputName: string]: LossOrMetricFn};\n  } else {\n    throw new TypeError(\n        'Type of metrics argument not understood. Expected an string,' +\n        `function, Array, or Object, found: ${metrics}`);\n  }\n\n  if (Array.isArray(wrappedMetrics)) {\n    // We then apply all metrics to all outputs.\n    return outputNames.map(\n        name => wrappedMetrics as Array<string|LossOrMetricFn>);\n  } else {\n    // In this case, metrics is a dict.\n    const nestedMetrics: Array<Array<string|LossOrMetricFn>> = [];\n    for (const name of outputNames) {\n      let outputMetrics: string|LossOrMetricFn|Array<string|LossOrMetricFn> =\n          wrappedMetrics.hasOwnProperty(name) ? wrappedMetrics[name] : [];\n      if (!Array.isArray(outputMetrics)) {\n        outputMetrics = [outputMetrics];\n      }\n      nestedMetrics.push(outputMetrics);\n    }\n    return nestedMetrics;\n  }\n}\n\nexport interface ModelEvaluateArgs {\n  /**\n   * Batch size (Integer). If unspecified, it will default to 32.\n   */\n  batchSize?: number;\n\n  /**\n   * Verbosity mode.\n   */\n  verbose?: ModelLoggingVerbosity;\n\n  /**\n   * Tensor of weights to weight the contribution of different samples to the\n   * loss and metrics.\n   */\n  sampleWeight?: Tensor;\n\n  /**\n   * integer: total number of steps (batches of samples)\n   * before declaring the evaluation round finished. Ignored with the default\n   * value of `undefined`.\n   */\n  steps?: number;\n}\n\n/**\n * Configuration for calls to `LayersModel.compile()`.\n */\nexport interface ModelCompileArgs {\n  /**\n   * An instance of `tf.train.Optimizer` or a string name for an Optimizer.\n   */\n  optimizer: string|Optimizer;\n\n  /**\n   * Object function(s) or name(s) of object function(s).\n   * If the model has multiple outputs, you can use a different loss\n   * on each output by passing a dictionary or an Array of losses.\n   * The loss value that will be minimized by the model will then be the sum\n   * of all individual losses.\n   */\n  loss: string|string[]|{[outputName: string]: string}|LossOrMetricFn|\n      LossOrMetricFn[]|{[outputName: string]: LossOrMetricFn};\n\n  /**\n   * List of metrics to be evaluated by the model during training and testing.\n   * Typically you will use `metrics=['accuracy']`.\n   * To specify different metrics for different outputs of a multi-output\n   * model, you could also pass a dictionary.\n   */\n  metrics?: string|LossOrMetricFn|Array<string|LossOrMetricFn>|\n      {[outputName: string]: string | LossOrMetricFn};\n\n  // TODO(cais): Add lossWeights, sampleWeightMode, weightedMetrics, and\n  //   targetTensors.\n}\n\nconst LAYERS_MODEL_FORMAT_NAME = 'layers-model';\n\n/**\n * A `tf.LayersModel` is a directed, acyclic graph of `tf.Layer`s plus methods\n * for training, evaluation, prediction and saving.\n *\n * `tf.LayersModel` is the basic unit of training, inference and evaluation in\n * TensorFlow.js. To create a `tf.LayersModel`, use `tf.LayersModel`.\n *\n * See also:\n *   `tf.Sequential`, `tf.loadLayersModel`.\n *\n * @doc {heading: 'Models', subheading: 'Classes'}\n */\nexport class LayersModel extends Container implements tfc.InferenceModel {\n  // The class name is 'Model' rather than 'LayersModel' for backwards\n  // compatibility since this class name shows up in the serialization format.\n  /** @nocollapse */\n  static className = 'Model';\n  protected optimizer_: Optimizer;\n  // Whether the model instance owns the optimizer: `true` if and only if\n  // `optimizer` is created from a string parameter during `compile()` call.\n  protected isOptimizerOwned: boolean;\n\n  loss: string|string[]|{[outputName: string]: string}|LossOrMetricFn|\n      LossOrMetricFn[]|{[outputName: string]: LossOrMetricFn};\n  lossFunctions: LossOrMetricFn[];\n\n  // TODO(cais): These private variables should probably not have the string\n  //   'feed' in their names, because we are not dealing with a symbolic\n  //   backend.\n  private feedOutputShapes: Shape[];\n  private feedLossFns: LossOrMetricFn[];\n  private collectedTrainableWeights: LayerVariable[];\n  private testFunction: (data: Tensor[]) => Scalar[];\n  history: History;\n\n  // A public property that can be set by Callbacks to order early stopping\n  // during `fit()` calls.\n  protected stopTraining_: boolean;\n  protected isTraining: boolean;\n\n  metrics: string|LossOrMetricFn|Array<string|LossOrMetricFn>|\n      {[outputName: string]: string | LossOrMetricFn};\n  metricsNames: string[];\n  // Porting Note: `metrics_tensors` in PyKeras is a symbolic tensor. But given\n  //   the imperative nature of tfjs-core, `metricsTensors` is a\n  //   TypeScript function here.\n  //   Also note that due to the imperative nature of tfjs-core, `metricsTensor`\n  //   here needs an output index to keep track of which output of the\n  //   LayersModel a metric belongs to. This is unlike `metrics_tensors` in\n  //   PyKeras, which is a `list` of symbolic tensors, each of which has\n  //   implicit \"knowledge\" of the outputs it depends on.\n  metricsTensors: Array<[LossOrMetricFn, number]>;\n\n  // User defind metadata (if any).\n  private userDefinedMetadata: {};\n\n  constructor(args: ContainerArgs) {\n    super(args);\n    this.isTraining = false;\n  }\n\n  /**\n   * Print a text summary of the model's layers.\n   *\n   * The summary includes\n   * - Name and type of all layers that comprise the model.\n   * - Output shape(s) of the layers\n   * - Number of weight parameters of each layer\n   * - If the model has non-sequential-like topology, the inputs each layer\n   *   receives\n   * - The total number of trainable and non-trainable parameters of the model.\n   *\n   * ```js\n   * const input1 = tf.input({shape: [10]});\n   * const input2 = tf.input({shape: [20]});\n   * const dense1 = tf.layers.dense({units: 4}).apply(input1);\n   * const dense2 = tf.layers.dense({units: 8}).apply(input2);\n   * const concat = tf.layers.concatenate().apply([dense1, dense2]);\n   * const output =\n   *     tf.layers.dense({units: 3, activation: 'softmax'}).apply(concat);\n   *\n   * const model = tf.model({inputs: [input1, input2], outputs: output});\n   * model.summary();\n   * ```\n   *\n   * @param lineLength Custom line length, in number of characters.\n   * @param positions Custom widths of each of the columns, as either\n   *   fractions of `lineLength` (e.g., `[0.5, 0.75, 1]`) or absolute number\n   *   of characters (e.g., `[30, 50, 65]`). Each number corresponds to\n   *   right-most (i.e., ending) position of a column.\n   * @param printFn Custom print function. Can be used to replace the default\n   *   `console.log`. For example, you can use `x => {}` to mute the printed\n   *   messages in the console.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  summary(\n      lineLength?: number, positions?: number[],\n      printFn:\n          // tslint:disable-next-line:no-any\n      (message?: any, ...optionalParams: any[]) => void = console.log) {\n    if (!this.built) {\n      throw new ValueError(\n          `This model has never been called, thus its weights have not been ` +\n          `created yet. So no summary can be displayed. Build the model ` +\n          `first (e.g., by calling it on some test data).`);\n    }\n    printSummary(this, lineLength, positions, printFn);\n  }\n\n  /**\n   * Configures and prepares the model for training and evaluation.  Compiling\n   * outfits the model with an optimizer, loss, and/or metrics.  Calling `fit`\n   * or `evaluate` on an un-compiled model will throw an error.\n   *\n   * @param args a `ModelCompileArgs` specifying the loss, optimizer, and\n   * metrics to be used for fitting and evaluating this model.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  compile(args: ModelCompileArgs): void {\n    if (args.loss == null) {\n      args.loss = [];\n    }\n    this.loss = args.loss;\n\n    if (typeof args.optimizer === 'string') {\n      this.optimizer_ = optimizers.getOptimizer(args.optimizer);\n      this.isOptimizerOwned = true;\n    } else {\n      if (!(args.optimizer instanceof Optimizer)) {\n        throw new ValueError(\n            `User-defined optimizer must be an instance of tf.Optimizer.`);\n      }\n      this.optimizer_ = args.optimizer;\n      this.isOptimizerOwned = false;\n    }\n\n    // TODO(cais): Add lossWeights.\n    // TODO(cais): Add sampleWeightMode.\n\n    // Prepare loss functions.\n    let lossFunctions: LossOrMetricFn[] = [];\n    if (!Array.isArray(args.loss) && typeof args.loss !== 'string' &&\n        typeof args.loss !== 'function') {\n      args.loss = args.loss as {[outputName: string]: string};\n      for (const name in args.loss) {\n        if (this.outputNames.indexOf(name) === -1) {\n          throw new ValueError(\n              `Unknown entry in loss dictionary: \"${name}\". ` +\n              `Only expected the following keys: ${this.outputNames}`);\n        }\n      }\n      for (const name of this.outputNames) {\n        if (args.loss[name] == null) {\n          console.warn(\n              `Output \"${name}\" is missing from loss dictionary. We assume ` +\n              `this was done on purpose, and we will not be expecting data ` +\n              `to be passed to ${name} during training`);\n        }\n        lossFunctions.push(losses.get(args.loss[name]));\n      }\n    } else if (Array.isArray(args.loss)) {\n      if (args.loss.length !== this.outputs.length) {\n        throw new ValueError(\n            `When passing an Array as loss, it should have one entry per ` +\n            `model output. The model has ${this.outputs.length} output(s), ` +\n            `but you passed loss=${args.loss}.`);\n      }\n      const theLosses = args.loss as Array<string|LossOrMetricFn>;\n      lossFunctions = theLosses.map(l => losses.get(l));\n    } else {\n      const lossFunction = losses.get(args.loss);\n      this.outputs.forEach(_ => {\n        lossFunctions.push(lossFunction);\n      });\n    }\n\n    this.lossFunctions = lossFunctions;\n\n    this.feedOutputNames = [];\n    this.feedOutputShapes = [];\n    this.feedLossFns = [];\n    for (let i = 0; i < this.outputs.length; ++i) {\n      // TODO(cais): Logic for skipping target(s).\n      const shape = this.internalOutputShapes[i];\n      const name = this.outputNames[i];\n      this.feedOutputNames.push(name);\n      this.feedOutputShapes.push(shape);\n      this.feedLossFns.push(this.lossFunctions[i]);\n    }\n\n    // TODO(cais): Add logic for output masks.\n    // TODO(cais): Add logic for sample weights.\n    const skipTargetIndices: number[] = [];\n\n    // Prepare metrics.\n    this.metrics = args.metrics;\n    // TODO(cais): Add weightedMetrics.\n    this.metricsNames = ['loss'];\n    this.metricsTensors = [];\n\n    // Compute total loss.\n    // Porting Note: In PyKeras, metrics_tensors are symbolic tensor objects.\n    //   Here, metricsTensors are TypeScript functions. This difference is due\n    //   to the difference in symbolic/imperative property of the backends.\n    nameScope('loss', () => {\n      for (let i = 0; i < this.outputs.length; ++i) {\n        if (skipTargetIndices.indexOf(i) !== -1) {\n          continue;\n        }\n        // TODO(cais): Add weightedLoss, sampleWeight and mask.\n        //   The following line should be weightedLoss\n        const weightedLoss = this.lossFunctions[i];\n        if (this.outputs.length > 1) {\n          this.metricsTensors.push([weightedLoss, i]);\n          this.metricsNames.push(this.outputNames[i] + '_loss');\n        }\n      }\n\n      // Porting Note: Due to the imperative nature of the backend, we calculate\n      //   the regularizer penalties in the totalLossFunction, instead of here.\n    });\n\n    const nestedMetrics = collectMetrics(args.metrics, this.outputNames);\n    // TODO(cais): Add nestedWeightedMetrics.\n\n    /**\n     * Helper function used in loop below.\n     */\n    const appendMetric =\n        (outputIndex: number, metricName: string,\n         metricTensor: LossOrMetricFn) => {\n          if (this.outputNames.length > 1) {\n            metricName = this.outputNames[outputIndex] + '_' + metricName;\n          }\n          this.metricsNames.push(metricName);\n          this.metricsTensors.push([metricTensor, outputIndex]);\n        };\n\n    nameScope('metric', () => {\n      for (let i = 0; i < this.outputs.length; ++i) {\n        if (skipTargetIndices.indexOf(i) !== -1) {\n          continue;\n        }\n        const outputMetrics = nestedMetrics[i];\n        // TODO(cais): Add weights and outputWeightedMetrics.\n\n        // TODO(cais): Add optional arg `weights` to the following function.\n        const handleMetrics = (metrics: Array<string|LossOrMetricFn>) => {\n          const metricNamePrefix = '';\n          let metricName: string;\n          let accFn: LossOrMetricFn;\n          let weightedMetricFn: LossOrMetricFn;\n          //  TODO(cais): Use 'weights_' for weighted metrics.\n\n          for (const metric of metrics) {\n            if (typeof metric === 'string' &&\n                ['accuracy', 'acc', 'crossentropy', 'ce'].indexOf(metric) !==\n                    -1) {\n              const outputShape = this.internalOutputShapes[i];\n\n              if (outputShape[outputShape.length - 1] === 1 ||\n                  this.lossFunctions[i] === losses.binaryCrossentropy) {\n                // case: binary accuracy/crossentropy.\n                if (['accuracy', 'acc'].indexOf(metric) !== -1) {\n                  accFn = Metrics.binaryAccuracy;\n                } else if (['crossentropy', 'ce'].indexOf(metric) !== -1) {\n                  accFn = Metrics.binaryCrossentropy;\n                }\n              } else if (\n                  this.lossFunctions[i] ===\n                  losses.sparseCategoricalCrossentropy) {\n                // case: categorical accuracy / crossentropy with sparse\n                // targets.\n                if (['accuracy', 'acc'].indexOf(metric) !== -1) {\n                  accFn = Metrics.sparseCategoricalAccuracy;\n                } else if (['crossentropy', 'ce'].indexOf(metric) !== -1) {\n                  accFn = Metrics.sparseCategoricalCrossentropy;\n                }\n              } else {\n                // case: categorical accuracy / crossentropy.\n                if (['accuracy', 'acc'].indexOf(metric) !== -1) {\n                  accFn = Metrics.categoricalAccuracy;\n                } else if (['crossentropy', 'ce'].indexOf(metric) !== -1) {\n                  accFn = Metrics.categoricalCrossentropy;\n                }\n              }\n              let suffix: string;\n              if (['accuracy', 'acc'].indexOf(metric) !== -1) {\n                suffix = 'acc';\n              } else if (['crossentropy', 'ce'].indexOf(metric) !== -1) {\n                suffix = 'ce';\n              }\n              // TODO(cais): Add weighting actually.\n              weightedMetricFn = accFn;\n              metricName = metricNamePrefix + suffix;\n            } else {\n              const metricFn = Metrics.get(metric);\n              // TODO(cais): Add weighting actually.\n              weightedMetricFn = metricFn;\n              metricName =\n                  metricNamePrefix + Metrics.getLossOrMetricName(metric);\n            }\n\n            // TODO(cais): Add weighting and masking to metricResult.\n            let metricResult: LossOrMetricFn;\n            nameScope(metricName, () => {\n              metricResult = weightedMetricFn;\n            });\n            appendMetric(i, metricName, metricResult);\n          }\n        };\n\n        handleMetrics(outputMetrics);\n        // TODO(cais): Call handleMetrics with weights.\n      }\n    });\n\n    // Porting Notes: Given the imperative backend of tfjs-core,\n    //   there is no need for constructing the symbolic graph and placeholders.\n    this.collectedTrainableWeights = this.trainableWeights;\n  }\n\n  /**\n   * Check trainable weights count consistency.\n   *\n   * This will raise a warning if `this.trainableWeights` and\n   * `this.collectedTrainableWeights` are inconsistent (i.e., have different\n   * numbers of parameters).\n   * Inconsistency will typically arise when one modifies `model.trainable`\n   * without calling `model.compile()` again.\n   */\n  protected checkTrainableWeightsConsistency(): void {\n    if (this.collectedTrainableWeights == null) {\n      return;\n    }\n    if (this.trainableWeights.length !==\n        this.collectedTrainableWeights.length) {\n      console.warn(\n          'Discrepancy between trainableweights and collected trainable ' +\n          'weights. Did you set `model.trainable` without calling ' +\n          '`model.compile()` afterwards?');\n    }\n  }\n\n  /**\n   * Returns the loss value & metrics values for the model in test mode.\n   *\n   * Loss and metrics are specified during `compile()`, which needs to happen\n   * before calls to `evaluate()`.\n   *\n   * Computation is done in batches.\n   *\n   * ```js\n   * const model = tf.sequential({\n   *   layers: [tf.layers.dense({units: 1, inputShape: [10]})]\n   * });\n   * model.compile({optimizer: 'sgd', loss: 'meanSquaredError'});\n   * const result = model.evaluate(\n   *     tf.ones([8, 10]), tf.ones([8, 1]), {batchSize: 4});\n   * result.print();\n   * ```\n   *\n   * @param x `tf.Tensor` of test data, or an `Array` of `tf.Tensor`s if the\n   * model has multiple inputs.\n   * @param y `tf.Tensor` of target data, or an `Array` of `tf.Tensor`s if the\n   * model has multiple outputs.\n   * @param args A `ModelEvaluateArgs`, containing optional fields.\n   *\n   * @return `Scalar` test loss (if the model has a single output and no\n   *   metrics) or `Array` of `Scalar`s (if the model has multiple outputs\n   *   and/or metrics). The attribute `model.metricsNames`\n   *   will give you the display labels for the scalar outputs.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  evaluate(\n      x: Tensor|Tensor[], y: Tensor|Tensor[],\n      args: ModelEvaluateArgs = {}): Scalar|Scalar[] {\n    const batchSize = args.batchSize == null ? 32 : args.batchSize;\n    checkBatchSize(batchSize);\n\n    // TODO(cais): Standardize `config.sampleWeights` as well.\n    // Validate user data.\n    const checkBatchAxis = true;\n    const standardizedOuts =\n        this.standardizeUserDataXY(x, y, checkBatchAxis, batchSize);\n    try {\n      // TODO(cais): If uses `useLearningPhase`, set the corresponding element\n      // of the input to 0.\n      const ins = standardizedOuts[0].concat(standardizedOuts[1]);\n      this.makeTestFunction();\n      const f = this.testFunction;\n      const testOuts =\n          this.testLoop(f, ins, batchSize, args.verbose, args.steps);\n      return singletonOrArray(testOuts);\n    } finally {\n      disposeNewTensors(standardizedOuts[0], x);\n      disposeNewTensors(standardizedOuts[1], y);\n    }\n  }\n\n  // TODO(cais): Add code snippet below once real dataset objects are\n  //   available.\n  /**\n   * Evaluate model using a dataset object.\n   *\n   * Note: Unlike `evaluate()`, this method is asynchronous (`async`).\n   *\n   * @param dataset A dataset object. Its `iterator()` method is expected\n   *   to generate a dataset iterator object, the `next()` method of which\n   *   is expected to produce data batches for evaluation. The return value\n   *   of the `next()` call ought to contain a boolean `done` field and a\n   *   `value` field. The `value` field is expected to be an array of two\n   *   `tf.Tensor`s or an array of two nested `tf.Tensor` structures. The former\n   *   case is for models with exactly one input and one output (e.g.\n   *   a sequential model). The latter case is for models with multiple\n   *   inputs and/or multiple outputs. Of the two items in the array, the\n   *   first is the input feature(s) and the second is the output target(s).\n   * @param args A configuration object for the dataset-based evaluation.\n   * @returns Loss and metric values as an Array of `Scalar` objects.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  async evaluateDataset(dataset: Dataset<{}>, args?: ModelEvaluateDatasetArgs):\n      Promise<Scalar|Scalar[]> {\n    this.makeTestFunction();\n    return evaluateDataset(this, dataset, args);\n  }\n\n  /**\n   * Get number of samples provided for training, evaluation or prediction.\n   *\n   * @param ins Input `tf.Tensor`.\n   * @param batchSize Integer batch size, optional.\n   * @param steps Total number of steps (batches of samples) before\n   * declaring loop finished. Optional.\n   * @param stepsName The public API's parameter name for `steps`.\n   * @returns Number of samples provided.\n   */\n  private checkNumSamples(\n      ins: Tensor|Tensor[], batchSize?: number, steps?: number,\n      stepsName = 'steps'): number {\n    let numSamples: number;\n    if (steps != null) {\n      numSamples = null;\n      if (batchSize != null) {\n        throw new ValueError(\n            `If ${stepsName} is set, batchSize must be null or undefined.` +\n            `Got batchSize = ${batchSize}`);\n      }\n    } else if (ins != null) {\n      if (Array.isArray(ins)) {\n        numSamples = ins[0].shape[0];\n      } else {\n        numSamples = ins.shape[0];\n      }\n    } else {\n      throw new ValueError(\n          `Either the input data should have a defined shape, or ` +\n          `${stepsName} shoud be specified.`);\n    }\n    return numSamples;\n  }\n\n  /**\n   * Execute internal tensors of the model with input data feed.\n   * @param inputs Input data feed. Must match the inputs of the model.\n   * @param outputs Names of the output tensors to be fetched. Must match\n   *   names of the SymbolicTensors that belong to the graph.\n   * @returns Fetched values for `outputs`.\n   */\n  execute(inputs: Tensor|Tensor[]|NamedTensorMap, outputs: string|string[]):\n      Tensor|Tensor[] {\n    if (Array.isArray(outputs) && outputs.length === 0) {\n      throw new ValueError(\n          '`outputs` is an empty Array, which is not allowed.');\n    }\n\n    const outputsIsArray = Array.isArray(outputs);\n    const outputNames =\n        (outputsIsArray ? outputs : [outputs]);\n    const outputSymbolicTensors = this.retrieveSymbolicTensors(outputNames);\n\n    // Format the input into a FeedDict.\n    const feedDict = new FeedDict();\n    if (inputs instanceof Tensor) {\n      inputs = [inputs];\n    }\n    if (Array.isArray(inputs)) {\n      if (inputs.length !== this.inputs.length) {\n        throw new ValueError(\n            `The number of inputs provided (${inputs.length}) ` +\n            `does not match the number of inputs of this model ` +\n            `(${this.inputs.length}).`);\n      }\n      for (let i = 0; i < this.inputs.length; ++i) {\n        feedDict.add(this.inputs[i], inputs[i]);\n      }\n    } else {\n      for (const input of this.inputs) {\n        const tensorValue = inputs[input.name];\n        if (tensorValue == null) {\n          throw new ValueError(\n              `No value is provided for the model's input ${input.name}`);\n        }\n        feedDict.add(input, tensorValue);\n      }\n    }\n\n    // Run execution.\n    const executeOutputs = execute(outputSymbolicTensors, feedDict) as Tensor[];\n    return outputsIsArray ? executeOutputs : executeOutputs[0];\n  }\n\n  /**\n   * Retrieve the model's internal symbolic tensors from symbolic-tensor names.\n   */\n  private retrieveSymbolicTensors(symbolicTensorNames: string[]):\n      SymbolicTensor[] {\n    const outputSymbolicTensors: SymbolicTensor[] =\n        pyListRepeat(null, symbolicTensorNames.length);\n    let outputsRemaining = symbolicTensorNames.length;\n    for (const layer of this.layers) {\n      const layerOutputs: SymbolicTensor[] =\n          Array.isArray(layer.output) ? layer.output : [layer.output];\n      const layerOutputNames = layerOutputs.map(output => output.name);\n      for (let i = 0; i < symbolicTensorNames.length; ++i) {\n        const index = layerOutputNames.indexOf(symbolicTensorNames[i]);\n        if (index !== -1) {\n          outputSymbolicTensors[i] = layerOutputs[index];\n          outputsRemaining--;\n        }\n        if (outputsRemaining === 0) {\n          break;\n        }\n      }\n      if (outputsRemaining === 0) {\n        break;\n      }\n    }\n\n    if (outputsRemaining > 0) {\n      const remainingNames: string[] = [];\n      outputSymbolicTensors.forEach((tensor, i) => {\n        if (tensor == null) {\n          remainingNames.push(symbolicTensorNames[i]);\n        }\n      });\n      throw new ValueError(\n          `Cannot find SymbolicTensors for output name(s): ` +\n          `${JSON.stringify(remainingNames)}`);\n    }\n    return outputSymbolicTensors;\n  }\n\n  /**\n   * Helper method to loop over some data in batches.\n   *\n   * Porting Note: Not using the functional approach in the Python equivalent\n   *   due to the imperative backend.\n   * Porting Note: Does not support step mode currently.\n   *\n   * @param ins: input data\n   * @param batchSize: integer batch size.\n   * @param verbose: verbosity model\n   * @returns: Predictions as `tf.Tensor` (if a single output) or an `Array` of\n   *   `tf.Tensor` (if multipe outputs).\n   */\n  private predictLoop(ins: Tensor|Tensor[], batchSize = 32, verbose = false):\n      Tensor|Tensor[] {\n    return tfc.tidy(() => {\n      const numSamples = this.checkNumSamples(ins);\n      if (verbose) {\n        throw new NotImplementedError(\n            'Verbose predictLoop() is not implemented yet.');\n      }\n\n      // Sample-based predictions.\n      // Porting Note: Tensor currently does not support sliced assignments as\n      //   in numpy, e.g., x[1:3] = y. Therefore we use concatenation while\n      //   iterating over the batches.\n\n      const batches = makeBatches(numSamples, batchSize);\n      const outsBatches: Tensor[][] = this.outputs.map(output => []);\n\n      // TODO(cais): Can the scope() be pushed down inside the for loop?\n      for (let batchIndex = 0; batchIndex < batches.length; ++batchIndex) {\n        const batchOuts = tfc.tidy(() => {\n          const batchStart = batches[batchIndex][0];\n          const batchEnd = batches[batchIndex][1];\n          // TODO(cais): Take care of the case of the last element is a flag for\n          //   training/test.\n          const insBatch = sliceArrays(ins, batchStart, batchEnd);\n\n          // Construct the feeds for execute();\n          const feeds = [];\n          if (Array.isArray(insBatch)) {\n            for (let i = 0; i < insBatch.length; ++i) {\n              feeds.push({key: this.inputs[i], value: insBatch[i]});\n            }\n          } else {\n            feeds.push({key: this.inputs[0], value: insBatch});\n          }\n          const feedDict = new FeedDict(feeds);\n          return execute(this.outputs, feedDict) as Tensor[];\n        });\n        batchOuts.forEach((batchOut, i) => outsBatches[i].push(batchOut));\n      }\n      return singletonOrArray(\n          outsBatches.map(batches => tfc.concat(batches, 0)));\n    });\n  }\n\n  /**\n   * Generates output predictions for the input samples.\n   *\n   * Computation is done in batches.\n   *\n   * Note: the \"step\" mode of predict() is currently not supported.\n   *   This is because the TensorFlow.js core backend is imperative only.\n   *\n   * ```js\n   * const model = tf.sequential({\n   *   layers: [tf.layers.dense({units: 1, inputShape: [10]})]\n   * });\n   * model.predict(tf.ones([8, 10]), {batchSize: 4}).print();\n   * ```\n   *\n   * @param x The input data, as a Tensor, or an `Array` of `tf.Tensor`s if\n   *   the model has multiple inputs.\n   * @param args A `ModelPredictArgs` object containing optional fields.\n   *\n   * @return Prediction results as a `tf.Tensor`(s).\n   *\n   * @exception ValueError In case of mismatch between the provided input data\n   *   and the model's expectations, or in case a stateful model receives a\n   *   number of samples that is not a multiple of the batch size.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  predict(x: Tensor|Tensor[], args: ModelPredictArgs = {}): Tensor|Tensor[] {\n    const xsRank2OrHigher = ensureTensorsRank2OrHigher(x);\n    checkInputData(\n        xsRank2OrHigher, this.inputNames, this.feedInputShapes, false);\n    try {\n      // TODO(cais): Take care of stateful models.\n      //   if (this.stateful) ...\n      // TODO(cais): Take care of the learning_phase boolean flag.\n      //   if (this.useLearningPhase) ...\n      const batchSize = args.batchSize == null ? 32 : args.batchSize;\n      checkBatchSize(batchSize);\n      return this.predictLoop(xsRank2OrHigher, batchSize);\n    } finally {\n      disposeNewTensors(xsRank2OrHigher, x);\n    }\n  }\n\n  /**\n   * Returns predictions for a single batch of samples.\n   *\n   * ```js\n   * const model = tf.sequential({\n   *   layers: [tf.layers.dense({units: 1, inputShape: [10]})]\n   * });\n   * model.predictOnBatch(tf.ones([8, 10])).print();\n   * ```\n   * @param x: Input samples, as a Tensor (for models with exactly one\n   *   input) or an array of Tensors (for models with more than one input).\n   * @return Tensor(s) of predictions\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  predictOnBatch(x: Tensor|Tensor[]): Tensor|Tensor[] {\n    checkInputData(x, this.inputNames, this.feedInputShapes, true);\n    // TODO(cais): Take care of the learning_phase boolean flag.\n    //   if (this.useLearningPhase) ...\n    const batchSize = (Array.isArray(x) ? x[0] : x).shape[0];\n    return this.predictLoop(x, batchSize);\n  }\n\n  protected standardizeUserDataXY(\n      x: Tensor|Tensor[]|{[inputName: string]: Tensor},\n      y: Tensor|Tensor[]|{[inputName: string]: Tensor}, checkBatchAxis = true,\n      batchSize?: number): [Tensor[], Tensor[]] {\n    // TODO(cais): Add sampleWeight, classWeight\n    if (this.optimizer_ == null) {\n      throw new RuntimeError(\n          'You must compile a model before training/testing. Use ' +\n          'LayersModel.compile(modelCompileArgs).');\n    }\n    const outputShapes: Shape[] = [];\n    for (let i = 0; i < this.feedOutputShapes.length; ++i) {\n      const outputShape = this.feedOutputShapes[i];\n      const lossFn = this.feedLossFns[i];\n      if (lossFn === losses.sparseCategoricalCrossentropy) {\n        outputShapes.push(\n            outputShape.slice(0, outputShape.length - 1).concat([1]));\n      } else {\n        // Porting Note: Because of strong typing `lossFn` must be a function.\n        outputShapes.push(outputShape);\n      }\n    }\n    x = standardizeInputData(\n        x, this.feedInputNames, this.feedInputShapes, false, 'input');\n    y = standardizeInputData(\n        y, this.feedOutputNames, outputShapes, false, 'target');\n    // TODO(cais): Standardize sampleWeights & classWeights.\n    checkArrayLengths(x, y, null);\n    // TODO(cais): Check sampleWeights as well.\n    checkLossAndTargetCompatibility(y, this.feedLossFns, this.feedOutputShapes);\n    if (this.stateful && batchSize != null && batchSize > 0) {\n      if (x[0].shape[0] % batchSize !== 0) {\n        throw new ValueError(\n            `In a stateful network, you should only pass inputs with a ` +\n            `number of samples that is divisible by the batch size ` +\n            `${batchSize}. Found: ${x[0].shape[0]} sample(s).`);\n      }\n    }\n    return [x, y];\n  }\n\n  protected async standardizeUserData(\n      x: Tensor|Tensor[]|{[inputName: string]: Tensor},\n      y: Tensor|Tensor[]|{[inputName: string]: Tensor},\n      sampleWeight?: Tensor|Tensor[]|{[outputName: string]: Tensor},\n      classWeight?: ClassWeight|ClassWeight[]|ClassWeightMap,\n      checkBatchAxis = true,\n      batchSize?: number): Promise<[Tensor[], Tensor[], Tensor[]]> {\n    const [standardXs, standardYs] =\n        this.standardizeUserDataXY(x, y, checkBatchAxis, batchSize);\n    // TODO(cais): Handle sampleWeights.\n    if (sampleWeight != null) {\n      throw new Error('sample weight is not supported yet.');\n    }\n\n    let standardSampleWeights: Tensor[] = null;\n    if (classWeight != null) {\n      const classWeights =\n          standardizeClassWeights(classWeight, this.outputNames);\n      standardSampleWeights = [];\n      for (let i = 0; i < classWeights.length; ++i) {\n        standardSampleWeights.push(\n            await standardizeWeights(standardYs[i], null, classWeights[i]));\n      }\n    }\n\n    // TODO(cais): Deal with the case of model.stateful == true.\n    return [standardXs, standardYs, standardSampleWeights];\n  }\n\n  /**\n   * Loop over some test data in batches.\n   * @param f A Function returning a list of tensors.\n   * @param ins Array of tensors to be fed to `f`.\n   * @param batchSize Integer batch size or `null` / `undefined`.\n   * @param verbose verbosity mode.\n   * @param steps Total number of steps (batches of samples) before\n   * declaring test finished. Ignored with the default value of `null` /\n   * `undefined`.\n   * @returns Array of Scalars.\n   */\n  private testLoop(\n      f: (data: Tensor[]) => Scalar[], ins: Tensor[], batchSize?: number,\n      verbose = 0, steps?: number): Scalar[] {\n    return tfc.tidy(() => {\n      const numSamples = this.checkNumSamples(ins, batchSize, steps, 'steps');\n      const outs: Scalar[] = [];\n      if (verbose > 0) {\n        throw new NotImplementedError('Verbose mode is not implemented yet.');\n      }\n      // TODO(cais): Use `indicesForConversionToDense' to prevent slow down.\n      if (steps != null) {\n        throw new NotImplementedError(\n            'steps mode in testLoop() is not implemented yet');\n      } else {\n        const batches = makeBatches(numSamples, batchSize);\n        const indexArray = tensor1d(range(0, numSamples));\n        for (let batchIndex = 0; batchIndex < batches.length; ++batchIndex) {\n          const batchStart = batches[batchIndex][0];\n          const batchEnd = batches[batchIndex][1];\n          const batchIds =\n              K.sliceAlongFirstAxis(\n                  indexArray, batchStart, batchEnd - batchStart) as Tensor1D;\n          // TODO(cais): In ins, train flag can be a number, instead of an\n          //   Tensor? Do we need to handle this in tfjs-layers?\n          const insBatch = sliceArraysByIndices(ins, batchIds) as Scalar[];\n          const batchOuts = f(insBatch);\n          if (batchIndex === 0) {\n            for (let i = 0; i < batchOuts.length; ++i) {\n              outs.push(scalar(0));\n            }\n          }\n          for (let i = 0; i < batchOuts.length; ++i) {\n            const batchOut = batchOuts[i];\n            outs[i] =\n                tfc.add(outs[i], tfc.mul(batchEnd - batchStart, batchOut));\n          }\n        }\n        for (let i = 0; i < outs.length; ++i) {\n          outs[i] = tfc.div(outs[i], numSamples);\n        }\n      }\n      return outs;\n    });\n  }\n\n  protected getDedupedMetricsNames(): string[] {\n    const outLabels = this.metricsNames;\n    // Rename duplicated metrics names (can happen with an output layer\n    // shared among multiple dataflows).\n    const dedupedOutLabels = [];\n    for (let i = 0; i < outLabels.length; ++i) {\n      const label = outLabels[i];\n      let newLabel = label;\n      if (count(outLabels, label) > 1) {\n        const dupIndex = count(outLabels.slice(0, i), label);\n        newLabel += `_${dupIndex}`;\n      }\n      dedupedOutLabels.push(newLabel);\n    }\n    return dedupedOutLabels;\n  }\n\n  /**\n   * Creates a function that performs the following actions:\n   *\n   * 1. computes the losses\n   * 2. sums them to get the total loss\n   * 3. call the optimizer computes the gradients of the LayersModel's\n   *    trainable weights w.r.t. the total loss and update the variables\n   * 4. calculates the metrics\n   * 5. returns the values of the losses and metrics.\n   */\n  protected makeTrainFunction(): (data: Tensor[]) => Scalar[] {\n    return (data: Tensor[]) => {\n      const lossValues: Scalar[] = [];\n\n      const inputs = data.slice(0, this.inputs.length);\n      const targets = data.slice(\n          this.inputs.length, this.inputs.length + this.outputs.length);\n      const sampleWeights = data.slice(\n          this.inputs.length + this.outputs.length,\n          this.inputs.length + this.outputs.length * 2);\n\n      const metricsValues: Scalar[] = [];\n\n      // Create a function that computes the total loss based on the\n      // inputs. This function is used for obtaining gradients through\n      // backprop.\n      const totalLossFunction = () => {\n        const feeds = [];\n        for (let i = 0; i < this.inputs.length; ++i) {\n          feeds.push({key: this.inputs[i], value: inputs[i]});\n        }\n        const feedDict = new FeedDict(feeds);\n        const outputs =\n            execute(this.outputs, feedDict, {'training': true}) as Tensor[];\n        // TODO(cais): Take care of the case of multiple outputs from a\n        //   single layer?\n\n        let totalLoss: Tensor;\n        for (let i = 0; i < this.lossFunctions.length; ++i) {\n          const lossFunction = this.lossFunctions[i];\n          let loss = lossFunction(targets[i], outputs[i]);\n          if (sampleWeights[i] != null) {\n            loss = computeWeightedLoss(loss, sampleWeights[i]);\n          }\n\n          // TODO(cais): push Scalar instead.\n          const meanLoss: Scalar = tfc.mean(loss);\n          // TODO(cais): Use a scope() instead, to avoid ownership.\n          lossValues.push(meanLoss);\n          if (i === 0) {\n            totalLoss = loss;\n          } else {\n            totalLoss = tfc.add(totalLoss, loss);\n          }\n        }\n\n        // Compute the metrics.\n        // TODO(cais): These should probably be calculated outside\n        //   totalLossFunction to benefit speed?\n        for (let i = 0; i < this.metricsTensors.length; ++i) {\n          let weightedMetric: Scalar;\n\n          if (this.outputs.length > 1 && i < this.outputs.length) {\n            weightedMetric = lossValues[i];\n          } else {\n            const metric = this.metricsTensors[i][0];\n            const outputIndex = this.metricsTensors[i][1];\n            weightedMetric =\n                tfc.mean(metric(targets[outputIndex], outputs[outputIndex]));\n          }\n\n          tfc.keep(weightedMetric);\n          // TODO(cais): Use a scope() instead, to avoid ownership.\n          metricsValues.push(weightedMetric);\n        }\n\n        totalLoss = tfc.mean(totalLoss);\n\n        // Add regularizer penalties.\n        this.calculateLosses().forEach(regularizerLoss => {\n          totalLoss = tfc.add(totalLoss, regularizerLoss);\n        });\n\n        return totalLoss as Scalar;\n      };\n\n      const variables = this.collectedTrainableWeights.map(\n          param => param.read() as tfc.Variable);\n      const returnCost = true;\n      const totalLossValue =\n          this.optimizer_.minimize(totalLossFunction, returnCost, variables);\n\n      return [totalLossValue].concat(metricsValues);\n    };\n  }\n\n  /**\n   * Create a function which, when invoked with an array of `tf.Tensor`s as a\n   * batch of inputs, returns the prespecified loss and metrics of the model\n   * under the batch of input data.\n   */\n  private makeTestFunction() {\n    this.testFunction = (data: Tensor[]) => {\n      return tfc.tidy(() => {\n        const valOutputs: Scalar[] = [];\n        let totalLoss: Scalar;\n        const inputs = data.slice(0, this.inputs.length);\n        const targets = data.slice(\n            this.inputs.length, this.inputs.length + this.outputs.length);\n        const feeds = [];\n        for (let i = 0; i < this.inputs.length; ++i) {\n          feeds.push({key: this.inputs[i], value: inputs[i]});\n        }\n        const feedDict = new FeedDict(feeds);\n        const outputs = execute(this.outputs, feedDict) as Tensor[];\n        // Compute total loss.\n        for (let i = 0; i < this.lossFunctions.length; ++i) {\n          const lossFunction = this.lossFunctions[i];\n          // TODO(cais): Add sample weighting and replace the simple\n          // averaging.\n          const loss: Scalar = tfc.mean(lossFunction(targets[i], outputs[i]));\n          if (i === 0) {\n            totalLoss = loss;\n          } else {\n            totalLoss = tfc.add(totalLoss, loss);\n          }\n          valOutputs.push(totalLoss);\n        }\n        // Compute the metrics.\n        for (let i = 0; i < this.metricsTensors.length; ++i) {\n          const metric = this.metricsTensors[i][0];\n          const outputIndex = this.metricsTensors[i][1];\n          // TODO(cais): Replace K.mean() with a proper weighting function.\n          const meanMetric =\n              tfc.mean(metric(targets[outputIndex], outputs[outputIndex]));\n          valOutputs.push(meanMetric as Scalar);\n        }\n        return valOutputs;\n      });\n    };\n  }\n\n  /**\n   * Trains the model for a fixed number of epochs (iterations on a\n   * dataset).\n   *\n   * ```js\n   * const model = tf.sequential({\n   *     layers: [tf.layers.dense({units: 1, inputShape: [10]})]\n   * });\n   * model.compile({optimizer: 'sgd', loss: 'meanSquaredError'});\n   * for (let i = 1; i < 5 ; ++i) {\n   *   const h = await model.fit(tf.ones([8, 10]), tf.ones([8, 1]), {\n   *       batchSize: 4,\n   *       epochs: 3\n   *   });\n   *   console.log(\"Loss after Epoch \" + i + \" : \" + h.history.loss[0]);\n   * }\n   * ```\n   *\n   * @param x `tf.Tensor` of training data, or an array of `tf.Tensor`s if the\n   * model has multiple inputs. If all inputs in the model are named, you\n   * can also pass a dictionary mapping input names to `tf.Tensor`s.\n   * @param y `tf.Tensor` of target (label) data, or an array of `tf.Tensor`s if\n   * the model has multiple outputs. If all outputs in the model are named,\n   * you can also pass a dictionary mapping output names to `tf.Tensor`s.\n   * @param args A `ModelFitArgs`, containing optional fields.\n   *\n   * @return A `History` instance. Its `history` attribute contains all\n   *   information collected during training.\n   *\n   * @exception ValueError In case of mismatch between the provided input\n   * data and what the model expects.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  async fit(\n      x: Tensor|Tensor[]|{[inputName: string]: Tensor},\n      y: Tensor|Tensor[]|{[inputName: string]: Tensor},\n      args: ModelFitArgs = {}): Promise<History> {\n    if (this.isTraining) {\n      throw new Error(\n          'Cannot start training because another fit() call is ongoing.');\n    }\n    this.isTraining = true;\n    let inputs: Tensor[];\n    let targets: Tensor[];\n    let originalInputs: Tensor[];\n    let originalTargets: Tensor[];\n    let inputValX: Tensor|Tensor[];\n    let inputValY: Tensor|Tensor[];\n    let valX: Tensor|Tensor[];\n    let valY: Tensor|Tensor[];\n    let sampleWeights: Tensor[];\n    try {\n      const batchSize = args.batchSize == null ? 32 : args.batchSize;\n      checkBatchSize(batchSize);\n\n      // Validate user data.\n      // TODO(cais): Support sampleWeight.\n      const checkBatchAxis = false;\n      const standardizedOuts =\n          await this.standardizeUserData(\n              x, y, args.sampleWeight, args.classWeight, checkBatchAxis,\n              batchSize) as [Tensor[], Tensor[], Tensor[]];\n      inputs = standardizedOuts[0];\n      targets = standardizedOuts[1];\n      sampleWeights = standardizedOuts[2];\n\n      // Prepare validation data.\n      let doValidation = false;\n      let valIns: Tensor[];\n      if (args.validationData != null && args.validationData.length > 0) {\n        doValidation = true;\n        if (args.validationData.length === 2) {\n          // config.validationData consists of valX and valY.\n          inputValX = args.validationData[0];\n          inputValY = args.validationData[1];\n        } else if (args.validationData.length === 3) {\n          throw new NotImplementedError(\n              'validationData including sample weights is not supported yet.');\n        } else {\n          throw new ValueError(\n              `When passing validation data, it must contain 2 (valX, valY) ` +\n              `or 3 (valX, valY, valSampleWeight) items; ` +\n              `${args.validationData} is invalid.`);\n        }\n\n        const checkBatchAxis = true;\n        const valStandardized =\n            await this.standardizeUserData(\n                inputValX, inputValY, null, /** Unused sample weights. */\n                null,                       /** Unused class weights. */\n                checkBatchAxis, batchSize) as [Tensor[], Tensor[], Tensor[]];\n        valX = valStandardized[0];\n        valY = valStandardized[1];\n        valIns = valX.concat(valY);\n        // TODO(cais): Add useLearningPhase data properly.\n      } else if (\n          args.validationSplit != null && args.validationSplit > 0 &&\n          args.validationSplit < 1) {\n        doValidation = true;\n        // Porting Note: In tfjs-layers, inputs[0] is always a Tensor.\n        const splitAt =\n            Math.floor(inputs[0].shape[0] * (1 - args.validationSplit));\n        const originalBatchSize = inputs[0].shape[0];\n        valX = sliceArrays(inputs, splitAt, originalBatchSize) as Tensor[];\n        originalInputs = inputs;\n        inputs = sliceArrays(inputs, 0, splitAt) as Tensor[];\n        valY = sliceArrays(targets, splitAt, originalBatchSize) as Tensor[];\n        originalTargets = targets;\n        targets = sliceArrays(targets, 0, splitAt) as Tensor[];\n        // TODO(cais): Once sampleWeights becomes available, slice it to get\n        //   valSampleWeights.\n        valIns = valX.concat(valY);\n\n        // TODO(cais): Add useLearningPhase data properly.\n      } else if (args.validationSteps != null) {\n        doValidation = true;\n        // TODO(cais): Add useLearningPhase.\n      }\n\n      const ins = inputs.concat(targets).concat(sampleWeights);\n\n      this.checkTrainableWeightsConsistency();\n\n      // TODO(cais): Handle use_learning_phase and learning_phase?\n\n      // Porting Note: Here we see a key deviation of tfjs-layers from\n      // Keras.\n      //  Due to the imperative nature of tfjs-layers' backend (tfjs-core),\n      //  we do not construct symbolic computation graphs to embody the\n      //  training process. Instead, we define a function that performs the\n      //  training action. In PyKeras, the data (inputs and targets) are fed\n      //  through graph placeholders. In tfjs-layers, the data are fed as\n      //  function arguments. Since the function are defined below in the\n      //  scope, we don't have equivalents of PyKeras's\n      //  `_make_train_funciton`.\n      const trainFunction = this.makeTrainFunction();\n      const outLabels = this.getDedupedMetricsNames();\n\n      let valFunction: (data: Tensor[]) => Scalar[];\n      let callbackMetrics: string[];\n      if (doValidation) {\n        this.makeTestFunction();\n        valFunction = this.testFunction;\n        callbackMetrics =\n            outLabels.slice().concat(outLabels.map(n => 'val_' + n));\n      } else {\n        valFunction = null;\n        valIns = [];\n        callbackMetrics = outLabels.slice();\n      }\n\n      const callbacks = standardizeCallbacks(args.callbacks, args.yieldEvery);\n      const out = await this.fitLoop(\n          trainFunction, ins, outLabels, batchSize, args.epochs,\n          args.verbose, callbacks, valFunction, valIns, args.shuffle,\n          callbackMetrics, args.initialEpoch, null, null);\n      return out;\n    } finally {\n      this.isTraining = false;\n      // Memory clean up.\n      disposeNewTensors(inputs, x);\n      disposeNewTensors(targets, y);\n      disposeNewTensors(originalInputs, x);\n      disposeNewTensors(originalTargets, y);\n      disposeNewTensors(valX as Tensor[], inputValX);\n      disposeNewTensors(valY as Tensor[], inputValY);\n      if (sampleWeights != null) {\n        tfc.dispose(sampleWeights);\n      }\n    }\n    // TODO(cais): Add value to outLabels.\n  }\n\n  /**\n   * Abstract fit function for `f(ins)`.\n   * @param f A Function returning a list of tensors. For training, this\n   *   function is expected to perform the updates to the variables.\n   * @param ins List of tensors to be fed to `f`.\n   * @param outLabels List of strings, display names of the outputs of `f`.\n   * @param batchSize Integer batch size or `== null` if unknown. Default : 32.\n   * @param epochs Number of times to iterate over the data. Default : 1.\n   * @param verbose Verbosity mode: 0, 1, or 2. Default: 1.\n   * @param callbacks List of callbacks to be called during training.\n   * @param valF Function to call for validation.\n   * @param valIns List of tensors to be fed to `valF`.\n   * @param shuffle Whether to shuffle the data at the beginning of every\n   * epoch. Default : true.\n   * @param callbackMetrics List of strings, the display names of the metrics\n   *   passed to the callbacks. They should be the concatenation of the\n   *   display names of the outputs of `f` and the list of display names\n   *   of the outputs of `valF`.\n   * @param initialEpoch Epoch at which to start training (useful for\n   *   resuming a previous training run). Default : 0.\n   * @param stepsPerEpoch Total number of steps (batches on samples) before\n   *   declaring one epoch finished and starting the next epoch. Ignored with\n   *   the default value of `undefined` or `null`.\n   * @param validationSteps Number of steps to run validation for (only if\n   *   doing validation from data tensors). Not applicable for tfjs-layers.\n   * @returns A `History` object.\n   */\n  async fitLoop(\n      f: (data: Tensor[]) => Scalar[], ins: Tensor[], outLabels?:\n      string[], batchSize?: number, epochs?: number, verbose?: number,\n      callbacks?: BaseCallback[], valF?: (data: Tensor[]) => Scalar[], valIns?:\n      Tensor[], shuffle?: boolean|string, callbackMetrics?: string[],\n      initialEpoch?: number, stepsPerEpoch?: number, validationSteps?: number):\n      Promise<History> {\n    if (batchSize == null) {\n      batchSize = 32;\n    }\n    if (epochs == null) {\n      epochs = 1;\n    }\n    if (shuffle == null) {\n      shuffle = true;\n    }\n    if (initialEpoch == null) {\n      initialEpoch = 0;\n    }\n\n    // TODO(cais): Change const to let below when implementing validation.\n    let doValidation = false;\n    if (valF != null && valIns != null) {\n      doValidation = true;\n      // TODO(cais): verbose message.\n    }\n    if (validationSteps != null) {\n      doValidation = true;\n      if (stepsPerEpoch == null) {\n        throw new ValueError(\n            'Can only use `validationSteps` when doing step-wise training, ' +\n            'i.e., `stepsPerEpoch` must be set.');\n      }\n    }\n\n    const numTrainSamples =\n        this.checkNumSamples(ins, batchSize, stepsPerEpoch, 'steps_per_epoch');\n    let indexArray: number[];\n    if (numTrainSamples != null) {\n      indexArray = range(0, numTrainSamples);\n    }\n\n    if (verbose == null) {\n      verbose = 1;\n    }\n\n    const {callbackList, history} = configureCallbacks(\n        callbacks, verbose, epochs, initialEpoch, numTrainSamples,\n        stepsPerEpoch, batchSize, doValidation, callbackMetrics);\n    callbackList.setModel(this);\n    this.history = history;\n    await callbackList.onTrainBegin();\n    this.stopTraining_ = false;\n    // TODO(cais): Take care of callbacks.validation_data as in PyKeras.\n    // TODO(cais): Pre-convert feeds for performance as in PyKeras.\n\n    for (let epoch = initialEpoch; epoch < epochs; ++epoch) {\n      await callbackList.onEpochBegin(epoch);\n      const epochLogs: UnresolvedLogs = {};\n      if (stepsPerEpoch != null) {\n        throw new NotImplementedError(\n            'stepsPerEpoch mode is not implemented yet.');\n      } else {\n        if (shuffle === 'batch') {\n          throw new NotImplementedError('batch shuffling is not implemneted'\n                                        + ' yet');\n        } else if (shuffle) {\n          util.shuffle(indexArray);\n        }\n        // Convert the potentially shuffled indices to Tensor1D, to avoid the\n        // cost of repeated creation of Array1Ds later on.\n        const epochIndexArray1D = tensor1d(indexArray);\n\n        const batches = makeBatches(numTrainSamples, batchSize);\n        for (let batchIndex = 0; batchIndex < batches.length; ++batchIndex) {\n          const batchLogs: UnresolvedLogs = {};\n          await callbackList.onBatchBegin(batchIndex, batchLogs);\n\n          tfc.tidy(() => {\n            const batchStart = batches[batchIndex][0];\n            const batchEnd = batches[batchIndex][1];\n            const batchIds = K.sliceAlongFirstAxis(\n                                 epochIndexArray1D, batchStart,\n                                 batchEnd - batchStart) as Tensor1D;\n            batchLogs['batch'] = batchIndex;\n            batchLogs['size'] = batchEnd - batchStart;\n\n            // TODO(cais): In ins, train flag can be a number, instead of an\n            //   Tensor? Do we need to handle this in tfjs-layers?\n            const insBatch = sliceArraysByIndices(ins, batchIds) as Tensor[];\n            const outs = f(insBatch);\n            for (let i = 0; i < outLabels.length; ++i) {\n              const label = outLabels[i];\n              const out = outs[i];\n              batchLogs[label] = out;\n              tfc.keep(out);\n              // TODO(cais): Use scope() to avoid ownership.\n            }\n\n            if (batchIndex === batches.length - 1) {  // Last batch.\n              if (doValidation) {\n                const valOuts = this.testLoop(valF, valIns, batchSize);\n                // Porting Notes: In tfjs-layers, valOuts is always an Array.\n                for (let i = 0; i < outLabels.length; ++i) {\n                  const label = outLabels[i];\n                  const out = valOuts[i];\n                  tfc.keep(out);\n                  // TODO(cais): Use scope() to avoid ownership.\n                  epochLogs['val_' + label] = out;\n                }\n              }\n            }\n          });\n\n          await callbackList.onBatchEnd(batchIndex, batchLogs);\n          disposeTensorsInLogs(batchLogs);\n\n          if (this.stopTraining_) {\n            break;\n          }\n          // TODO(cais): return outs as list of Tensor.\n        }\n\n        epochIndexArray1D.dispose();\n      }\n      // TODO(cais): Run validation at the end of the epoch.\n      await callbackList.onEpochEnd(epoch, epochLogs);\n      if (this.stopTraining_) {\n        break;\n      }\n    }\n    await callbackList.onTrainEnd();\n\n    await this.history.syncData();\n    return this.history;\n  }\n\n  // TODO(cais): Add code snippet below when it's possible to instantiate\n  //   actual dataset objects.\n  /**\n   * Trains the model using a dataset object.\n   *\n   * @param dataset A dataset object. Its `iterator()` method is expected\n   *   to generate a dataset iterator object, the `next()` method of which\n   *   is expected to produce data batches for training. The return value\n   *   of the `next()` call ought to contain a boolean `done` field and a\n   *   `value` field. The `value` field is expected to be an array of two\n   *   `tf.Tensor`s or an array of two nested `tf.Tensor` structures. The former\n   *   case is for models with exactly one input and one output (e.g.\n   *   a sequential model). The latter case is for models with multiple\n   *   inputs and/or multiple outputs.\n   *   Of the two items in the array, the first is the input feature(s) and\n   *   the second is the output target(s).\n   * @param args A `ModelFitDatasetArgs`, containing optional fields.\n   *\n   * @return A `History` instance. Its `history` attribute contains all\n   *   information collected during training.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  async fitDataset<T>(dataset: Dataset<T>, args: ModelFitDatasetArgs<T>):\n      Promise<History> {\n    return fitDataset(this, dataset, args);\n  }\n\n  /**\n   * Runs a single gradient update on a single batch of data.\n   *\n   * This method differs from `fit()` and `fitDataset()` in the following\n   * regards:\n   *   - It operates on exactly one batch of data.\n   *   - It returns only the loss and metric values, instead of\n   *     returning the batch-by-batch loss and metric values.\n   *   - It doesn't support fine-grained options such as verbosity and\n   *     callbacks.\n   *\n   * @param x Input data. It could be one of the following:\n   *   - A `tf.Tensor`, or an Array of `tf.Tensor`s (in case the model has\n   *     multiple inputs).\n   *   - An Object mapping input names to corresponding `tf.Tensor` (if the\n   *     model has named inputs).\n   * @param y Target data. It could be either a `tf.Tensor` or multiple\n   *   `tf.Tensor`s. It should be consistent with `x`.\n   * @returns Training loss or losses (in case the model has\n   *   multiple outputs), along with metrics (if any), as numbers.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes'}\n   */\n  async trainOnBatch(\n      x: Tensor|Tensor[]|{[inputName: string]: Tensor},\n      y: Tensor|Tensor[]|\n      {[inputName: string]: Tensor}): Promise<number|number[]> {\n    // TODO(cais): Support sampleWeight and classWeight.\n    // TODO(cais): Support Dataset objects.\n    const standardizeOut = await this.standardizeUserData(x, y);\n    const inputs = standardizeOut[0];\n    const targets = standardizeOut[1];\n    const trainFunction = this.makeTrainFunction();\n    const losses = trainFunction(inputs.concat(targets));\n    const lossValues: number[] = [];\n    for (const loss of losses) {\n      const v = await loss.data();\n      lossValues.push(v[0]);\n    }\n    tfc.dispose(losses);\n    disposeNewTensors(standardizeOut[0], x);\n    disposeNewTensors(standardizeOut[1], y);\n    return singletonOrArray(lossValues);\n  }\n\n  /**\n   * Extract weight values of the model.\n   *\n   * @param config: An instance of `io.SaveConfig`, which specifies\n   * model-saving options such as whether only trainable weights are to be\n   * saved.\n   * @returns A `NamedTensorMap` mapping original weight names (i.e.,\n   *   non-uniqueified weight names) to their values.\n   */\n  protected getNamedWeights(config?: io.SaveConfig): NamedTensor[] {\n    const namedWeights: NamedTensor[] = [];\n\n    const trainableOnly = config != null && config.trainableOnly;\n    const weights = trainableOnly ? this.trainableWeights : this.weights;\n    const weightValues = this.getWeights(trainableOnly);\n    for (let i = 0; i < weights.length; ++i) {\n      if (trainableOnly && !weights[i].trainable) {\n        // Optionally skip non-trainable weights.\n        continue;\n      }\n      namedWeights.push(\n          {name: weights[i].originalName, tensor: weightValues[i]});\n    }\n    return namedWeights;\n  }\n\n  /**\n   * Setter used for force stopping of LayersModel.fit() (i.e., training).\n   *\n   * Example:\n   *\n   * ```js\n   * const input = tf.input({shape: [10]});\n   * const output = tf.layers.dense({units: 1}).apply(input);\n   * const model = tf.model({inputs: [input], outputs: [output]});\n   * model.compile({loss: 'meanSquaredError', optimizer: 'sgd'});\n   * const xs = tf.ones([8, 10]);\n   * const ys = tf.zeros([8, 1]);\n   *\n   * const history = await model.fit(xs, ys, {\n   *   epochs: 10,\n   *   callbacks: {\n   *     onEpochEnd: async (epoch, logs) => {\n   *       if (epoch === 2) {\n   *         model.stopTraining = true;\n   *       }\n   *     }\n   *   }\n   * });\n   *\n   * // There should be only 3 values in the loss array, instead of 10\n   * values,\n   * // due to the stopping after 3 epochs.\n   * console.log(history.history.loss);\n   * ```\n   */\n  set stopTraining(stop: boolean) {\n    this.stopTraining_ = stop;\n  }\n\n  get stopTraining(): boolean {\n    return this.stopTraining_;\n  }\n\n  get optimizer(): Optimizer {\n    return this.optimizer_;\n  }\n\n  set optimizer(optimizer: Optimizer) {\n    if (this.optimizer_ !== optimizer) {\n      this.optimizer_ = optimizer;\n      this.isOptimizerOwned = false;\n    }\n  }\n\n  override dispose(): DisposeResult {\n    const result = super.dispose();\n    if (result.refCountAfterDispose === 0 && this.optimizer != null &&\n        this.isOptimizerOwned) {\n      const numTensorsBeforeOptmizerDisposal = tfc.memory().numTensors;\n      this.optimizer_.dispose();\n      result.numDisposedVariables +=\n          numTensorsBeforeOptmizerDisposal - tfc.memory().numTensors;\n    }\n    return result;\n  }\n\n  private getLossIdentifiers(): LossIdentifier|LossIdentifier[]|\n      {[outputName: string]: LossIdentifier} {\n    let lossNames: LossIdentifier|LossIdentifier[]|\n        {[outputName: string]: LossIdentifier};\n    if (typeof this.loss === 'string') {\n      lossNames = toSnakeCase(this.loss) as LossIdentifier;\n    } else if (Array.isArray(this.loss)) {\n      for (const loss of this.loss) {\n        if (typeof loss !== 'string') {\n          throw new Error('Serialization of non-string loss is not supported.');\n        }\n      }\n      lossNames = (this.loss as string[]).map(name => toSnakeCase(name)) as\n          LossIdentifier[];\n    } else {\n      const outputNames = Object.keys(this.loss);\n      lossNames = {} as {[outputName: string]: LossIdentifier};\n      const losses =\n          this.loss as {[outputName: string]: LossOrMetricFn | string};\n      for (const outputName of outputNames) {\n        if (typeof losses[outputName] === 'string') {\n          lossNames[outputName] =\n              toSnakeCase(losses[outputName] as string) as LossIdentifier;\n        } else {\n          throw new Error('Serialization of non-string loss is not supported.');\n        }\n      }\n    }\n    return lossNames;\n  }\n\n  private getMetricIdentifiers(): MetricsIdentifier[]|\n      {[key: string]: MetricsIdentifier} {\n    if (typeof this.metrics === 'string' ||\n        typeof this.metrics === 'function') {\n      return [toSnakeCase(Metrics.getLossOrMetricName(this.metrics))];\n    } else if (Array.isArray(this.metrics)) {\n      return this.metrics.map(\n          metric => toSnakeCase(Metrics.getLossOrMetricName(metric)));\n    } else {\n      const metricsIdentifiers: {[key: string]: MetricsIdentifier} = {};\n      for (const key in this.metrics) {\n        metricsIdentifiers[key] =\n            toSnakeCase(Metrics.getLossOrMetricName(this.metrics[key]));\n      }\n      return metricsIdentifiers;\n    }\n  }\n\n  protected getTrainingConfig(): TrainingConfig {\n    return {\n      loss: this.getLossIdentifiers(),\n      metrics: this.getMetricIdentifiers(),\n      optimizer_config: {\n        class_name: this.optimizer.getClassName(),\n        config: this.optimizer.getConfig()\n      } as OptimizerSerialization\n    };\n    // TODO(cais): Add weight_metrics when they are supported.\n    // TODO(cais): Add sample_weight_mode when it's supported.\n    // TODO(cais): Add loss_weights when it's supported.\n  }\n\n  loadTrainingConfig(trainingConfig: TrainingConfig) {\n    if (trainingConfig.weighted_metrics != null) {\n      throw new Error('Loading weight_metrics is not supported yet.');\n    }\n    if (trainingConfig.loss_weights != null) {\n      throw new Error('Loading loss_weights is not supported yet.');\n    }\n    if (trainingConfig.sample_weight_mode != null) {\n      throw new Error('Loading sample_weight_mode is not supported yet.');\n    }\n\n    const tsConfig = convertPythonicToTs(trainingConfig.optimizer_config) as\n        serialization.ConfigDict;\n    const optimizer = deserialize(tsConfig) as Optimizer;\n\n    let loss;\n    if (typeof trainingConfig.loss === 'string') {\n      loss = toCamelCase(trainingConfig.loss);\n    } else if (Array.isArray(trainingConfig.loss)) {\n      loss = trainingConfig.loss.map(lossEntry => toCamelCase(lossEntry));\n    } else if (trainingConfig.loss != null) {\n      loss = {} as {[outputName: string]: LossIdentifier};\n      for (const key in trainingConfig.loss) {\n        loss[key] = toCamelCase(trainingConfig.loss[key]) as LossIdentifier;\n      }\n    }\n\n    let metrics;\n    if (Array.isArray(trainingConfig.metrics)) {\n      metrics = trainingConfig.metrics.map(metric => toCamelCase(metric));\n    } else if (trainingConfig.metrics != null) {\n      metrics = {} as {[outputName: string]: MetricsIdentifier};\n      for (const key in trainingConfig.metrics) {\n        metrics[key] = toCamelCase(trainingConfig.metrics[key]);\n      }\n    }\n\n    this.compile({loss, metrics, optimizer});\n  }\n\n  /**\n   * Save the configuration and/or weights of the LayersModel.\n   *\n   * An `IOHandler` is an object that has a `save` method of the proper\n   * signature defined. The `save` method manages the storing or\n   * transmission of serialized data (\"artifacts\") that represent the\n   * model's topology and weights onto or via a specific medium, such as\n   * file downloads, local storage, IndexedDB in the web browser and HTTP\n   * requests to a server. TensorFlow.js provides `IOHandler`\n   * implementations for a number of frequently used saving mediums, such as\n   * `tf.io.browserDownloads` and `tf.io.browserLocalStorage`. See `tf.io`\n   * for more details.\n   *\n   * This method also allows you to refer to certain types of `IOHandler`s\n   * as URL-like string shortcuts, such as 'localstorage://' and\n   * 'indexeddb://'.\n   *\n   * Example 1: Save `model`'s topology and weights to browser [local\n   * storage](https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage);\n   * then load it back.\n   *\n   * ```js\n   * const model = tf.sequential(\n   *     {layers: [tf.layers.dense({units: 1, inputShape: [3]})]});\n   * console.log('Prediction from original model:');\n   * model.predict(tf.ones([1, 3])).print();\n   *\n   * const saveResults = await model.save('localstorage://my-model-1');\n   *\n   * const loadedModel = await tf.loadLayersModel('localstorage://my-model-1');\n   * console.log('Prediction from loaded model:');\n   * loadedModel.predict(tf.ones([1, 3])).print();\n   * ```\n   *\n   * Example 2. Saving `model`'s topology and weights to browser\n   * [IndexedDB](https://developer.mozilla.org/en-US/docs/Web/API/IndexedDB_API);\n   * then load it back.\n   *\n   * ```js\n   * const model = tf.sequential(\n   *     {layers: [tf.layers.dense({units: 1, inputShape: [3]})]});\n   * console.log('Prediction from original model:');\n   * model.predict(tf.ones([1, 3])).print();\n   *\n   * const saveResults = await model.save('indexeddb://my-model-1');\n   *\n   * const loadedModel = await tf.loadLayersModel('indexeddb://my-model-1');\n   * console.log('Prediction from loaded model:');\n   * loadedModel.predict(tf.ones([1, 3])).print();\n   * ```\n   *\n   * Example 3. Saving `model`'s topology and weights as two files\n   * (`my-model-1.json` and `my-model-1.weights.bin`) downloaded from\n   * browser.\n   *\n   * ```js\n   * const model = tf.sequential(\n   *     {layers: [tf.layers.dense({units: 1, inputShape: [3]})]});\n   * const saveResults = await model.save('downloads://my-model-1');\n   * ```\n   *\n   * Example 4. Send  `model`'s topology and weights to an HTTP server.\n   * See the documentation of `tf.io.http` for more details\n   * including specifying request parameters and implementation of the\n   * server.\n   *\n   * ```js\n   * const model = tf.sequential(\n   *     {layers: [tf.layers.dense({units: 1, inputShape: [3]})]});\n   * const saveResults = await model.save('http://my-server/model/upload');\n   * ```\n   *\n   * @param handlerOrURL An instance of `IOHandler` or a URL-like,\n   * scheme-based string shortcut for `IOHandler`.\n   * @param config Options for saving the model.\n   * @returns A `Promise` of `SaveResult`, which summarizes the result of\n   * the saving, such as byte sizes of the saved artifacts for the model's\n   *   topology and weight values.\n   *\n   * @doc {heading: 'Models', subheading: 'Classes', ignoreCI: true}\n   */\n  async save(handlerOrURL: io.IOHandler|string, config?: io.SaveConfig):\n      Promise<io.SaveResult> {\n    if (typeof handlerOrURL === 'string') {\n      const handlers = io.getSaveHandlers(handlerOrURL);\n      if (handlers.length === 0) {\n        throw new ValueError(\n            `Cannot find any save handlers for URL '${handlerOrURL}'`);\n      } else if (handlers.length > 1) {\n        throw new ValueError(\n            `Found more than one (${handlers.length}) save handlers for ` +\n            `URL '${handlerOrURL}'`);\n      }\n      handlerOrURL = handlers[0];\n    }\n    if (handlerOrURL.save == null) {\n      throw new ValueError(\n          'LayersModel.save() cannot proceed because the IOHandler ' +\n          'provided does not have the `save` attribute defined.');\n    }\n\n    const weightDataAndSpecs =\n        await io.encodeWeights(this.getNamedWeights(config));\n\n    const returnString = false;\n    const unusedArg: {} = null;\n    const modelConfig = this.toJSON(unusedArg, returnString);\n    const modelArtifacts: io.ModelArtifacts = {\n      modelTopology: modelConfig,\n      format: LAYERS_MODEL_FORMAT_NAME,\n      generatedBy: `TensorFlow.js tfjs-layers v${version}`,\n      convertedBy: null,\n    };\n\n    const includeOptimizer = config == null ? false : config.includeOptimizer;\n    if (includeOptimizer && this.optimizer != null) {\n      modelArtifacts.trainingConfig = this.getTrainingConfig();\n      const weightType = 'optimizer';\n      const {data: optimizerWeightData, specs: optimizerWeightSpecs} =\n          await io.encodeWeights(await this.optimizer.getWeights(), weightType);\n      weightDataAndSpecs.specs.push(...optimizerWeightSpecs);\n      weightDataAndSpecs.data = io.concatenateArrayBuffers(\n          [weightDataAndSpecs.data, optimizerWeightData]);\n    }\n\n    if (this.userDefinedMetadata != null) {\n      // Check serialized size of user-defined metadata.\n      const checkSize = true;\n      checkUserDefinedMetadata(this.userDefinedMetadata, this.name, checkSize);\n      modelArtifacts.userDefinedMetadata = this.userDefinedMetadata;\n    }\n\n    modelArtifacts.weightData = weightDataAndSpecs.data;\n    modelArtifacts.weightSpecs = weightDataAndSpecs.specs;\n    return handlerOrURL.save(modelArtifacts);\n  }\n\n  /**\n   * Set user-defined metadata.\n   *\n   * The set metadata will be serialized together with the topology\n   * and weights of the model during `save()` calls.\n   *\n   * @param setUserDefinedMetadata\n   */\n  setUserDefinedMetadata(userDefinedMetadata: {}): void {\n    checkUserDefinedMetadata(userDefinedMetadata, this.name);\n    this.userDefinedMetadata = userDefinedMetadata;\n  }\n\n  /**\n   * Get user-defined metadata.\n   *\n   * The metadata is supplied via one of the two routes:\n   *   1. By calling `setUserDefinedMetadata()`.\n   *   2. Loaded during model loading (if the model is constructed\n   *      via `tf.loadLayersModel()`.)\n   *\n   * If no user-defined metadata is available from either of the\n   * two routes, this function will return `undefined`.\n   */\n  getUserDefinedMetadata(): {} {\n    return this.userDefinedMetadata;\n  }\n}\nserialization.registerClass(LayersModel);\n\n/**\n * A `tf.Functional` is an alias to `tf.LayersModel`.\n *\n * See also:\n *   `tf.LayersModel`, `tf.Sequential`, `tf.loadLayersModel`.\n */\n/** @doc {heading: 'Models', subheading: 'Classes'} */\nexport class Functional extends LayersModel {\n  static override className = 'Functional';\n}\nserialization.registerClass(Functional);\n"],"mappings":";;;;;;;;;;;AAAA;;;;;;;;;AAUA;AAEA,OAAO,KAAKA,GAAG,MAAM,uBAAuB;AAC5C,SAAQC,EAAE,EAA0DC,SAAS,EAAUC,MAAM,EAAEC,aAAa,EAAEC,MAAM,EAAYC,QAAQ,EAAEC,IAAI,QAAO,uBAAuB;AAE5K,OAAO,KAAKC,CAAC,MAAM,yBAAyB;AAC5C,SAAsBC,kBAAkB,EAAkCC,oBAAoB,QAAO,mBAAmB;AACxH,SAAQC,SAAS,QAAO,WAAW;AACnC,SAAQC,mBAAmB,EAAEC,YAAY,EAAEC,UAAU,QAAO,WAAW;AAKvE,SAAQC,WAAW,QAAO,yBAAyB;AACnD,SAASC,oBAAoB,QAAwB,SAAS;AAC9D,OAAO,KAAKC,MAAM,MAAM,WAAW;AACnC,OAAO,KAAKC,OAAO,MAAM,YAAY;AACrC,OAAO,KAAKC,UAAU,MAAM,eAAe;AAE3C,SAAQC,wBAAwB,QAAO,0BAA0B;AACjE,SAAQC,KAAK,EAAEC,YAAY,EAAEC,gBAAgB,EAAEC,WAAW,EAAEC,WAAW,EAAEC,MAAM,QAAO,wBAAwB;AAC9G,SAAQC,YAAY,QAAO,sBAAsB;AACjD,SAAQC,KAAK,QAAO,qBAAqB;AACzC,SAAQC,mBAAmB,QAAO,8BAA8B;AAEhE,SAAQC,OAAO,QAAO,YAAY;AAElC,SAAQC,SAAS,QAAsB,aAAa;AAEpD,SAAQC,OAAO,IAAPA,QAAO,EAAEC,QAAQ,QAAO,YAAY;AAE5C,SAAQC,eAAe,IAAfA,gBAAe,EAAEC,UAAU,IAAVA,WAAU,QAAsD,oBAAoB;AAC7G,SAAQC,cAAc,EAAEC,iBAAiB,EAAEC,0BAA0B,EAAEC,WAAW,EAAgBC,WAAW,EAAEC,oBAAoB,QAAO,oBAAoB;AAC9J,SAAqCC,mBAAmB,EAAEC,uBAAuB,EAAEC,kBAAkB,QAAO,kBAAkB;AAE9H;;;AAGA,OAAM,SAAUC,YAAYA,CAACC,CAC+B;EAC1D,OAAOA,CAAC,YAAYzC,MAAM;AAC5B;AAEA;;;AAGA,OAAM,SAAU0C,WAAWA,CAACD,CAC6B;EACvD,OAAOE,KAAK,CAACC,OAAO,CAACH,CAAC,CAAC;AACzB;AAEA;;;AAGA,OAAM,SAAUI,UAAUA,CAACJ,CAC6B;EACtD,OAAO,CAACD,YAAY,CAACC,CAAC,CAAC,IAAI,CAACC,WAAW,CAACD,CAAC,CAAC;AAC5C;AAEA;;;;;;;;;;;AAWA,OAAM,SAAUK,oBAAoBA,CAChCC,IAAmD,EAAEC,KAAe,EACpEC,MAAgB,EAA6C;EAAA,IAA3CC,cAAc,GAAAC,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,IAAI;EAAA,IAAEG,eAAe,GAAAH,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,EAAE;EAC/D,IAAIH,KAAK,IAAI,IAAI,IAAIA,KAAK,CAACI,MAAM,KAAK,CAAC,EAAE;IACvC;IACA;IACA,IAAIL,IAAI,IAAI,IAAI,EAAE;MAChB,IAAIQ,iBAAiB,GAAG,KAAK;MAC7B,IAAIb,WAAW,CAACK,IAAI,CAAC,IAAKA,IAAiB,CAACK,MAAM,GAAG,CAAC,EAAE;QACtDG,iBAAiB,GAAG,IAAI;OACzB,MAAM,IAAIV,UAAU,CAACE,IAAI,CAAC,EAAE;QAC3B,KAAK,IAAMS,GAAG,IAAIT,IAAI,EAAE;UACtB,IAAIA,IAAI,CAACU,cAAc,CAACD,GAAG,CAAC,EAAE;YAC5BD,iBAAiB,GAAG,IAAI;YACxB;;;OAGL,MAAM;QACL;QACAA,iBAAiB,GAAG,IAAI;;MAE1B,IAAIA,iBAAiB,EAAE;QACrB,MAAM,IAAI9C,UAAU,CAChB,6BAAAiD,MAAA,CAA6BJ,eAAe,sCAAAI,MAAA,CACjCX,IAAI,CAAE,CAAC;;;IAG1B,OAAO,EAAE;;EAEX,IAAIA,IAAI,IAAI,IAAI,EAAE;IAChB,OAAOC,KAAK,CAACW,GAAG,CAAC,UAAAC,IAAI;MAAA,OAAI,IAAI;IAAA,EAAC;;EAGhC,IAAIC,MAAgB;EACpB,IAAIhB,UAAU,CAACE,IAAI,CAAC,EAAE;IACpBA,IAAI,GAAGA,IAAqC;IAC5Cc,MAAM,GAAG,EAAE;IAAC,IAAAC,SAAA,GAAAC,0BAAA,CACOf,KAAK;MAAAgB,KAAA;IAAA;MAAxB,KAAAF,SAAA,CAAAG,CAAA,MAAAD,KAAA,GAAAF,SAAA,CAAAI,CAAA,IAAAC,IAAA,GAA0B;QAAA,IAAfP,IAAI,GAAAI,KAAA,CAAAI,KAAA;QACb,IAAIrB,IAAI,CAACa,IAAI,CAAC,IAAI,IAAI,EAAE;UACtB,MAAM,IAAInD,UAAU,CAChB,0BAAAiD,MAAA,CAAyBE,IAAI,0CAAAF,MAAA,CAC1BV,KAAK,CAAE,CAAC;;QAEjBa,MAAM,CAACQ,IAAI,CAACtB,IAAI,CAACa,IAAI,CAAC,CAAC;;IACxB,SAAAU,GAAA;MAAAR,SAAA,CAAAS,CAAA,CAAAD,GAAA;IAAA;MAAAR,SAAA,CAAAU,CAAA;IAAA;GACF,MAAM,IAAI9B,WAAW,CAACK,IAAI,CAAC,EAAE;IAC5BA,IAAI,GAAGA,IAAgB;IACvB,IAAIA,IAAI,CAACK,MAAM,KAAKJ,KAAK,CAACI,MAAM,EAAE;MAChC,MAAM,IAAI3C,UAAU,CAChB,6BAAAiD,MAAA,CAA6BJ,eAAe,wFACqB,sCAAAI,MAAA,CAC9BV,KAAK,CAACI,MAAM,qBAAkB,mDAAAM,MAAA,CACjBX,IAAI,CAAE,CAAC;;IAE7Dc,MAAM,GAAGd,IAAI;GACd,MAAM;IACLA,IAAI,GAAGA,IAAc;IACrB,IAAIC,KAAK,CAACI,MAAM,GAAG,CAAC,EAAE;MACpB,MAAM,IAAI3C,UAAU,CAChB,aAAAiD,MAAA,CAAaJ,eAAe,eAAAI,MAAA,CAAYV,KAAK,CAACI,MAAM,8EAAAM,MAAA,CAEhDX,IAAI,CAAC0B,KAAK,CAAE,CAAC;;IAEvBZ,MAAM,GAAG,CAACd,IAAI,CAAC;;EAGjBc,MAAM,GAAG5B,0BAA0B,CAAC4B,MAAM,CAAC;EAE3C;EACA,IAAIZ,MAAM,IAAI,IAAI,EAAE;IAClB,KAAK,IAAIyB,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG1B,KAAK,CAACI,MAAM,EAAE,EAAEsB,CAAC,EAAE;MACrC,IAAIzB,MAAM,CAACyB,CAAC,CAAC,IAAI,IAAI,EAAE;QACrB;;MAEF,IAAMC,KAAK,GAAGd,MAAM,CAACa,CAAC,CAAC;MACvB,IAAIC,KAAK,CAACF,KAAK,CAACrB,MAAM,KAAKH,MAAM,CAACyB,CAAC,CAAC,CAACtB,MAAM,EAAE;QAC3C,MAAM,IAAI3C,UAAU,CAChB,uBAAAiD,MAAA,CAAuBJ,eAAe,iBAAAI,MAAA,CAAcV,KAAK,CAAC0B,CAAC,CAAC,oBAAAhB,MAAA,CACjDT,MAAM,CAACyB,CAAC,CAAC,CAACtB,MAAM,uCAAoC,YAAAM,MAAA,CACtDiB,KAAK,CAACF,KAAK,CAAE,CAAC;;MAE7B,KAAK,IAAIG,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG3B,MAAM,CAACyB,CAAC,CAAC,CAACtB,MAAM,EAAE,EAAEwB,CAAC,EAAE;QACzC,IAAIA,CAAC,KAAK,CAAC,IAAI,CAAC1B,cAAc,EAAE;UAC9B;UACA;;QAEF,IAAM2B,GAAG,GAAGF,KAAK,CAACF,KAAK,CAACG,CAAC,CAAC;QAC1B,IAAME,MAAM,GAAG7B,MAAM,CAACyB,CAAC,CAAC,CAACE,CAAC,CAAC;QAC3B,IAAIE,MAAM,IAAI,IAAI,IAAIA,MAAM,IAAI,CAAC,IAAID,GAAG,KAAKC,MAAM,EAAE;UACnD,MAAM,IAAIrE,UAAU,CAChB,GAAAiD,MAAA,CAAGJ,eAAe,uEAAAI,MAAA,CACIT,MAAM,CAACyB,CAAC,CAAC,CAACK,KAAK,CAAC,CAAC,EAAE9B,MAAM,CAACyB,CAAC,CAAC,CAACtB,MAAM,CAAC,OAAI,4BAAAM,MAAA,CAE1DT,MAAM,CAACyB,CAAC,CAAC,CAACK,KAAK,CAAC,CAAC,EAAE9B,MAAM,CAACyB,CAAC,CAAC,CAACtB,MAAM,CAAC,OAAI,eAAAM,MAAA,CAChCJ,eAAe,8BAAAI,MAAA,CACvBiB,KAAK,CAACF,KAAK,CAAC,CAAC,CAAC,CAAE,kCAAAf,MAAA,CAEhBiB,KAAK,CAACF,KAAK,CAACM,KAAK,CAAC,CAAC,EAAEJ,KAAK,CAACF,KAAK,CAACrB,MAAM,CAAC,MAAG,sBAAAM,MAAA,CAC5BiB,KAAK,CAACF,KAAK,OAAI,CAAC;;;;;EAK/C,OAAOZ,MAAM;AACf;AAEA;;;;;;;AAOA,OAAM,SAAUmB,iBAAiBA,CAC7BC,MAAgB,EAAEC,OAAiB,EAAEC,OAAkB;EACzD,IAAMC,IAAI,GAAG/D,MAAM,CAAC4D,MAAM,CAACtB,GAAG,CAAC,UAAA0B,KAAK;IAAA,OAAIA,KAAK,CAACZ,KAAK,CAAC,CAAC,CAAC;EAAA,EAAC,CAAC;EACxDW,IAAI,CAACE,IAAI,EAAE;EACX,IAAMC,IAAI,GAAGlE,MAAM,CAAC6D,OAAO,CAACvB,GAAG,CAAC,UAAA6B,MAAM;IAAA,OAAIA,MAAM,CAACf,KAAK,CAAC,CAAC,CAAC;EAAA,EAAC,CAAC;EAC3Dc,IAAI,CAACD,IAAI,EAAE;EACX;EACA,IAAIF,IAAI,CAAChC,MAAM,GAAG,CAAC,EAAE;IACnB,MAAM,IAAI3C,UAAU,CAChB,uFACoB,MAAAiD,MAAA,CACjB+B,IAAI,CAACC,SAAS,CAACT,MAAM,CAACtB,GAAG,CAAC,UAAA0B,KAAK;MAAA,OAAIA,KAAK,CAACZ,KAAK;IAAA,EAAC,CAAC,CAAE,CAAC;;EAE5D,IAAIc,IAAI,CAACnC,MAAM,GAAG,CAAC,EAAE;IACnB,MAAM,IAAI3C,UAAU,CAChB,wFACoB,MAAAiD,MAAA,CACjB+B,IAAI,CAACC,SAAS,CAACR,OAAO,CAACvB,GAAG,CAAC,UAAA6B,MAAM;MAAA,OAAIA,MAAM,CAACf,KAAK;IAAA,EAAC,CAAC,CAAE,CAAC;;EAE/D,IAAIW,IAAI,CAAChC,MAAM,GAAG,CAAC,IAAImC,IAAI,CAACnC,MAAM,GAAG,CAAC,IAAI,CAAClD,IAAI,CAACyF,WAAW,CAACP,IAAI,EAAEG,IAAI,CAAC,EAAE;IACvE,MAAM,IAAI9E,UAAU,CAChB,sFAAAiD,MAAA,CACkB0B,IAAI,CAAC,CAAC,CAAC,2BAAA1B,MAAA,CAAwB6B,IAAI,CAAC,CAAC,CAAC,aAAU,eACtD,CAAC;;AAErB;AAEA;;;;;;;;;AASA,SAASK,+BAA+BA,CACpCV,OAAiB,EAAEW,OAAyB,EAAEC,YAAqB;EACrE;EACA,IAAMC,SAAS,GAAG,CAChBnF,MAAM,CAACoF,gBAAgB,EAAEpF,MAAM,CAACqF,kBAAkB,EAClDrF,MAAM,CAACsF,uBAAuB,CAC/B;EACD,KAAK,IAAIxB,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGQ,OAAO,CAAC9B,MAAM,EAAE,EAAEsB,CAAC,EAAE;IACvC,IAAMyB,CAAC,GAAGjB,OAAO,CAACR,CAAC,CAAC;IACpB,IAAM0B,IAAI,GAAGP,OAAO,CAACnB,CAAC,CAAC;IACvB,IAAMD,KAAK,GAAGqB,YAAY,CAACpB,CAAC,CAAC;IAC7B,IAAI0B,IAAI,IAAI,IAAI,EAAE;MAChB;;IAEF,IAAIA,IAAI,KAAKxF,MAAM,CAACsF,uBAAuB,EAAE;MAC3C,IAAIC,CAAC,CAAC1B,KAAK,CAAC0B,CAAC,CAAC1B,KAAK,CAACrB,MAAM,GAAG,CAAC,CAAC,KAAK,CAAC,EAAE;QACrC,MAAM,IAAI3C,UAAU,CAChB,2CAAAiD,MAAA,CAA2CyC,CAAC,CAAC1B,KAAK,oFACa,gEACF,wBACxC,CAAC;QAC1B;;;;IAGJ,IAAIsB,SAAS,CAACM,OAAO,CAACD,IAAI,CAAC,KAAK,CAAC,CAAC,EAAE;MAClC,IAAME,YAAY,GAAGH,CAAC,CAAC1B,KAAK,CAACM,KAAK,CAAC,CAAC,CAAC;MACrC,IAAMwB,WAAW,GAAG9B,KAAK,CAACM,KAAK,CAAC,CAAC,CAAC;MAClC,KAAK,IAAIH,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG0B,YAAY,CAAClD,MAAM,EAAE,EAAEwB,CAAC,EAAE;QAC5C,IAAM4B,SAAS,GAAGF,YAAY,CAAC1B,CAAC,CAAC;QACjC,IAAM6B,MAAM,GAAGF,WAAW,CAAC3B,CAAC,CAAC;QAC7B,IAAI6B,MAAM,IAAI,IAAI,IAAID,SAAS,KAAKC,MAAM,EAAE;UAC1C,MAAM,IAAIhG,UAAU,CAChB,8BAAAiD,MAAA,CAA8ByC,CAAC,CAAC1B,KAAK,8CAAAf,MAAA,CAClBe,KAAK,wCAAqC,0DACN,CAAC;;;;;AAKtE;AAEA;;;;;;;;;;;;;;;;;;;;;;;;;;AA0BA,SAASiC,cAAcA,CACnB3D,IAAqB,EAAEC,KAAe,EAAEC,MAAgB,EACb;EAAA,IAA3CC,cAAc,GAAAC,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,IAAI;EAAA,IAAEG,eAAe,GAAAH,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,EAAE;EAC7C,IAAIU,MAAgB;EACpB,IAAIlB,KAAK,CAACC,OAAO,CAACG,IAAI,CAAC,EAAE;IACvB,IAAIA,IAAI,CAACK,MAAM,KAAKJ,KAAK,CAACI,MAAM,EAAE;MAChC,MAAM,IAAI3C,UAAU,CAChB,6BAAAiD,MAAA,CAA6BJ,eAAe,wFACqB,0CAAAI,MAAA,CAC1BV,KAAK,CAACI,MAAM,gBAAa,uBAAAM,MAAA,CAC5CX,IAAI,CAACK,MAAM,iBAAc,CAAC;;IAEpDS,MAAM,GAAGd,IAAI;GACd,MAAM;IACL,IAAIC,KAAK,CAACI,MAAM,GAAG,CAAC,EAAE;MACpB,MAAM,IAAI3C,UAAU,CAChB,qBAAAiD,MAAA,CAAqBV,KAAK,CAACI,MAAM,OAAAM,MAAA,CAAIJ,eAAe,0EACI,MAAAI,MAAA,CACrD+B,IAAI,CAACC,SAAS,CAAC3C,IAAI,CAAC0B,KAAK,CAAC,MAAG,CAAC;;IAEvCZ,MAAM,GAAG,CAACd,IAAI,CAAC;;EAGjB,IAAIE,MAAM,IAAI,IAAI,EAAE;IAClB,KAAK,IAAIyB,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG1B,KAAK,CAACI,MAAM,EAAE,EAAEsB,CAAC,EAAE;MACrC,IAAIzB,MAAM,CAACyB,CAAC,CAAC,IAAI,IAAI,EAAE;QACrB;;MAEF,IAAMC,KAAK,GAAGd,MAAM,CAACa,CAAC,CAAC;MACvB,IAAIC,KAAK,CAACF,KAAK,CAACrB,MAAM,KAAKH,MAAM,CAACyB,CAAC,CAAC,CAACtB,MAAM,EAAE;QAC3C,MAAM,IAAI3C,UAAU,CAChB,uBAAAiD,MAAA,CAAuBJ,eAAe,iBAAAI,MAAA,CAAcV,KAAK,CAAC0B,CAAC,CAAC,oBAAAhB,MAAA,CACjDT,MAAM,CAACyB,CAAC,CAAC,CAACtB,MAAM,uCAAoC,YAAAM,MAAA,CACtD+B,IAAI,CAACC,SAAS,CAACf,KAAK,CAACF,KAAK,CAAC,CAAE,CAAC;;MAE7C,KAAK,IAAIG,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG3B,MAAM,CAACyB,CAAC,CAAC,CAACtB,MAAM,EAAE,EAAEwB,CAAC,EAAE;QACzC,IAAIA,CAAC,KAAK,CAAC,IAAI,CAAC1B,cAAc,EAAE;UAC9B;;QAEF,IAAM2B,GAAG,GAAGF,KAAK,CAACF,KAAK,CAACG,CAAC,CAAC;QAC1B,IAAME,MAAM,GAAG7B,MAAM,CAACyB,CAAC,CAAC,CAACE,CAAC,CAAC;QAC3B,IAAIE,MAAM,IAAI,IAAI,EAAE;UAClB,IAAIA,MAAM,KAAKD,GAAG,EAAE;YAClB,MAAM,IAAIpE,UAAU,CAChB,uBAAAiD,MAAA,CAAuBJ,eAAe,sBAAAI,MAAA,CACnCV,KAAK,CAAC0B,CAAC,CAAC,qBAAAhB,MAAA,CAAkB+B,IAAI,CAACC,SAAS,CAACzC,MAAM,CAACyB,CAAC,CAAC,CAAC,UAAO,2BAAAhB,MAAA,CACrC+B,IAAI,CAACC,SAAS,CAACf,KAAK,CAACF,KAAK,CAAC,MAAG,CAAC;;;;;;AAMvE;AAEA;;;;;;;;;;;;;AAaA,OAAM,SAAUkC,cAAcA,CAC1BC,OAC+C,EAC/CC,WAAqB;EACvB,IAAID,OAAO,IAAI,IAAI,IAAIjE,KAAK,CAACC,OAAO,CAACgE,OAAO,CAAC,IAAIA,OAAO,CAACxD,MAAM,KAAK,CAAC,EAAE;IACrE,OAAOyD,WAAW,CAAClD,GAAG,CAAC,UAAAC,IAAI;MAAA,OAAI,EAAE;IAAA,EAAC;;EAGpC,IAAIkD,cAC+C;EACnD,IAAI,OAAOF,OAAO,KAAK,QAAQ,IAAI,OAAOA,OAAO,KAAK,UAAU,EAAE;IAChEE,cAAc,GAAG,CAACF,OAAO,CAAC;GAC3B,MAAM,IAAIjE,KAAK,CAACC,OAAO,CAACgE,OAAO,CAAC,IAAI,OAAOA,OAAO,KAAK,QAAQ,EAAE;IAChEE,cAAc,GAAGF,OAC0D;GAC5E,MAAM;IACL,MAAM,IAAIG,SAAS,CACf,8DAA8D,yCAAArD,MAAA,CACxBkD,OAAO,CAAE,CAAC;;EAGtD,IAAIjE,KAAK,CAACC,OAAO,CAACkE,cAAc,CAAC,EAAE;IACjC;IACA,OAAOD,WAAW,CAAClD,GAAG,CAClB,UAAAC,IAAI;MAAA,OAAIkD,cAA8C;IAAA,EAAC;GAC5D,MAAM;IACL;IACA,IAAME,aAAa,GAAwC,EAAE;IAAC,IAAAC,UAAA,GAAAlD,0BAAA,CAC3C8C,WAAW;MAAAK,MAAA;IAAA;MAA9B,KAAAD,UAAA,CAAAhD,CAAA,MAAAiD,MAAA,GAAAD,UAAA,CAAA/C,CAAA,IAAAC,IAAA,GAAgC;QAAA,IAArBP,IAAI,GAAAsD,MAAA,CAAA9C,KAAA;QACb,IAAI+C,aAAa,GACbL,cAAc,CAACrD,cAAc,CAACG,IAAI,CAAC,GAAGkD,cAAc,CAAClD,IAAI,CAAC,GAAG,EAAE;QACnE,IAAI,CAACjB,KAAK,CAACC,OAAO,CAACuE,aAAa,CAAC,EAAE;UACjCA,aAAa,GAAG,CAACA,aAAa,CAAC;;QAEjCH,aAAa,CAAC3C,IAAI,CAAC8C,aAAa,CAAC;;IAClC,SAAA7C,GAAA;MAAA2C,UAAA,CAAA1C,CAAA,CAAAD,GAAA;IAAA;MAAA2C,UAAA,CAAAzC,CAAA;IAAA;IACD,OAAOwC,aAAa;;AAExB;AA2DA,IAAMI,wBAAwB,GAAG,cAAc;AAE/C;;;;;;;;;;;;AAYA,WAAaC,WAAY,0BAAAC,UAAA;EAAAC,SAAA,CAAAF,WAAA,EAAAC,UAAA;EAAA,IAAAE,MAAA,GAAAC,YAAA,CAAAJ,WAAA;EA4CvB,SAAAA,YAAYK,IAAmB;IAAA,IAAAC,KAAA;IAAAC,eAAA,OAAAP,WAAA;IAC7BM,KAAA,GAAAH,MAAA,CAAAK,IAAA,OAAMH,IAAI;IACVC,KAAA,CAAKG,UAAU,GAAG,KAAK;IAAC,OAAAH,KAAA;EAC1B;EAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;EAAAI,YAAA,CAAAV,WAAA;IAAA7D,GAAA;IAAAY,KAAA,EAmCA,SAAA4D,QACIC,UAAmB,EAAEC,SAAoB,EAGsB;MAAA,IAF/DC,OAAA,GAAAhF,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAEoDiF,OAAO,CAACC,GAAG;MACjE,IAAI,CAAC,IAAI,CAACC,KAAK,EAAE;QACf,MAAM,IAAI7H,UAAU,CAChB,qIAC+D,mDACf,CAAC;;MAEvDa,YAAY,CAAC,IAAI,EAAE2G,UAAU,EAAEC,SAAS,EAAEC,OAAO,CAAC;IACpD;IAEA;;;;;;;;;;EAAA;IAAA3E,GAAA;IAAAY,KAAA,EAUA,SAAAmE,QAAQb,IAAsB;MAAA,IAAAc,MAAA;MAC5B,IAAId,IAAI,CAACtB,IAAI,IAAI,IAAI,EAAE;QACrBsB,IAAI,CAACtB,IAAI,GAAG,EAAE;;MAEhB,IAAI,CAACA,IAAI,GAAGsB,IAAI,CAACtB,IAAI;MAErB,IAAI,OAAOsB,IAAI,CAACe,SAAS,KAAK,QAAQ,EAAE;QACtC,IAAI,CAACC,UAAU,GAAG5H,UAAU,CAAC6H,YAAY,CAACjB,IAAI,CAACe,SAAS,CAAC;QACzD,IAAI,CAACG,gBAAgB,GAAG,IAAI;OAC7B,MAAM;QACL,IAAI,EAAElB,IAAI,CAACe,SAAS,YAAY5I,SAAS,CAAC,EAAE;UAC1C,MAAM,IAAIY,UAAU,+DAC8C;;QAEpE,IAAI,CAACiI,UAAU,GAAGhB,IAAI,CAACe,SAAS;QAChC,IAAI,CAACG,gBAAgB,GAAG,KAAK;;MAG/B;MACA;MAEA;MACA,IAAIC,aAAa,GAAqB,EAAE;MACxC,IAAI,CAAClG,KAAK,CAACC,OAAO,CAAC8E,IAAI,CAACtB,IAAI,CAAC,IAAI,OAAOsB,IAAI,CAACtB,IAAI,KAAK,QAAQ,IAC1D,OAAOsB,IAAI,CAACtB,IAAI,KAAK,UAAU,EAAE;QACnCsB,IAAI,CAACtB,IAAI,GAAGsB,IAAI,CAACtB,IAAsC;QACvD,KAAK,IAAMxC,IAAI,IAAI8D,IAAI,CAACtB,IAAI,EAAE;UAC5B,IAAI,IAAI,CAACS,WAAW,CAACR,OAAO,CAACzC,IAAI,CAAC,KAAK,CAAC,CAAC,EAAE;YACzC,MAAM,IAAInD,UAAU,CAChB,uCAAAiD,MAAA,CAAsCE,IAAI,iDAAAF,MAAA,CACL,IAAI,CAACmD,WAAW,CAAE,CAAC;;;QAE/D,IAAAiC,UAAA,GAAA/E,0BAAA,CACkB,IAAI,CAAC8C,WAAW;UAAAkC,MAAA;QAAA;UAAnC,KAAAD,UAAA,CAAA7E,CAAA,MAAA8E,MAAA,GAAAD,UAAA,CAAA5E,CAAA,IAAAC,IAAA,GAAqC;YAAA,IAA1BP,KAAI,GAAAmF,MAAA,CAAA3E,KAAA;YACb,IAAIsD,IAAI,CAACtB,IAAI,CAACxC,KAAI,CAAC,IAAI,IAAI,EAAE;cAC3BwE,OAAO,CAACY,IAAI,CACR,YAAAtF,MAAA,CAAWE,KAAI,oHAC+C,sBAAAF,MAAA,CAC3CE,KAAI,qBAAkB,CAAC;;YAEhDiF,aAAa,CAACxE,IAAI,CAACzD,MAAM,CAACqI,GAAG,CAACvB,IAAI,CAACtB,IAAI,CAACxC,KAAI,CAAC,CAAC,CAAC;;QAChD,SAAAU,GAAA;UAAAwE,UAAA,CAAAvE,CAAA,CAAAD,GAAA;QAAA;UAAAwE,UAAA,CAAAtE,CAAA;QAAA;OACF,MAAM,IAAI7B,KAAK,CAACC,OAAO,CAAC8E,IAAI,CAACtB,IAAI,CAAC,EAAE;QACnC,IAAIsB,IAAI,CAACtB,IAAI,CAAChD,MAAM,KAAK,IAAI,CAAC8F,OAAO,CAAC9F,MAAM,EAAE;UAC5C,MAAM,IAAI3C,UAAU,CAChB,gGAAAiD,MAAA,CAC+B,IAAI,CAACwF,OAAO,CAAC9F,MAAM,iBAAc,0BAAAM,MAAA,CACzCgE,IAAI,CAACtB,IAAI,MAAG,CAAC;;QAE1C,IAAM+C,SAAS,GAAGzB,IAAI,CAACtB,IAAoC;QAC3DyC,aAAa,GAAGM,SAAS,CAACxF,GAAG,CAAC,UAAAyF,CAAC;UAAA,OAAIxI,MAAM,CAACqI,GAAG,CAACG,CAAC,CAAC;QAAA,EAAC;OAClD,MAAM;QACL,IAAMC,YAAY,GAAGzI,MAAM,CAACqI,GAAG,CAACvB,IAAI,CAACtB,IAAI,CAAC;QAC1C,IAAI,CAAC8C,OAAO,CAACI,OAAO,CAAC,UAAAC,CAAC,EAAG;UACvBV,aAAa,CAACxE,IAAI,CAACgF,YAAY,CAAC;QAClC,CAAC,CAAC;;MAGJ,IAAI,CAACR,aAAa,GAAGA,aAAa;MAElC,IAAI,CAACW,eAAe,GAAG,EAAE;MACzB,IAAI,CAACC,gBAAgB,GAAG,EAAE;MAC1B,IAAI,CAACC,WAAW,GAAG,EAAE;MACrB,KAAK,IAAIhF,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG,IAAI,CAACwE,OAAO,CAAC9F,MAAM,EAAE,EAAEsB,CAAC,EAAE;QAC5C;QACA,IAAMD,KAAK,GAAG,IAAI,CAACkF,oBAAoB,CAACjF,CAAC,CAAC;QAC1C,IAAMd,MAAI,GAAG,IAAI,CAACiD,WAAW,CAACnC,CAAC,CAAC;QAChC,IAAI,CAAC8E,eAAe,CAACnF,IAAI,CAACT,MAAI,CAAC;QAC/B,IAAI,CAAC6F,gBAAgB,CAACpF,IAAI,CAACI,KAAK,CAAC;QACjC,IAAI,CAACiF,WAAW,CAACrF,IAAI,CAAC,IAAI,CAACwE,aAAa,CAACnE,CAAC,CAAC,CAAC;;MAG9C;MACA;MACA,IAAMkF,iBAAiB,GAAa,EAAE;MAEtC;MACA,IAAI,CAAChD,OAAO,GAAGc,IAAI,CAACd,OAAO;MAC3B;MACA,IAAI,CAACiD,YAAY,GAAG,CAAC,MAAM,CAAC;MAC5B,IAAI,CAACC,cAAc,GAAG,EAAE;MAExB;MACA;MACA;MACA;MACAxJ,SAAS,CAAC,MAAM,EAAE,YAAK;QACrB,KAAK,IAAIoE,EAAC,GAAG,CAAC,EAAEA,EAAC,GAAG8D,MAAI,CAACU,OAAO,CAAC9F,MAAM,EAAE,EAAEsB,EAAC,EAAE;UAC5C,IAAIkF,iBAAiB,CAACvD,OAAO,CAAC3B,EAAC,CAAC,KAAK,CAAC,CAAC,EAAE;YACvC;;UAEF;UACA;UACA,IAAMqF,YAAY,GAAGvB,MAAI,CAACK,aAAa,CAACnE,EAAC,CAAC;UAC1C,IAAI8D,MAAI,CAACU,OAAO,CAAC9F,MAAM,GAAG,CAAC,EAAE;YAC3BoF,MAAI,CAACsB,cAAc,CAACzF,IAAI,CAAC,CAAC0F,YAAY,EAAErF,EAAC,CAAC,CAAC;YAC3C8D,MAAI,CAACqB,YAAY,CAACxF,IAAI,CAACmE,MAAI,CAAC3B,WAAW,CAACnC,EAAC,CAAC,GAAG,OAAO,CAAC;;;QAIzD;QACA;MACF,CAAC,CAAC;;MAEF,IAAMsC,aAAa,GAAGL,cAAc,CAACe,IAAI,CAACd,OAAO,EAAE,IAAI,CAACC,WAAW,CAAC;MACpE;MAEA;;;MAGA,IAAMmD,YAAY,GACd,SADEA,YAAYA,CACbC,WAAmB,EAAEC,UAAkB,EACvCC,YAA4B,EAAI;QAC/B,IAAI3B,MAAI,CAAC3B,WAAW,CAACzD,MAAM,GAAG,CAAC,EAAE;UAC/B8G,UAAU,GAAG1B,MAAI,CAAC3B,WAAW,CAACoD,WAAW,CAAC,GAAG,GAAG,GAAGC,UAAU;;QAE/D1B,MAAI,CAACqB,YAAY,CAACxF,IAAI,CAAC6F,UAAU,CAAC;QAClC1B,MAAI,CAACsB,cAAc,CAACzF,IAAI,CAAC,CAAC8F,YAAY,EAAEF,WAAW,CAAC,CAAC;MACvD,CAAC;MAEL3J,SAAS,CAAC,QAAQ,EAAE,YAAK;QAAA,IAAA8J,KAAA,YAAAA,MAAAC,GAAA,EACuB;UAC5C,IAAIT,iBAAiB,CAACvD,OAAO,CAAC3B,GAAC,CAAC,KAAK,CAAC,CAAC,EAAE;YAAA;;UAGzC,IAAMyC,aAAa,GAAGH,aAAa,CAACtC,GAAC,CAAC;UACtC;UAEA;UACA,IAAM4F,aAAa,GAAG,SAAhBA,aAAaA,CAAI1D,OAAqC,EAAI;YAC9D,IAAM2D,gBAAgB,GAAG,EAAE;YAC3B,IAAIL,UAAkB;YACtB,IAAIM,KAAqB;YACzB,IAAIC,gBAAgC;YACpC;YAAA,IAAAC,UAAA,GAAA3G,0BAAA,CAEqB6C,OAAO;cAAA+D,MAAA;YAAA;cAAA,IAAAC,MAAA,YAAAA,OAAA,EAAE;gBAAA,IAAnBC,MAAM,GAAAF,MAAA,CAAAvG,KAAA;gBACf,IAAI,OAAOyG,MAAM,KAAK,QAAQ,IAC1B,CAAC,UAAU,EAAE,KAAK,EAAE,cAAc,EAAE,IAAI,CAAC,CAACxE,OAAO,CAACwE,MAAM,CAAC,KACrD,CAAC,CAAC,EAAE;kBACV,IAAMC,WAAW,GAAGtC,MAAI,CAACmB,oBAAoB,CAACjF,GAAC,CAAC;kBAEhD,IAAIoG,WAAW,CAACA,WAAW,CAAC1H,MAAM,GAAG,CAAC,CAAC,KAAK,CAAC,IACzCoF,MAAI,CAACK,aAAa,CAACnE,GAAC,CAAC,KAAK9D,MAAM,CAACqF,kBAAkB,EAAE;oBACvD;oBACA,IAAI,CAAC,UAAU,EAAE,KAAK,CAAC,CAACI,OAAO,CAACwE,MAAM,CAAC,KAAK,CAAC,CAAC,EAAE;sBAC9CL,KAAK,GAAG3J,OAAO,CAACkK,cAAc;qBAC/B,MAAM,IAAI,CAAC,cAAc,EAAE,IAAI,CAAC,CAAC1E,OAAO,CAACwE,MAAM,CAAC,KAAK,CAAC,CAAC,EAAE;sBACxDL,KAAK,GAAG3J,OAAO,CAACoF,kBAAkB;;mBAErC,MAAM,IACHuC,MAAI,CAACK,aAAa,CAACnE,GAAC,CAAC,KACrB9D,MAAM,CAACoK,6BAA6B,EAAE;oBACxC;oBACA;oBACA,IAAI,CAAC,UAAU,EAAE,KAAK,CAAC,CAAC3E,OAAO,CAACwE,MAAM,CAAC,KAAK,CAAC,CAAC,EAAE;sBAC9CL,KAAK,GAAG3J,OAAO,CAACoK,yBAAyB;qBAC1C,MAAM,IAAI,CAAC,cAAc,EAAE,IAAI,CAAC,CAAC5E,OAAO,CAACwE,MAAM,CAAC,KAAK,CAAC,CAAC,EAAE;sBACxDL,KAAK,GAAG3J,OAAO,CAACmK,6BAA6B;;mBAEhD,MAAM;oBACL;oBACA,IAAI,CAAC,UAAU,EAAE,KAAK,CAAC,CAAC3E,OAAO,CAACwE,MAAM,CAAC,KAAK,CAAC,CAAC,EAAE;sBAC9CL,KAAK,GAAG3J,OAAO,CAACqK,mBAAmB;qBACpC,MAAM,IAAI,CAAC,cAAc,EAAE,IAAI,CAAC,CAAC7E,OAAO,CAACwE,MAAM,CAAC,KAAK,CAAC,CAAC,EAAE;sBACxDL,KAAK,GAAG3J,OAAO,CAACqF,uBAAuB;;;kBAG3C,IAAIiF,MAAc;kBAClB,IAAI,CAAC,UAAU,EAAE,KAAK,CAAC,CAAC9E,OAAO,CAACwE,MAAM,CAAC,KAAK,CAAC,CAAC,EAAE;oBAC9CM,MAAM,GAAG,KAAK;mBACf,MAAM,IAAI,CAAC,cAAc,EAAE,IAAI,CAAC,CAAC9E,OAAO,CAACwE,MAAM,CAAC,KAAK,CAAC,CAAC,EAAE;oBACxDM,MAAM,GAAG,IAAI;;kBAEf;kBACAV,gBAAgB,GAAGD,KAAK;kBACxBN,UAAU,GAAGK,gBAAgB,GAAGY,MAAM;iBACvC,MAAM;kBACL,IAAMC,QAAQ,GAAGvK,OAAO,CAACoI,GAAG,CAAC4B,MAAM,CAAC;kBACpC;kBACAJ,gBAAgB,GAAGW,QAAQ;kBAC3BlB,UAAU,GACNK,gBAAgB,GAAG1J,OAAO,CAACwK,mBAAmB,CAACR,MAAM,CAAC;;gBAG5D;gBACA,IAAIS,YAA4B;gBAChChL,SAAS,CAAC4J,UAAU,EAAE,YAAK;kBACzBoB,YAAY,GAAGb,gBAAgB;gBACjC,CAAC,CAAC;gBACFT,YAAY,CAACtF,GAAC,EAAEwF,UAAU,EAAEoB,YAAY,CAAC;eAC1C;cAvDD,KAAAZ,UAAA,CAAAzG,CAAA,MAAA0G,MAAA,GAAAD,UAAA,CAAAxG,CAAA,IAAAC,IAAA;gBAAAyG,MAAA;cAAA;YAuDC,SAAAtG,GAAA;cAAAoG,UAAA,CAAAnG,CAAA,CAAAD,GAAA;YAAA;cAAAoG,UAAA,CAAAlG,CAAA;YAAA;UACH,CAAC;UAED8F,aAAa,CAACnD,aAAa,CAAC;UAC5B;SACD;QA3ED,KAAK,IAAIzC,GAAC,GAAG,CAAC,EAAEA,GAAC,GAAG8D,MAAI,CAACU,OAAO,CAAC9F,MAAM,EAAE,EAAEsB,GAAC;UAAA,IAAA6G,IAAA,GAAAnB,KAAA,CAAAC,GAAA;UAAA,IAAAkB,IAAA,iBAExC;QAAS;MA0Ef,CAAC,CAAC;MAEF;MACA;MACA,IAAI,CAACC,yBAAyB,GAAG,IAAI,CAACC,gBAAgB;IACxD;IAEA;;;;;;;;;EAAA;IAAAjI,GAAA;IAAAY,KAAA,EASU,SAAAsH,iCAAA,EAAgC;MACxC,IAAI,IAAI,CAACF,yBAAyB,IAAI,IAAI,EAAE;QAC1C;;MAEF,IAAI,IAAI,CAACC,gBAAgB,CAACrI,MAAM,KAC5B,IAAI,CAACoI,yBAAyB,CAACpI,MAAM,EAAE;QACzCgF,OAAO,CAACY,IAAI,CACR,+DAA+D,GAC/D,yDAAyD,GACzD,+BAA+B,CAAC;;IAExC;IAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;EAAA;IAAAxF,GAAA;IAAAY,KAAA,EA+BA,SAAAuH,SACIlJ,CAAkB,EAAE0D,CAAkB,EACV;MAAA,IAA5BuB,IAAA,GAAAvE,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAA0B,EAAE;MAC9B,IAAMyI,SAAS,GAAGlE,IAAI,CAACkE,SAAS,IAAI,IAAI,GAAG,EAAE,GAAGlE,IAAI,CAACkE,SAAS;MAC9D7J,cAAc,CAAC6J,SAAS,CAAC;MAEzB;MACA;MACA,IAAM1I,cAAc,GAAG,IAAI;MAC3B,IAAM2I,gBAAgB,GAClB,IAAI,CAACC,qBAAqB,CAACrJ,CAAC,EAAE0D,CAAC,EAAEjD,cAAc,EAAE0I,SAAS,CAAC;MAC/D,IAAI;QACF;QACA;QACA,IAAMG,GAAG,GAAGF,gBAAgB,CAAC,CAAC,CAAC,CAACnI,MAAM,CAACmI,gBAAgB,CAAC,CAAC,CAAC,CAAC;QAC3D,IAAI,CAACG,gBAAgB,EAAE;QACvB,IAAMxH,CAAC,GAAG,IAAI,CAACyH,YAAY;QAC3B,IAAMC,QAAQ,GACV,IAAI,CAACC,QAAQ,CAAC3H,CAAC,EAAEuH,GAAG,EAAEH,SAAS,EAAElE,IAAI,CAAC0E,OAAO,EAAE1E,IAAI,CAAC2E,KAAK,CAAC;QAC9D,OAAOnL,gBAAgB,CAACgL,QAAQ,CAAC;OAClC,SAAS;QACRlK,iBAAiB,CAAC6J,gBAAgB,CAAC,CAAC,CAAC,EAAEpJ,CAAC,CAAC;QACzCT,iBAAiB,CAAC6J,gBAAgB,CAAC,CAAC,CAAC,EAAE1F,CAAC,CAAC;;IAE7C;IAEA;IACA;IACA;;;;;;;;;;;;;;;;;;;;EAAA;IAAA3C,GAAA;IAAAY,KAAA;MAAA,IAAAkI,iBAAA,GAAAC,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CAoBA,SAAAC,QAAsBC,OAAoB,EAAEjF,IAA+B;QAAA,OAAA8E,mBAAA,GAAAI,IAAA,UAAAC,SAAAC,QAAA;UAAA,kBAAAA,QAAA,CAAAC,IAAA,GAAAD,QAAA,CAAAE,IAAA;YAAA;cAEzE,IAAI,CAAChB,gBAAgB,EAAE;cAAC,OAAAc,QAAA,CAAAG,MAAA,WACjBpL,gBAAe,CAAC,IAAI,EAAE8K,OAAO,EAAEjF,IAAI,CAAC;YAAA;YAAA;cAAA,OAAAoF,QAAA,CAAAI,IAAA;UAAA;QAAA,GAAAR,OAAA;MAAA,CAC5C;MAAA,SAAA7K,gBAAAsL,EAAA,EAAAC,GAAA;QAAA,OAAAd,iBAAA,CAAAe,KAAA,OAAAlK,SAAA;MAAA;MAAA,OAAAtB,eAAA;IAAA;IAED;;;;;;;;;;EAAA;IAAA2B,GAAA;IAAAY,KAAA,EAUQ,SAAAkJ,gBACJvB,GAAoB,EAAEH,SAAkB,EAAES,KAAc,EACrC;MAAA,IAAnBkB,SAAS,GAAApK,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,OAAO;MACrB,IAAIqK,UAAkB;MACtB,IAAInB,KAAK,IAAI,IAAI,EAAE;QACjBmB,UAAU,GAAG,IAAI;QACjB,IAAI5B,SAAS,IAAI,IAAI,EAAE;UACrB,MAAM,IAAInL,UAAU,CAChB,MAAAiD,MAAA,CAAM6J,SAAS,wEAAA7J,MAAA,CACIkI,SAAS,CAAE,CAAC;;OAEtC,MAAM,IAAIG,GAAG,IAAI,IAAI,EAAE;QACtB,IAAIpJ,KAAK,CAACC,OAAO,CAACmJ,GAAG,CAAC,EAAE;UACtByB,UAAU,GAAGzB,GAAG,CAAC,CAAC,CAAC,CAACtH,KAAK,CAAC,CAAC,CAAC;SAC7B,MAAM;UACL+I,UAAU,GAAGzB,GAAG,CAACtH,KAAK,CAAC,CAAC,CAAC;;OAE5B,MAAM;QACL,MAAM,IAAIhE,UAAU,CAChB,8DAAAiD,MAAA,CACG6J,SAAS,yBAAsB,CAAC;;MAEzC,OAAOC,UAAU;IACnB;IAEA;;;;;;;EAAA;IAAAhK,GAAA;IAAAY,KAAA,EAOA,SAAAzC,QAAQsD,MAAsC,EAAEiE,OAAwB;MAEtE,IAAIvG,KAAK,CAACC,OAAO,CAACsG,OAAO,CAAC,IAAIA,OAAO,CAAC9F,MAAM,KAAK,CAAC,EAAE;QAClD,MAAM,IAAI3C,UAAU,CAChB,oDAAoD,CAAC;;MAG3D,IAAMgN,cAAc,GAAG9K,KAAK,CAACC,OAAO,CAACsG,OAAO,CAAC;MAC7C,IAAMrC,WAAW,GACZ4G,cAAc,GAAGvE,OAAO,GAAG,CAACA,OAAO,CAAE;MAC1C,IAAMwE,qBAAqB,GAAG,IAAI,CAACC,uBAAuB,CAAC9G,WAAW,CAAC;MAEvE;MACA,IAAM+G,QAAQ,GAAG,IAAIhM,QAAQ,EAAE;MAC/B,IAAIqD,MAAM,YAAYjF,MAAM,EAAE;QAC5BiF,MAAM,GAAG,CAACA,MAAM,CAAC;;MAEnB,IAAItC,KAAK,CAACC,OAAO,CAACqC,MAAM,CAAC,EAAE;QACzB,IAAIA,MAAM,CAAC7B,MAAM,KAAK,IAAI,CAAC6B,MAAM,CAAC7B,MAAM,EAAE;UACxC,MAAM,IAAI3C,UAAU,CAChB,kCAAAiD,MAAA,CAAkCuB,MAAM,CAAC7B,MAAM,8DACK,OAAAM,MAAA,CAChD,IAAI,CAACuB,MAAM,CAAC7B,MAAM,OAAI,CAAC;;QAEjC,KAAK,IAAIsB,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG,IAAI,CAACO,MAAM,CAAC7B,MAAM,EAAE,EAAEsB,CAAC,EAAE;UAC3CkJ,QAAQ,CAACC,GAAG,CAAC,IAAI,CAAC5I,MAAM,CAACP,CAAC,CAAC,EAAEO,MAAM,CAACP,CAAC,CAAC,CAAC;;OAE1C,MAAM;QAAA,IAAAoJ,UAAA,GAAA/J,0BAAA,CACe,IAAI,CAACkB,MAAM;UAAA8I,MAAA;QAAA;UAA/B,KAAAD,UAAA,CAAA7J,CAAA,MAAA8J,MAAA,GAAAD,UAAA,CAAA5J,CAAA,IAAAC,IAAA,GAAiC;YAAA,IAAtBkB,KAAK,GAAA0I,MAAA,CAAA3J,KAAA;YACd,IAAM4J,WAAW,GAAG/I,MAAM,CAACI,KAAK,CAACzB,IAAI,CAAC;YACtC,IAAIoK,WAAW,IAAI,IAAI,EAAE;cACvB,MAAM,IAAIvN,UAAU,+CAAAiD,MAAA,CAC8B2B,KAAK,CAACzB,IAAI,EAAG;;YAEjEgK,QAAQ,CAACC,GAAG,CAACxI,KAAK,EAAE2I,WAAW,CAAC;;QACjC,SAAA1J,GAAA;UAAAwJ,UAAA,CAAAvJ,CAAA,CAAAD,GAAA;QAAA;UAAAwJ,UAAA,CAAAtJ,CAAA;QAAA;;MAGH;MACA,IAAMyJ,cAAc,GAAGtM,QAAO,CAAC+L,qBAAqB,EAAEE,QAAQ,CAAa;MAC3E,OAAOH,cAAc,GAAGQ,cAAc,GAAGA,cAAc,CAAC,CAAC,CAAC;IAC5D;IAEA;;;EAAA;IAAAzK,GAAA;IAAAY,KAAA,EAGQ,SAAAuJ,wBAAwBO,mBAA6B;MAE3D,IAAMR,qBAAqB,GACvBzM,YAAY,CAAC,IAAI,EAAEiN,mBAAmB,CAAC9K,MAAM,CAAC;MAClD,IAAI+K,gBAAgB,GAAGD,mBAAmB,CAAC9K,MAAM;MAAC,IAAAgL,UAAA,GAAArK,0BAAA,CAC9B,IAAI,CAACsK,MAAM;QAAAC,MAAA;MAAA;QAA/B,KAAAF,UAAA,CAAAnK,CAAA,MAAAqK,MAAA,GAAAF,UAAA,CAAAlK,CAAA,IAAAC,IAAA,GAAiC;UAAA,IAAtBoK,KAAK,GAAAD,MAAA,CAAAlK,KAAA;UACd,IAAMoK,YAAY,GACd7L,KAAK,CAACC,OAAO,CAAC2L,KAAK,CAACE,MAAM,CAAC,GAAGF,KAAK,CAACE,MAAM,GAAG,CAACF,KAAK,CAACE,MAAM,CAAC;UAC/D,IAAMC,gBAAgB,GAAGF,YAAY,CAAC7K,GAAG,CAAC,UAAA8K,MAAM;YAAA,OAAIA,MAAM,CAAC7K,IAAI;UAAA,EAAC;UAChE,KAAK,IAAIc,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGwJ,mBAAmB,CAAC9K,MAAM,EAAE,EAAEsB,CAAC,EAAE;YACnD,IAAMiK,KAAK,GAAGD,gBAAgB,CAACrI,OAAO,CAAC6H,mBAAmB,CAACxJ,CAAC,CAAC,CAAC;YAC9D,IAAIiK,KAAK,KAAK,CAAC,CAAC,EAAE;cAChBjB,qBAAqB,CAAChJ,CAAC,CAAC,GAAG8J,YAAY,CAACG,KAAK,CAAC;cAC9CR,gBAAgB,EAAE;;YAEpB,IAAIA,gBAAgB,KAAK,CAAC,EAAE;cAC1B;;;UAGJ,IAAIA,gBAAgB,KAAK,CAAC,EAAE;YAC1B;;;MAEH,SAAA7J,GAAA;QAAA8J,UAAA,CAAA7J,CAAA,CAAAD,GAAA;MAAA;QAAA8J,UAAA,CAAA5J,CAAA;MAAA;MAED,IAAI2J,gBAAgB,GAAG,CAAC,EAAE;QACxB,IAAMS,cAAc,GAAa,EAAE;QACnClB,qBAAqB,CAACpE,OAAO,CAAC,UAACuF,MAAM,EAAEnK,CAAC,EAAI;UAC1C,IAAImK,MAAM,IAAI,IAAI,EAAE;YAClBD,cAAc,CAACvK,IAAI,CAAC6J,mBAAmB,CAACxJ,CAAC,CAAC,CAAC;;QAE/C,CAAC,CAAC;QACF,MAAM,IAAIjE,UAAU,CAChB,wDAAAiD,MAAA,CACG+B,IAAI,CAACC,SAAS,CAACkJ,cAAc,CAAC,CAAE,CAAC;;MAE1C,OAAOlB,qBAAqB;IAC9B;IAEA;;;;;;;;;;;;;EAAA;IAAAlK,GAAA;IAAAY,KAAA,EAaQ,SAAA0K,YAAY/C,GAAoB,EAAiC;MAAA,IAAAgD,MAAA;MAAA,IAA/BnD,SAAS,GAAAzI,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,EAAE;MAAA,IAAEiJ,OAAO,GAAAjJ,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,KAAK;MAEvE,OAAOxD,GAAG,CAACqP,IAAI,CAAC,YAAK;QACnB,IAAMxB,UAAU,GAAGuB,MAAI,CAACzB,eAAe,CAACvB,GAAG,CAAC;QAC5C,IAAIK,OAAO,EAAE;UACX,MAAM,IAAI7L,mBAAmB,CACzB,+CAA+C,CAAC;;QAGtD;QACA;QACA;QACA;QAEA,IAAM0O,OAAO,GAAG/M,WAAW,CAACsL,UAAU,EAAE5B,SAAS,CAAC;QAClD,IAAMsD,WAAW,GAAeH,MAAI,CAAC7F,OAAO,CAACvF,GAAG,CAAC,UAAA8K,MAAM;UAAA,OAAI,EAAE;QAAA,EAAC;QAE9D;QAAA,IAAAU,MAAA,YAAAA,OAAAC,UAAA,EACoE;UAClE,IAAMC,SAAS,GAAG1P,GAAG,CAACqP,IAAI,CAAC,YAAK;YAC9B,IAAMM,UAAU,GAAGL,OAAO,CAACG,UAAU,CAAC,CAAC,CAAC,CAAC;YACzC,IAAMG,QAAQ,GAAGN,OAAO,CAACG,UAAU,CAAC,CAAC,CAAC,CAAC;YACvC;YACA;YACA,IAAMI,QAAQ,GAAGrN,WAAW,CAAC4J,GAAG,EAAEuD,UAAU,EAAEC,QAAQ,CAAC;YAEvD;YACA,IAAME,KAAK,GAAG,EAAE;YAChB,IAAI9M,KAAK,CAACC,OAAO,CAAC4M,QAAQ,CAAC,EAAE;cAC3B,KAAK,IAAI9K,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG8K,QAAQ,CAACpM,MAAM,EAAE,EAAEsB,CAAC,EAAE;gBACxC+K,KAAK,CAACpL,IAAI,CAAC;kBAACb,GAAG,EAAEuL,MAAI,CAAC9J,MAAM,CAACP,CAAC,CAAC;kBAAEN,KAAK,EAAEoL,QAAQ,CAAC9K,CAAC;gBAAC,CAAC,CAAC;;aAExD,MAAM;cACL+K,KAAK,CAACpL,IAAI,CAAC;gBAACb,GAAG,EAAEuL,MAAI,CAAC9J,MAAM,CAAC,CAAC,CAAC;gBAAEb,KAAK,EAAEoL;cAAQ,CAAC,CAAC;;YAEpD,IAAM5B,QAAQ,GAAG,IAAIhM,QAAQ,CAAC6N,KAAK,CAAC;YACpC,OAAO9N,QAAO,CAACoN,MAAI,CAAC7F,OAAO,EAAE0E,QAAQ,CAAa;UACpD,CAAC,CAAC;UACFyB,SAAS,CAAC/F,OAAO,CAAC,UAACoG,QAAQ,EAAEhL,CAAC;YAAA,OAAKwK,WAAW,CAACxK,CAAC,CAAC,CAACL,IAAI,CAACqL,QAAQ,CAAC;UAAA,EAAC;SAClE;QArBD,KAAK,IAAIN,UAAU,GAAG,CAAC,EAAEA,UAAU,GAAGH,OAAO,CAAC7L,MAAM,EAAE,EAAEgM,UAAU;UAAAD,MAAA,CAAAC,UAAA;QAAA;QAsBlE,OAAOlO,gBAAgB,CACnBgO,WAAW,CAACvL,GAAG,CAAC,UAAAsL,OAAO;UAAA,OAAItP,GAAG,CAAC+D,MAAM,CAACuL,OAAO,EAAE,CAAC,CAAC;QAAA,EAAC,CAAC;MACzD,CAAC,CAAC;IACJ;IAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;EAAA;IAAAzL,GAAA;IAAAY,KAAA,EA2BA,SAAAuL,QAAQlN,CAAkB,EAA6B;MAAA,IAA3BiF,IAAA,GAAAvE,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAyB,EAAE;MACrD,IAAMyM,eAAe,GAAG3N,0BAA0B,CAACQ,CAAC,CAAC;MACrDiE,cAAc,CACVkJ,eAAe,EAAE,IAAI,CAACC,UAAU,EAAE,IAAI,CAACC,eAAe,EAAE,KAAK,CAAC;MAClE,IAAI;QACF;QACA;QACA;QACA;QACA,IAAMlE,SAAS,GAAGlE,IAAI,CAACkE,SAAS,IAAI,IAAI,GAAG,EAAE,GAAGlE,IAAI,CAACkE,SAAS;QAC9D7J,cAAc,CAAC6J,SAAS,CAAC;QACzB,OAAO,IAAI,CAACkD,WAAW,CAACc,eAAe,EAAEhE,SAAS,CAAC;OACpD,SAAS;QACR5J,iBAAiB,CAAC4N,eAAe,EAAEnN,CAAC,CAAC;;IAEzC;IAEA;;;;;;;;;;;;;;;EAAA;IAAAe,GAAA;IAAAY,KAAA,EAeA,SAAA2L,eAAetN,CAAkB;MAC/BiE,cAAc,CAACjE,CAAC,EAAE,IAAI,CAACoN,UAAU,EAAE,IAAI,CAACC,eAAe,EAAE,IAAI,CAAC;MAC9D;MACA;MACA,IAAMlE,SAAS,GAAG,CAACjJ,KAAK,CAACC,OAAO,CAACH,CAAC,CAAC,GAAGA,CAAC,CAAC,CAAC,CAAC,GAAGA,CAAC,EAAEgC,KAAK,CAAC,CAAC,CAAC;MACxD,OAAO,IAAI,CAACqK,WAAW,CAACrM,CAAC,EAAEmJ,SAAS,CAAC;IACvC;EAAC;IAAApI,GAAA;IAAAY,KAAA,EAES,SAAA0H,sBACNrJ,CAAgD,EAChD0D,CAAgD,EAC9B;MAAA,IADgCjD,cAAc,GAAAC,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,IAAI;MAAA,IACvEyI,SAAkB,GAAAzI,SAAA,CAAAC,MAAA,OAAAD,SAAA,MAAAE,SAAA;MACpB;MACA,IAAI,IAAI,CAACqF,UAAU,IAAI,IAAI,EAAE;QAC3B,MAAM,IAAIlI,YAAY,CAClB,wDAAwD,GACxD,wCAAwC,CAAC;;MAE/C,IAAMsF,YAAY,GAAY,EAAE;MAChC,KAAK,IAAIpB,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG,IAAI,CAAC+E,gBAAgB,CAACrG,MAAM,EAAE,EAAEsB,CAAC,EAAE;QACrD,IAAMoG,WAAW,GAAG,IAAI,CAACrB,gBAAgB,CAAC/E,CAAC,CAAC;QAC5C,IAAMsL,MAAM,GAAG,IAAI,CAACtG,WAAW,CAAChF,CAAC,CAAC;QAClC,IAAIsL,MAAM,KAAKpP,MAAM,CAACoK,6BAA6B,EAAE;UACnDlF,YAAY,CAACzB,IAAI,CACbyG,WAAW,CAAC/F,KAAK,CAAC,CAAC,EAAE+F,WAAW,CAAC1H,MAAM,GAAG,CAAC,CAAC,CAACM,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;SAC9D,MAAM;UACL;UACAoC,YAAY,CAACzB,IAAI,CAACyG,WAAW,CAAC;;;MAGlCrI,CAAC,GAAGK,oBAAoB,CACpBL,CAAC,EAAE,IAAI,CAACwN,cAAc,EAAE,IAAI,CAACH,eAAe,EAAE,KAAK,EAAE,OAAO,CAAC;MACjE3J,CAAC,GAAGrD,oBAAoB,CACpBqD,CAAC,EAAE,IAAI,CAACqD,eAAe,EAAE1D,YAAY,EAAE,KAAK,EAAE,QAAQ,CAAC;MAC3D;MACAd,iBAAiB,CAACvC,CAAC,EAAE0D,CAAC,EAAE,IAAI,CAAC;MAC7B;MACAP,+BAA+B,CAACO,CAAC,EAAE,IAAI,CAACuD,WAAW,EAAE,IAAI,CAACD,gBAAgB,CAAC;MAC3E,IAAI,IAAI,CAACyG,QAAQ,IAAItE,SAAS,IAAI,IAAI,IAAIA,SAAS,GAAG,CAAC,EAAE;QACvD,IAAInJ,CAAC,CAAC,CAAC,CAAC,CAACgC,KAAK,CAAC,CAAC,CAAC,GAAGmH,SAAS,KAAK,CAAC,EAAE;UACnC,MAAM,IAAInL,UAAU,CAChB,uHACwD,MAAAiD,MAAA,CACrDkI,SAAS,eAAAlI,MAAA,CAAYjB,CAAC,CAAC,CAAC,CAAC,CAACgC,KAAK,CAAC,CAAC,CAAC,gBAAa,CAAC;;;MAG3D,OAAO,CAAChC,CAAC,EAAE0D,CAAC,CAAC;IACf;EAAC;IAAA3C,GAAA;IAAAY,KAAA;MAAA,IAAA+L,oBAAA,GAAA5D,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CAES,SAAA2D,SACN3N,CAAgD,EAChD0D,CAAgD,EAChDkK,YAA6D,EAC7DC,WAAsD;QAAA,IAAApN,cAAA;UAAA0I,SAAA;UAAA2E,qBAAA;UAAAC,sBAAA;UAAAC,UAAA;UAAAC,UAAA;UAAAC,qBAAA;UAAAC,YAAA;UAAAlM,CAAA;UAAAmM,MAAA,GAAA1N,SAAA;QAAA,OAAAqJ,mBAAA,GAAAI,IAAA,UAAAkE,UAAAC,SAAA;UAAA,kBAAAA,SAAA,CAAAhE,IAAA,GAAAgE,SAAA,CAAA/D,IAAA;YAAA;cACtD9J,cAAc,GAAA2N,MAAA,CAAAzN,MAAA,QAAAyN,MAAA,QAAAxN,SAAA,GAAAwN,MAAA,MAAG,IAAI;cACrBjF,SAAkB,GAAAiF,MAAA,CAAAzN,MAAA,OAAAyN,MAAA,MAAAxN,SAAA;cAAAkN,qBAAA,GAEhB,IAAI,CAACzE,qBAAqB,CAACrJ,CAAC,EAAE0D,CAAC,EAAEjD,cAAc,EAAE0I,SAAS,CAAC,EAAA4E,sBAAA,GAAAQ,cAAA,CAAAT,qBAAA,MADxDE,UAAU,GAAAD,sBAAA,KAAEE,UAAU,GAAAF,sBAAA,KAE7B;cAAA,MACIH,YAAY,IAAI,IAAI;gBAAAU,SAAA,CAAA/D,IAAA;gBAAA;cAAA;cAAA,MAChB,IAAIiE,KAAK,CAAC,qCAAqC,CAAC;YAAA;cAGpDN,qBAAqB,GAAa,IAAI;cAAA,MACtCL,WAAW,IAAI,IAAI;gBAAAS,SAAA,CAAA/D,IAAA;gBAAA;cAAA;cACf4D,YAAY,GACdtO,uBAAuB,CAACgO,WAAW,EAAE,IAAI,CAACzJ,WAAW,CAAC;cAC1D8J,qBAAqB,GAAG,EAAE;cACjBjM,CAAC,GAAG,CAAC;YAAA;cAAA,MAAEA,CAAC,GAAGkM,YAAY,CAACxN,MAAM;gBAAA2N,SAAA,CAAA/D,IAAA;gBAAA;cAAA;cAAA+D,SAAA,CAAAG,EAAA,GACrCP,qBAAqB;cAAAI,SAAA,CAAA/D,IAAA;cAAA,OACXzK,kBAAkB,CAACmO,UAAU,CAAChM,CAAC,CAAC,EAAE,IAAI,EAAEkM,YAAY,CAAClM,CAAC,CAAC,CAAC;YAAA;cAAAqM,SAAA,CAAAI,EAAA,GAAAJ,SAAA,CAAAK,IAAA;cAAAL,SAAA,CAAAG,EAAA,CAD5C7M,IAAI,CAAAwD,IAAA,CAAAkJ,SAAA,CAAAG,EAAA,EAAAH,SAAA,CAAAI,EAAA;YAAA;cADa,EAAEzM,CAAC;cAAAqM,SAAA,CAAA/D,IAAA;cAAA;YAAA;cAAA,OAAA+D,SAAA,CAAA9D,MAAA,WAOvC,CAACwD,UAAU,EAAEC,UAAU,EAAEC,qBAAqB,CAAC;YAAA;YAAA;cAAA,OAAAI,SAAA,CAAA7D,IAAA;UAAA;QAAA,GAAAkD,QAAA;MAAA,CACvD;MAAA,SAAAiB,oBAAAC,GAAA,EAAAC,GAAA,EAAAC,GAAA,EAAAC,GAAA;QAAA,OAAAtB,oBAAA,CAAA9C,KAAA,OAAAlK,SAAA;MAAA;MAAA,OAAAkO,mBAAA;IAAA;IAED;;;;;;;;;;;EAAA;IAAA7N,GAAA;IAAAY,KAAA,EAWQ,SAAA+H,SACJ3H,CAA+B,EAAEuH,GAAa,EAAEH,SAAkB,EACvC;MAAA,IAAA8F,MAAA;MAAA,IAA3BtF,OAAO,GAAAjJ,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,CAAC;MAAA,IAAEkJ,KAAc,GAAAlJ,SAAA,CAAAC,MAAA,OAAAD,SAAA,MAAAE,SAAA;MAC7B,OAAO1D,GAAG,CAACqP,IAAI,CAAC,YAAK;QACnB,IAAMxB,UAAU,GAAGkE,MAAI,CAACpE,eAAe,CAACvB,GAAG,EAAEH,SAAS,EAAES,KAAK,EAAE,OAAO,CAAC;QACvE,IAAMsF,IAAI,GAAa,EAAE;QACzB,IAAIvF,OAAO,GAAG,CAAC,EAAE;UACf,MAAM,IAAI7L,mBAAmB,CAAC,sCAAsC,CAAC;;QAEvE;QACA,IAAI8L,KAAK,IAAI,IAAI,EAAE;UACjB,MAAM,IAAI9L,mBAAmB,CACzB,iDAAiD,CAAC;SACvD,MAAM;UACL,IAAM0O,OAAO,GAAG/M,WAAW,CAACsL,UAAU,EAAE5B,SAAS,CAAC;UAClD,IAAMgG,UAAU,GAAG3R,QAAQ,CAACsB,KAAK,CAAC,CAAC,EAAEiM,UAAU,CAAC,CAAC;UACjD,KAAK,IAAI4B,UAAU,GAAG,CAAC,EAAEA,UAAU,GAAGH,OAAO,CAAC7L,MAAM,EAAE,EAAEgM,UAAU,EAAE;YAClE,IAAME,UAAU,GAAGL,OAAO,CAACG,UAAU,CAAC,CAAC,CAAC,CAAC;YACzC,IAAMG,QAAQ,GAAGN,OAAO,CAACG,UAAU,CAAC,CAAC,CAAC,CAAC;YACvC,IAAMyC,QAAQ,GACV1R,CAAC,CAAC2R,mBAAmB,CACjBF,UAAU,EAAEtC,UAAU,EAAEC,QAAQ,GAAGD,UAAU,CAAa;YAClE;YACA;YACA,IAAME,QAAQ,GAAGpN,oBAAoB,CAAC2J,GAAG,EAAE8F,QAAQ,CAAa;YAChE,IAAMxC,SAAS,GAAG7K,CAAC,CAACgL,QAAQ,CAAC;YAC7B,IAAIJ,UAAU,KAAK,CAAC,EAAE;cACpB,KAAK,IAAI1K,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG2K,SAAS,CAACjM,MAAM,EAAE,EAAEsB,CAAC,EAAE;gBACzCiN,IAAI,CAACtN,IAAI,CAACvE,MAAM,CAAC,CAAC,CAAC,CAAC;;;YAGxB,KAAK,IAAI4E,GAAC,GAAG,CAAC,EAAEA,GAAC,GAAG2K,SAAS,CAACjM,MAAM,EAAE,EAAEsB,GAAC,EAAE;cACzC,IAAMgL,QAAQ,GAAGL,SAAS,CAAC3K,GAAC,CAAC;cAC7BiN,IAAI,CAACjN,GAAC,CAAC,GACH/E,GAAG,CAACkO,GAAG,CAAC8D,IAAI,CAACjN,GAAC,CAAC,EAAE/E,GAAG,CAACoS,GAAG,CAACxC,QAAQ,GAAGD,UAAU,EAAEI,QAAQ,CAAC,CAAC;;;UAGlE,KAAK,IAAIhL,GAAC,GAAG,CAAC,EAAEA,GAAC,GAAGiN,IAAI,CAACvO,MAAM,EAAE,EAAEsB,GAAC,EAAE;YACpCiN,IAAI,CAACjN,GAAC,CAAC,GAAG/E,GAAG,CAACqS,GAAG,CAACL,IAAI,CAACjN,GAAC,CAAC,EAAE8I,UAAU,CAAC;;;QAG1C,OAAOmE,IAAI;MACb,CAAC,CAAC;IACJ;EAAC;IAAAnO,GAAA;IAAAY,KAAA,EAES,SAAA6N,uBAAA,EAAsB;MAC9B,IAAMC,SAAS,GAAG,IAAI,CAACrI,YAAY;MACnC;MACA;MACA,IAAMsI,gBAAgB,GAAG,EAAE;MAC3B,KAAK,IAAIzN,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGwN,SAAS,CAAC9O,MAAM,EAAE,EAAEsB,CAAC,EAAE;QACzC,IAAM0N,KAAK,GAAGF,SAAS,CAACxN,CAAC,CAAC;QAC1B,IAAI2N,QAAQ,GAAGD,KAAK;QACpB,IAAIpR,KAAK,CAACkR,SAAS,EAAEE,KAAK,CAAC,GAAG,CAAC,EAAE;UAC/B,IAAME,QAAQ,GAAGtR,KAAK,CAACkR,SAAS,CAACnN,KAAK,CAAC,CAAC,EAAEL,CAAC,CAAC,EAAE0N,KAAK,CAAC;UACpDC,QAAQ,QAAA3O,MAAA,CAAQ4O,QAAQ,CAAE;;QAE5BH,gBAAgB,CAAC9N,IAAI,CAACgO,QAAQ,CAAC;;MAEjC,OAAOF,gBAAgB;IACzB;IAEA;;;;;;;;;;EAAA;IAAA3O,GAAA;IAAAY,KAAA,EAUU,SAAAmO,kBAAA,EAAiB;MAAA,IAAAC,MAAA;MACzB,OAAO,UAACzP,IAAc,EAAI;QACxB,IAAM0P,UAAU,GAAa,EAAE;QAE/B,IAAMxN,MAAM,GAAGlC,IAAI,CAACgC,KAAK,CAAC,CAAC,EAAEyN,MAAI,CAACvN,MAAM,CAAC7B,MAAM,CAAC;QAChD,IAAM8B,OAAO,GAAGnC,IAAI,CAACgC,KAAK,CACtByN,MAAI,CAACvN,MAAM,CAAC7B,MAAM,EAAEoP,MAAI,CAACvN,MAAM,CAAC7B,MAAM,GAAGoP,MAAI,CAACtJ,OAAO,CAAC9F,MAAM,CAAC;QACjE,IAAMsP,aAAa,GAAG3P,IAAI,CAACgC,KAAK,CAC5ByN,MAAI,CAACvN,MAAM,CAAC7B,MAAM,GAAGoP,MAAI,CAACtJ,OAAO,CAAC9F,MAAM,EACxCoP,MAAI,CAACvN,MAAM,CAAC7B,MAAM,GAAGoP,MAAI,CAACtJ,OAAO,CAAC9F,MAAM,GAAG,CAAC,CAAC;QAEjD,IAAMuP,aAAa,GAAa,EAAE;QAElC;QACA;QACA;QACA,IAAMC,iBAAiB,GAAG,SAApBA,iBAAiBA,CAAA,EAAQ;UAC7B,IAAMnD,KAAK,GAAG,EAAE;UAChB,KAAK,IAAI/K,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG8N,MAAI,CAACvN,MAAM,CAAC7B,MAAM,EAAE,EAAEsB,CAAC,EAAE;YAC3C+K,KAAK,CAACpL,IAAI,CAAC;cAACb,GAAG,EAAEgP,MAAI,CAACvN,MAAM,CAACP,CAAC,CAAC;cAAEN,KAAK,EAAEa,MAAM,CAACP,CAAC;YAAC,CAAC,CAAC;;UAErD,IAAMkJ,QAAQ,GAAG,IAAIhM,QAAQ,CAAC6N,KAAK,CAAC;UACpC,IAAMvG,OAAO,GACTvH,QAAO,CAAC6Q,MAAI,CAACtJ,OAAO,EAAE0E,QAAQ,EAAE;YAAC,UAAU,EAAE;UAAI,CAAC,CAAa;UACnE;UACA;UAEA,IAAIiF,SAAiB;UACrB,KAAK,IAAInO,GAAC,GAAG,CAAC,EAAEA,GAAC,GAAG8N,MAAI,CAAC3J,aAAa,CAACzF,MAAM,EAAE,EAAEsB,GAAC,EAAE;YAClD,IAAM2E,YAAY,GAAGmJ,MAAI,CAAC3J,aAAa,CAACnE,GAAC,CAAC;YAC1C,IAAI0B,IAAI,GAAGiD,YAAY,CAACnE,OAAO,CAACR,GAAC,CAAC,EAAEwE,OAAO,CAACxE,GAAC,CAAC,CAAC;YAC/C,IAAIgO,aAAa,CAAChO,GAAC,CAAC,IAAI,IAAI,EAAE;cAC5B0B,IAAI,GAAG/D,mBAAmB,CAAC+D,IAAI,EAAEsM,aAAa,CAAChO,GAAC,CAAC,CAAC;;YAGpD;YACA,IAAMoO,QAAQ,GAAWnT,GAAG,CAACoT,IAAI,CAAC3M,IAAI,CAAC;YACvC;YACAqM,UAAU,CAACpO,IAAI,CAACyO,QAAQ,CAAC;YACzB,IAAIpO,GAAC,KAAK,CAAC,EAAE;cACXmO,SAAS,GAAGzM,IAAI;aACjB,MAAM;cACLyM,SAAS,GAAGlT,GAAG,CAACkO,GAAG,CAACgF,SAAS,EAAEzM,IAAI,CAAC;;;UAIxC;UACA;UACA;UACA,KAAK,IAAI1B,GAAC,GAAG,CAAC,EAAEA,GAAC,GAAG8N,MAAI,CAAC1I,cAAc,CAAC1G,MAAM,EAAE,EAAEsB,GAAC,EAAE;YACnD,IAAIsO,cAAsB;YAE1B,IAAIR,MAAI,CAACtJ,OAAO,CAAC9F,MAAM,GAAG,CAAC,IAAIsB,GAAC,GAAG8N,MAAI,CAACtJ,OAAO,CAAC9F,MAAM,EAAE;cACtD4P,cAAc,GAAGP,UAAU,CAAC/N,GAAC,CAAC;aAC/B,MAAM;cACL,IAAMmG,MAAM,GAAG2H,MAAI,CAAC1I,cAAc,CAACpF,GAAC,CAAC,CAAC,CAAC,CAAC;cACxC,IAAMuF,WAAW,GAAGuI,MAAI,CAAC1I,cAAc,CAACpF,GAAC,CAAC,CAAC,CAAC,CAAC;cAC7CsO,cAAc,GACVrT,GAAG,CAACoT,IAAI,CAAClI,MAAM,CAAC3F,OAAO,CAAC+E,WAAW,CAAC,EAAEf,OAAO,CAACe,WAAW,CAAC,CAAC,CAAC;;YAGlEtK,GAAG,CAACsT,IAAI,CAACD,cAAc,CAAC;YACxB;YACAL,aAAa,CAACtO,IAAI,CAAC2O,cAAc,CAAC;;UAGpCH,SAAS,GAAGlT,GAAG,CAACoT,IAAI,CAACF,SAAS,CAAC;UAE/B;UACAL,MAAI,CAACU,eAAe,EAAE,CAAC5J,OAAO,CAAC,UAAA6J,eAAe,EAAG;YAC/CN,SAAS,GAAGlT,GAAG,CAACkO,GAAG,CAACgF,SAAS,EAAEM,eAAe,CAAC;UACjD,CAAC,CAAC;UAEF,OAAON,SAAmB;QAC5B,CAAC;QAED,IAAMO,SAAS,GAAGZ,MAAI,CAAChH,yBAAyB,CAAC7H,GAAG,CAChD,UAAA0P,KAAK;UAAA,OAAIA,KAAK,CAACC,IAAI,EAAkB;QAAA,EAAC;QAC1C,IAAMC,UAAU,GAAG,IAAI;QACvB,IAAMC,cAAc,GAChBhB,MAAI,CAAC9J,UAAU,CAAC+K,QAAQ,CAACb,iBAAiB,EAAEW,UAAU,EAAEH,SAAS,CAAC;QAEtE,OAAO,CAACI,cAAc,CAAC,CAAC9P,MAAM,CAACiP,aAAa,CAAC;MAC/C,CAAC;IACH;IAEA;;;;;EAAA;IAAAnP,GAAA;IAAAY,KAAA,EAKQ,SAAA4H,iBAAA,EAAgB;MAAA,IAAA0H,MAAA;MACtB,IAAI,CAACzH,YAAY,GAAG,UAAClJ,IAAc,EAAI;QACrC,OAAOpD,GAAG,CAACqP,IAAI,CAAC,YAAK;UACnB,IAAM2E,UAAU,GAAa,EAAE;UAC/B,IAAId,SAAiB;UACrB,IAAM5N,MAAM,GAAGlC,IAAI,CAACgC,KAAK,CAAC,CAAC,EAAE2O,MAAI,CAACzO,MAAM,CAAC7B,MAAM,CAAC;UAChD,IAAM8B,OAAO,GAAGnC,IAAI,CAACgC,KAAK,CACtB2O,MAAI,CAACzO,MAAM,CAAC7B,MAAM,EAAEsQ,MAAI,CAACzO,MAAM,CAAC7B,MAAM,GAAGsQ,MAAI,CAACxK,OAAO,CAAC9F,MAAM,CAAC;UACjE,IAAMqM,KAAK,GAAG,EAAE;UAChB,KAAK,IAAI/K,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGgP,MAAI,CAACzO,MAAM,CAAC7B,MAAM,EAAE,EAAEsB,CAAC,EAAE;YAC3C+K,KAAK,CAACpL,IAAI,CAAC;cAACb,GAAG,EAAEkQ,MAAI,CAACzO,MAAM,CAACP,CAAC,CAAC;cAAEN,KAAK,EAAEa,MAAM,CAACP,CAAC;YAAC,CAAC,CAAC;;UAErD,IAAMkJ,QAAQ,GAAG,IAAIhM,QAAQ,CAAC6N,KAAK,CAAC;UACpC,IAAMvG,OAAO,GAAGvH,QAAO,CAAC+R,MAAI,CAACxK,OAAO,EAAE0E,QAAQ,CAAa;UAC3D;UACA,KAAK,IAAIlJ,GAAC,GAAG,CAAC,EAAEA,GAAC,GAAGgP,MAAI,CAAC7K,aAAa,CAACzF,MAAM,EAAE,EAAEsB,GAAC,EAAE;YAClD,IAAM2E,YAAY,GAAGqK,MAAI,CAAC7K,aAAa,CAACnE,GAAC,CAAC;YAC1C;YACA;YACA,IAAM0B,IAAI,GAAWzG,GAAG,CAACoT,IAAI,CAAC1J,YAAY,CAACnE,OAAO,CAACR,GAAC,CAAC,EAAEwE,OAAO,CAACxE,GAAC,CAAC,CAAC,CAAC;YACnE,IAAIA,GAAC,KAAK,CAAC,EAAE;cACXmO,SAAS,GAAGzM,IAAI;aACjB,MAAM;cACLyM,SAAS,GAAGlT,GAAG,CAACkO,GAAG,CAACgF,SAAS,EAAEzM,IAAI,CAAC;;YAEtCuN,UAAU,CAACtP,IAAI,CAACwO,SAAS,CAAC;;UAE5B;UACA,KAAK,IAAInO,GAAC,GAAG,CAAC,EAAEA,GAAC,GAAGgP,MAAI,CAAC5J,cAAc,CAAC1G,MAAM,EAAE,EAAEsB,GAAC,EAAE;YACnD,IAAMmG,MAAM,GAAG6I,MAAI,CAAC5J,cAAc,CAACpF,GAAC,CAAC,CAAC,CAAC,CAAC;YACxC,IAAMuF,WAAW,GAAGyJ,MAAI,CAAC5J,cAAc,CAACpF,GAAC,CAAC,CAAC,CAAC,CAAC;YAC7C;YACA,IAAMkP,UAAU,GACZjU,GAAG,CAACoT,IAAI,CAAClI,MAAM,CAAC3F,OAAO,CAAC+E,WAAW,CAAC,EAAEf,OAAO,CAACe,WAAW,CAAC,CAAC,CAAC;YAChE0J,UAAU,CAACtP,IAAI,CAACuP,UAAoB,CAAC;;UAEvC,OAAOD,UAAU;QACnB,CAAC,CAAC;MACJ,CAAC;IACH;IAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;EAAA;IAAAnQ,GAAA;IAAAY,KAAA;MAAA,IAAAyP,IAAA,GAAAtH,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CAkCA,SAAAqH,SACIrR,CAAgD,EAChD0D,CAAgD;QAAA,IAAAuB,IAAA;UAAAzC,MAAA;UAAAC,OAAA;UAAA6O,cAAA;UAAAC,eAAA;UAAAC,SAAA;UAAAC,SAAA;UAAAC,IAAA;UAAAC,IAAA;UAAA1B,aAAA;UAAA9G,SAAA;UAAA1I,cAAA;UAAA2I,gBAAA;UAAAwI,YAAA;UAAAC,MAAA;UAAAC,eAAA;UAAAC,eAAA;UAAAC,OAAA;UAAAC,iBAAA;UAAA3I,GAAA;UAAA4I,aAAA;UAAAzC,SAAA;UAAA0C,WAAA;UAAAC,eAAA;UAAAC,SAAA;UAAAC,GAAA;UAAAC,MAAA,GAAA7R,SAAA;QAAA,OAAAqJ,mBAAA,GAAAI,IAAA,UAAAqI,UAAAC,SAAA;UAAA,kBAAAA,SAAA,CAAAnI,IAAA,GAAAmI,SAAA,CAAAlI,IAAA;YAAA;cAChDtF,IAAA,GAAAsN,MAAA,CAAA5R,MAAA,QAAA4R,MAAA,QAAA3R,SAAA,GAAA2R,MAAA,MAAqB,EAAE;cAAA,KACrB,IAAI,CAAClN,UAAU;gBAAAoN,SAAA,CAAAlI,IAAA;gBAAA;cAAA;cAAA,MACX,IAAIiE,KAAK,CACX,8DAA8D,CAAC;YAAA;cAErE,IAAI,CAACnJ,UAAU,GAAG,IAAI;cAACoN,SAAA,CAAAnI,IAAA;cAWfnB,SAAS,GAAGlE,IAAI,CAACkE,SAAS,IAAI,IAAI,GAAG,EAAE,GAAGlE,IAAI,CAACkE,SAAS;cAC9D7J,cAAc,CAAC6J,SAAS,CAAC;cAEzB;cACA;cACM1I,cAAc,GAAG,KAAK;cAAAgS,SAAA,CAAAlI,IAAA;cAAA,OAElB,IAAI,CAACqE,mBAAmB,CAC1B5O,CAAC,EAAE0D,CAAC,EAAEuB,IAAI,CAAC2I,YAAY,EAAE3I,IAAI,CAAC4I,WAAW,EAAEpN,cAAc,EACzD0I,SAAS,CAAmC;YAAA;cAH9CC,gBAAgB,GAAAqJ,SAAA,CAAA9D,IAAA;cAItBnM,MAAM,GAAG4G,gBAAgB,CAAC,CAAC,CAAC;cAC5B3G,OAAO,GAAG2G,gBAAgB,CAAC,CAAC,CAAC;cAC7B6G,aAAa,GAAG7G,gBAAgB,CAAC,CAAC,CAAC;cAEnC;cACIwI,YAAY,GAAG,KAAK;cAAA,MAEpB3M,IAAI,CAACyN,cAAc,IAAI,IAAI,IAAIzN,IAAI,CAACyN,cAAc,CAAC/R,MAAM,GAAG,CAAC;gBAAA8R,SAAA,CAAAlI,IAAA;gBAAA;cAAA;cAC/DqH,YAAY,GAAG,IAAI;cAAC,MAChB3M,IAAI,CAACyN,cAAc,CAAC/R,MAAM,KAAK,CAAC;gBAAA8R,SAAA,CAAAlI,IAAA;gBAAA;cAAA;cAClC;cACAiH,SAAS,GAAGvM,IAAI,CAACyN,cAAc,CAAC,CAAC,CAAC;cAClCjB,SAAS,GAAGxM,IAAI,CAACyN,cAAc,CAAC,CAAC,CAAC;cAACD,SAAA,CAAAlI,IAAA;cAAA;YAAA;cAAA,MAC1BtF,IAAI,CAACyN,cAAc,CAAC/R,MAAM,KAAK,CAAC;gBAAA8R,SAAA,CAAAlI,IAAA;gBAAA;cAAA;cAAA,MACnC,IAAIzM,mBAAmB,CACzB,+DAA+D,CAAC;YAAA;cAAA,MAE9D,IAAIE,UAAU,CAChB,8GAC4C,MAAAiD,MAAA,CACzCgE,IAAI,CAACyN,cAAc,iBAAc,CAAC;YAAA;cAGrCjS,eAAc,GAAG,IAAI;cAAAgS,SAAA,CAAAlI,IAAA;cAAA,OAEjB,IAAI,CAACqE,mBAAmB,CAC1B4C,SAAS,EAAEC,SAAS,EAAE,IAAI,EAAE,6BAC5B,IAAI,EAAwB,4BAC5BhR,eAAc,EAAE0I,SAAS,CAAmC;YAAA;cAJ9D4I,eAAe,GAAAU,SAAA,CAAA9D,IAAA;cAKrB+C,IAAI,GAAGK,eAAe,CAAC,CAAC,CAAC;cACzBJ,IAAI,GAAGI,eAAe,CAAC,CAAC,CAAC;cACzBF,MAAM,GAAGH,IAAI,CAACzQ,MAAM,CAAC0Q,IAAI,CAAC;cAC1B;cAAAc,SAAA,CAAAlI,IAAA;cAAA;YAAA;cACK,IACHtF,IAAI,CAAC0N,eAAe,IAAI,IAAI,IAAI1N,IAAI,CAAC0N,eAAe,GAAG,CAAC,IACxD1N,IAAI,CAAC0N,eAAe,GAAG,CAAC,EAAE;gBAC5Bf,YAAY,GAAG,IAAI;gBACnB;gBACMI,OAAO,GACTY,IAAI,CAACC,KAAK,CAACrQ,MAAM,CAAC,CAAC,CAAC,CAACR,KAAK,CAAC,CAAC,CAAC,IAAI,CAAC,GAAGiD,IAAI,CAAC0N,eAAe,CAAC,CAAC;gBACzDV,iBAAiB,GAAGzP,MAAM,CAAC,CAAC,CAAC,CAACR,KAAK,CAAC,CAAC,CAAC;gBAC5C0P,IAAI,GAAGhS,WAAW,CAAC8C,MAAM,EAAEwP,OAAO,EAAEC,iBAAiB,CAAa;gBAClEX,cAAc,GAAG9O,MAAM;gBACvBA,MAAM,GAAG9C,WAAW,CAAC8C,MAAM,EAAE,CAAC,EAAEwP,OAAO,CAAa;gBACpDL,IAAI,GAAGjS,WAAW,CAAC+C,OAAO,EAAEuP,OAAO,EAAEC,iBAAiB,CAAa;gBACnEV,eAAe,GAAG9O,OAAO;gBACzBA,OAAO,GAAG/C,WAAW,CAAC+C,OAAO,EAAE,CAAC,EAAEuP,OAAO,CAAa;gBACtD;gBACA;gBACAH,MAAM,GAAGH,IAAI,CAACzQ,MAAM,CAAC0Q,IAAI,CAAC;gBAE1B;eACD,MAAM,IAAI1M,IAAI,CAAC6N,eAAe,IAAI,IAAI,EAAE;gBACvClB,YAAY,GAAG,IAAI;gBACnB;;YACD;cAEKtI,GAAG,GAAG9G,MAAM,CAACvB,MAAM,CAACwB,OAAO,CAAC,CAACxB,MAAM,CAACgP,aAAa,CAAC;cAExD,IAAI,CAAChH,gCAAgC,EAAE;cAEvC;cAEA;cACA;cACA;cACA;cACA;cACA;cACA;cACA;cACA;cACA;cACMiJ,aAAa,GAAG,IAAI,CAACpC,iBAAiB,EAAE;cACxCL,SAAS,GAAG,IAAI,CAACD,sBAAsB,EAAE;cAI/C,IAAIoC,YAAY,EAAE;gBAChB,IAAI,CAACrI,gBAAgB,EAAE;gBACvB4I,WAAW,GAAG,IAAI,CAAC3I,YAAY;gBAC/B4I,eAAe,GACX3C,SAAS,CAACnN,KAAK,EAAE,CAACrB,MAAM,CAACwO,SAAS,CAACvO,GAAG,CAAC,UAAAO,CAAC;kBAAA,OAAI,MAAM,GAAGA,CAAC;gBAAA,EAAC,CAAC;eAC7D,MAAM;gBACL0Q,WAAW,GAAG,IAAI;gBAClBN,MAAM,GAAG,EAAE;gBACXO,eAAe,GAAG3C,SAAS,CAACnN,KAAK,EAAE;;cAG/B+P,SAAS,GAAGzU,oBAAoB,CAACqH,IAAI,CAACoN,SAAS,EAAEpN,IAAI,CAAC8N,UAAU,CAAC;cAAAN,SAAA,CAAAlI,IAAA;cAAA,OACrD,IAAI,CAACyI,OAAO,CAC1Bd,aAAa,EAAE5I,GAAG,EAAEmG,SAAS,EAAEtG,SAAS,EAAElE,IAAI,CAACgO,MAAM,EACrDhO,IAAI,CAAC0E,OAAO,EAAE0I,SAAS,EAAEF,WAAW,EAAEN,MAAM,EAAE5M,IAAI,CAACiO,OAAO,EAC1Dd,eAAe,EAAEnN,IAAI,CAACkO,YAAY,EAAE,IAAI,EAAE,IAAI,CAAC;YAAA;cAH7Cb,GAAG,GAAAG,SAAA,CAAA9D,IAAA;cAAA,OAAA8D,SAAA,CAAAjI,MAAA,WAIF8H,GAAG;YAAA;cAAAG,SAAA,CAAAnI,IAAA;cAEV,IAAI,CAACjF,UAAU,GAAG,KAAK;cACvB;cACA9F,iBAAiB,CAACiD,MAAM,EAAExC,CAAC,CAAC;cAC5BT,iBAAiB,CAACkD,OAAO,EAAEiB,CAAC,CAAC;cAC7BnE,iBAAiB,CAAC+R,cAAc,EAAEtR,CAAC,CAAC;cACpCT,iBAAiB,CAACgS,eAAe,EAAE7N,CAAC,CAAC;cACrCnE,iBAAiB,CAACmS,IAAgB,EAAEF,SAAS,CAAC;cAC9CjS,iBAAiB,CAACoS,IAAgB,EAAEF,SAAS,CAAC;cAC9C,IAAIxB,aAAa,IAAI,IAAI,EAAE;gBACzB/S,GAAG,CAACkW,OAAO,CAACnD,aAAa,CAAC;;cAC3B,OAAAwC,SAAA,CAAAY,MAAA;YAAA;YAAA;cAAA,OAAAZ,SAAA,CAAAhI,IAAA;UAAA;QAAA,GAAA4G,QAAA;MAAA,CAGJ;MAAA,SAAAiC,IAAAC,GAAA,EAAAC,GAAA;QAAA,OAAApC,IAAA,CAAAxG,KAAA,OAAAlK,SAAA;MAAA;MAAA,OAAA4S,GAAA;IAAA;IAED;;;;;;;;;;;;;;;;;;;;;;;;;;;EAAA;IAAAvS,GAAA;IAAAY,KAAA;MAAA,IAAA8R,QAAA,GAAA3J,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CA2BA,SAAA0J,SACI3R,CAA+B,EAAEuH,GAAa,EAAEmG,SACxC,EAAEtG,SAAkB,EAAE8J,MAAe,EAAEtJ,OAAgB,EAC/D0I,SAA0B,EAAEsB,IAAmC,EAAE9B,MACzD,EAAEqB,OAAwB,EAAEd,eAA0B,EAC9De,YAAqB,EAAES,aAAsB,EAAEd,eAAwB;QAAA,IAAAe,MAAA;QAAA,IAAAjC,YAAA,EAAAkC,eAAA,EAAA3E,UAAA,EAAA4E,mBAAA,EAAAC,YAAA,EAAAC,OAAA,EAAAC,MAAA,EAAAC,KAAA,EAAAC,KAAA;QAAA,OAAArK,mBAAA,GAAAI,IAAA,UAAAkK,UAAAC,SAAA;UAAA,kBAAAA,SAAA,CAAAhK,IAAA,GAAAgK,SAAA,CAAA/J,IAAA;YAAA;cAEzE,IAAIpB,SAAS,IAAI,IAAI,EAAE;gBACrBA,SAAS,GAAG,EAAE;;cAEhB,IAAI8J,MAAM,IAAI,IAAI,EAAE;gBAClBA,MAAM,GAAG,CAAC;;cAEZ,IAAIC,OAAO,IAAI,IAAI,EAAE;gBACnBA,OAAO,GAAG,IAAI;;cAEhB,IAAIC,YAAY,IAAI,IAAI,EAAE;gBACxBA,YAAY,GAAG,CAAC;;cAGlB;cACIvB,YAAY,GAAG,KAAK;cACxB,IAAI+B,IAAI,IAAI,IAAI,IAAI9B,MAAM,IAAI,IAAI,EAAE;gBAClCD,YAAY,GAAG,IAAI;gBACnB;;cACD,MACGkB,eAAe,IAAI,IAAI;gBAAAwB,SAAA,CAAA/J,IAAA;gBAAA;cAAA;cACzBqH,YAAY,GAAG,IAAI;cAAC,MAChBgC,aAAa,IAAI,IAAI;gBAAAU,SAAA,CAAA/J,IAAA;gBAAA;cAAA;cAAA,MACjB,IAAIvM,UAAU,CAChB,gEAAgE,GAChE,oCAAoC,CAAC;YAAA;cAIvC8V,eAAe,GACjB,IAAI,CAACjJ,eAAe,CAACvB,GAAG,EAAEH,SAAS,EAAEyK,aAAa,EAAE,iBAAiB,CAAC;cAE1E,IAAIE,eAAe,IAAI,IAAI,EAAE;gBAC3B3E,UAAU,GAAGrQ,KAAK,CAAC,CAAC,EAAEgV,eAAe,CAAC;;cAGxC,IAAInK,OAAO,IAAI,IAAI,EAAE;gBACnBA,OAAO,GAAG,CAAC;;cACZoK,mBAAA,GAE+BpW,kBAAkB,CAC9C0U,SAAS,EAAE1I,OAAO,EAAEsJ,MAAM,EAAEE,YAAY,EAAEW,eAAe,EACzDF,aAAa,EAAEzK,SAAS,EAAEyI,YAAY,EAAEQ,eAAe,CAAC,EAFrD4B,YAAY,GAAAD,mBAAA,CAAZC,YAAY,EAAEC,OAAO,GAAAF,mBAAA,CAAPE,OAAO;cAG5BD,YAAY,CAACO,QAAQ,CAAC,IAAI,CAAC;cAC3B,IAAI,CAACN,OAAO,GAAGA,OAAO;cAACK,SAAA,CAAA/J,IAAA;cAAA,OACjByJ,YAAY,CAACQ,YAAY,EAAE;YAAA;cACjC,IAAI,CAACC,aAAa,GAAG,KAAK;cAC1B;cACA;cAAAP,MAAA,gBAAAnK,mBAAA,GAAAC,IAAA,UAAAkK,OAAA;gBAAA,IAAAQ,SAAA,EAAAC,iBAAA,EAAAnI,OAAA,EAAAoI,MAAA,EAAAjI,UAAA,EAAAkI,KAAA;gBAAA,OAAA9K,mBAAA,GAAAI,IAAA,UAAA2K,QAAAC,SAAA;kBAAA,kBAAAA,SAAA,CAAAzK,IAAA,GAAAyK,SAAA,CAAAxK,IAAA;oBAAA;sBAAAwK,SAAA,CAAAxK,IAAA;sBAAA,OAGQyJ,YAAY,CAACgB,YAAY,CAACb,KAAK,CAAC;oBAAA;sBAChCO,SAAS,GAAmB,EAAE;sBAAA,MAChCd,aAAa,IAAI,IAAI;wBAAAmB,SAAA,CAAAxK,IAAA;wBAAA;sBAAA;sBAAA,MACjB,IAAIzM,mBAAmB,CACzB,4CAA4C,CAAC;oBAAA;sBAAA,MAE7CoV,OAAO,KAAK,OAAO;wBAAA6B,SAAA,CAAAxK,IAAA;wBAAA;sBAAA;sBAAA,MACf,IAAIzM,mBAAmB,CAAC,oCAAoC,GAClC,MAAM,CAAC;oBAAA;sBAClC,IAAIoV,OAAO,EAAE;wBAClBzV,IAAI,CAACyV,OAAO,CAAC/D,UAAU,CAAC;;oBACzB;sBACD;sBACA;sBACMwF,iBAAiB,GAAGnX,QAAQ,CAAC2R,UAAU,CAAC;sBAExC3C,OAAO,GAAG/M,WAAW,CAACqU,eAAe,EAAE3K,SAAS,CAAC;sBAAAyL,MAAA,gBAAA7K,mBAAA,GAAAC,IAAA,UAAA4K,OAAAjI,UAAA;wBAAA,IAAAsI,SAAA;wBAAA,OAAAlL,mBAAA,GAAAI,IAAA,UAAA+K,QAAAC,SAAA;0BAAA,kBAAAA,SAAA,CAAA7K,IAAA,GAAA6K,SAAA,CAAA5K,IAAA;4BAAA;8BAE/C0K,SAAS,GAAmB,EAAE;8BAAAE,SAAA,CAAA5K,IAAA;8BAAA,OAC9ByJ,YAAY,CAACoB,YAAY,CAACzI,UAAU,EAAEsI,SAAS,CAAC;4BAAA;8BAEtD/X,GAAG,CAACqP,IAAI,CAAC,YAAK;gCACZ,IAAMM,UAAU,GAAGL,OAAO,CAACG,UAAU,CAAC,CAAC,CAAC,CAAC;gCACzC,IAAMG,QAAQ,GAAGN,OAAO,CAACG,UAAU,CAAC,CAAC,CAAC,CAAC;gCACvC,IAAMyC,QAAQ,GAAG1R,CAAC,CAAC2R,mBAAmB,CACjBsF,iBAAiB,EAAE9H,UAAU,EAC7BC,QAAQ,GAAGD,UAAU,CAAa;gCACvDoI,SAAS,CAAC,OAAO,CAAC,GAAGtI,UAAU;gCAC/BsI,SAAS,CAAC,MAAM,CAAC,GAAGnI,QAAQ,GAAGD,UAAU;gCAEzC;gCACA;gCACA,IAAME,QAAQ,GAAGpN,oBAAoB,CAAC2J,GAAG,EAAE8F,QAAQ,CAAa;gCAChE,IAAMF,IAAI,GAAGnN,CAAC,CAACgL,QAAQ,CAAC;gCACxB,KAAK,IAAI9K,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGwN,SAAS,CAAC9O,MAAM,EAAE,EAAEsB,CAAC,EAAE;kCACzC,IAAM0N,KAAK,GAAGF,SAAS,CAACxN,CAAC,CAAC;kCAC1B,IAAMqQ,GAAG,GAAGpD,IAAI,CAACjN,CAAC,CAAC;kCACnBgT,SAAS,CAACtF,KAAK,CAAC,GAAG2C,GAAG;kCACtBpV,GAAG,CAACsT,IAAI,CAAC8B,GAAG,CAAC;kCACb;;;gCAGF,IAAI3F,UAAU,KAAKH,OAAO,CAAC7L,MAAM,GAAG,CAAC,EAAE;kCAAG;kCACxC,IAAIiR,YAAY,EAAE;oCAChB,IAAMyD,OAAO,GAAGxB,MAAI,CAACnK,QAAQ,CAACiK,IAAI,EAAE9B,MAAM,EAAE1I,SAAS,CAAC;oCACtD;oCACA,KAAK,IAAIlH,GAAC,GAAG,CAAC,EAAEA,GAAC,GAAGwN,SAAS,CAAC9O,MAAM,EAAE,EAAEsB,GAAC,EAAE;sCACzC,IAAM0N,MAAK,GAAGF,SAAS,CAACxN,GAAC,CAAC;sCAC1B,IAAMqQ,IAAG,GAAG+C,OAAO,CAACpT,GAAC,CAAC;sCACtB/E,GAAG,CAACsT,IAAI,CAAC8B,IAAG,CAAC;sCACb;sCACAoC,SAAS,CAAC,MAAM,GAAG/E,MAAK,CAAC,GAAG2C,IAAG;;;;8BAIvC,CAAC,CAAC;8BAAC6C,SAAA,CAAA5K,IAAA;8BAAA,OAEGyJ,YAAY,CAACsB,UAAU,CAAC3I,UAAU,EAAEsI,SAAS,CAAC;4BAAA;8BACpD/W,oBAAoB,CAAC+W,SAAS,CAAC;8BAAC,KAE5BpB,MAAI,CAACY,aAAa;gCAAAU,SAAA,CAAA5K,IAAA;gCAAA;8BAAA;8BAAA,OAAA4K,SAAA,CAAA3K,MAAA;4BAAA;4BAAA;8BAAA,OAAA2K,SAAA,CAAA1K,IAAA;0BAAA;wBAAA,GAAAmK,MAAA;sBAAA;sBA3CfjI,UAAU,GAAG,CAAC;oBAAA;sBAAA,MAAEA,UAAU,GAAGH,OAAO,CAAC7L,MAAM;wBAAAoU,SAAA,CAAAxK,IAAA;wBAAA;sBAAA;sBAAA,OAAAwK,SAAA,CAAAQ,aAAA,CAAAX,MAAA,CAAAjI,UAAA;oBAAA;sBAAAkI,KAAA,GAAAE,SAAA,CAAAtG,EAAA;sBAAA,MAAAoG,KAAA;wBAAAE,SAAA,CAAAxK,IAAA;wBAAA;sBAAA;sBAAA,OAAAwK,SAAA,CAAAvK,MAAA;oBAAA;sBAAE,EAAEmC,UAAU;sBAAAoI,SAAA,CAAAxK,IAAA;sBAAA;oBAAA;sBAiDlEoK,iBAAiB,CAACvB,OAAO,EAAE;oBAAC;sBAAA2B,SAAA,CAAAxK,IAAA;sBAAA,OAGxByJ,YAAY,CAACwB,UAAU,CAACrB,KAAK,EAAEO,SAAS,CAAC;oBAAA;sBAAA,KAC3Cb,MAAI,CAACY,aAAa;wBAAAM,SAAA,CAAAxK,IAAA;wBAAA;sBAAA;sBAAA,OAAAwK,SAAA,CAAAvK,MAAA;oBAAA;oBAAA;sBAAA,OAAAuK,SAAA,CAAAtK,IAAA;kBAAA;gBAAA,GAAAyJ,MAAA;cAAA;cAvEfC,KAAK,GAAGhB,YAAY;YAAA;cAAA,MAAEgB,KAAK,GAAGlB,MAAM;gBAAAqB,SAAA,CAAA/J,IAAA;gBAAA;cAAA;cAAA,OAAA+J,SAAA,CAAAiB,aAAA,CAAArB,MAAA;YAAA;cAAAE,KAAA,GAAAE,SAAA,CAAA7F,EAAA;cAAA,MAAA2F,KAAA;gBAAAE,SAAA,CAAA/J,IAAA;gBAAA;cAAA;cAAA,OAAA+J,SAAA,CAAA9J,MAAA;YAAA;cAAE,EAAE2J,KAAK;cAAAG,SAAA,CAAA/J,IAAA;cAAA;YAAA;cAAA+J,SAAA,CAAA/J,IAAA;cAAA,OA2EhDyJ,YAAY,CAACyB,UAAU,EAAE;YAAA;cAAAnB,SAAA,CAAA/J,IAAA;cAAA,OAEzB,IAAI,CAAC0J,OAAO,CAACyB,QAAQ,EAAE;YAAA;cAAA,OAAApB,SAAA,CAAA9J,MAAA,WACtB,IAAI,CAACyJ,OAAO;YAAA;YAAA;cAAA,OAAAK,SAAA,CAAA7J,IAAA;UAAA;QAAA,GAAAiJ,QAAA;MAAA,CACpB;MAAA,SAAAV,QAAA2C,GAAA,EAAAC,IAAA,EAAAC,IAAA,EAAAC,IAAA,EAAAC,IAAA,EAAAC,IAAA,EAAAC,IAAA,EAAAC,IAAA,EAAAC,IAAA,EAAAC,IAAA,EAAAC,IAAA,EAAAC,IAAA,EAAAC,IAAA,EAAAC,IAAA;QAAA,OAAA/C,QAAA,CAAA7I,KAAA,OAAAlK,SAAA;MAAA;MAAA,OAAAsS,OAAA;IAAA,IAED;IACA;IACA;;;;;;;;;;;;;;;;;;;;;EAAA;IAAAjS,GAAA;IAAAY,KAAA;MAAA,IAAA8U,YAAA,GAAA3M,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CAqBA,SAAA0M,SAAoBxM,OAAmB,EAAEjF,IAA4B;QAAA,OAAA8E,mBAAA,GAAAI,IAAA,UAAAwM,UAAAC,SAAA;UAAA,kBAAAA,SAAA,CAAAtM,IAAA,GAAAsM,SAAA,CAAArM,IAAA;YAAA;cAAA,OAAAqM,SAAA,CAAApM,MAAA,WAE5DnL,WAAU,CAAC,IAAI,EAAE6K,OAAO,EAAEjF,IAAI,CAAC;YAAA;YAAA;cAAA,OAAA2R,SAAA,CAAAnM,IAAA;UAAA;QAAA,GAAAiM,QAAA;MAAA,CACvC;MAAA,SAAArX,WAAAwX,IAAA,EAAAC,IAAA;QAAA,OAAAL,YAAA,CAAA7L,KAAA,OAAAlK,SAAA;MAAA;MAAA,OAAArB,UAAA;IAAA;IAED;;;;;;;;;;;;;;;;;;;;;;;EAAA;IAAA0B,GAAA;IAAAY,KAAA;MAAA,IAAAoV,aAAA,GAAAjN,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CAuBA,SAAAgN,SACIhX,CAAgD,EAChD0D,CAC6B;QAAA,IAAAuT,cAAA,EAAAzU,MAAA,EAAAC,OAAA,EAAAyP,aAAA,EAAA/T,MAAA,EAAA6R,UAAA,EAAAkH,UAAA,EAAAC,MAAA,EAAAxT,IAAA,EAAAyT,CAAA;QAAA,OAAArN,mBAAA,GAAAI,IAAA,UAAAkN,UAAAC,SAAA;UAAA,kBAAAA,SAAA,CAAAhN,IAAA,GAAAgN,SAAA,CAAA/M,IAAA;YAAA;cAAA+M,SAAA,CAAA/M,IAAA;cAAA,OAGF,IAAI,CAACqE,mBAAmB,CAAC5O,CAAC,EAAE0D,CAAC,CAAC;YAAA;cAArDuT,cAAc,GAAAK,SAAA,CAAA3I,IAAA;cACdnM,MAAM,GAAGyU,cAAc,CAAC,CAAC,CAAC;cAC1BxU,OAAO,GAAGwU,cAAc,CAAC,CAAC,CAAC;cAC3B/E,aAAa,GAAG,IAAI,CAACpC,iBAAiB,EAAE;cACxC3R,MAAM,GAAG+T,aAAa,CAAC1P,MAAM,CAACvB,MAAM,CAACwB,OAAO,CAAC,CAAC;cAC9CuN,UAAU,GAAa,EAAE;cAAAkH,UAAA,GAAA5V,0BAAA,CACZnD,MAAM;cAAAmZ,SAAA,CAAAhN,IAAA;cAAA4M,UAAA,CAAA1V,CAAA;YAAA;cAAA,KAAA2V,MAAA,GAAAD,UAAA,CAAAzV,CAAA,IAAAC,IAAA;gBAAA4V,SAAA,CAAA/M,IAAA;gBAAA;cAAA;cAAd5G,IAAI,GAAAwT,MAAA,CAAAxV,KAAA;cAAA2V,SAAA,CAAA/M,IAAA;cAAA,OACG5G,IAAI,CAACrD,IAAI,EAAE;YAAA;cAArB8W,CAAC,GAAAE,SAAA,CAAA3I,IAAA;cACPqB,UAAU,CAACpO,IAAI,CAACwV,CAAC,CAAC,CAAC,CAAC,CAAC;YAAC;cAAAE,SAAA,CAAA/M,IAAA;cAAA;YAAA;cAAA+M,SAAA,CAAA/M,IAAA;cAAA;YAAA;cAAA+M,SAAA,CAAAhN,IAAA;cAAAgN,SAAA,CAAA7I,EAAA,GAAA6I,SAAA;cAAAJ,UAAA,CAAApV,CAAA,CAAAwV,SAAA,CAAA7I,EAAA;YAAA;cAAA6I,SAAA,CAAAhN,IAAA;cAAA4M,UAAA,CAAAnV,CAAA;cAAA,OAAAuV,SAAA,CAAAjE,MAAA;YAAA;cAExBnW,GAAG,CAACkW,OAAO,CAACjV,MAAM,CAAC;cACnBoB,iBAAiB,CAAC0X,cAAc,CAAC,CAAC,CAAC,EAAEjX,CAAC,CAAC;cACvCT,iBAAiB,CAAC0X,cAAc,CAAC,CAAC,CAAC,EAAEvT,CAAC,CAAC;cAAC,OAAA4T,SAAA,CAAA9M,MAAA,WACjC/L,gBAAgB,CAACuR,UAAU,CAAC;YAAA;YAAA;cAAA,OAAAsH,SAAA,CAAA7M,IAAA;UAAA;QAAA,GAAAuM,QAAA;MAAA,CACpC;MAAA,SAAAO,aAAAC,IAAA,EAAAC,IAAA;QAAA,OAAAV,aAAA,CAAAnM,KAAA,OAAAlK,SAAA;MAAA;MAAA,OAAA6W,YAAA;IAAA;IAED;;;;;;;;;EAAA;IAAAxW,GAAA;IAAAY,KAAA,EASU,SAAA+V,gBAAgBC,MAAsB;MAC9C,IAAMC,YAAY,GAAkB,EAAE;MAEtC,IAAMC,aAAa,GAAGF,MAAM,IAAI,IAAI,IAAIA,MAAM,CAACE,aAAa;MAC5D,IAAMnV,OAAO,GAAGmV,aAAa,GAAG,IAAI,CAAC7O,gBAAgB,GAAG,IAAI,CAACtG,OAAO;MACpE,IAAMoV,YAAY,GAAG,IAAI,CAACC,UAAU,CAACF,aAAa,CAAC;MACnD,KAAK,IAAI5V,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGS,OAAO,CAAC/B,MAAM,EAAE,EAAEsB,CAAC,EAAE;QACvC,IAAI4V,aAAa,IAAI,CAACnV,OAAO,CAACT,CAAC,CAAC,CAAC+V,SAAS,EAAE;UAC1C;UACA;;QAEFJ,YAAY,CAAChW,IAAI,CACb;UAACT,IAAI,EAAEuB,OAAO,CAACT,CAAC,CAAC,CAACgW,YAAY;UAAE7L,MAAM,EAAE0L,YAAY,CAAC7V,CAAC;QAAC,CAAC,CAAC;;MAE/D,OAAO2V,YAAY;IACrB;IAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;EAAA;IAAA7W,GAAA;IAAAyF,GAAA,EAkCA,SAAAA,IAAA,EAAgB;MACd,OAAO,IAAI,CAACiO,aAAa;IAC3B,CAAC;IAAAyD,GAAA,EAND,SAAAA,IAAiBzN,IAAa;MAC5B,IAAI,CAACgK,aAAa,GAAGhK,IAAI;IAC3B;EAAC;IAAA1J,GAAA;IAAAyF,GAAA,EAMD,SAAAA,IAAA,EAAa;MACX,OAAO,IAAI,CAACP,UAAU;IACxB,CAAC;IAAAiS,GAAA,EAED,SAAAA,IAAclS,SAAoB;MAChC,IAAI,IAAI,CAACC,UAAU,KAAKD,SAAS,EAAE;QACjC,IAAI,CAACC,UAAU,GAAGD,SAAS;QAC3B,IAAI,CAACG,gBAAgB,GAAG,KAAK;;IAEjC;EAAC;IAAApF,GAAA;IAAAY,KAAA,EAEQ,SAAAyR,QAAA,EAAO;MACd,IAAM+E,MAAM,GAAAC,IAAA,CAAAC,eAAA,CAAAzT,WAAA,CAAA0T,SAAA,oBAAAlT,IAAA,MAAkB;MAC9B,IAAI+S,MAAM,CAACI,oBAAoB,KAAK,CAAC,IAAI,IAAI,CAACvS,SAAS,IAAI,IAAI,IAC3D,IAAI,CAACG,gBAAgB,EAAE;QACzB,IAAMqS,gCAAgC,GAAGtb,GAAG,CAACub,MAAM,EAAE,CAACC,UAAU;QAChE,IAAI,CAACzS,UAAU,CAACmN,OAAO,EAAE;QACzB+E,MAAM,CAACQ,oBAAoB,IACvBH,gCAAgC,GAAGtb,GAAG,CAACub,MAAM,EAAE,CAACC,UAAU;;MAEhE,OAAOP,MAAM;IACf;EAAC;IAAApX,GAAA;IAAAY,KAAA,EAEO,SAAAiX,mBAAA,EAAkB;MAExB,IAAIC,SACsC;MAC1C,IAAI,OAAO,IAAI,CAAClV,IAAI,KAAK,QAAQ,EAAE;QACjCkV,SAAS,GAAGla,WAAW,CAAC,IAAI,CAACgF,IAAI,CAAmB;OACrD,MAAM,IAAIzD,KAAK,CAACC,OAAO,CAAC,IAAI,CAACwD,IAAI,CAAC,EAAE;QAAA,IAAAmV,UAAA,GAAAxX,0BAAA,CAChB,IAAI,CAACqC,IAAI;UAAAoV,MAAA;QAAA;UAA5B,KAAAD,UAAA,CAAAtX,CAAA,MAAAuX,MAAA,GAAAD,UAAA,CAAArX,CAAA,IAAAC,IAAA,GAA8B;YAAA,IAAnBiC,IAAI,GAAAoV,MAAA,CAAApX,KAAA;YACb,IAAI,OAAOgC,IAAI,KAAK,QAAQ,EAAE;cAC5B,MAAM,IAAI6K,KAAK,CAAC,oDAAoD,CAAC;;;QAExE,SAAA3M,GAAA;UAAAiX,UAAA,CAAAhX,CAAA,CAAAD,GAAA;QAAA;UAAAiX,UAAA,CAAA/W,CAAA;QAAA;QACD8W,SAAS,GAAI,IAAI,CAAClV,IAAiB,CAACzC,GAAG,CAAC,UAAAC,IAAI;UAAA,OAAIxC,WAAW,CAACwC,IAAI,CAAC;QAAA,EAC7C;OACrB,MAAM;QACL,IAAMiD,WAAW,GAAG4U,MAAM,CAACC,IAAI,CAAC,IAAI,CAACtV,IAAI,CAAC;QAC1CkV,SAAS,GAAG,EAA4C;QACxD,IAAM1a,OAAM,GACR,IAAI,CAACwF,IAAuD;QAChE,SAAAuV,IAAA,MAAAC,YAAA,GAAyB/U,WAAW,EAAA8U,IAAA,GAAAC,YAAA,CAAAxY,MAAA,EAAAuY,IAAA,IAAE;UAAjC,IAAME,UAAU,GAAAD,YAAA,CAAAD,IAAA;UACnB,IAAI,OAAO/a,OAAM,CAACib,UAAU,CAAC,KAAK,QAAQ,EAAE;YAC1CP,SAAS,CAACO,UAAU,CAAC,GACjBza,WAAW,CAACR,OAAM,CAACib,UAAU,CAAW,CAAmB;WAChE,MAAM;YACL,MAAM,IAAI5K,KAAK,CAAC,oDAAoD,CAAC;;;;MAI3E,OAAOqK,SAAS;IAClB;EAAC;IAAA9X,GAAA;IAAAY,KAAA,EAEO,SAAA0X,qBAAA,EAAoB;MAE1B,IAAI,OAAO,IAAI,CAAClV,OAAO,KAAK,QAAQ,IAChC,OAAO,IAAI,CAACA,OAAO,KAAK,UAAU,EAAE;QACtC,OAAO,CAACxF,WAAW,CAACP,OAAO,CAACwK,mBAAmB,CAAC,IAAI,CAACzE,OAAO,CAAC,CAAC,CAAC;OAChE,MAAM,IAAIjE,KAAK,CAACC,OAAO,CAAC,IAAI,CAACgE,OAAO,CAAC,EAAE;QACtC,OAAO,IAAI,CAACA,OAAO,CAACjD,GAAG,CACnB,UAAAkH,MAAM;UAAA,OAAIzJ,WAAW,CAACP,OAAO,CAACwK,mBAAmB,CAACR,MAAM,CAAC,CAAC;QAAA,EAAC;OAChE,MAAM;QACL,IAAMkR,kBAAkB,GAAuC,EAAE;QACjE,KAAK,IAAMvY,GAAG,IAAI,IAAI,CAACoD,OAAO,EAAE;UAC9BmV,kBAAkB,CAACvY,GAAG,CAAC,GACnBpC,WAAW,CAACP,OAAO,CAACwK,mBAAmB,CAAC,IAAI,CAACzE,OAAO,CAACpD,GAAG,CAAC,CAAC,CAAC;;QAEjE,OAAOuY,kBAAkB;;IAE7B;EAAC;IAAAvY,GAAA;IAAAY,KAAA,EAES,SAAA4X,kBAAA,EAAiB;MACzB,OAAO;QACL5V,IAAI,EAAE,IAAI,CAACiV,kBAAkB,EAAE;QAC/BzU,OAAO,EAAE,IAAI,CAACkV,oBAAoB,EAAE;QACpCG,gBAAgB,EAAE;UAChBC,UAAU,EAAE,IAAI,CAACzT,SAAS,CAAC0T,YAAY,EAAE;UACzC/B,MAAM,EAAE,IAAI,CAAC3R,SAAS,CAAC2T,SAAS;;OAEnC;MACD;MACA;MACA;IACF;EAAC;IAAA5Y,GAAA;IAAAY,KAAA,EAED,SAAAiY,mBAAmBC,cAA8B;MAC/C,IAAIA,cAAc,CAACC,gBAAgB,IAAI,IAAI,EAAE;QAC3C,MAAM,IAAItL,KAAK,CAAC,8CAA8C,CAAC;;MAEjE,IAAIqL,cAAc,CAACE,YAAY,IAAI,IAAI,EAAE;QACvC,MAAM,IAAIvL,KAAK,CAAC,4CAA4C,CAAC;;MAE/D,IAAIqL,cAAc,CAACG,kBAAkB,IAAI,IAAI,EAAE;QAC7C,MAAM,IAAIxL,KAAK,CAAC,kDAAkD,CAAC;;MAGrE,IAAMyL,QAAQ,GAAGlb,mBAAmB,CAAC8a,cAAc,CAACL,gBAAgB,CACxC;MAC5B,IAAMxT,SAAS,GAAG/H,WAAW,CAACgc,QAAQ,CAAc;MAEpD,IAAItW,IAAI;MACR,IAAI,OAAOkW,cAAc,CAAClW,IAAI,KAAK,QAAQ,EAAE;QAC3CA,IAAI,GAAGjF,WAAW,CAACmb,cAAc,CAAClW,IAAI,CAAC;OACxC,MAAM,IAAIzD,KAAK,CAACC,OAAO,CAAC0Z,cAAc,CAAClW,IAAI,CAAC,EAAE;QAC7CA,IAAI,GAAGkW,cAAc,CAAClW,IAAI,CAACzC,GAAG,CAAC,UAAAgZ,SAAS;UAAA,OAAIxb,WAAW,CAACwb,SAAS,CAAC;QAAA,EAAC;OACpE,MAAM,IAAIL,cAAc,CAAClW,IAAI,IAAI,IAAI,EAAE;QACtCA,IAAI,GAAG,EAA4C;QACnD,KAAK,IAAM5C,GAAG,IAAI8Y,cAAc,CAAClW,IAAI,EAAE;UACrCA,IAAI,CAAC5C,GAAG,CAAC,GAAGrC,WAAW,CAACmb,cAAc,CAAClW,IAAI,CAAC5C,GAAG,CAAC,CAAmB;;;MAIvE,IAAIoD,OAAO;MACX,IAAIjE,KAAK,CAACC,OAAO,CAAC0Z,cAAc,CAAC1V,OAAO,CAAC,EAAE;QACzCA,OAAO,GAAG0V,cAAc,CAAC1V,OAAO,CAACjD,GAAG,CAAC,UAAAkH,MAAM;UAAA,OAAI1J,WAAW,CAAC0J,MAAM,CAAC;QAAA,EAAC;OACpE,MAAM,IAAIyR,cAAc,CAAC1V,OAAO,IAAI,IAAI,EAAE;QACzCA,OAAO,GAAG,EAA+C;QACzD,KAAK,IAAMpD,IAAG,IAAI8Y,cAAc,CAAC1V,OAAO,EAAE;UACxCA,OAAO,CAACpD,IAAG,CAAC,GAAGrC,WAAW,CAACmb,cAAc,CAAC1V,OAAO,CAACpD,IAAG,CAAC,CAAC;;;MAI3D,IAAI,CAAC+E,OAAO,CAAC;QAACnC,IAAI,EAAJA,IAAI;QAAEQ,OAAO,EAAPA,OAAO;QAAE6B,SAAS,EAATA;MAAS,CAAC,CAAC;IAC1C;IAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;EAAA;IAAAjF,GAAA;IAAAY,KAAA;MAAA,IAAAwY,KAAA,GAAArQ,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CAiFA,SAAAoQ,SAAWC,YAAiC,EAAE1C,MAAsB;QAAA,IAAA2C,QAAA,EAAAC,kBAAA,EAAAC,YAAA,EAAAC,SAAA,EAAAC,WAAA,EAAAC,cAAA,EAAAC,gBAAA,EAAAC,qBAAA,EAAAC,UAAA,EAAAC,qBAAA,EAAAC,mBAAA,EAAAC,oBAAA,EAAAC,SAAA;QAAA,OAAAnR,mBAAA,GAAAI,IAAA,UAAAgR,UAAAC,SAAA;UAAA,kBAAAA,SAAA,CAAA9Q,IAAA,GAAA8Q,SAAA,CAAA7Q,IAAA;YAAA;cAAA,MAE9D,OAAO8P,YAAY,KAAK,QAAQ;gBAAAe,SAAA,CAAA7Q,IAAA;gBAAA;cAAA;cAC5B+P,QAAQ,GAAGnd,EAAE,CAACke,eAAe,CAAChB,YAAY,CAAC;cAAA,MAC7CC,QAAQ,CAAC3Z,MAAM,KAAK,CAAC;gBAAAya,SAAA,CAAA7Q,IAAA;gBAAA;cAAA;cAAA,MACjB,IAAIvM,UAAU,2CAAAiD,MAAA,CAC0BoZ,YAAY,OAAI;YAAA;cAAA,MACrDC,QAAQ,CAAC3Z,MAAM,GAAG,CAAC;gBAAAya,SAAA,CAAA7Q,IAAA;gBAAA;cAAA;cAAA,MACtB,IAAIvM,UAAU,CAChB,wBAAAiD,MAAA,CAAwBqZ,QAAQ,CAAC3Z,MAAM,oCAAAM,MAAA,CAC/BoZ,YAAY,MAAG,CAAC;YAAA;cAE9BA,YAAY,GAAGC,QAAQ,CAAC,CAAC,CAAC;YAAC;cAAA,MAEzBD,YAAY,CAACiB,IAAI,IAAI,IAAI;gBAAAF,SAAA,CAAA7Q,IAAA;gBAAA;cAAA;cAAA,MACrB,IAAIvM,UAAU,CAChB,0DAA0D,GAC1D,sDAAsD,CAAC;YAAA;cAAAod,SAAA,CAAA7Q,IAAA;cAAA,OAInDpN,EAAE,CAACoe,aAAa,CAAC,IAAI,CAAC7D,eAAe,CAACC,MAAM,CAAC,CAAC;YAAA;cADlD4C,kBAAkB,GAAAa,SAAA,CAAAzM,IAAA;cAGlB6L,YAAY,GAAG,KAAK;cACpBC,SAAS,GAAO,IAAI;cACpBC,WAAW,GAAG,IAAI,CAACc,MAAM,CAACf,SAAS,EAAED,YAAY,CAAC;cAClDG,cAAc,GAAsB;gBACxCc,aAAa,EAAEf,WAAW;gBAC1BgB,MAAM,EAAE/W,wBAAwB;gBAChCgX,WAAW,gCAAA1a,MAAA,CAAgCjC,OAAO,CAAE;gBACpD4c,WAAW,EAAE;eACd;cAEKhB,gBAAgB,GAAGjD,MAAM,IAAI,IAAI,GAAG,KAAK,GAAGA,MAAM,CAACiD,gBAAgB;cAAA,MACrEA,gBAAgB,IAAI,IAAI,CAAC5U,SAAS,IAAI,IAAI;gBAAAoV,SAAA,CAAA7Q,IAAA;gBAAA;cAAA;cAC5CoQ,cAAc,CAACd,cAAc,GAAG,IAAI,CAACN,iBAAiB,EAAE;cAClDuB,UAAU,GAAG,WAAW;cAAAM,SAAA,CAAA3M,EAAA,GAEpBtR,EAAE;cAAAie,SAAA,CAAA7Q,IAAA;cAAA,OAAqB,IAAI,CAACvE,SAAS,CAAC+R,UAAU,EAAE;YAAA;cAAAqD,SAAA,CAAA1M,EAAA,GAAA0M,SAAA,CAAAzM,IAAA;cAAAyM,SAAA,CAAAS,EAAA,GAAEf,UAAU;cAAAM,SAAA,CAAA7Q,IAAA;cAAA,OAAA6Q,SAAA,CAAA3M,EAAA,CAA3D8M,aAAa,CAAAnW,IAAA,CAAAgW,SAAA,CAAA3M,EAAA,EAAA2M,SAAA,CAAA1M,EAAA,EAAA0M,SAAA,CAAAS,EAAA;YAAA;cAAAd,qBAAA,GAAAK,SAAA,CAAAzM,IAAA;cADbqM,mBAAmB,GAAAD,qBAAA,CAAzBza,IAAI;cAA8B2a,oBAAoB,GAAAF,qBAAA,CAA3Be,KAAK;cAEvC,CAAAjB,qBAAA,GAAAN,kBAAkB,CAACuB,KAAK,EAACla,IAAI,CAAAgJ,KAAA,CAAAiQ,qBAAA,EAAAkB,kBAAA,CAAId,oBAAoB,EAAC;cACtDV,kBAAkB,CAACja,IAAI,GAAGnD,EAAE,CAAC6e,uBAAuB,CAChD,CAACzB,kBAAkB,CAACja,IAAI,EAAE0a,mBAAmB,CAAC,CAAC;YAAC;cAGtD,IAAI,IAAI,CAACiB,mBAAmB,IAAI,IAAI,EAAE;gBACpC;gBACMf,SAAS,GAAG,IAAI;gBACtB5c,wBAAwB,CAAC,IAAI,CAAC2d,mBAAmB,EAAE,IAAI,CAAC9a,IAAI,EAAE+Z,SAAS,CAAC;gBACxEP,cAAc,CAACsB,mBAAmB,GAAG,IAAI,CAACA,mBAAmB;;cAG/DtB,cAAc,CAACuB,UAAU,GAAG3B,kBAAkB,CAACja,IAAI;cACnDqa,cAAc,CAACwB,WAAW,GAAG5B,kBAAkB,CAACuB,KAAK;cAAC,OAAAV,SAAA,CAAA5Q,MAAA,WAC/C6P,YAAY,CAACiB,IAAI,CAACX,cAAc,CAAC;YAAA;YAAA;cAAA,OAAAS,SAAA,CAAA3Q,IAAA;UAAA;QAAA,GAAA2P,QAAA;MAAA,CACzC;MAAA,SAAAkB,KAAAc,IAAA,EAAAC,IAAA;QAAA,OAAAlC,KAAA,CAAAvP,KAAA,OAAAlK,SAAA;MAAA;MAAA,OAAA4a,IAAA;IAAA;IAED;;;;;;;;EAAA;IAAAva,GAAA;IAAAY,KAAA,EAQA,SAAA2a,uBAAuBL,mBAAuB;MAC5C3d,wBAAwB,CAAC2d,mBAAmB,EAAE,IAAI,CAAC9a,IAAI,CAAC;MACxD,IAAI,CAAC8a,mBAAmB,GAAGA,mBAAmB;IAChD;IAEA;;;;;;;;;;;EAAA;IAAAlb,GAAA;IAAAY,KAAA,EAWA,SAAA4a,uBAAA,EAAsB;MACpB,OAAO,IAAI,CAACN,mBAAmB;IACjC;EAAC;EAAA,OAAArX,WAAA;AAAA,EAvrD8B3F,SAAS;AACxC;AACA;AACA;AACO2F,WAAA,CAAA4X,SAAS,GAAG,OAAO;AAqrD5Blf,aAAa,CAACmf,aAAa,CAAC7X,WAAW,CAAC;AAExC;;;;;;AAMA;AACA,WAAa8X,UAAW,0BAAAC,YAAA;EAAA7X,SAAA,CAAA4X,UAAA,EAAAC,YAAA;EAAA,IAAAC,OAAA,GAAA5X,YAAA,CAAA0X,UAAA;EAAA,SAAAA,WAAA;IAAAvX,eAAA,OAAAuX,UAAA;IAAA,OAAAE,OAAA,CAAAhS,KAAA,OAAAlK,SAAA;EAAA;EAAA,OAAA4E,YAAA,CAAAoX,UAAA;AAAA,EAAQ9X,WAAW;AACzB8X,UAAA,CAAAF,SAAS,GAAG,YAAY;AAE1Clf,aAAa,CAACmf,aAAa,CAACC,UAAU,CAAC"},"metadata":{},"sourceType":"module","externalDependencies":[]}