{"ast":null,"code":"import _toConsumableArray from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/toConsumableArray.js\";\n/**\r\n * @license\r\n * Copyright 2020 Google LLC. All Rights Reserved.\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n * http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n * =============================================================================\r\n */\nimport { util } from '@tensorflow/tfjs-core';\nimport { Im2ColPackedProgram } from '../im2col_packed_gpu';\nimport { mapActivationToShaderProgram } from '../kernel_utils/kernel_funcs_utils';\nimport { MatMulPackedProgram } from '../mulmat_packed_gpu';\nimport * as webgl_util from '../webgl_util';\nimport { batchMatMulImpl, MATMUL_SHARED_DIM_THRESHOLD } from './BatchMatMul_impl';\nimport { identity } from './Identity';\nimport { reshape } from './Reshape';\n// Both conv2dByMatMul and conv2dWithIm2Row fuse height and width into one\n// dimension to compute batchMatMul, so bias and activation weights are also\n// supposed to fuse the two dimensions into one.\n//\n// This function computes the target shape for fusing height and width\n// dimensions. Returning null means the shape is already compatible.\n//\n// Even though the bias is not supposed to be a 3-D or a 4-D (including\n// batch) tensor and PReLU activiation weights is not supposed to be a 4-D\n// tensor, we still need to support them, because we haven't disabled\n// them for NHWC format.\n// https://github.com/tensorflow/tfjs/blob/b53bd47e880367ae57493f0ea628abaf08db2d5d/tfjs-core/src/ops/fused/conv2d.ts#L181-L196\nfunction getShapeForBatchMatMul(shape, isChannelsLast) {\n  var length = shape.length;\n  if (length >= 3) {\n    return isChannelsLast ? [].concat(_toConsumableArray(shape.slice(0, -3)), [shape[length - 3] * shape[length - 2] /* height * width */, shape[length - 1] /* channel */]) : [].concat(_toConsumableArray(shape.slice(0, -3)), [shape[length - 3] /* channel */, shape[length - 2] * shape[length - 1] /* height * width */]);\n  } else if (!isChannelsLast && length === 1 && shape[0] > 1) {\n    return [shape[0], 1];\n  } else {\n    return null;\n  }\n}\n// For 1x1 kernels that iterate through every point in the input, convolution\n// can be expressed as matrix multiplication (without need for memory\n// remapping).\nexport function conv2dByMatMul(_ref) {\n  var x = _ref.x,\n    filter = _ref.filter,\n    convInfo = _ref.convInfo,\n    backend = _ref.backend,\n    _ref$bias = _ref.bias,\n    bias = _ref$bias === void 0 ? null : _ref$bias,\n    _ref$preluActivationW = _ref.preluActivationWeights,\n    preluActivationWeights = _ref$preluActivationW === void 0 ? null : _ref$preluActivationW,\n    _ref$leakyreluAlpha = _ref.leakyreluAlpha,\n    leakyreluAlpha = _ref$leakyreluAlpha === void 0 ? 0 : _ref$leakyreluAlpha,\n    _ref$activation = _ref.activation,\n    activation = _ref$activation === void 0 ? null : _ref$activation;\n  // Reshapes conv2D input to 2D tensors, uses matMul and then reshape the\n  // result from 2D to 4D.\n  var xShape = x.shape;\n  var xTexData = backend.texData.get(x.dataId);\n  var sharedMatMulDim = convInfo.inChannels;\n  var outerShapeX = xShape[0] * xShape[1] * xShape[2];\n  var outerShapeFilter = convInfo.outChannels;\n  var isChannelsLast = convInfo.dataFormat === 'channelsLast';\n  var transposeA = false;\n  var transposeB = false;\n  var out;\n  var intermediates = [];\n  if (preluActivationWeights != null) {\n    var targetShape = getShapeForBatchMatMul(preluActivationWeights.shape, isChannelsLast);\n    if (targetShape != null) {\n      preluActivationWeights = reshape({\n        inputs: {\n          x: preluActivationWeights\n        },\n        backend: backend,\n        attrs: {\n          shape: targetShape\n        }\n      });\n      intermediates.push(preluActivationWeights);\n    }\n  }\n  if (bias != null) {\n    var _targetShape = getShapeForBatchMatMul(bias.shape, isChannelsLast);\n    if (_targetShape != null) {\n      bias = reshape({\n        inputs: {\n          x: bias\n        },\n        backend: backend,\n        attrs: {\n          shape: _targetShape\n        }\n      });\n      intermediates.push(bias);\n    }\n  }\n  // TODO: Once reduction ops are packed, batchMatMul will always be packed\n  // and we can remove this condition.\n  var batchMatMulWillBeUnpacked = (outerShapeX === 1 || outerShapeFilter === 1) && sharedMatMulDim > MATMUL_SHARED_DIM_THRESHOLD;\n  // The algorithm in the if condition assumes (1) the output will be packed,\n  // (2) x is packed, (3) x isChannelsLast, (4)  x's packed texture is already\n  // on GPU, (5) col is odd, (6) the width, height and inChannels are the same\n  // for xTexData.shape and xShape.\n  var canOptimize = !batchMatMulWillBeUnpacked && xTexData.isPacked && isChannelsLast && xTexData.texture != null && xShape[2] % 2 !== 0 && util.arraysEqual(xTexData.shape.slice(-3), xShape.slice(-3));\n  if (canOptimize) {\n    // We avoid expensive packed 2x2 reshape by padding col count to next,\n    // even number. When col is odd, the result of packed batchMatMul is\n    // the same (has the same texture layout and and values in the texture) as\n    // it is for next even col. We make the odd-cols tensor to look like\n    // even-cols tensor before the operation and, after the batchMatMul,\n    // fix the even-cols result to have odd number of cols.\n    var _targetShape2 = xShape[0] * xShape[1] * (xShape[2] + 1);\n    var xReshaped = {\n      dataId: x.dataId,\n      shape: [1, _targetShape2, convInfo.inChannels],\n      dtype: x.dtype\n    };\n    // xTexData.shape gets referenced from GPGPUBinary.inShapeInfos.\n    // Decrementing col count, after batchMatMul->...->compileProgram leads to\n    // invalid col count within the reference in GPGPUBinary.inShapeInfos.\n    // Alternative fix would be to provide a copy to GPGPUBinary.inShapeInfos\n    // in compileProgram method, but that would affect compilation of all\n    // programs - instead, provide a copy here, with even col count, before\n    // calling batchMatMul->...->compileProgram and after that, the original\n    // xTexData.shape is restored.\n    var originalXTexDataShape = xTexData.shape;\n    xTexData.shape = xTexData.shape.slice();\n    xTexData.shape[xTexData.shape.length - 2]++;\n    util.assert(webgl_util.isReshapeFree(xTexData.shape, xReshaped.shape), function () {\n      return \"packed reshape \".concat(xTexData.shape, \" to \").concat(xReshaped.shape, \" isn't free\");\n    });\n    var filterReshaped = reshape({\n      inputs: {\n        x: filter\n      },\n      backend: backend,\n      attrs: {\n        shape: [1, convInfo.inChannels, convInfo.outChannels]\n      }\n    });\n    intermediates.push(filterReshaped);\n    var pointwiseConv = batchMatMulImpl({\n      a: xReshaped,\n      b: filterReshaped,\n      backend: backend,\n      transposeA: transposeA,\n      transposeB: transposeB,\n      bias: bias,\n      activation: activation,\n      preluActivationWeights: preluActivationWeights,\n      leakyreluAlpha: leakyreluAlpha\n    });\n    var pointwiseConvTexData = backend.texData.get(pointwiseConv.dataId);\n    util.assert(pointwiseConvTexData.isPacked, function () {\n      return 'batchMatMul result is expected to be packed';\n    });\n    // Restore the input shape to original.\n    xTexData.shape = originalXTexDataShape;\n    // Set the output shape - there is no need for expensive reshape as data\n    // layout is already correct.\n    pointwiseConvTexData.shape = convInfo.outShape;\n    out = identity({\n      inputs: {\n        x: pointwiseConv\n      },\n      backend: backend\n    });\n    out.shape = convInfo.outShape;\n    intermediates.push(pointwiseConv);\n  } else {\n    var numCols = convInfo.outHeight * convInfo.outWidth;\n    var _xReshaped = reshape({\n      inputs: {\n        x: x\n      },\n      backend: backend,\n      attrs: {\n        shape: isChannelsLast ? [convInfo.batchSize, numCols, convInfo.inChannels] : [convInfo.batchSize, convInfo.inChannels, numCols]\n      }\n    });\n    var _filterReshaped = reshape({\n      inputs: {\n        x: filter\n      },\n      backend: backend,\n      attrs: {\n        shape: [1, convInfo.inChannels, convInfo.outChannels]\n      }\n    });\n    var result = batchMatMulImpl({\n      a: isChannelsLast ? _xReshaped : _filterReshaped,\n      b: isChannelsLast ? _filterReshaped : _xReshaped,\n      transposeA: !isChannelsLast,\n      transposeB: transposeB,\n      backend: backend,\n      bias: bias,\n      activation: activation,\n      preluActivationWeights: preluActivationWeights,\n      leakyreluAlpha: leakyreluAlpha\n    });\n    out = reshape({\n      inputs: {\n        x: result\n      },\n      backend: backend,\n      attrs: {\n        shape: convInfo.outShape\n      }\n    });\n    intermediates.push(_xReshaped);\n    intermediates.push(_filterReshaped);\n    intermediates.push(result);\n  }\n  for (var _i = 0, _intermediates = intermediates; _i < _intermediates.length; _i++) {\n    var i = _intermediates[_i];\n    backend.disposeIntermediateTensorInfo(i);\n  }\n  return out;\n}\n// Implements the im2row algorithm as outlined in \"High Performance\n// Convolutional Neural Networks for Document Processing\" (Suvisoft, 2006)\nexport function conv2dWithIm2Row(_ref2) {\n  var x = _ref2.x,\n    filter = _ref2.filter,\n    convInfo = _ref2.convInfo,\n    backend = _ref2.backend,\n    _ref2$bias = _ref2.bias,\n    bias = _ref2$bias === void 0 ? null : _ref2$bias,\n    _ref2$preluActivation = _ref2.preluActivationWeights,\n    preluActivationWeights = _ref2$preluActivation === void 0 ? null : _ref2$preluActivation,\n    _ref2$leakyreluAlpha = _ref2.leakyreluAlpha,\n    leakyreluAlpha = _ref2$leakyreluAlpha === void 0 ? 0 : _ref2$leakyreluAlpha,\n    _ref2$activation = _ref2.activation,\n    activation = _ref2$activation === void 0 ? null : _ref2$activation;\n  // Rearranges conv2d input so each block to be convolved over forms the\n  // column of a new matrix with shape [filterWidth * filterHeight *\n  // inChannels, outHeight * outWidth]. The filter is also rearranged so each\n  // output channel forms a row of a new matrix with shape [outChannels,\n  // filterWidth * filterHeight * inChannels]. The convolution is then\n  // computed by multiplying these matrices and reshaping the result.\n  var filterWidth = convInfo.filterWidth,\n    filterHeight = convInfo.filterHeight,\n    inChannels = convInfo.inChannels,\n    outWidth = convInfo.outWidth,\n    outHeight = convInfo.outHeight,\n    dataFormat = convInfo.dataFormat;\n  var isChannelsLast = dataFormat === 'channelsLast';\n  var sharedDim = filterWidth * filterHeight * inChannels;\n  var numCols = outHeight * outWidth;\n  var x2ColShape = [convInfo.batchSize, sharedDim, numCols];\n  var transposeA = true;\n  var transposeB = false;\n  var intermediates = [];\n  if (preluActivationWeights != null) {\n    var targetShape = getShapeForBatchMatMul(preluActivationWeights.shape, isChannelsLast);\n    if (targetShape != null) {\n      preluActivationWeights = reshape({\n        inputs: {\n          x: preluActivationWeights\n        },\n        backend: backend,\n        attrs: {\n          shape: targetShape\n        }\n      });\n      intermediates.push(preluActivationWeights);\n    }\n  }\n  if (bias != null) {\n    var _targetShape3 = getShapeForBatchMatMul(bias.shape, isChannelsLast);\n    if (_targetShape3 != null) {\n      bias = reshape({\n        inputs: {\n          x: bias\n        },\n        backend: backend,\n        attrs: {\n          shape: _targetShape3\n        }\n      });\n      intermediates.push(bias);\n    }\n  }\n  var w2Row = reshape({\n    inputs: {\n      x: filter\n    },\n    backend: backend,\n    attrs: {\n      shape: [1, sharedDim, util.sizeFromShape(filter.shape) / sharedDim]\n    }\n  });\n  intermediates.push(w2Row);\n  var im2ColProgram = new Im2ColPackedProgram(x2ColShape, convInfo);\n  var customValues = [x.shape, [convInfo.padInfo.top, convInfo.padInfo.left], [convInfo.strideHeight, convInfo.strideWidth], [convInfo.dilationHeight, convInfo.dilationWidth], [convInfo.inChannels], [convInfo.filterWidth * convInfo.inChannels], [convInfo.outWidth]];\n  var im2Col = backend.runWebGLProgram(im2ColProgram, [x], 'float32', customValues);\n  var im2ColReshaped = reshape({\n    inputs: {\n      x: im2Col\n    },\n    backend: backend,\n    attrs: {\n      shape: x2ColShape\n    }\n  });\n  intermediates.push(im2Col);\n  intermediates.push(im2ColReshaped);\n  var hasBias = bias != null;\n  var hasPreluActivationWeights = preluActivationWeights != null;\n  var hasLeakyreluAlpha = activation === 'leakyrelu';\n  var fusedActivation = activation ? mapActivationToShaderProgram(activation, true) : null;\n  var matmulProgram = new MatMulPackedProgram(isChannelsLast ? im2ColReshaped.shape : w2Row.shape, isChannelsLast ? w2Row.shape : im2ColReshaped.shape, isChannelsLast ? [convInfo.batchSize, numCols, convInfo.outChannels] : [convInfo.batchSize, convInfo.outChannels, numCols], transposeA, transposeB, hasBias, fusedActivation, hasPreluActivationWeights, hasLeakyreluAlpha);\n  var inputs = isChannelsLast ? [im2ColReshaped, w2Row] : [w2Row, im2ColReshaped];\n  if (bias) {\n    inputs.push(bias);\n  }\n  if (hasPreluActivationWeights) {\n    inputs.push(preluActivationWeights);\n  }\n  if (hasLeakyreluAlpha) {\n    var $leakyreluAlpha = backend.makeTensorInfo([], 'float32', util.createScalarValue(leakyreluAlpha, 'float32'));\n    inputs.push($leakyreluAlpha);\n    intermediates.push($leakyreluAlpha);\n  }\n  var product = backend.runWebGLProgram(matmulProgram, inputs, 'float32');\n  var out = reshape({\n    inputs: {\n      x: product\n    },\n    backend: backend,\n    attrs: {\n      shape: convInfo.outShape\n    }\n  });\n  intermediates.push(product);\n  for (var _i2 = 0, _intermediates2 = intermediates; _i2 < _intermediates2.length; _i2++) {\n    var i = _intermediates2[_i2];\n    backend.disposeIntermediateTensorInfo(i);\n  }\n  return out;\n}","map":{"version":3,"names":["util","Im2ColPackedProgram","mapActivationToShaderProgram","MatMulPackedProgram","webgl_util","batchMatMulImpl","MATMUL_SHARED_DIM_THRESHOLD","identity","reshape","getShapeForBatchMatMul","shape","isChannelsLast","length","concat","_toConsumableArray","slice","conv2dByMatMul","_ref","x","filter","convInfo","backend","_ref$bias","bias","_ref$preluActivationW","preluActivationWeights","_ref$leakyreluAlpha","leakyreluAlpha","_ref$activation","activation","xShape","xTexData","texData","get","dataId","sharedMatMulDim","inChannels","outerShapeX","outerShapeFilter","outChannels","dataFormat","transposeA","transposeB","out","intermediates","targetShape","inputs","attrs","push","batchMatMulWillBeUnpacked","canOptimize","isPacked","texture","arraysEqual","xReshaped","dtype","originalXTexDataShape","assert","isReshapeFree","filterReshaped","pointwiseConv","a","b","pointwiseConvTexData","outShape","numCols","outHeight","outWidth","batchSize","result","_i","_intermediates","i","disposeIntermediateTensorInfo","conv2dWithIm2Row","_ref2","_ref2$bias","_ref2$preluActivation","_ref2$leakyreluAlpha","_ref2$activation","filterWidth","filterHeight","sharedDim","x2ColShape","w2Row","sizeFromShape","im2ColProgram","customValues","padInfo","top","left","strideHeight","strideWidth","dilationHeight","dilationWidth","im2Col","runWebGLProgram","im2ColReshaped","hasBias","hasPreluActivationWeights","hasLeakyreluAlpha","fusedActivation","matmulProgram","$leakyreluAlpha","makeTensorInfo","createScalarValue","product","_i2","_intermediates2"],"sources":["C:\\Users\\vince\\OneDrive\\Documents\\GitHub\\tfjs-backend-webgl\\src\\kernels\\Conv2D_impl.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, TensorInfo, util} from '@tensorflow/tfjs-core';\n\n// import {assertAndGetBroadcastShape} from\n// '../../../tfjs-core/src/ops/broadcast_util';\nimport {MathBackendWebGL} from '../backend_webgl';\nimport {Im2ColPackedProgram} from '../im2col_packed_gpu';\nimport {mapActivationToShaderProgram} from '../kernel_utils/kernel_funcs_utils';\nimport {MatMulPackedProgram} from '../mulmat_packed_gpu';\nimport * as webgl_util from '../webgl_util';\n\nimport {batchMatMulImpl, MATMUL_SHARED_DIM_THRESHOLD} from './BatchMatMul_impl';\nimport {identity} from './Identity';\nimport {reshape} from './Reshape';\n\ntype Conv2DConfig = {\n  x: TensorInfo,\n  filter: TensorInfo,\n  convInfo: backend_util.Conv2DInfo,\n  backend: MathBackendWebGL,\n  bias?: TensorInfo,\n  preluActivationWeights?: TensorInfo,\n  leakyreluAlpha?: number,\n  activation?: backend_util.Activation\n};\n\n// Both conv2dByMatMul and conv2dWithIm2Row fuse height and width into one\n// dimension to compute batchMatMul, so bias and activation weights are also\n// supposed to fuse the two dimensions into one.\n//\n// This function computes the target shape for fusing height and width\n// dimensions. Returning null means the shape is already compatible.\n//\n// Even though the bias is not supposed to be a 3-D or a 4-D (including\n// batch) tensor and PReLU activiation weights is not supposed to be a 4-D\n// tensor, we still need to support them, because we haven't disabled\n// them for NHWC format.\n// https://github.com/tensorflow/tfjs/blob/b53bd47e880367ae57493f0ea628abaf08db2d5d/tfjs-core/src/ops/fused/conv2d.ts#L181-L196\nfunction getShapeForBatchMatMul(\n    shape: number[], isChannelsLast: boolean): number[] {\n  const length = shape.length;\n  if (length >= 3) {\n    return isChannelsLast ?\n        [\n          ...shape.slice(0, -3) /* batch */,\n          shape[length - 3] * shape[length - 2] /* height * width */,\n          shape[length - 1] /* channel */\n        ] :\n        [\n          ...shape.slice(0, -3) /* batch */, shape[length - 3] /* channel */,\n          shape[length - 2] * shape[length - 1] /* height * width */\n        ];\n  } else if (!isChannelsLast && length === 1 && shape[0] > 1) {\n    return [shape[0], 1];\n  } else {\n    return null;\n  }\n}\n\n// For 1x1 kernels that iterate through every point in the input, convolution\n// can be expressed as matrix multiplication (without need for memory\n// remapping).\nexport function conv2dByMatMul({\n  x,\n  filter,\n  convInfo,\n  backend,\n  bias = null,\n  preluActivationWeights = null,\n  leakyreluAlpha = 0,\n  activation = null\n}: Conv2DConfig) {\n  // Reshapes conv2D input to 2D tensors, uses matMul and then reshape the\n  // result from 2D to 4D.\n  const xShape = x.shape;\n  const xTexData = backend.texData.get(x.dataId);\n  const sharedMatMulDim = convInfo.inChannels;\n  const outerShapeX = xShape[0] * xShape[1] * xShape[2];\n  const outerShapeFilter = convInfo.outChannels;\n  const isChannelsLast = convInfo.dataFormat === 'channelsLast';\n  const transposeA = false;\n  const transposeB = false;\n\n  let out: TensorInfo;\n  const intermediates: TensorInfo[] = [];\n\n  if (preluActivationWeights != null) {\n    const targetShape =\n        getShapeForBatchMatMul(preluActivationWeights.shape, isChannelsLast);\n    if (targetShape != null) {\n      preluActivationWeights = reshape({\n        inputs: {x: preluActivationWeights},\n        backend,\n        attrs: {shape: targetShape}\n      });\n      intermediates.push(preluActivationWeights);\n    }\n  }\n\n  if (bias != null) {\n    const targetShape = getShapeForBatchMatMul(bias.shape, isChannelsLast);\n    if (targetShape != null) {\n      bias = reshape({inputs: {x: bias}, backend, attrs: {shape: targetShape}});\n      intermediates.push(bias);\n    }\n  }\n\n  // TODO: Once reduction ops are packed, batchMatMul will always be packed\n  // and we can remove this condition.\n  const batchMatMulWillBeUnpacked =\n      (outerShapeX === 1 || outerShapeFilter === 1) &&\n      sharedMatMulDim > MATMUL_SHARED_DIM_THRESHOLD;\n\n  // The algorithm in the if condition assumes (1) the output will be packed,\n  // (2) x is packed, (3) x isChannelsLast, (4)  x's packed texture is already\n  // on GPU, (5) col is odd, (6) the width, height and inChannels are the same\n  // for xTexData.shape and xShape.\n  const canOptimize = !batchMatMulWillBeUnpacked && xTexData.isPacked &&\n      isChannelsLast && xTexData.texture != null && xShape[2] % 2 !== 0 &&\n      util.arraysEqual(xTexData.shape.slice(-3), xShape.slice(-3));\n\n  if (canOptimize) {\n    // We avoid expensive packed 2x2 reshape by padding col count to next,\n    // even number. When col is odd, the result of packed batchMatMul is\n    // the same (has the same texture layout and and values in the texture) as\n    // it is for next even col. We make the odd-cols tensor to look like\n    // even-cols tensor before the operation and, after the batchMatMul,\n    // fix the even-cols result to have odd number of cols.\n    const targetShape = xShape[0] * xShape[1] * (xShape[2] + 1);\n    const xReshaped: TensorInfo = {\n      dataId: x.dataId,\n      shape: [1, targetShape, convInfo.inChannels],\n      dtype: x.dtype\n    };\n    // xTexData.shape gets referenced from GPGPUBinary.inShapeInfos.\n    // Decrementing col count, after batchMatMul->...->compileProgram leads to\n    // invalid col count within the reference in GPGPUBinary.inShapeInfos.\n    // Alternative fix would be to provide a copy to GPGPUBinary.inShapeInfos\n    // in compileProgram method, but that would affect compilation of all\n    // programs - instead, provide a copy here, with even col count, before\n    // calling batchMatMul->...->compileProgram and after that, the original\n    // xTexData.shape is restored.\n    const originalXTexDataShape = xTexData.shape;\n    xTexData.shape = xTexData.shape.slice();\n    xTexData.shape[xTexData.shape.length - 2]++;\n    util.assert(\n        webgl_util.isReshapeFree(xTexData.shape, xReshaped.shape),\n        () => `packed reshape ${xTexData.shape} to ${\n            xReshaped.shape} isn't free`);\n    const filterReshaped = reshape({\n      inputs: {x: filter},\n      backend,\n      attrs: {shape: [1, convInfo.inChannels, convInfo.outChannels]}\n    });\n    intermediates.push(filterReshaped);\n    const pointwiseConv = batchMatMulImpl({\n      a: xReshaped,\n      b: filterReshaped,\n      backend,\n      transposeA,\n      transposeB,\n      bias,\n      activation,\n      preluActivationWeights,\n      leakyreluAlpha\n    });\n\n    const pointwiseConvTexData = backend.texData.get(pointwiseConv.dataId);\n    util.assert(\n        pointwiseConvTexData.isPacked,\n        () => 'batchMatMul result is expected to be packed');\n    // Restore the input shape to original.\n    xTexData.shape = originalXTexDataShape;\n    // Set the output shape - there is no need for expensive reshape as data\n    // layout is already correct.\n    pointwiseConvTexData.shape = convInfo.outShape;\n\n    out = identity({inputs: {x: pointwiseConv}, backend});\n    out.shape = convInfo.outShape;\n\n    intermediates.push(pointwiseConv);\n  } else {\n    const numCols = convInfo.outHeight * convInfo.outWidth;\n    const xReshaped = reshape({\n      inputs: {x},\n      backend,\n      attrs: {\n        shape: isChannelsLast ?\n            [convInfo.batchSize, numCols, convInfo.inChannels] :\n            [convInfo.batchSize, convInfo.inChannels, numCols]\n      }\n    });\n    const filterReshaped = reshape({\n      inputs: {x: filter},\n      backend,\n      attrs: {shape: [1, convInfo.inChannels, convInfo.outChannels]}\n    });\n    const result = batchMatMulImpl({\n      a: isChannelsLast ? xReshaped : filterReshaped,\n      b: isChannelsLast ? filterReshaped : xReshaped,\n      transposeA: !isChannelsLast,\n      transposeB,\n      backend,\n      bias,\n      activation,\n      preluActivationWeights,\n      leakyreluAlpha\n    });\n\n    out = reshape(\n        {inputs: {x: result}, backend, attrs: {shape: convInfo.outShape}});\n\n    intermediates.push(xReshaped);\n    intermediates.push(filterReshaped);\n    intermediates.push(result);\n  }\n\n  for (const i of intermediates) {\n    backend.disposeIntermediateTensorInfo(i);\n  }\n\n  return out;\n}\n\n// Implements the im2row algorithm as outlined in \"High Performance\n// Convolutional Neural Networks for Document Processing\" (Suvisoft, 2006)\nexport function conv2dWithIm2Row({\n  x,\n  filter,\n  convInfo,\n  backend,\n  bias = null,\n  preluActivationWeights = null,\n  leakyreluAlpha = 0,\n  activation = null\n}: Conv2DConfig) {\n  // Rearranges conv2d input so each block to be convolved over forms the\n  // column of a new matrix with shape [filterWidth * filterHeight *\n  // inChannels, outHeight * outWidth]. The filter is also rearranged so each\n  // output channel forms a row of a new matrix with shape [outChannels,\n  // filterWidth * filterHeight * inChannels]. The convolution is then\n  // computed by multiplying these matrices and reshaping the result.\n  const {\n    filterWidth,\n    filterHeight,\n    inChannels,\n    outWidth,\n    outHeight,\n    dataFormat\n  } = convInfo;\n\n  const isChannelsLast = dataFormat === 'channelsLast';\n\n  const sharedDim = filterWidth * filterHeight * inChannels;\n  const numCols = outHeight * outWidth;\n  const x2ColShape = [convInfo.batchSize, sharedDim, numCols];\n  const transposeA = true;\n  const transposeB = false;\n\n  const intermediates: TensorInfo[] = [];\n\n  if (preluActivationWeights != null) {\n    const targetShape =\n        getShapeForBatchMatMul(preluActivationWeights.shape, isChannelsLast);\n    if (targetShape != null) {\n      preluActivationWeights = reshape({\n        inputs: {x: preluActivationWeights},\n        backend,\n        attrs: {shape: targetShape}\n      });\n      intermediates.push(preluActivationWeights);\n    }\n  }\n\n  if (bias != null) {\n    const targetShape = getShapeForBatchMatMul(bias.shape, isChannelsLast);\n    if (targetShape != null) {\n      bias = reshape({inputs: {x: bias}, backend, attrs: {shape: targetShape}});\n      intermediates.push(bias);\n    }\n  }\n\n  const w2Row = reshape({\n    inputs: {x: filter},\n    backend,\n    attrs: {shape: [1, sharedDim, util.sizeFromShape(filter.shape) / sharedDim]}\n  });\n  intermediates.push(w2Row);\n\n  const im2ColProgram = new Im2ColPackedProgram(x2ColShape, convInfo);\n  const customValues = [\n    x.shape, [convInfo.padInfo.top, convInfo.padInfo.left],\n    [convInfo.strideHeight, convInfo.strideWidth],\n    [convInfo.dilationHeight, convInfo.dilationWidth], [convInfo.inChannels],\n    [convInfo.filterWidth * convInfo.inChannels], [convInfo.outWidth]\n  ];\n  const im2Col =\n      backend.runWebGLProgram(im2ColProgram, [x], 'float32', customValues);\n  const im2ColReshaped =\n      reshape({inputs: {x: im2Col}, backend, attrs: {shape: x2ColShape}});\n\n  intermediates.push(im2Col);\n  intermediates.push(im2ColReshaped);\n\n  const hasBias = bias != null;\n  const hasPreluActivationWeights = preluActivationWeights != null;\n  const hasLeakyreluAlpha = activation === 'leakyrelu';\n  const fusedActivation =\n      activation ? mapActivationToShaderProgram(activation, true) : null;\n  const matmulProgram = new MatMulPackedProgram(\n      isChannelsLast ? im2ColReshaped.shape as [number, number, number] :\n                       w2Row.shape as [number, number, number],\n      isChannelsLast ? w2Row.shape as [number, number, number] :\n                       im2ColReshaped.shape as [number, number, number],\n      isChannelsLast ? [convInfo.batchSize, numCols, convInfo.outChannels] :\n                       [convInfo.batchSize, convInfo.outChannels, numCols],\n      transposeA, transposeB, hasBias, fusedActivation,\n      hasPreluActivationWeights, hasLeakyreluAlpha);\n  const inputs: TensorInfo[] =\n      isChannelsLast ? [im2ColReshaped, w2Row] : [w2Row, im2ColReshaped];\n  if (bias) {\n    inputs.push(bias);\n  }\n  if (hasPreluActivationWeights) {\n    inputs.push(preluActivationWeights);\n  }\n  if (hasLeakyreluAlpha) {\n    const $leakyreluAlpha = backend.makeTensorInfo(\n        [], 'float32',\n        util.createScalarValue(leakyreluAlpha as unknown as 'float32',\n                               'float32'));\n    inputs.push($leakyreluAlpha);\n    intermediates.push($leakyreluAlpha);\n  }\n  const product = backend.runWebGLProgram(matmulProgram, inputs, 'float32');\n  const out = reshape(\n      {inputs: {x: product}, backend, attrs: {shape: convInfo.outShape}});\n\n  intermediates.push(product);\n  for (const i of intermediates) {\n    backend.disposeIntermediateTensorInfo(i);\n  }\n\n  return out;\n}\n"],"mappings":";AAAA;;;;;;;;;;;;;;;;AAiBA,SAAkCA,IAAI,QAAO,uBAAuB;AAKpE,SAAQC,mBAAmB,QAAO,sBAAsB;AACxD,SAAQC,4BAA4B,QAAO,oCAAoC;AAC/E,SAAQC,mBAAmB,QAAO,sBAAsB;AACxD,OAAO,KAAKC,UAAU,MAAM,eAAe;AAE3C,SAAQC,eAAe,EAAEC,2BAA2B,QAAO,oBAAoB;AAC/E,SAAQC,QAAQ,QAAO,YAAY;AACnC,SAAQC,OAAO,QAAO,WAAW;AAajC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAASC,sBAAsBA,CAC3BC,KAAe,EAAEC,cAAuB;EAC1C,IAAMC,MAAM,GAAGF,KAAK,CAACE,MAAM;EAC3B,IAAIA,MAAM,IAAI,CAAC,EAAE;IACf,OAAOD,cAAc,MAAAE,MAAA,CAAAC,kBAAA,CAEZJ,KAAK,CAACK,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,IACrBL,KAAK,CAACE,MAAM,GAAG,CAAC,CAAC,GAAGF,KAAK,CAACE,MAAM,GAAG,CAAC,CAAC,CAAC,sBACtCF,KAAK,CAACE,MAAM,GAAG,CAAC,CAAC,CAAC,qBAAAC,MAAA,CAAAC,kBAAA,CAGfJ,KAAK,CAACK,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,IAAcL,KAAK,CAACE,MAAM,GAAG,CAAC,CAAC,CAAC,eACrDF,KAAK,CAACE,MAAM,GAAG,CAAC,CAAC,GAAGF,KAAK,CAACE,MAAM,GAAG,CAAC,CAAC,CAAC,sBACvC;GACN,MAAM,IAAI,CAACD,cAAc,IAAIC,MAAM,KAAK,CAAC,IAAIF,KAAK,CAAC,CAAC,CAAC,GAAG,CAAC,EAAE;IAC1D,OAAO,CAACA,KAAK,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC;GACrB,MAAM;IACL,OAAO,IAAI;;AAEf;AAEA;AACA;AACA;AACA,OAAM,SAAUM,cAAcA,CAAAC,IAAA,EASf;EAAA,IARbC,CAAC,GAAAD,IAAA,CAADC,CAAC;IACDC,MAAM,GAAAF,IAAA,CAANE,MAAM;IACNC,QAAQ,GAAAH,IAAA,CAARG,QAAQ;IACRC,OAAO,GAAAJ,IAAA,CAAPI,OAAO;IAAAC,SAAA,GAAAL,IAAA,CACPM,IAAI;IAAJA,IAAI,GAAAD,SAAA,cAAG,IAAI,GAAAA,SAAA;IAAAE,qBAAA,GAAAP,IAAA,CACXQ,sBAAsB;IAAtBA,sBAAsB,GAAAD,qBAAA,cAAG,IAAI,GAAAA,qBAAA;IAAAE,mBAAA,GAAAT,IAAA,CAC7BU,cAAc;IAAdA,cAAc,GAAAD,mBAAA,cAAG,CAAC,GAAAA,mBAAA;IAAAE,eAAA,GAAAX,IAAA,CAClBY,UAAU;IAAVA,UAAU,GAAAD,eAAA,cAAG,IAAI,GAAAA,eAAA;EAEjB;EACA;EACA,IAAME,MAAM,GAAGZ,CAAC,CAACR,KAAK;EACtB,IAAMqB,QAAQ,GAAGV,OAAO,CAACW,OAAO,CAACC,GAAG,CAACf,CAAC,CAACgB,MAAM,CAAC;EAC9C,IAAMC,eAAe,GAAGf,QAAQ,CAACgB,UAAU;EAC3C,IAAMC,WAAW,GAAGP,MAAM,CAAC,CAAC,CAAC,GAAGA,MAAM,CAAC,CAAC,CAAC,GAAGA,MAAM,CAAC,CAAC,CAAC;EACrD,IAAMQ,gBAAgB,GAAGlB,QAAQ,CAACmB,WAAW;EAC7C,IAAM5B,cAAc,GAAGS,QAAQ,CAACoB,UAAU,KAAK,cAAc;EAC7D,IAAMC,UAAU,GAAG,KAAK;EACxB,IAAMC,UAAU,GAAG,KAAK;EAExB,IAAIC,GAAe;EACnB,IAAMC,aAAa,GAAiB,EAAE;EAEtC,IAAInB,sBAAsB,IAAI,IAAI,EAAE;IAClC,IAAMoB,WAAW,GACbpC,sBAAsB,CAACgB,sBAAsB,CAACf,KAAK,EAAEC,cAAc,CAAC;IACxE,IAAIkC,WAAW,IAAI,IAAI,EAAE;MACvBpB,sBAAsB,GAAGjB,OAAO,CAAC;QAC/BsC,MAAM,EAAE;UAAC5B,CAAC,EAAEO;QAAsB,CAAC;QACnCJ,OAAO,EAAPA,OAAO;QACP0B,KAAK,EAAE;UAACrC,KAAK,EAAEmC;QAAW;OAC3B,CAAC;MACFD,aAAa,CAACI,IAAI,CAACvB,sBAAsB,CAAC;;;EAI9C,IAAIF,IAAI,IAAI,IAAI,EAAE;IAChB,IAAMsB,YAAW,GAAGpC,sBAAsB,CAACc,IAAI,CAACb,KAAK,EAAEC,cAAc,CAAC;IACtE,IAAIkC,YAAW,IAAI,IAAI,EAAE;MACvBtB,IAAI,GAAGf,OAAO,CAAC;QAACsC,MAAM,EAAE;UAAC5B,CAAC,EAAEK;QAAI,CAAC;QAAEF,OAAO,EAAPA,OAAO;QAAE0B,KAAK,EAAE;UAACrC,KAAK,EAAEmC;QAAW;MAAC,CAAC,CAAC;MACzED,aAAa,CAACI,IAAI,CAACzB,IAAI,CAAC;;;EAI5B;EACA;EACA,IAAM0B,yBAAyB,GAC3B,CAACZ,WAAW,KAAK,CAAC,IAAIC,gBAAgB,KAAK,CAAC,KAC5CH,eAAe,GAAG7B,2BAA2B;EAEjD;EACA;EACA;EACA;EACA,IAAM4C,WAAW,GAAG,CAACD,yBAAyB,IAAIlB,QAAQ,CAACoB,QAAQ,IAC/DxC,cAAc,IAAIoB,QAAQ,CAACqB,OAAO,IAAI,IAAI,IAAItB,MAAM,CAAC,CAAC,CAAC,GAAG,CAAC,KAAK,CAAC,IACjE9B,IAAI,CAACqD,WAAW,CAACtB,QAAQ,CAACrB,KAAK,CAACK,KAAK,CAAC,CAAC,CAAC,CAAC,EAAEe,MAAM,CAACf,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC;EAEhE,IAAImC,WAAW,EAAE;IACf;IACA;IACA;IACA;IACA;IACA;IACA,IAAML,aAAW,GAAGf,MAAM,CAAC,CAAC,CAAC,GAAGA,MAAM,CAAC,CAAC,CAAC,IAAIA,MAAM,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC;IAC3D,IAAMwB,SAAS,GAAe;MAC5BpB,MAAM,EAAEhB,CAAC,CAACgB,MAAM;MAChBxB,KAAK,EAAE,CAAC,CAAC,EAAEmC,aAAW,EAAEzB,QAAQ,CAACgB,UAAU,CAAC;MAC5CmB,KAAK,EAAErC,CAAC,CAACqC;KACV;IACD;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA,IAAMC,qBAAqB,GAAGzB,QAAQ,CAACrB,KAAK;IAC5CqB,QAAQ,CAACrB,KAAK,GAAGqB,QAAQ,CAACrB,KAAK,CAACK,KAAK,EAAE;IACvCgB,QAAQ,CAACrB,KAAK,CAACqB,QAAQ,CAACrB,KAAK,CAACE,MAAM,GAAG,CAAC,CAAC,EAAE;IAC3CZ,IAAI,CAACyD,MAAM,CACPrD,UAAU,CAACsD,aAAa,CAAC3B,QAAQ,CAACrB,KAAK,EAAE4C,SAAS,CAAC5C,KAAK,CAAC,EACzD;MAAA,yBAAAG,MAAA,CAAwBkB,QAAQ,CAACrB,KAAK,UAAAG,MAAA,CAClCyC,SAAS,CAAC5C,KAAK;IAAA,CAAa,CAAC;IACrC,IAAMiD,cAAc,GAAGnD,OAAO,CAAC;MAC7BsC,MAAM,EAAE;QAAC5B,CAAC,EAAEC;MAAM,CAAC;MACnBE,OAAO,EAAPA,OAAO;MACP0B,KAAK,EAAE;QAACrC,KAAK,EAAE,CAAC,CAAC,EAAEU,QAAQ,CAACgB,UAAU,EAAEhB,QAAQ,CAACmB,WAAW;MAAC;KAC9D,CAAC;IACFK,aAAa,CAACI,IAAI,CAACW,cAAc,CAAC;IAClC,IAAMC,aAAa,GAAGvD,eAAe,CAAC;MACpCwD,CAAC,EAAEP,SAAS;MACZQ,CAAC,EAAEH,cAAc;MACjBtC,OAAO,EAAPA,OAAO;MACPoB,UAAU,EAAVA,UAAU;MACVC,UAAU,EAAVA,UAAU;MACVnB,IAAI,EAAJA,IAAI;MACJM,UAAU,EAAVA,UAAU;MACVJ,sBAAsB,EAAtBA,sBAAsB;MACtBE,cAAc,EAAdA;KACD,CAAC;IAEF,IAAMoC,oBAAoB,GAAG1C,OAAO,CAACW,OAAO,CAACC,GAAG,CAAC2B,aAAa,CAAC1B,MAAM,CAAC;IACtElC,IAAI,CAACyD,MAAM,CACPM,oBAAoB,CAACZ,QAAQ,EAC7B;MAAA,OAAM,6CAA6C;IAAA,EAAC;IACxD;IACApB,QAAQ,CAACrB,KAAK,GAAG8C,qBAAqB;IACtC;IACA;IACAO,oBAAoB,CAACrD,KAAK,GAAGU,QAAQ,CAAC4C,QAAQ;IAE9CrB,GAAG,GAAGpC,QAAQ,CAAC;MAACuC,MAAM,EAAE;QAAC5B,CAAC,EAAE0C;MAAa,CAAC;MAAEvC,OAAO,EAAPA;IAAO,CAAC,CAAC;IACrDsB,GAAG,CAACjC,KAAK,GAAGU,QAAQ,CAAC4C,QAAQ;IAE7BpB,aAAa,CAACI,IAAI,CAACY,aAAa,CAAC;GAClC,MAAM;IACL,IAAMK,OAAO,GAAG7C,QAAQ,CAAC8C,SAAS,GAAG9C,QAAQ,CAAC+C,QAAQ;IACtD,IAAMb,UAAS,GAAG9C,OAAO,CAAC;MACxBsC,MAAM,EAAE;QAAC5B,CAAC,EAADA;MAAC,CAAC;MACXG,OAAO,EAAPA,OAAO;MACP0B,KAAK,EAAE;QACLrC,KAAK,EAAEC,cAAc,GACjB,CAACS,QAAQ,CAACgD,SAAS,EAAEH,OAAO,EAAE7C,QAAQ,CAACgB,UAAU,CAAC,GAClD,CAAChB,QAAQ,CAACgD,SAAS,EAAEhD,QAAQ,CAACgB,UAAU,EAAE6B,OAAO;;KAExD,CAAC;IACF,IAAMN,eAAc,GAAGnD,OAAO,CAAC;MAC7BsC,MAAM,EAAE;QAAC5B,CAAC,EAAEC;MAAM,CAAC;MACnBE,OAAO,EAAPA,OAAO;MACP0B,KAAK,EAAE;QAACrC,KAAK,EAAE,CAAC,CAAC,EAAEU,QAAQ,CAACgB,UAAU,EAAEhB,QAAQ,CAACmB,WAAW;MAAC;KAC9D,CAAC;IACF,IAAM8B,MAAM,GAAGhE,eAAe,CAAC;MAC7BwD,CAAC,EAAElD,cAAc,GAAG2C,UAAS,GAAGK,eAAc;MAC9CG,CAAC,EAAEnD,cAAc,GAAGgD,eAAc,GAAGL,UAAS;MAC9Cb,UAAU,EAAE,CAAC9B,cAAc;MAC3B+B,UAAU,EAAVA,UAAU;MACVrB,OAAO,EAAPA,OAAO;MACPE,IAAI,EAAJA,IAAI;MACJM,UAAU,EAAVA,UAAU;MACVJ,sBAAsB,EAAtBA,sBAAsB;MACtBE,cAAc,EAAdA;KACD,CAAC;IAEFgB,GAAG,GAAGnC,OAAO,CACT;MAACsC,MAAM,EAAE;QAAC5B,CAAC,EAAEmD;MAAM,CAAC;MAAEhD,OAAO,EAAPA,OAAO;MAAE0B,KAAK,EAAE;QAACrC,KAAK,EAAEU,QAAQ,CAAC4C;MAAQ;IAAC,CAAC,CAAC;IAEtEpB,aAAa,CAACI,IAAI,CAACM,UAAS,CAAC;IAC7BV,aAAa,CAACI,IAAI,CAACW,eAAc,CAAC;IAClCf,aAAa,CAACI,IAAI,CAACqB,MAAM,CAAC;;EAG5B,SAAAC,EAAA,MAAAC,cAAA,GAAgB3B,aAAa,EAAA0B,EAAA,GAAAC,cAAA,CAAA3D,MAAA,EAAA0D,EAAA,IAAE;IAA1B,IAAME,CAAC,GAAAD,cAAA,CAAAD,EAAA;IACVjD,OAAO,CAACoD,6BAA6B,CAACD,CAAC,CAAC;;EAG1C,OAAO7B,GAAG;AACZ;AAEA;AACA;AACA,OAAM,SAAU+B,gBAAgBA,CAAAC,KAAA,EASjB;EAAA,IARbzD,CAAC,GAAAyD,KAAA,CAADzD,CAAC;IACDC,MAAM,GAAAwD,KAAA,CAANxD,MAAM;IACNC,QAAQ,GAAAuD,KAAA,CAARvD,QAAQ;IACRC,OAAO,GAAAsD,KAAA,CAAPtD,OAAO;IAAAuD,UAAA,GAAAD,KAAA,CACPpD,IAAI;IAAJA,IAAI,GAAAqD,UAAA,cAAG,IAAI,GAAAA,UAAA;IAAAC,qBAAA,GAAAF,KAAA,CACXlD,sBAAsB;IAAtBA,sBAAsB,GAAAoD,qBAAA,cAAG,IAAI,GAAAA,qBAAA;IAAAC,oBAAA,GAAAH,KAAA,CAC7BhD,cAAc;IAAdA,cAAc,GAAAmD,oBAAA,cAAG,CAAC,GAAAA,oBAAA;IAAAC,gBAAA,GAAAJ,KAAA,CAClB9C,UAAU;IAAVA,UAAU,GAAAkD,gBAAA,cAAG,IAAI,GAAAA,gBAAA;EAEjB;EACA;EACA;EACA;EACA;EACA;EACA,IACEC,WAAW,GAMT5D,QAAQ,CANV4D,WAAW;IACXC,YAAY,GAKV7D,QAAQ,CALV6D,YAAY;IACZ7C,UAAU,GAIRhB,QAAQ,CAJVgB,UAAU;IACV+B,QAAQ,GAGN/C,QAAQ,CAHV+C,QAAQ;IACRD,SAAS,GAEP9C,QAAQ,CAFV8C,SAAS;IACT1B,UAAU,GACRpB,QAAQ,CADVoB,UAAU;EAGZ,IAAM7B,cAAc,GAAG6B,UAAU,KAAK,cAAc;EAEpD,IAAM0C,SAAS,GAAGF,WAAW,GAAGC,YAAY,GAAG7C,UAAU;EACzD,IAAM6B,OAAO,GAAGC,SAAS,GAAGC,QAAQ;EACpC,IAAMgB,UAAU,GAAG,CAAC/D,QAAQ,CAACgD,SAAS,EAAEc,SAAS,EAAEjB,OAAO,CAAC;EAC3D,IAAMxB,UAAU,GAAG,IAAI;EACvB,IAAMC,UAAU,GAAG,KAAK;EAExB,IAAME,aAAa,GAAiB,EAAE;EAEtC,IAAInB,sBAAsB,IAAI,IAAI,EAAE;IAClC,IAAMoB,WAAW,GACbpC,sBAAsB,CAACgB,sBAAsB,CAACf,KAAK,EAAEC,cAAc,CAAC;IACxE,IAAIkC,WAAW,IAAI,IAAI,EAAE;MACvBpB,sBAAsB,GAAGjB,OAAO,CAAC;QAC/BsC,MAAM,EAAE;UAAC5B,CAAC,EAAEO;QAAsB,CAAC;QACnCJ,OAAO,EAAPA,OAAO;QACP0B,KAAK,EAAE;UAACrC,KAAK,EAAEmC;QAAW;OAC3B,CAAC;MACFD,aAAa,CAACI,IAAI,CAACvB,sBAAsB,CAAC;;;EAI9C,IAAIF,IAAI,IAAI,IAAI,EAAE;IAChB,IAAMsB,aAAW,GAAGpC,sBAAsB,CAACc,IAAI,CAACb,KAAK,EAAEC,cAAc,CAAC;IACtE,IAAIkC,aAAW,IAAI,IAAI,EAAE;MACvBtB,IAAI,GAAGf,OAAO,CAAC;QAACsC,MAAM,EAAE;UAAC5B,CAAC,EAAEK;QAAI,CAAC;QAAEF,OAAO,EAAPA,OAAO;QAAE0B,KAAK,EAAE;UAACrC,KAAK,EAAEmC;QAAW;MAAC,CAAC,CAAC;MACzED,aAAa,CAACI,IAAI,CAACzB,IAAI,CAAC;;;EAI5B,IAAM6D,KAAK,GAAG5E,OAAO,CAAC;IACpBsC,MAAM,EAAE;MAAC5B,CAAC,EAAEC;IAAM,CAAC;IACnBE,OAAO,EAAPA,OAAO;IACP0B,KAAK,EAAE;MAACrC,KAAK,EAAE,CAAC,CAAC,EAAEwE,SAAS,EAAElF,IAAI,CAACqF,aAAa,CAAClE,MAAM,CAACT,KAAK,CAAC,GAAGwE,SAAS;IAAC;GAC5E,CAAC;EACFtC,aAAa,CAACI,IAAI,CAACoC,KAAK,CAAC;EAEzB,IAAME,aAAa,GAAG,IAAIrF,mBAAmB,CAACkF,UAAU,EAAE/D,QAAQ,CAAC;EACnE,IAAMmE,YAAY,GAAG,CACnBrE,CAAC,CAACR,KAAK,EAAE,CAACU,QAAQ,CAACoE,OAAO,CAACC,GAAG,EAAErE,QAAQ,CAACoE,OAAO,CAACE,IAAI,CAAC,EACtD,CAACtE,QAAQ,CAACuE,YAAY,EAAEvE,QAAQ,CAACwE,WAAW,CAAC,EAC7C,CAACxE,QAAQ,CAACyE,cAAc,EAAEzE,QAAQ,CAAC0E,aAAa,CAAC,EAAE,CAAC1E,QAAQ,CAACgB,UAAU,CAAC,EACxE,CAAChB,QAAQ,CAAC4D,WAAW,GAAG5D,QAAQ,CAACgB,UAAU,CAAC,EAAE,CAAChB,QAAQ,CAAC+C,QAAQ,CAAC,CAClE;EACD,IAAM4B,MAAM,GACR1E,OAAO,CAAC2E,eAAe,CAACV,aAAa,EAAE,CAACpE,CAAC,CAAC,EAAE,SAAS,EAAEqE,YAAY,CAAC;EACxE,IAAMU,cAAc,GAChBzF,OAAO,CAAC;IAACsC,MAAM,EAAE;MAAC5B,CAAC,EAAE6E;IAAM,CAAC;IAAE1E,OAAO,EAAPA,OAAO;IAAE0B,KAAK,EAAE;MAACrC,KAAK,EAAEyE;IAAU;EAAC,CAAC,CAAC;EAEvEvC,aAAa,CAACI,IAAI,CAAC+C,MAAM,CAAC;EAC1BnD,aAAa,CAACI,IAAI,CAACiD,cAAc,CAAC;EAElC,IAAMC,OAAO,GAAG3E,IAAI,IAAI,IAAI;EAC5B,IAAM4E,yBAAyB,GAAG1E,sBAAsB,IAAI,IAAI;EAChE,IAAM2E,iBAAiB,GAAGvE,UAAU,KAAK,WAAW;EACpD,IAAMwE,eAAe,GACjBxE,UAAU,GAAG3B,4BAA4B,CAAC2B,UAAU,EAAE,IAAI,CAAC,GAAG,IAAI;EACtE,IAAMyE,aAAa,GAAG,IAAInG,mBAAmB,CACzCQ,cAAc,GAAGsF,cAAc,CAACvF,KAAiC,GAChD0E,KAAK,CAAC1E,KAAiC,EACxDC,cAAc,GAAGyE,KAAK,CAAC1E,KAAiC,GACvCuF,cAAc,CAACvF,KAAiC,EACjEC,cAAc,GAAG,CAACS,QAAQ,CAACgD,SAAS,EAAEH,OAAO,EAAE7C,QAAQ,CAACmB,WAAW,CAAC,GACnD,CAACnB,QAAQ,CAACgD,SAAS,EAAEhD,QAAQ,CAACmB,WAAW,EAAE0B,OAAO,CAAC,EACpExB,UAAU,EAAEC,UAAU,EAAEwD,OAAO,EAAEG,eAAe,EAChDF,yBAAyB,EAAEC,iBAAiB,CAAC;EACjD,IAAMtD,MAAM,GACRnC,cAAc,GAAG,CAACsF,cAAc,EAAEb,KAAK,CAAC,GAAG,CAACA,KAAK,EAAEa,cAAc,CAAC;EACtE,IAAI1E,IAAI,EAAE;IACRuB,MAAM,CAACE,IAAI,CAACzB,IAAI,CAAC;;EAEnB,IAAI4E,yBAAyB,EAAE;IAC7BrD,MAAM,CAACE,IAAI,CAACvB,sBAAsB,CAAC;;EAErC,IAAI2E,iBAAiB,EAAE;IACrB,IAAMG,eAAe,GAAGlF,OAAO,CAACmF,cAAc,CAC1C,EAAE,EAAE,SAAS,EACbxG,IAAI,CAACyG,iBAAiB,CAAC9E,cAAsC,EACtC,SAAS,CAAC,CAAC;IACtCmB,MAAM,CAACE,IAAI,CAACuD,eAAe,CAAC;IAC5B3D,aAAa,CAACI,IAAI,CAACuD,eAAe,CAAC;;EAErC,IAAMG,OAAO,GAAGrF,OAAO,CAAC2E,eAAe,CAACM,aAAa,EAAExD,MAAM,EAAE,SAAS,CAAC;EACzE,IAAMH,GAAG,GAAGnC,OAAO,CACf;IAACsC,MAAM,EAAE;MAAC5B,CAAC,EAAEwF;IAAO,CAAC;IAAErF,OAAO,EAAPA,OAAO;IAAE0B,KAAK,EAAE;MAACrC,KAAK,EAAEU,QAAQ,CAAC4C;IAAQ;EAAC,CAAC,CAAC;EAEvEpB,aAAa,CAACI,IAAI,CAAC0D,OAAO,CAAC;EAC3B,SAAAC,GAAA,MAAAC,eAAA,GAAgBhE,aAAa,EAAA+D,GAAA,GAAAC,eAAA,CAAAhG,MAAA,EAAA+F,GAAA,IAAE;IAA1B,IAAMnC,CAAC,GAAAoC,eAAA,CAAAD,GAAA;IACVtF,OAAO,CAACoD,6BAA6B,CAACD,CAAC,CAAC;;EAG1C,OAAO7B,GAAG;AACZ"},"metadata":{},"sourceType":"module","externalDependencies":[]}