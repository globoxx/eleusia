{"ast":null,"code":"import _slicedToArray from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/slicedToArray.js\";\n/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { customGrad } from '../../gradients';\nimport { _FusedMatMul } from '../../kernel_names';\nimport { makeTypesMatch } from '../../tensor_util';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { add } from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport { applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse } from '../fused_util';\nimport { matMul as unfusedMatMul } from '../mat_mul';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\n/**\n * Computes the dot product of two matrices with optional activation and bias.\n *\n * ```js\n * const a = tf.tensor2d([-1, -2], [1, 2]);\n * const b = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n * const bias = tf.tensor2d([1, 2], [1, 2]);\n *\n * tf.fused.matMul({a, b, bias, activation: 'relu'}).print();\n * ```\n *\n * @param obj An object with the following properties:\n * - `a` First matrix in dot product operation.\n * - `b` Second matrix in dot product operation.\n * - `transposeA` If true, `a` is transposed before multiplication.\n * - `transposeB` If true, `b` is transposed before multiplication.\n * - `bias` Matrix to be added to the result.\n * - `activation` Name of activation kernel (defaults to `linear`).\n * - `preluActivationWeights` Tensor of prelu weights.\n * - `leakyreluAlpha` Alpha of leakyrelu.\n */\nfunction fusedMatMul_(_ref) {\n  var a = _ref.a,\n    b = _ref.b,\n    _ref$transposeA = _ref.transposeA,\n    transposeA = _ref$transposeA === void 0 ? false : _ref$transposeA,\n    _ref$transposeB = _ref.transposeB,\n    transposeB = _ref$transposeB === void 0 ? false : _ref$transposeB,\n    bias = _ref.bias,\n    _ref$activation = _ref.activation,\n    activation = _ref$activation === void 0 ? 'linear' : _ref$activation,\n    preluActivationWeights = _ref.preluActivationWeights,\n    _ref$leakyreluAlpha = _ref.leakyreluAlpha,\n    leakyreluAlpha = _ref$leakyreluAlpha === void 0 ? 0.2 : _ref$leakyreluAlpha;\n  if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n    var result = unfusedMatMul(a, b, transposeA, transposeB);\n    if (bias != null) {\n      result = add(result, bias);\n    }\n    return applyActivation(result, activation, preluActivationWeights, leakyreluAlpha);\n  }\n  var $a = convertToTensor(a, 'a', 'fused matMul');\n  var $b = convertToTensor(b, 'b', 'fused matMul');\n  var _makeTypesMatch = makeTypesMatch($a, $b);\n  var _makeTypesMatch2 = _slicedToArray(_makeTypesMatch, 2);\n  $a = _makeTypesMatch2[0];\n  $b = _makeTypesMatch2[1];\n  var innerShapeA = transposeA ? $a.shape[$a.rank - 2] : $a.shape[$a.rank - 1];\n  var innerShapeB = transposeB ? $b.shape[$b.rank - 1] : $b.shape[$b.rank - 2];\n  var outerShapeA = transposeA ? $a.shape[$a.rank - 1] : $a.shape[$a.rank - 2];\n  var outerShapeB = transposeB ? $b.shape[$b.rank - 2] : $b.shape[$b.rank - 1];\n  var outerDimsA = $a.shape.slice(0, -2);\n  var outerDimsB = $b.shape.slice(0, -2);\n  var batchDimA = util.sizeFromShape(outerDimsA);\n  var batchDimB = util.sizeFromShape(outerDimsB);\n  util.assert(innerShapeA === innerShapeB, function () {\n    return \"Error in fused matMul: inner shapes (\".concat(innerShapeA, \") and (\") + \"\".concat(innerShapeB, \") of Tensors with shapes \").concat($a.shape, \" and \") + \"\".concat($b.shape, \" and transposeA=\").concat(transposeA) + \" and transposeB=\".concat(transposeB, \" must match.\");\n  });\n  var outShapeOuterDims = broadcast_util.assertAndGetBroadcastShape($a.shape.slice(0, -2), $b.shape.slice(0, -2));\n  var outShape = outShapeOuterDims.concat([outerShapeA, outerShapeB]);\n  var a3D = transposeA ? reshape($a, [batchDimA, innerShapeA, outerShapeA]) : reshape($a, [batchDimA, outerShapeA, innerShapeA]);\n  var b3D = transposeB ? reshape($b, [batchDimB, outerShapeB, innerShapeB]) : reshape($b, [batchDimB, innerShapeB, outerShapeB]);\n  var $bias;\n  if (bias != null) {\n    $bias = convertToTensor(bias, 'bias', 'fused matMul');\n    var _makeTypesMatch3 = makeTypesMatch($bias, $a);\n    var _makeTypesMatch4 = _slicedToArray(_makeTypesMatch3, 1);\n    $bias = _makeTypesMatch4[0];\n    broadcast_util.assertAndGetBroadcastShape(outShape, $bias.shape);\n  }\n  var $preluActivationWeights;\n  if (preluActivationWeights != null) {\n    $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused matMul');\n  }\n  var grad = function grad(dy, saved) {\n    var _saved = _slicedToArray(saved, 4),\n      a3D = _saved[0],\n      b3D = _saved[1],\n      y = _saved[2],\n      $bias = _saved[3];\n    // we reshape dy because the result of the forward is not\n    // necessarily going to be a 3d tensor due to a reshape done at the end of\n    // the customOp.\n    var dyActivation = getFusedDyActivation(reshape(dy, y.shape), y, activation);\n    var aDer;\n    var bDer;\n    if (!transposeA && !transposeB) {\n      aDer = unfusedMatMul(dyActivation, b3D, false, true);\n      bDer = unfusedMatMul(a3D, dyActivation, true, false);\n    } else if (!transposeA && transposeB) {\n      aDer = unfusedMatMul(dyActivation, b3D, false, false);\n      bDer = unfusedMatMul(dyActivation, a3D, true, false);\n    } else if (transposeA && !transposeB) {\n      aDer = unfusedMatMul(b3D, dyActivation, false, true);\n      bDer = unfusedMatMul(a3D, dyActivation, false, false);\n    } else {\n      aDer = unfusedMatMul(b3D, dyActivation, true, true);\n      bDer = unfusedMatMul(dyActivation, a3D, true, true);\n    }\n    if (bias != null) {\n      var biasDer = getFusedBiasGradient($bias, dyActivation);\n      return [aDer, bDer, biasDer];\n    } else {\n      return [aDer, bDer];\n    }\n  };\n  var inputs = {\n    a: a3D,\n    b: b3D,\n    bias: $bias,\n    preluActivationWeights: $preluActivationWeights\n  };\n  var attrs = {\n    transposeA: transposeA,\n    transposeB: transposeB,\n    activation: activation,\n    leakyreluAlpha: leakyreluAlpha\n  };\n  // Depending on the the params passed in we will have different number of\n  // inputs and thus a a different number of elements in the gradient.\n  if (bias == null) {\n    var customOp = customGrad(function (a3D, b3D, save) {\n      var res =\n      // tslint:disable-next-line: no-unnecessary-type-assertion\n      ENGINE.runKernel(_FusedMatMul, inputs, attrs);\n      save([a3D, b3D, res]);\n      return {\n        value: reshape(res, outShape),\n        gradFunc: grad\n      };\n    });\n    return customOp(a3D, b3D);\n  } else {\n    var customOpWithBias = customGrad(function (a3D, b3D, $bias, save) {\n      var res =\n      // tslint:disable-next-line: no-unnecessary-type-assertion\n      ENGINE.runKernel(_FusedMatMul, inputs, attrs);\n      save([a3D, b3D, res, $bias]);\n      return {\n        value: reshape(res, outShape),\n        gradFunc: grad\n      };\n    });\n    return customOpWithBias(a3D, b3D, $bias);\n  }\n}\nexport var matMul = /* @__PURE__ */op({\n  fusedMatMul_: fusedMatMul_\n});","map":{"version":3,"names":["ENGINE","customGrad","_FusedMatMul","makeTypesMatch","convertToTensor","util","add","broadcast_util","applyActivation","getFusedBiasGradient","getFusedDyActivation","shouldFuse","matMul","unfusedMatMul","op","reshape","fusedMatMul_","_ref","a","b","_ref$transposeA","transposeA","_ref$transposeB","transposeB","bias","_ref$activation","activation","preluActivationWeights","_ref$leakyreluAlpha","leakyreluAlpha","state","gradientDepth","result","$a","$b","_makeTypesMatch","_makeTypesMatch2","_slicedToArray","innerShapeA","shape","rank","innerShapeB","outerShapeA","outerShapeB","outerDimsA","slice","outerDimsB","batchDimA","sizeFromShape","batchDimB","assert","concat","outShapeOuterDims","assertAndGetBroadcastShape","outShape","a3D","b3D","$bias","_makeTypesMatch3","_makeTypesMatch4","$preluActivationWeights","grad","dy","saved","_saved","y","dyActivation","aDer","bDer","biasDer","inputs","attrs","customOp","save","res","runKernel","value","gradFunc","customOpWithBias"],"sources":["C:\\Users\\vince\\OneDrive\\Documents\\GitHub\\tfjs-core\\src\\ops\\fused\\mat_mul.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from '../../engine';\nimport {customGrad} from '../../gradients';\nimport {_FusedMatMul, _FusedMatMulAttrs, _FusedMatMulInputs} from '../../kernel_names';\nimport {NamedAttrMap} from '../../kernel_registry';\nimport {Tensor, Tensor3D} from '../../tensor';\nimport {GradSaveFunc, NamedTensorMap} from '../../tensor_types';\nimport {makeTypesMatch} from '../../tensor_util';\nimport {convertToTensor} from '../../tensor_util_env';\nimport {TensorLike} from '../../types';\nimport * as util from '../../util';\n\nimport {add} from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport {Activation} from '../fused_types';\nimport {applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse} from '../fused_util';\nimport {matMul as unfusedMatMul} from '../mat_mul';\nimport {op} from '../operation';\nimport {reshape} from '../reshape';\n\n/**\n * Computes the dot product of two matrices with optional activation and bias.\n *\n * ```js\n * const a = tf.tensor2d([-1, -2], [1, 2]);\n * const b = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n * const bias = tf.tensor2d([1, 2], [1, 2]);\n *\n * tf.fused.matMul({a, b, bias, activation: 'relu'}).print();\n * ```\n *\n * @param obj An object with the following properties:\n * - `a` First matrix in dot product operation.\n * - `b` Second matrix in dot product operation.\n * - `transposeA` If true, `a` is transposed before multiplication.\n * - `transposeB` If true, `b` is transposed before multiplication.\n * - `bias` Matrix to be added to the result.\n * - `activation` Name of activation kernel (defaults to `linear`).\n * - `preluActivationWeights` Tensor of prelu weights.\n * - `leakyreluAlpha` Alpha of leakyrelu.\n */\nfunction fusedMatMul_({\n  a,\n  b,\n  transposeA = false,\n  transposeB = false,\n  bias,\n  activation = 'linear',\n  preluActivationWeights,\n  leakyreluAlpha = 0.2,\n}: {\n  a: Tensor|TensorLike,\n  b: Tensor|TensorLike,\n  transposeA?: boolean,\n  transposeB?: boolean,\n  bias?: Tensor|TensorLike,\n  activation?: Activation,\n  preluActivationWeights?: Tensor\n  leakyreluAlpha?: number\n}): Tensor {\n    if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n      let result = unfusedMatMul(a, b, transposeA, transposeB);\n      if (bias != null) {\n        result = add(result, bias);\n      }\n\n      return applyActivation(\n                 result, activation, preluActivationWeights, leakyreluAlpha);\n    }\n\n    let $a = convertToTensor(a, 'a', 'fused matMul');\n    let $b = convertToTensor(b, 'b', 'fused matMul');\n    [$a, $b] = makeTypesMatch($a, $b);\n\n    const innerShapeA =\n        transposeA ? $a.shape[$a.rank - 2] : $a.shape[$a.rank - 1];\n    const innerShapeB =\n        transposeB ? $b.shape[$b.rank - 1] : $b.shape[$b.rank - 2];\n\n    const outerShapeA =\n        transposeA ? $a.shape[$a.rank - 1] : $a.shape[$a.rank - 2];\n    const outerShapeB =\n        transposeB ? $b.shape[$b.rank - 2] : $b.shape[$b.rank - 1];\n\n    const outerDimsA = $a.shape.slice(0, -2);\n    const outerDimsB = $b.shape.slice(0, -2);\n    const batchDimA = util.sizeFromShape(outerDimsA);\n    const batchDimB = util.sizeFromShape(outerDimsB);\n\n    util.assert(\n        innerShapeA === innerShapeB,\n        () => `Error in fused matMul: inner shapes (${innerShapeA}) and (` +\n            `${innerShapeB}) of Tensors with shapes ${$a.shape} and ` +\n            `${$b.shape} and transposeA=${transposeA}` +\n            ` and transposeB=${transposeB} must match.`);\n\n    const outShapeOuterDims = broadcast_util.assertAndGetBroadcastShape(\n        $a.shape.slice(0, -2), $b.shape.slice(0, -2));\n    const outShape = outShapeOuterDims.concat([outerShapeA, outerShapeB]);\n\n    const a3D: Tensor3D = transposeA ?\n        reshape($a, [batchDimA, innerShapeA, outerShapeA]) :\n        reshape($a, [batchDimA, outerShapeA, innerShapeA]);\n    const b3D: Tensor3D = transposeB ?\n        reshape($b, [batchDimB, outerShapeB, innerShapeB]) :\n        reshape($b, [batchDimB, innerShapeB, outerShapeB]);\n\n    let $bias: Tensor;\n    if (bias != null) {\n      $bias = convertToTensor(bias, 'bias', 'fused matMul');\n      [$bias] = makeTypesMatch($bias, $a);\n\n      broadcast_util.assertAndGetBroadcastShape(outShape, $bias.shape);\n    }\n\n    let $preluActivationWeights: Tensor;\n    if (preluActivationWeights != null) {\n      $preluActivationWeights = convertToTensor(\n          preluActivationWeights, 'prelu weights', 'fused matMul');\n    }\n\n    const grad = (dy: Tensor3D, saved: Tensor[]) => {\n      const [a3D, b3D, y, $bias] = saved;\n      // we reshape dy because the result of the forward is not\n      // necessarily going to be a 3d tensor due to a reshape done at the end of\n      // the customOp.\n      const dyActivation =\n          getFusedDyActivation(reshape(dy, y.shape), y, activation);\n      let aDer: Tensor;\n      let bDer: Tensor;\n\n      if (!transposeA && !transposeB) {\n        aDer = unfusedMatMul(dyActivation, b3D, false, true);\n        bDer = unfusedMatMul(a3D, dyActivation, true, false);\n      } else if (!transposeA && transposeB) {\n        aDer = unfusedMatMul(dyActivation, b3D, false, false);\n        bDer = unfusedMatMul(dyActivation, a3D, true, false);\n      } else if (transposeA && !transposeB) {\n        aDer = unfusedMatMul(b3D, dyActivation, false, true);\n        bDer = unfusedMatMul(a3D, dyActivation, false, false);\n      } else {\n        aDer = unfusedMatMul(b3D, dyActivation, true, true);\n        bDer = unfusedMatMul(dyActivation, a3D, true, true);\n      }\n\n      if (bias != null) {\n        const biasDer = getFusedBiasGradient($bias, dyActivation);\n        return [aDer, bDer, biasDer];\n      } else {\n        return [aDer, bDer];\n      }\n    };\n\n    const inputs: _FusedMatMulInputs = {\n      a: a3D,\n      b: b3D,\n      bias: $bias,\n      preluActivationWeights: $preluActivationWeights\n    };\n    const attrs: _FusedMatMulAttrs =\n        {transposeA, transposeB, activation, leakyreluAlpha};\n\n    // Depending on the the params passed in we will have different number of\n    // inputs and thus a a different number of elements in the gradient.\n    if (bias == null) {\n      const customOp =\n          customGrad((a3D: Tensor3D, b3D: Tensor3D, save: GradSaveFunc) => {\n            const res =\n                // tslint:disable-next-line: no-unnecessary-type-assertion\n                ENGINE.runKernel(\n                    _FusedMatMul, inputs as unknown as NamedTensorMap,\n                    attrs as unknown as NamedAttrMap) as Tensor;\n\n            save([a3D, b3D, res]);\n\n            return {value: reshape(res, outShape), gradFunc: grad};\n          });\n      return customOp(a3D, b3D);\n    } else {\n      const customOpWithBias = customGrad(\n          (a3D: Tensor3D, b3D: Tensor3D, $bias: Tensor, save: GradSaveFunc) => {\n            const res =\n                // tslint:disable-next-line: no-unnecessary-type-assertion\n                ENGINE.runKernel(\n                    _FusedMatMul, inputs as unknown as NamedTensorMap,\n                    attrs as unknown as NamedAttrMap) as Tensor;\n\n            save([a3D, b3D, res, $bias]);\n\n            return {value: reshape(res, outShape), gradFunc: grad};\n          });\n\n      return customOpWithBias(a3D, b3D, $bias);\n    }\n  }\n\n  export const matMul = /* @__PURE__ */ op({fusedMatMul_});\n"],"mappings":";AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQA,MAAM,QAAO,cAAc;AACnC,SAAQC,UAAU,QAAO,iBAAiB;AAC1C,SAAQC,YAAY,QAA8C,oBAAoB;AAItF,SAAQC,cAAc,QAAO,mBAAmB;AAChD,SAAQC,eAAe,QAAO,uBAAuB;AAErD,OAAO,KAAKC,IAAI,MAAM,YAAY;AAElC,SAAQC,GAAG,QAAO,QAAQ;AAC1B,OAAO,KAAKC,cAAc,MAAM,mBAAmB;AAEnD,SAAQC,eAAe,EAAEC,oBAAoB,EAAEC,oBAAoB,EAAEC,UAAU,QAAO,eAAe;AACrG,SAAQC,MAAM,IAAIC,aAAa,QAAO,YAAY;AAClD,SAAQC,EAAE,QAAO,cAAc;AAC/B,SAAQC,OAAO,QAAO,YAAY;AAElC;;;;;;;;;;;;;;;;;;;;;AAqBA,SAASC,YAAYA,CAAAC,IAAA,EAkBpB;EAAA,IAjBCC,CAAC,GAAAD,IAAA,CAADC,CAAC;IACDC,CAAC,GAAAF,IAAA,CAADE,CAAC;IAAAC,eAAA,GAAAH,IAAA,CACDI,UAAU;IAAVA,UAAU,GAAAD,eAAA,cAAG,KAAK,GAAAA,eAAA;IAAAE,eAAA,GAAAL,IAAA,CAClBM,UAAU;IAAVA,UAAU,GAAAD,eAAA,cAAG,KAAK,GAAAA,eAAA;IAClBE,IAAI,GAAAP,IAAA,CAAJO,IAAI;IAAAC,eAAA,GAAAR,IAAA,CACJS,UAAU;IAAVA,UAAU,GAAAD,eAAA,cAAG,QAAQ,GAAAA,eAAA;IACrBE,sBAAsB,GAAAV,IAAA,CAAtBU,sBAAsB;IAAAC,mBAAA,GAAAX,IAAA,CACtBY,cAAc;IAAdA,cAAc,GAAAD,mBAAA,cAAG,GAAG,GAAAA,mBAAA;EAWlB,IAAIjB,UAAU,CAACX,MAAM,CAAC8B,KAAK,CAACC,aAAa,EAAEL,UAAU,CAAC,KAAK,KAAK,EAAE;IAChE,IAAIM,MAAM,GAAGnB,aAAa,CAACK,CAAC,EAAEC,CAAC,EAAEE,UAAU,EAAEE,UAAU,CAAC;IACxD,IAAIC,IAAI,IAAI,IAAI,EAAE;MAChBQ,MAAM,GAAG1B,GAAG,CAAC0B,MAAM,EAAER,IAAI,CAAC;;IAG5B,OAAOhB,eAAe,CACXwB,MAAM,EAAEN,UAAU,EAAEC,sBAAsB,EAAEE,cAAc,CAAC;;EAGxE,IAAII,EAAE,GAAG7B,eAAe,CAACc,CAAC,EAAE,GAAG,EAAE,cAAc,CAAC;EAChD,IAAIgB,EAAE,GAAG9B,eAAe,CAACe,CAAC,EAAE,GAAG,EAAE,cAAc,CAAC;EAAC,IAAAgB,eAAA,GACtChC,cAAc,CAAC8B,EAAE,EAAEC,EAAE,CAAC;EAAA,IAAAE,gBAAA,GAAAC,cAAA,CAAAF,eAAA;EAAhCF,EAAE,GAAAG,gBAAA;EAAEF,EAAE,GAAAE,gBAAA;EAEP,IAAME,WAAW,GACbjB,UAAU,GAAGY,EAAE,CAACM,KAAK,CAACN,EAAE,CAACO,IAAI,GAAG,CAAC,CAAC,GAAGP,EAAE,CAACM,KAAK,CAACN,EAAE,CAACO,IAAI,GAAG,CAAC,CAAC;EAC9D,IAAMC,WAAW,GACblB,UAAU,GAAGW,EAAE,CAACK,KAAK,CAACL,EAAE,CAACM,IAAI,GAAG,CAAC,CAAC,GAAGN,EAAE,CAACK,KAAK,CAACL,EAAE,CAACM,IAAI,GAAG,CAAC,CAAC;EAE9D,IAAME,WAAW,GACbrB,UAAU,GAAGY,EAAE,CAACM,KAAK,CAACN,EAAE,CAACO,IAAI,GAAG,CAAC,CAAC,GAAGP,EAAE,CAACM,KAAK,CAACN,EAAE,CAACO,IAAI,GAAG,CAAC,CAAC;EAC9D,IAAMG,WAAW,GACbpB,UAAU,GAAGW,EAAE,CAACK,KAAK,CAACL,EAAE,CAACM,IAAI,GAAG,CAAC,CAAC,GAAGN,EAAE,CAACK,KAAK,CAACL,EAAE,CAACM,IAAI,GAAG,CAAC,CAAC;EAE9D,IAAMI,UAAU,GAAGX,EAAE,CAACM,KAAK,CAACM,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;EACxC,IAAMC,UAAU,GAAGZ,EAAE,CAACK,KAAK,CAACM,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;EACxC,IAAME,SAAS,GAAG1C,IAAI,CAAC2C,aAAa,CAACJ,UAAU,CAAC;EAChD,IAAMK,SAAS,GAAG5C,IAAI,CAAC2C,aAAa,CAACF,UAAU,CAAC;EAEhDzC,IAAI,CAAC6C,MAAM,CACPZ,WAAW,KAAKG,WAAW,EAC3B;IAAA,OAAM,wCAAAU,MAAA,CAAwCb,WAAW,kBAAAa,MAAA,CAClDV,WAAW,+BAAAU,MAAA,CAA4BlB,EAAE,CAACM,KAAK,UAAO,MAAAY,MAAA,CACtDjB,EAAE,CAACK,KAAK,sBAAAY,MAAA,CAAmB9B,UAAU,CAAE,sBAAA8B,MAAA,CACvB5B,UAAU,iBAAc;EAAA,EAAC;EAEpD,IAAM6B,iBAAiB,GAAG7C,cAAc,CAAC8C,0BAA0B,CAC/DpB,EAAE,CAACM,KAAK,CAACM,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAEX,EAAE,CAACK,KAAK,CAACM,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;EACjD,IAAMS,QAAQ,GAAGF,iBAAiB,CAACD,MAAM,CAAC,CAACT,WAAW,EAAEC,WAAW,CAAC,CAAC;EAErE,IAAMY,GAAG,GAAalC,UAAU,GAC5BN,OAAO,CAACkB,EAAE,EAAE,CAACc,SAAS,EAAET,WAAW,EAAEI,WAAW,CAAC,CAAC,GAClD3B,OAAO,CAACkB,EAAE,EAAE,CAACc,SAAS,EAAEL,WAAW,EAAEJ,WAAW,CAAC,CAAC;EACtD,IAAMkB,GAAG,GAAajC,UAAU,GAC5BR,OAAO,CAACmB,EAAE,EAAE,CAACe,SAAS,EAAEN,WAAW,EAAEF,WAAW,CAAC,CAAC,GAClD1B,OAAO,CAACmB,EAAE,EAAE,CAACe,SAAS,EAAER,WAAW,EAAEE,WAAW,CAAC,CAAC;EAEtD,IAAIc,KAAa;EACjB,IAAIjC,IAAI,IAAI,IAAI,EAAE;IAChBiC,KAAK,GAAGrD,eAAe,CAACoB,IAAI,EAAE,MAAM,EAAE,cAAc,CAAC;IAAC,IAAAkC,gBAAA,GAC5CvD,cAAc,CAACsD,KAAK,EAAExB,EAAE,CAAC;IAAA,IAAA0B,gBAAA,GAAAtB,cAAA,CAAAqB,gBAAA;IAAlCD,KAAK,GAAAE,gBAAA;IAENpD,cAAc,CAAC8C,0BAA0B,CAACC,QAAQ,EAAEG,KAAK,CAAClB,KAAK,CAAC;;EAGlE,IAAIqB,uBAA+B;EACnC,IAAIjC,sBAAsB,IAAI,IAAI,EAAE;IAClCiC,uBAAuB,GAAGxD,eAAe,CACrCuB,sBAAsB,EAAE,eAAe,EAAE,cAAc,CAAC;;EAG9D,IAAMkC,IAAI,GAAG,SAAPA,IAAIA,CAAIC,EAAY,EAAEC,KAAe,EAAI;IAC7C,IAAAC,MAAA,GAAA3B,cAAA,CAA6B0B,KAAK;MAA3BR,GAAG,GAAAS,MAAA;MAAER,GAAG,GAAAQ,MAAA;MAAEC,CAAC,GAAAD,MAAA;MAAEP,KAAK,GAAAO,MAAA;IACzB;IACA;IACA;IACA,IAAME,YAAY,GACdxD,oBAAoB,CAACK,OAAO,CAAC+C,EAAE,EAAEG,CAAC,CAAC1B,KAAK,CAAC,EAAE0B,CAAC,EAAEvC,UAAU,CAAC;IAC7D,IAAIyC,IAAY;IAChB,IAAIC,IAAY;IAEhB,IAAI,CAAC/C,UAAU,IAAI,CAACE,UAAU,EAAE;MAC9B4C,IAAI,GAAGtD,aAAa,CAACqD,YAAY,EAAEV,GAAG,EAAE,KAAK,EAAE,IAAI,CAAC;MACpDY,IAAI,GAAGvD,aAAa,CAAC0C,GAAG,EAAEW,YAAY,EAAE,IAAI,EAAE,KAAK,CAAC;KACrD,MAAM,IAAI,CAAC7C,UAAU,IAAIE,UAAU,EAAE;MACpC4C,IAAI,GAAGtD,aAAa,CAACqD,YAAY,EAAEV,GAAG,EAAE,KAAK,EAAE,KAAK,CAAC;MACrDY,IAAI,GAAGvD,aAAa,CAACqD,YAAY,EAAEX,GAAG,EAAE,IAAI,EAAE,KAAK,CAAC;KACrD,MAAM,IAAIlC,UAAU,IAAI,CAACE,UAAU,EAAE;MACpC4C,IAAI,GAAGtD,aAAa,CAAC2C,GAAG,EAAEU,YAAY,EAAE,KAAK,EAAE,IAAI,CAAC;MACpDE,IAAI,GAAGvD,aAAa,CAAC0C,GAAG,EAAEW,YAAY,EAAE,KAAK,EAAE,KAAK,CAAC;KACtD,MAAM;MACLC,IAAI,GAAGtD,aAAa,CAAC2C,GAAG,EAAEU,YAAY,EAAE,IAAI,EAAE,IAAI,CAAC;MACnDE,IAAI,GAAGvD,aAAa,CAACqD,YAAY,EAAEX,GAAG,EAAE,IAAI,EAAE,IAAI,CAAC;;IAGrD,IAAI/B,IAAI,IAAI,IAAI,EAAE;MAChB,IAAM6C,OAAO,GAAG5D,oBAAoB,CAACgD,KAAK,EAAES,YAAY,CAAC;MACzD,OAAO,CAACC,IAAI,EAAEC,IAAI,EAAEC,OAAO,CAAC;KAC7B,MAAM;MACL,OAAO,CAACF,IAAI,EAAEC,IAAI,CAAC;;EAEvB,CAAC;EAED,IAAME,MAAM,GAAuB;IACjCpD,CAAC,EAAEqC,GAAG;IACNpC,CAAC,EAAEqC,GAAG;IACNhC,IAAI,EAAEiC,KAAK;IACX9B,sBAAsB,EAAEiC;GACzB;EACD,IAAMW,KAAK,GACP;IAAClD,UAAU,EAAVA,UAAU;IAAEE,UAAU,EAAVA,UAAU;IAAEG,UAAU,EAAVA,UAAU;IAAEG,cAAc,EAAdA;EAAc,CAAC;EAExD;EACA;EACA,IAAIL,IAAI,IAAI,IAAI,EAAE;IAChB,IAAMgD,QAAQ,GACVvE,UAAU,CAAC,UAACsD,GAAa,EAAEC,GAAa,EAAEiB,IAAkB,EAAI;MAC9D,IAAMC,GAAG;MACL;MACA1E,MAAM,CAAC2E,SAAS,CACZzE,YAAY,EAAEoE,MAAmC,EACjDC,KAAgC,CAAW;MAEnDE,IAAI,CAAC,CAAClB,GAAG,EAAEC,GAAG,EAAEkB,GAAG,CAAC,CAAC;MAErB,OAAO;QAACE,KAAK,EAAE7D,OAAO,CAAC2D,GAAG,EAAEpB,QAAQ,CAAC;QAAEuB,QAAQ,EAAEhB;MAAI,CAAC;IACxD,CAAC,CAAC;IACN,OAAOW,QAAQ,CAACjB,GAAG,EAAEC,GAAG,CAAC;GAC1B,MAAM;IACL,IAAMsB,gBAAgB,GAAG7E,UAAU,CAC/B,UAACsD,GAAa,EAAEC,GAAa,EAAEC,KAAa,EAAEgB,IAAkB,EAAI;MAClE,IAAMC,GAAG;MACL;MACA1E,MAAM,CAAC2E,SAAS,CACZzE,YAAY,EAAEoE,MAAmC,EACjDC,KAAgC,CAAW;MAEnDE,IAAI,CAAC,CAAClB,GAAG,EAAEC,GAAG,EAAEkB,GAAG,EAAEjB,KAAK,CAAC,CAAC;MAE5B,OAAO;QAACmB,KAAK,EAAE7D,OAAO,CAAC2D,GAAG,EAAEpB,QAAQ,CAAC;QAAEuB,QAAQ,EAAEhB;MAAI,CAAC;IACxD,CAAC,CAAC;IAEN,OAAOiB,gBAAgB,CAACvB,GAAG,EAAEC,GAAG,EAAEC,KAAK,CAAC;;AAE5C;AAEA,OAAO,IAAM7C,MAAM,GAAG,eAAgBE,EAAE,CAAC;EAACE,YAAY,EAAZA;AAAY,CAAC,CAAC"},"metadata":{},"sourceType":"module","externalDependencies":[]}