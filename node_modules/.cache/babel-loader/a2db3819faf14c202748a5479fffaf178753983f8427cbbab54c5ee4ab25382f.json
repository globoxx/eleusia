{"ast":null,"code":"import _slicedToArray from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/slicedToArray.js\";\nimport _toConsumableArray from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/toConsumableArray.js\";\nimport _createForOfIteratorHelper from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/createForOfIteratorHelper.js\";\nimport _classCallCheck from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/classCallCheck.js\";\nimport _createClass from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/createClass.js\";\nimport _get from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/get.js\";\nimport _getPrototypeOf from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/getPrototypeOf.js\";\nimport _inherits from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/inherits.js\";\nimport _createSuper from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/createSuper.js\";\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * TensorFlow.js Layers: Recurrent Neural Network Layers.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy, util } from '@tensorflow/tfjs-core';\nimport { getActivation, serializeActivation } from '../activations';\nimport * as K from '../backend/tfjs_backend';\nimport { nameScope } from '../common';\nimport { getConstraint, serializeConstraint } from '../constraints';\nimport { InputSpec, SymbolicTensor } from '../engine/topology';\nimport { Layer } from '../engine/topology';\nimport { AttributeError, NotImplementedError, ValueError } from '../errors';\nimport { getInitializer, Initializer, Ones, serializeInitializer } from '../initializers';\nimport { getRegularizer, serializeRegularizer } from '../regularizers';\nimport { assertPositiveInteger } from '../utils/generic_utils';\nimport * as math_utils from '../utils/math_utils';\nimport { getExactlyOneShape, getExactlyOneTensor, isArrayOfShapes } from '../utils/types_utils';\nimport { batchGetValue, batchSetValue } from '../variables';\nimport { deserialize } from './serialization';\n/**\n * Standardize `apply()` args to a single list of tensor inputs.\n *\n * When running a model loaded from file, the input tensors `initialState` and\n * `constants` are passed to `RNN.apply()` as part of `inputs` instead of the\n * dedicated kwargs fields. `inputs` consists of\n * `[inputs, initialState0, initialState1, ..., constant0, constant1]` in this\n * case.\n * This method makes sure that arguments are\n * separated and that `initialState` and `constants` are `Array`s of tensors\n * (or None).\n *\n * @param inputs Tensor or `Array` of  tensors.\n * @param initialState Tensor or `Array` of tensors or `null`/`undefined`.\n * @param constants Tensor or `Array` of tensors or `null`/`undefined`.\n * @returns An object consisting of\n *   inputs: A tensor.\n *   initialState: `Array` of tensors or `null`.\n *   constants: `Array` of tensors or `null`.\n * @throws ValueError, if `inputs` is an `Array` but either `initialState` or\n *   `constants` is provided.\n */\nexport function standardizeArgs(inputs, initialState, constants, numConstants) {\n  if (Array.isArray(inputs)) {\n    if (initialState != null || constants != null) {\n      throw new ValueError('When inputs is an array, neither initialState or constants ' + 'should be provided');\n    }\n    if (numConstants != null) {\n      constants = inputs.slice(inputs.length - numConstants, inputs.length);\n      inputs = inputs.slice(0, inputs.length - numConstants);\n    }\n    if (inputs.length > 1) {\n      initialState = inputs.slice(1, inputs.length);\n    }\n    inputs = inputs[0];\n  }\n  function toListOrNull(x) {\n    if (x == null || Array.isArray(x)) {\n      return x;\n    } else {\n      return [x];\n    }\n  }\n  initialState = toListOrNull(initialState);\n  constants = toListOrNull(constants);\n  return {\n    inputs: inputs,\n    initialState: initialState,\n    constants: constants\n  };\n}\n/**\n * Iterates over the time dimension of a tensor.\n *\n * @param stepFunction RNN step function.\n *   Parameters:\n *     inputs: tensor with shape `[samples, ...]` (no time dimension),\n *       representing input for the batch of samples at a certain time step.\n *     states: an Array of tensors.\n *   Returns:\n *     outputs: tensor with shape `[samples, outputDim]` (no time dimension).\n *     newStates: list of tensors, same length and shapes as `states`. The first\n *       state in the list must be the output tensor at the previous timestep.\n * @param inputs Tensor of temporal data of shape `[samples, time, ...]` (at\n *   least 3D).\n * @param initialStates Tensor with shape `[samples, outputDim]` (no time\n *   dimension), containing the initial values of the states used in the step\n *   function.\n * @param goBackwards If `true`, do the iteration over the time dimension in\n *   reverse order and return the reversed sequence.\n * @param mask Binary tensor with shape `[sample, time, 1]`, with a zero for\n *   every element that is masked.\n * @param constants An Array of constant values passed at each step.\n * @param unroll Whether to unroll the RNN or to use a symbolic loop. *Not*\n *   applicable to this imperative deeplearn.js backend. Its value is ignored.\n * @param needPerStepOutputs Whether the per-step outputs are to be\n *   concatenated into a single tensor and returned (as the second return\n *   value). Default: `false`. This arg is included so that the relatively\n *   expensive concatenation of the stepwise outputs can be omitted unless\n *   the stepwise outputs need to be kept (e.g., for an LSTM layer of which\n *   `returnSequence` is `true`.)\n * @returns An Array: `[lastOutput, outputs, newStates]`.\n *   lastOutput: the lastest output of the RNN, of shape `[samples, ...]`.\n *   outputs: tensor with shape `[samples, time, ...]` where each entry\n *     `output[s, t]` is the output of the step function at time `t` for sample\n *     `s`. This return value is provided if and only if the\n *     `needPerStepOutputs` is set as `true`. If it is set as `false`, this\n *     return value will be `undefined`.\n *   newStates: Array of tensors, latest states returned by the step function,\n *      of shape `(samples, ...)`.\n * @throws ValueError If input dimension is less than 3.\n *\n * TODO(nielsene): This needs to be tidy-ed.\n */\nexport function rnn(stepFunction, inputs, initialStates) {\n  var goBackwards = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : false;\n  var mask = arguments.length > 4 ? arguments[4] : undefined;\n  var constants = arguments.length > 5 ? arguments[5] : undefined;\n  var unroll = arguments.length > 6 && arguments[6] !== undefined ? arguments[6] : false;\n  var needPerStepOutputs = arguments.length > 7 && arguments[7] !== undefined ? arguments[7] : false;\n  return tfc.tidy(function () {\n    var ndim = inputs.shape.length;\n    if (ndim < 3) {\n      throw new ValueError(\"Input should be at least 3D, but is \".concat(ndim, \"D.\"));\n    }\n    // Transpose to time-major, i.e., from [batch, time, ...] to [time, batch,\n    // ...].\n    var axes = [1, 0].concat(math_utils.range(2, ndim));\n    inputs = tfc.transpose(inputs, axes);\n    if (constants != null) {\n      throw new NotImplementedError('The rnn() functoin of the deeplearn.js backend does not support ' + 'constants yet.');\n    }\n    // Porting Note: the unroll option is ignored by the imperative backend.\n    if (unroll) {\n      console.warn('Backend rnn(): the unroll = true option is not applicable to the ' + 'imperative deeplearn.js backend.');\n    }\n    if (mask != null) {\n      mask = tfc.cast(tfc.cast(mask, 'bool'), 'float32');\n      if (mask.rank === ndim - 1) {\n        mask = tfc.expandDims(mask, -1);\n      }\n      mask = tfc.transpose(mask, axes);\n    }\n    if (goBackwards) {\n      inputs = tfc.reverse(inputs, 0);\n      if (mask != null) {\n        mask = tfc.reverse(mask, 0);\n      }\n    }\n    // Porting Note: PyKeras with TensorFlow backend uses a symbolic loop\n    //   (tf.while_loop). But for the imperative deeplearn.js backend, we just\n    //   use the usual TypeScript control flow to iterate over the time steps in\n    //   the inputs.\n    // Porting Note: PyKeras patches a \"_use_learning_phase\" attribute to\n    // outputs.\n    //   This is not idiomatic in TypeScript. The info regarding whether we are\n    //   in a learning (i.e., training) phase for RNN is passed in a different\n    //   way.\n    var perStepOutputs = [];\n    var lastOutput;\n    var states = initialStates;\n    var timeSteps = inputs.shape[0];\n    var perStepInputs = tfc.unstack(inputs);\n    var perStepMasks;\n    if (mask != null) {\n      perStepMasks = tfc.unstack(mask);\n    }\n    var _loop = function _loop(t) {\n      var currentInput = perStepInputs[t];\n      var stepOutputs = tfc.tidy(function () {\n        return stepFunction(currentInput, states);\n      });\n      if (mask == null) {\n        lastOutput = stepOutputs[0];\n        states = stepOutputs[1];\n      } else {\n        var maskedOutputs = tfc.tidy(function () {\n          var stepMask = perStepMasks[t];\n          var negStepMask = tfc.sub(tfc.onesLike(stepMask), stepMask);\n          // TODO(cais): Would tfc.where() be better for performance?\n          var output = tfc.add(tfc.mul(stepOutputs[0], stepMask), tfc.mul(states[0], negStepMask));\n          var newStates = states.map(function (state, i) {\n            return tfc.add(tfc.mul(stepOutputs[1][i], stepMask), tfc.mul(state, negStepMask));\n          });\n          return {\n            output: output,\n            newStates: newStates\n          };\n        });\n        lastOutput = maskedOutputs.output;\n        states = maskedOutputs.newStates;\n      }\n      if (needPerStepOutputs) {\n        perStepOutputs.push(lastOutput);\n      }\n    };\n    for (var t = 0; t < timeSteps; ++t) {\n      _loop(t);\n    }\n    var outputs;\n    if (needPerStepOutputs) {\n      var axis = 1;\n      outputs = tfc.stack(perStepOutputs, axis);\n    }\n    return [lastOutput, outputs, states];\n  });\n}\nexport var RNN = /*#__PURE__*/function (_Layer) {\n  _inherits(RNN, _Layer);\n  var _super = _createSuper(RNN);\n  function RNN(args) {\n    var _this;\n    _classCallCheck(this, RNN);\n    _this = _super.call(this, args);\n    var cell;\n    if (args.cell == null) {\n      throw new ValueError('cell property is missing for the constructor of RNN.');\n    } else if (Array.isArray(args.cell)) {\n      cell = new StackedRNNCells({\n        cells: args.cell\n      });\n    } else {\n      cell = args.cell;\n    }\n    if (cell.stateSize == null) {\n      throw new ValueError('The RNN cell should have an attribute `stateSize` (tuple of ' + 'integers, one integer per RNN state).');\n    }\n    _this.cell = cell;\n    _this.returnSequences = args.returnSequences == null ? false : args.returnSequences;\n    _this.returnState = args.returnState == null ? false : args.returnState;\n    _this.goBackwards = args.goBackwards == null ? false : args.goBackwards;\n    _this._stateful = args.stateful == null ? false : args.stateful;\n    _this.unroll = args.unroll == null ? false : args.unroll;\n    _this.supportsMasking = true;\n    _this.inputSpec = [new InputSpec({\n      ndim: 3\n    })];\n    _this.stateSpec = null;\n    _this.states_ = null;\n    // TODO(cais): Add constantsSpec and numConstants.\n    _this.numConstants = null;\n    // TODO(cais): Look into the use of initial_state in the kwargs of the\n    //   constructor.\n    _this.keptStates = [];\n    return _this;\n  }\n  // Porting Note: This is the equivalent of `RNN.states` property getter in\n  //   PyKeras.\n  _createClass(RNN, [{\n    key: \"getStates\",\n    value: function getStates() {\n      if (this.states_ == null) {\n        var numStates = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;\n        return math_utils.range(0, numStates).map(function (x) {\n          return null;\n        });\n      } else {\n        return this.states_;\n      }\n    }\n    // Porting Note: This is the equivalent of the `RNN.states` property setter in\n    //   PyKeras.\n  }, {\n    key: \"setStates\",\n    value: function setStates(states) {\n      this.states_ = states;\n    }\n  }, {\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      if (isArrayOfShapes(inputShape)) {\n        inputShape = inputShape[0];\n      }\n      inputShape = inputShape;\n      // TODO(cais): Remove the casting once stacked RNN cells become supported.\n      var stateSize = this.cell.stateSize;\n      if (!Array.isArray(stateSize)) {\n        stateSize = [stateSize];\n      }\n      var outputDim = stateSize[0];\n      var outputShape;\n      if (this.returnSequences) {\n        outputShape = [inputShape[0], inputShape[1], outputDim];\n      } else {\n        outputShape = [inputShape[0], outputDim];\n      }\n      if (this.returnState) {\n        var stateShape = [];\n        var _iterator = _createForOfIteratorHelper(stateSize),\n          _step;\n        try {\n          for (_iterator.s(); !(_step = _iterator.n()).done;) {\n            var dim = _step.value;\n            stateShape.push([inputShape[0], dim]);\n          }\n        } catch (err) {\n          _iterator.e(err);\n        } finally {\n          _iterator.f();\n        }\n        return [outputShape].concat(stateShape);\n      } else {\n        return outputShape;\n      }\n    }\n  }, {\n    key: \"computeMask\",\n    value: function computeMask(inputs, mask) {\n      var _this2 = this;\n      return tfc.tidy(function () {\n        if (Array.isArray(mask)) {\n          mask = mask[0];\n        }\n        var outputMask = _this2.returnSequences ? mask : null;\n        if (_this2.returnState) {\n          var stateMask = _this2.states.map(function (s) {\n            return null;\n          });\n          return [outputMask].concat(stateMask);\n        } else {\n          return outputMask;\n        }\n      });\n    }\n    /**\n     * Get the current state tensors of the RNN.\n     *\n     * If the state hasn't been set, return an array of `null`s of the correct\n     * length.\n     */\n  }, {\n    key: \"states\",\n    get: function get() {\n      if (this.states_ == null) {\n        var numStates = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;\n        var output = [];\n        for (var i = 0; i < numStates; ++i) {\n          output.push(null);\n        }\n        return output;\n      } else {\n        return this.states_;\n      }\n    },\n    set: function set(s) {\n      this.states_ = s;\n    }\n  }, {\n    key: \"build\",\n    value: function build(inputShape) {\n      // Note inputShape will be an Array of Shapes of initial states and\n      // constants if these are passed in apply().\n      var constantShape = null;\n      if (this.numConstants != null) {\n        throw new NotImplementedError('Constants support is not implemented in RNN yet.');\n      }\n      if (isArrayOfShapes(inputShape)) {\n        inputShape = inputShape[0];\n      }\n      inputShape = inputShape;\n      var batchSize = this.stateful ? inputShape[0] : null;\n      var inputDim = inputShape.slice(2);\n      this.inputSpec[0] = new InputSpec({\n        shape: [batchSize, null].concat(_toConsumableArray(inputDim))\n      });\n      // Allow cell (if RNNCell Layer) to build before we set or validate\n      // stateSpec.\n      var stepInputShape = [inputShape[0]].concat(inputShape.slice(2));\n      if (constantShape != null) {\n        throw new NotImplementedError('Constants support is not implemented in RNN yet.');\n      } else {\n        this.cell.build(stepInputShape);\n      }\n      // Set or validate stateSpec.\n      var stateSize;\n      if (Array.isArray(this.cell.stateSize)) {\n        stateSize = this.cell.stateSize;\n      } else {\n        stateSize = [this.cell.stateSize];\n      }\n      if (this.stateSpec != null) {\n        if (!util.arraysEqual(this.stateSpec.map(function (spec) {\n          return spec.shape[spec.shape.length - 1];\n        }), stateSize)) {\n          throw new ValueError(\"An initialState was passed that is not compatible with \" + \"cell.stateSize. Received stateSpec=\".concat(this.stateSpec, \"; \") + \"However cell.stateSize is \".concat(this.cell.stateSize));\n        }\n      } else {\n        this.stateSpec = stateSize.map(function (dim) {\n          return new InputSpec({\n            shape: [null, dim]\n          });\n        });\n      }\n      if (this.stateful) {\n        this.resetStates();\n      }\n    }\n    /**\n     * Reset the state tensors of the RNN.\n     *\n     * If the `states` argument is `undefined` or `null`, will set the\n     * state tensor(s) of the RNN to all-zero tensors of the appropriate\n     * shape(s).\n     *\n     * If `states` is provided, will set the state tensors of the RNN to its\n     * value.\n     *\n     * @param states Optional externally-provided initial states.\n     * @param training Whether this call is done during training. For stateful\n     *   RNNs, this affects whether the old states are kept or discarded. In\n     *   particular, if `training` is `true`, the old states will be kept so\n     *   that subsequent backpropgataion through time (BPTT) may work properly.\n     *   Else, the old states will be discarded.\n     */\n  }, {\n    key: \"resetStates\",\n    value: function resetStates(states) {\n      var _this3 = this;\n      var training = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : false;\n      tidy(function () {\n        if (!_this3.stateful) {\n          throw new AttributeError('Cannot call resetStates() on an RNN Layer that is not stateful.');\n        }\n        var batchSize = _this3.inputSpec[0].shape[0];\n        if (batchSize == null) {\n          throw new ValueError('If an RNN is stateful, it needs to know its batch size. Specify ' + 'the batch size of your input tensors: \\n' + '- If using a Sequential model, specify the batch size by ' + 'passing a `batchInputShape` option to your first layer.\\n' + '- If using the functional API, specify the batch size by ' + 'passing a `batchShape` option to your Input layer.');\n        }\n        // Initialize state if null.\n        if (_this3.states_ == null) {\n          if (Array.isArray(_this3.cell.stateSize)) {\n            _this3.states_ = _this3.cell.stateSize.map(function (dim) {\n              return tfc.zeros([batchSize, dim]);\n            });\n          } else {\n            _this3.states_ = [tfc.zeros([batchSize, _this3.cell.stateSize])];\n          }\n        } else if (states == null) {\n          // Dispose old state tensors.\n          tfc.dispose(_this3.states_);\n          // For stateful RNNs, fully dispose kept old states.\n          if (_this3.keptStates != null) {\n            tfc.dispose(_this3.keptStates);\n            _this3.keptStates = [];\n          }\n          if (Array.isArray(_this3.cell.stateSize)) {\n            _this3.states_ = _this3.cell.stateSize.map(function (dim) {\n              return tfc.zeros([batchSize, dim]);\n            });\n          } else {\n            _this3.states_[0] = tfc.zeros([batchSize, _this3.cell.stateSize]);\n          }\n        } else {\n          if (!Array.isArray(states)) {\n            states = [states];\n          }\n          if (states.length !== _this3.states_.length) {\n            throw new ValueError(\"Layer \".concat(_this3.name, \" expects \").concat(_this3.states_.length, \" state(s), \") + \"but it received \".concat(states.length, \" state value(s). Input \") + \"received: \".concat(states));\n          }\n          if (training === true) {\n            // Store old state tensors for complete disposal later, i.e., during\n            // the next no-arg call to this method. We do not dispose the old\n            // states immediately because that BPTT (among other things) require\n            // them.\n            _this3.keptStates.push(_this3.states_.slice());\n          } else {\n            tfc.dispose(_this3.states_);\n          }\n          for (var index = 0; index < _this3.states_.length; ++index) {\n            var value = states[index];\n            var dim = Array.isArray(_this3.cell.stateSize) ? _this3.cell.stateSize[index] : _this3.cell.stateSize;\n            var expectedShape = [batchSize, dim];\n            if (!util.arraysEqual(value.shape, expectedShape)) {\n              throw new ValueError(\"State \".concat(index, \" is incompatible with layer \").concat(_this3.name, \": \") + \"expected shape=\".concat(expectedShape, \", received shape=\").concat(value.shape));\n            }\n            _this3.states_[index] = value;\n          }\n        }\n        _this3.states_ = _this3.states_.map(function (state) {\n          return tfc.keep(state.clone());\n        });\n      });\n    }\n  }, {\n    key: \"apply\",\n    value: function apply(inputs, kwargs) {\n      // TODO(cais): Figure out whether initialState is in kwargs or inputs.\n      var initialState = kwargs == null ? null : kwargs['initialState'];\n      var constants = kwargs == null ? null : kwargs['constants'];\n      if (kwargs == null) {\n        kwargs = {};\n      }\n      var standardized = standardizeArgs(inputs, initialState, constants, this.numConstants);\n      inputs = standardized.inputs;\n      initialState = standardized.initialState;\n      constants = standardized.constants;\n      // If any of `initial_state` or `constants` are specified and are\n      // `tf.SymbolicTensor`s, then add them to the inputs and temporarily modify\n      // the input_spec to include them.\n      var additionalInputs = [];\n      var additionalSpecs = [];\n      if (initialState != null) {\n        kwargs['initialState'] = initialState;\n        additionalInputs = additionalInputs.concat(initialState);\n        this.stateSpec = [];\n        var _iterator2 = _createForOfIteratorHelper(initialState),\n          _step2;\n        try {\n          for (_iterator2.s(); !(_step2 = _iterator2.n()).done;) {\n            var state = _step2.value;\n            this.stateSpec.push(new InputSpec({\n              shape: state.shape\n            }));\n          }\n          // TODO(cais): Use the following instead.\n          // this.stateSpec = initialState.map(state => new InputSpec({shape:\n          // state.shape}));\n        } catch (err) {\n          _iterator2.e(err);\n        } finally {\n          _iterator2.f();\n        }\n        additionalSpecs = additionalSpecs.concat(this.stateSpec);\n      }\n      if (constants != null) {\n        kwargs['constants'] = constants;\n        additionalInputs = additionalInputs.concat(constants);\n        // TODO(cais): Add this.constantsSpec.\n        this.numConstants = constants.length;\n      }\n      var isTensor = additionalInputs[0] instanceof SymbolicTensor;\n      if (isTensor) {\n        // Compute full input spec, including state and constants.\n        var fullInput = [inputs].concat(additionalInputs);\n        var fullInputSpec = this.inputSpec.concat(additionalSpecs);\n        // Perform the call with temporarily replaced inputSpec.\n        var originalInputSpec = this.inputSpec;\n        this.inputSpec = fullInputSpec;\n        var output = _get(_getPrototypeOf(RNN.prototype), \"apply\", this).call(this, fullInput, kwargs);\n        this.inputSpec = originalInputSpec;\n        return output;\n      } else {\n        return _get(_getPrototypeOf(RNN.prototype), \"apply\", this).call(this, inputs, kwargs);\n      }\n    }\n    // tslint:disable-next-line:no-any\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this4 = this;\n      // Input shape: `[samples, time (padded with zeros), input_dim]`.\n      // Note that the .build() method of subclasses **must** define\n      // this.inputSpec and this.stateSpec owith complete input shapes.\n      return tidy(function () {\n        var mask = kwargs == null ? null : kwargs['mask'];\n        var training = kwargs == null ? null : kwargs['training'];\n        var initialState = kwargs == null ? null : kwargs['initialState'];\n        inputs = getExactlyOneTensor(inputs);\n        if (initialState == null) {\n          if (_this4.stateful) {\n            initialState = _this4.states_;\n          } else {\n            initialState = _this4.getInitialState(inputs);\n          }\n        }\n        var numStates = Array.isArray(_this4.cell.stateSize) ? _this4.cell.stateSize.length : 1;\n        if (initialState.length !== numStates) {\n          throw new ValueError(\"RNN Layer has \".concat(numStates, \" state(s) but was passed \") + \"\".concat(initialState.length, \" initial state(s).\"));\n        }\n        if (_this4.unroll) {\n          console.warn('Ignoring unroll = true for RNN layer, due to imperative backend.');\n        }\n        var cellCallKwargs = {\n          training: training\n        };\n        // TODO(cais): Add support for constants.\n        var step = function step(inputs, states) {\n          // `inputs` and `states` are concatenated to form a single `Array` of\n          // `tf.Tensor`s as the input to `cell.call()`.\n          var outputs = _this4.cell.call([inputs].concat(states), cellCallKwargs);\n          // Marshall the return value into output and new states.\n          return [outputs[0], outputs.slice(1)];\n        };\n        // TODO(cais): Add support for constants.\n        var rnnOutputs = rnn(step, inputs, initialState, _this4.goBackwards, mask, null, _this4.unroll, _this4.returnSequences);\n        var lastOutput = rnnOutputs[0];\n        var outputs = rnnOutputs[1];\n        var states = rnnOutputs[2];\n        if (_this4.stateful) {\n          _this4.resetStates(states, training);\n        }\n        var output = _this4.returnSequences ? outputs : lastOutput;\n        // TODO(cais): Porperty set learning phase flag.\n        if (_this4.returnState) {\n          return [output].concat(states);\n        } else {\n          return output;\n        }\n      });\n    }\n  }, {\n    key: \"getInitialState\",\n    value: function getInitialState(inputs) {\n      var _this5 = this;\n      return tidy(function () {\n        // Build an all-zero tensor of shape [samples, outputDim].\n        // [Samples, timeSteps, inputDim].\n        var initialState = tfc.zeros(inputs.shape);\n        // [Samples].\n        initialState = tfc.sum(initialState, [1, 2]);\n        initialState = K.expandDims(initialState); // [Samples, 1].\n        if (Array.isArray(_this5.cell.stateSize)) {\n          return _this5.cell.stateSize.map(function (dim) {\n            return dim > 1 ? K.tile(initialState, [1, dim]) : initialState;\n          });\n        } else {\n          return _this5.cell.stateSize > 1 ? [K.tile(initialState, [1, _this5.cell.stateSize])] : [initialState];\n        }\n      });\n    }\n  }, {\n    key: \"trainableWeights\",\n    get: function get() {\n      if (!this.trainable) {\n        return [];\n      }\n      // Porting Note: In TypeScript, `this` is always an instance of `Layer`.\n      return this.cell.trainableWeights;\n    }\n  }, {\n    key: \"nonTrainableWeights\",\n    get: function get() {\n      // Porting Note: In TypeScript, `this` is always an instance of `Layer`.\n      if (!this.trainable) {\n        return this.cell.weights;\n      }\n      return this.cell.nonTrainableWeights;\n    }\n  }, {\n    key: \"setFastWeightInitDuringBuild\",\n    value: function setFastWeightInitDuringBuild(value) {\n      _get(_getPrototypeOf(RNN.prototype), \"setFastWeightInitDuringBuild\", this).call(this, value);\n      if (this.cell != null) {\n        this.cell.setFastWeightInitDuringBuild(value);\n      }\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var baseConfig = _get(_getPrototypeOf(RNN.prototype), \"getConfig\", this).call(this);\n      var config = {\n        returnSequences: this.returnSequences,\n        returnState: this.returnState,\n        goBackwards: this.goBackwards,\n        stateful: this.stateful,\n        unroll: this.unroll\n      };\n      if (this.numConstants != null) {\n        config['numConstants'] = this.numConstants;\n      }\n      var cellConfig = this.cell.getConfig();\n      if (this.getClassName() === RNN.className) {\n        config['cell'] = {\n          'className': this.cell.getClassName(),\n          'config': cellConfig\n        };\n      }\n      // this order is necessary, to prevent cell name from replacing layer name\n      return Object.assign(Object.assign(Object.assign({}, cellConfig), baseConfig), config);\n    }\n    /** @nocollapse */\n  }], [{\n    key: \"fromConfig\",\n    value: function fromConfig(cls, config) {\n      var customObjects = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n      var cellConfig = config['cell'];\n      var cell = deserialize(cellConfig, customObjects);\n      return new cls(Object.assign(config, {\n        cell: cell\n      }));\n    }\n  }]);\n  return RNN;\n}(Layer);\n/** @nocollapse */\nRNN.className = 'RNN';\nserialization.registerClass(RNN);\n// Porting Note: This is a common parent class for RNN cells. There is no\n// equivalent of this in PyKeras. Having a common parent class forgoes the\n//  need for `has_attr(cell, ...)` checks or its TypeScript equivalent.\n/**\n * An RNNCell layer.\n *\n * @doc {heading: 'Layers', subheading: 'Classes'}\n */\nexport var RNNCell = /*#__PURE__*/function (_Layer2) {\n  _inherits(RNNCell, _Layer2);\n  var _super2 = _createSuper(RNNCell);\n  function RNNCell() {\n    _classCallCheck(this, RNNCell);\n    return _super2.apply(this, arguments);\n  }\n  return _createClass(RNNCell);\n}(Layer);\nexport var SimpleRNNCell = /*#__PURE__*/function (_RNNCell) {\n  _inherits(SimpleRNNCell, _RNNCell);\n  var _super3 = _createSuper(SimpleRNNCell);\n  function SimpleRNNCell(args) {\n    var _this6;\n    _classCallCheck(this, SimpleRNNCell);\n    _this6 = _super3.call(this, args);\n    _this6.DEFAULT_ACTIVATION = 'tanh';\n    _this6.DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n    _this6.DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n    _this6.DEFAULT_BIAS_INITIALIZER = 'zeros';\n    _this6.units = args.units;\n    assertPositiveInteger(_this6.units, \"units\");\n    _this6.activation = getActivation(args.activation == null ? _this6.DEFAULT_ACTIVATION : args.activation);\n    _this6.useBias = args.useBias == null ? true : args.useBias;\n    _this6.kernelInitializer = getInitializer(args.kernelInitializer || _this6.DEFAULT_KERNEL_INITIALIZER);\n    _this6.recurrentInitializer = getInitializer(args.recurrentInitializer || _this6.DEFAULT_RECURRENT_INITIALIZER);\n    _this6.biasInitializer = getInitializer(args.biasInitializer || _this6.DEFAULT_BIAS_INITIALIZER);\n    _this6.kernelRegularizer = getRegularizer(args.kernelRegularizer);\n    _this6.recurrentRegularizer = getRegularizer(args.recurrentRegularizer);\n    _this6.biasRegularizer = getRegularizer(args.biasRegularizer);\n    _this6.kernelConstraint = getConstraint(args.kernelConstraint);\n    _this6.recurrentConstraint = getConstraint(args.recurrentConstraint);\n    _this6.biasConstraint = getConstraint(args.biasConstraint);\n    _this6.dropout = math_utils.min([1, math_utils.max([0, args.dropout == null ? 0 : args.dropout])]);\n    _this6.recurrentDropout = math_utils.min([1, math_utils.max([0, args.recurrentDropout == null ? 0 : args.recurrentDropout])]);\n    _this6.dropoutFunc = args.dropoutFunc;\n    _this6.stateSize = _this6.units;\n    _this6.dropoutMask = null;\n    _this6.recurrentDropoutMask = null;\n    return _this6;\n  }\n  _createClass(SimpleRNNCell, [{\n    key: \"build\",\n    value: function build(inputShape) {\n      inputShape = getExactlyOneShape(inputShape);\n      // TODO(cais): Use regularizer.\n      this.kernel = this.addWeight('kernel', [inputShape[inputShape.length - 1], this.units], null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);\n      this.recurrentKernel = this.addWeight('recurrent_kernel', [this.units, this.units], null, this.recurrentInitializer, this.recurrentRegularizer, true, this.recurrentConstraint);\n      if (this.useBias) {\n        this.bias = this.addWeight('bias', [this.units], null, this.biasInitializer, this.biasRegularizer, true, this.biasConstraint);\n      } else {\n        this.bias = null;\n      }\n      this.built = true;\n    }\n    // Porting Note: PyKeras' equivalent of this method takes two tensor inputs:\n    //   `inputs` and `states`. Here, the two tensors are combined into an\n    //   `Tensor[]` Array as the first input argument.\n    //   Similarly, PyKeras' equivalent of this method returns two values:\n    //    `output` and `[output]`. Here the two are combined into one length-2\n    //    `Tensor[]`, consisting of `output` repeated.\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this7 = this;\n      return tidy(function () {\n        inputs = inputs;\n        if (inputs.length !== 2) {\n          throw new ValueError(\"SimpleRNNCell expects 2 input Tensors, got \".concat(inputs.length, \".\"));\n        }\n        var prevOutput = inputs[1];\n        inputs = inputs[0];\n        var training = kwargs['training'] == null ? false : kwargs['training'];\n        if (0 < _this7.dropout && _this7.dropout < 1 && _this7.dropoutMask == null) {\n          _this7.dropoutMask = generateDropoutMask({\n            ones: function ones() {\n              return tfc.onesLike(inputs);\n            },\n            rate: _this7.dropout,\n            training: training,\n            dropoutFunc: _this7.dropoutFunc\n          });\n        }\n        if (0 < _this7.recurrentDropout && _this7.recurrentDropout < 1 && _this7.recurrentDropoutMask == null) {\n          _this7.recurrentDropoutMask = generateDropoutMask({\n            ones: function ones() {\n              return tfc.onesLike(prevOutput);\n            },\n            rate: _this7.recurrentDropout,\n            training: training,\n            dropoutFunc: _this7.dropoutFunc\n          });\n        }\n        var h;\n        var dpMask = _this7.dropoutMask;\n        var recDpMask = _this7.recurrentDropoutMask;\n        if (dpMask != null) {\n          h = K.dot(tfc.mul(inputs, dpMask), _this7.kernel.read());\n        } else {\n          h = K.dot(inputs, _this7.kernel.read());\n        }\n        if (_this7.bias != null) {\n          h = K.biasAdd(h, _this7.bias.read());\n        }\n        if (recDpMask != null) {\n          prevOutput = tfc.mul(prevOutput, recDpMask);\n        }\n        var output = tfc.add(h, K.dot(prevOutput, _this7.recurrentKernel.read()));\n        if (_this7.activation != null) {\n          output = _this7.activation.apply(output);\n        }\n        // TODO(cais): Properly set learning phase on output tensor?\n        return [output, output];\n      });\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var baseConfig = _get(_getPrototypeOf(SimpleRNNCell.prototype), \"getConfig\", this).call(this);\n      var config = {\n        units: this.units,\n        activation: serializeActivation(this.activation),\n        useBias: this.useBias,\n        kernelInitializer: serializeInitializer(this.kernelInitializer),\n        recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n        biasInitializer: serializeInitializer(this.biasInitializer),\n        kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n        recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n        biasRegularizer: serializeRegularizer(this.biasRegularizer),\n        activityRegularizer: serializeRegularizer(this.activityRegularizer),\n        kernelConstraint: serializeConstraint(this.kernelConstraint),\n        recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n        biasConstraint: serializeConstraint(this.biasConstraint),\n        dropout: this.dropout,\n        recurrentDropout: this.recurrentDropout\n      };\n      return Object.assign(Object.assign({}, baseConfig), config);\n    }\n  }]);\n  return SimpleRNNCell;\n}(RNNCell);\n/** @nocollapse */\nSimpleRNNCell.className = 'SimpleRNNCell';\nserialization.registerClass(SimpleRNNCell);\nexport var SimpleRNN = /*#__PURE__*/function (_RNN) {\n  _inherits(SimpleRNN, _RNN);\n  var _super4 = _createSuper(SimpleRNN);\n  function SimpleRNN(args) {\n    _classCallCheck(this, SimpleRNN);\n    args.cell = new SimpleRNNCell(args);\n    return _super4.call(this, args); // TODO(cais): Add activityRegularizer.\n  }\n  _createClass(SimpleRNN, [{\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this8 = this;\n      return tidy(function () {\n        if (_this8.cell.dropoutMask != null) {\n          tfc.dispose(_this8.cell.dropoutMask);\n          _this8.cell.dropoutMask = null;\n        }\n        if (_this8.cell.recurrentDropoutMask != null) {\n          tfc.dispose(_this8.cell.recurrentDropoutMask);\n          _this8.cell.recurrentDropoutMask = null;\n        }\n        var mask = kwargs == null ? null : kwargs['mask'];\n        var training = kwargs == null ? null : kwargs['training'];\n        var initialState = kwargs == null ? null : kwargs['initialState'];\n        return _get(_getPrototypeOf(SimpleRNN.prototype), \"call\", _this8).call(_this8, inputs, {\n          mask: mask,\n          training: training,\n          initialState: initialState\n        });\n      });\n    }\n    /** @nocollapse */\n  }], [{\n    key: \"fromConfig\",\n    value: function fromConfig(cls, config) {\n      return new cls(config);\n    }\n  }]);\n  return SimpleRNN;\n}(RNN);\n/** @nocollapse */\nSimpleRNN.className = 'SimpleRNN';\nserialization.registerClass(SimpleRNN);\nexport var GRUCell = /*#__PURE__*/function (_RNNCell2) {\n  _inherits(GRUCell, _RNNCell2);\n  var _super5 = _createSuper(GRUCell);\n  function GRUCell(args) {\n    var _this9;\n    _classCallCheck(this, GRUCell);\n    _this9 = _super5.call(this, args);\n    _this9.DEFAULT_ACTIVATION = 'tanh';\n    _this9.DEFAULT_RECURRENT_ACTIVATION = 'hardSigmoid';\n    _this9.DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n    _this9.DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n    _this9.DEFAULT_BIAS_INITIALIZER = 'zeros';\n    if (args.resetAfter) {\n      throw new ValueError(\"GRUCell does not support reset_after parameter set to true.\");\n    }\n    _this9.units = args.units;\n    assertPositiveInteger(_this9.units, 'units');\n    _this9.activation = getActivation(args.activation === undefined ? _this9.DEFAULT_ACTIVATION : args.activation);\n    _this9.recurrentActivation = getActivation(args.recurrentActivation === undefined ? _this9.DEFAULT_RECURRENT_ACTIVATION : args.recurrentActivation);\n    _this9.useBias = args.useBias == null ? true : args.useBias;\n    _this9.kernelInitializer = getInitializer(args.kernelInitializer || _this9.DEFAULT_KERNEL_INITIALIZER);\n    _this9.recurrentInitializer = getInitializer(args.recurrentInitializer || _this9.DEFAULT_RECURRENT_INITIALIZER);\n    _this9.biasInitializer = getInitializer(args.biasInitializer || _this9.DEFAULT_BIAS_INITIALIZER);\n    _this9.kernelRegularizer = getRegularizer(args.kernelRegularizer);\n    _this9.recurrentRegularizer = getRegularizer(args.recurrentRegularizer);\n    _this9.biasRegularizer = getRegularizer(args.biasRegularizer);\n    _this9.kernelConstraint = getConstraint(args.kernelConstraint);\n    _this9.recurrentConstraint = getConstraint(args.recurrentConstraint);\n    _this9.biasConstraint = getConstraint(args.biasConstraint);\n    _this9.dropout = math_utils.min([1, math_utils.max([0, args.dropout == null ? 0 : args.dropout])]);\n    _this9.recurrentDropout = math_utils.min([1, math_utils.max([0, args.recurrentDropout == null ? 0 : args.recurrentDropout])]);\n    _this9.dropoutFunc = args.dropoutFunc;\n    _this9.implementation = args.implementation;\n    _this9.stateSize = _this9.units;\n    _this9.dropoutMask = null;\n    _this9.recurrentDropoutMask = null;\n    return _this9;\n  }\n  _createClass(GRUCell, [{\n    key: \"build\",\n    value: function build(inputShape) {\n      inputShape = getExactlyOneShape(inputShape);\n      var inputDim = inputShape[inputShape.length - 1];\n      this.kernel = this.addWeight('kernel', [inputDim, this.units * 3], null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);\n      this.recurrentKernel = this.addWeight('recurrent_kernel', [this.units, this.units * 3], null, this.recurrentInitializer, this.recurrentRegularizer, true, this.recurrentConstraint);\n      if (this.useBias) {\n        this.bias = this.addWeight('bias', [this.units * 3], null, this.biasInitializer, this.biasRegularizer, true, this.biasConstraint);\n      } else {\n        this.bias = null;\n      }\n      // Porting Notes: Unlike the PyKeras implementation, we perform slicing\n      //   of the weights and bias in the call() method, at execution time.\n      this.built = true;\n    }\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this10 = this;\n      return tidy(function () {\n        inputs = inputs;\n        if (inputs.length !== 2) {\n          throw new ValueError(\"GRUCell expects 2 input Tensors (inputs, h, c), got \" + \"\".concat(inputs.length, \".\"));\n        }\n        var training = kwargs['training'] == null ? false : kwargs['training'];\n        var hTMinus1 = inputs[1]; // Previous memory state.\n        inputs = inputs[0];\n        // Note: For superior performance, TensorFlow.js always uses\n        // implementation 2, regardless of the actual value of\n        // config.implementation.\n        if (0 < _this10.dropout && _this10.dropout < 1 && _this10.dropoutMask == null) {\n          _this10.dropoutMask = generateDropoutMask({\n            ones: function ones() {\n              return tfc.onesLike(inputs);\n            },\n            rate: _this10.dropout,\n            training: training,\n            count: 3,\n            dropoutFunc: _this10.dropoutFunc\n          });\n        }\n        if (0 < _this10.recurrentDropout && _this10.recurrentDropout < 1 && _this10.recurrentDropoutMask == null) {\n          _this10.recurrentDropoutMask = generateDropoutMask({\n            ones: function ones() {\n              return tfc.onesLike(hTMinus1);\n            },\n            rate: _this10.recurrentDropout,\n            training: training,\n            count: 3,\n            dropoutFunc: _this10.dropoutFunc\n          });\n        }\n        var dpMask = _this10.dropoutMask;\n        var recDpMask = _this10.recurrentDropoutMask;\n        var z;\n        var r;\n        var hh;\n        if (0 < _this10.dropout && _this10.dropout < 1) {\n          inputs = tfc.mul(inputs, dpMask[0]);\n        }\n        var matrixX = K.dot(inputs, _this10.kernel.read());\n        if (_this10.useBias) {\n          matrixX = K.biasAdd(matrixX, _this10.bias.read());\n        }\n        if (0 < _this10.recurrentDropout && _this10.recurrentDropout < 1) {\n          hTMinus1 = tfc.mul(hTMinus1, recDpMask[0]);\n        }\n        var recurrentKernelValue = _this10.recurrentKernel.read();\n        var _tfc$split = tfc.split(recurrentKernelValue, [2 * _this10.units, _this10.units], recurrentKernelValue.rank - 1),\n          _tfc$split2 = _slicedToArray(_tfc$split, 2),\n          rk1 = _tfc$split2[0],\n          rk2 = _tfc$split2[1];\n        var matrixInner = K.dot(hTMinus1, rk1);\n        var _tfc$split3 = tfc.split(matrixX, 3, matrixX.rank - 1),\n          _tfc$split4 = _slicedToArray(_tfc$split3, 3),\n          xZ = _tfc$split4[0],\n          xR = _tfc$split4[1],\n          xH = _tfc$split4[2];\n        var _tfc$split5 = tfc.split(matrixInner, 2, matrixInner.rank - 1),\n          _tfc$split6 = _slicedToArray(_tfc$split5, 2),\n          recurrentZ = _tfc$split6[0],\n          recurrentR = _tfc$split6[1];\n        z = _this10.recurrentActivation.apply(tfc.add(xZ, recurrentZ));\n        r = _this10.recurrentActivation.apply(tfc.add(xR, recurrentR));\n        var recurrentH = K.dot(tfc.mul(r, hTMinus1), rk2);\n        hh = _this10.activation.apply(tfc.add(xH, recurrentH));\n        var h = tfc.add(tfc.mul(z, hTMinus1), tfc.mul(tfc.add(1, tfc.neg(z)), hh));\n        // TODO(cais): Add use_learning_phase flag properly.\n        return [h, h];\n      });\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var baseConfig = _get(_getPrototypeOf(GRUCell.prototype), \"getConfig\", this).call(this);\n      var config = {\n        units: this.units,\n        activation: serializeActivation(this.activation),\n        recurrentActivation: serializeActivation(this.recurrentActivation),\n        useBias: this.useBias,\n        kernelInitializer: serializeInitializer(this.kernelInitializer),\n        recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n        biasInitializer: serializeInitializer(this.biasInitializer),\n        kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n        recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n        biasRegularizer: serializeRegularizer(this.biasRegularizer),\n        activityRegularizer: serializeRegularizer(this.activityRegularizer),\n        kernelConstraint: serializeConstraint(this.kernelConstraint),\n        recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n        biasConstraint: serializeConstraint(this.biasConstraint),\n        dropout: this.dropout,\n        recurrentDropout: this.recurrentDropout,\n        implementation: this.implementation,\n        resetAfter: false\n      };\n      return Object.assign(Object.assign({}, baseConfig), config);\n    }\n  }]);\n  return GRUCell;\n}(RNNCell);\n/** @nocollapse */\nGRUCell.className = 'GRUCell';\nserialization.registerClass(GRUCell);\nexport var GRU = /*#__PURE__*/function (_RNN2) {\n  _inherits(GRU, _RNN2);\n  var _super6 = _createSuper(GRU);\n  function GRU(args) {\n    _classCallCheck(this, GRU);\n    if (args.implementation === 0) {\n      console.warn('`implementation=0` has been deprecated, and now defaults to ' + '`implementation=1`. Please update your layer call.');\n    }\n    args.cell = new GRUCell(args);\n    return _super6.call(this, args); // TODO(cais): Add activityRegularizer.\n  }\n  _createClass(GRU, [{\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this11 = this;\n      return tidy(function () {\n        if (_this11.cell.dropoutMask != null) {\n          tfc.dispose(_this11.cell.dropoutMask);\n          _this11.cell.dropoutMask = null;\n        }\n        if (_this11.cell.recurrentDropoutMask != null) {\n          tfc.dispose(_this11.cell.recurrentDropoutMask);\n          _this11.cell.recurrentDropoutMask = null;\n        }\n        var mask = kwargs == null ? null : kwargs['mask'];\n        var training = kwargs == null ? null : kwargs['training'];\n        var initialState = kwargs == null ? null : kwargs['initialState'];\n        return _get(_getPrototypeOf(GRU.prototype), \"call\", _this11).call(_this11, inputs, {\n          mask: mask,\n          training: training,\n          initialState: initialState\n        });\n      });\n    }\n    /** @nocollapse */\n  }], [{\n    key: \"fromConfig\",\n    value: function fromConfig(cls, config) {\n      if (config['implmentation'] === 0) {\n        config['implementation'] = 1;\n      }\n      return new cls(config);\n    }\n  }]);\n  return GRU;\n}(RNN);\n/** @nocollapse */\nGRU.className = 'GRU';\nserialization.registerClass(GRU);\nexport var LSTMCell = /*#__PURE__*/function (_RNNCell3) {\n  _inherits(LSTMCell, _RNNCell3);\n  var _super7 = _createSuper(LSTMCell);\n  function LSTMCell(args) {\n    var _this12;\n    _classCallCheck(this, LSTMCell);\n    _this12 = _super7.call(this, args);\n    _this12.DEFAULT_ACTIVATION = 'tanh';\n    _this12.DEFAULT_RECURRENT_ACTIVATION = 'hardSigmoid';\n    _this12.DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n    _this12.DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n    _this12.DEFAULT_BIAS_INITIALIZER = 'zeros';\n    _this12.units = args.units;\n    assertPositiveInteger(_this12.units, 'units');\n    _this12.activation = getActivation(args.activation === undefined ? _this12.DEFAULT_ACTIVATION : args.activation);\n    _this12.recurrentActivation = getActivation(args.recurrentActivation === undefined ? _this12.DEFAULT_RECURRENT_ACTIVATION : args.recurrentActivation);\n    _this12.useBias = args.useBias == null ? true : args.useBias;\n    _this12.kernelInitializer = getInitializer(args.kernelInitializer || _this12.DEFAULT_KERNEL_INITIALIZER);\n    _this12.recurrentInitializer = getInitializer(args.recurrentInitializer || _this12.DEFAULT_RECURRENT_INITIALIZER);\n    _this12.biasInitializer = getInitializer(args.biasInitializer || _this12.DEFAULT_BIAS_INITIALIZER);\n    _this12.unitForgetBias = args.unitForgetBias;\n    _this12.kernelRegularizer = getRegularizer(args.kernelRegularizer);\n    _this12.recurrentRegularizer = getRegularizer(args.recurrentRegularizer);\n    _this12.biasRegularizer = getRegularizer(args.biasRegularizer);\n    _this12.kernelConstraint = getConstraint(args.kernelConstraint);\n    _this12.recurrentConstraint = getConstraint(args.recurrentConstraint);\n    _this12.biasConstraint = getConstraint(args.biasConstraint);\n    _this12.dropout = math_utils.min([1, math_utils.max([0, args.dropout == null ? 0 : args.dropout])]);\n    _this12.recurrentDropout = math_utils.min([1, math_utils.max([0, args.recurrentDropout == null ? 0 : args.recurrentDropout])]);\n    _this12.dropoutFunc = args.dropoutFunc;\n    _this12.implementation = args.implementation;\n    _this12.stateSize = [_this12.units, _this12.units];\n    _this12.dropoutMask = null;\n    _this12.recurrentDropoutMask = null;\n    return _this12;\n  }\n  _createClass(LSTMCell, [{\n    key: \"build\",\n    value: function build(inputShape) {\n      var _a;\n      inputShape = getExactlyOneShape(inputShape);\n      var inputDim = inputShape[inputShape.length - 1];\n      this.kernel = this.addWeight('kernel', [inputDim, this.units * 4], null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);\n      this.recurrentKernel = this.addWeight('recurrent_kernel', [this.units, this.units * 4], null, this.recurrentInitializer, this.recurrentRegularizer, true, this.recurrentConstraint);\n      var biasInitializer;\n      if (this.useBias) {\n        if (this.unitForgetBias) {\n          var capturedBiasInit = this.biasInitializer;\n          var capturedUnits = this.units;\n          biasInitializer = new (_a = /*#__PURE__*/function (_Initializer) {\n            _inherits(CustomInit, _Initializer);\n            var _super8 = _createSuper(CustomInit);\n            function CustomInit() {\n              _classCallCheck(this, CustomInit);\n              return _super8.apply(this, arguments);\n            }\n            _createClass(CustomInit, [{\n              key: \"apply\",\n              value: function apply(shape, dtype) {\n                // TODO(cais): More informative variable names?\n                var bI = capturedBiasInit.apply([capturedUnits]);\n                var bF = new Ones().apply([capturedUnits]);\n                var bCAndH = capturedBiasInit.apply([capturedUnits * 2]);\n                return K.concatAlongFirstAxis(K.concatAlongFirstAxis(bI, bF), bCAndH);\n              }\n            }]);\n            return CustomInit;\n          }(Initializer), /** @nocollapse */\n          _a.className = 'CustomInit', _a)();\n        } else {\n          biasInitializer = this.biasInitializer;\n        }\n        this.bias = this.addWeight('bias', [this.units * 4], null, biasInitializer, this.biasRegularizer, true, this.biasConstraint);\n      } else {\n        this.bias = null;\n      }\n      // Porting Notes: Unlike the PyKeras implementation, we perform slicing\n      //   of the weights and bias in the call() method, at execution time.\n      this.built = true;\n    }\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this13 = this;\n      return tidy(function () {\n        var training = kwargs['training'] == null ? false : kwargs['training'];\n        inputs = inputs;\n        if (inputs.length !== 3) {\n          throw new ValueError(\"LSTMCell expects 3 input Tensors (inputs, h, c), got \" + \"\".concat(inputs.length, \".\"));\n        }\n        var hTMinus1 = inputs[1]; // Previous memory state.\n        var cTMinus1 = inputs[2]; // Previous carry state.\n        inputs = inputs[0];\n        if (0 < _this13.dropout && _this13.dropout < 1 && _this13.dropoutMask == null) {\n          _this13.dropoutMask = generateDropoutMask({\n            ones: function ones() {\n              return tfc.onesLike(inputs);\n            },\n            rate: _this13.dropout,\n            training: training,\n            count: 4,\n            dropoutFunc: _this13.dropoutFunc\n          });\n        }\n        if (0 < _this13.recurrentDropout && _this13.recurrentDropout < 1 && _this13.recurrentDropoutMask == null) {\n          _this13.recurrentDropoutMask = generateDropoutMask({\n            ones: function ones() {\n              return tfc.onesLike(hTMinus1);\n            },\n            rate: _this13.recurrentDropout,\n            training: training,\n            count: 4,\n            dropoutFunc: _this13.dropoutFunc\n          });\n        }\n        var dpMask = _this13.dropoutMask;\n        var recDpMask = _this13.recurrentDropoutMask;\n        // Note: For superior performance, TensorFlow.js always uses\n        // implementation 2 regardless of the actual value of\n        // config.implementation.\n        var i;\n        var f;\n        var c;\n        var o;\n        if (0 < _this13.dropout && _this13.dropout < 1) {\n          inputs = tfc.mul(inputs, dpMask[0]);\n        }\n        var z = K.dot(inputs, _this13.kernel.read());\n        if (0 < _this13.recurrentDropout && _this13.recurrentDropout < 1) {\n          hTMinus1 = tfc.mul(hTMinus1, recDpMask[0]);\n        }\n        z = tfc.add(z, K.dot(hTMinus1, _this13.recurrentKernel.read()));\n        if (_this13.useBias) {\n          z = K.biasAdd(z, _this13.bias.read());\n        }\n        var _tfc$split7 = tfc.split(z, 4, z.rank - 1),\n          _tfc$split8 = _slicedToArray(_tfc$split7, 4),\n          z0 = _tfc$split8[0],\n          z1 = _tfc$split8[1],\n          z2 = _tfc$split8[2],\n          z3 = _tfc$split8[3];\n        i = _this13.recurrentActivation.apply(z0);\n        f = _this13.recurrentActivation.apply(z1);\n        c = tfc.add(tfc.mul(f, cTMinus1), tfc.mul(i, _this13.activation.apply(z2)));\n        o = _this13.recurrentActivation.apply(z3);\n        var h = tfc.mul(o, _this13.activation.apply(c));\n        // TODO(cais): Add use_learning_phase flag properly.\n        return [h, h, c];\n      });\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var baseConfig = _get(_getPrototypeOf(LSTMCell.prototype), \"getConfig\", this).call(this);\n      var config = {\n        units: this.units,\n        activation: serializeActivation(this.activation),\n        recurrentActivation: serializeActivation(this.recurrentActivation),\n        useBias: this.useBias,\n        kernelInitializer: serializeInitializer(this.kernelInitializer),\n        recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n        biasInitializer: serializeInitializer(this.biasInitializer),\n        unitForgetBias: this.unitForgetBias,\n        kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n        recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n        biasRegularizer: serializeRegularizer(this.biasRegularizer),\n        activityRegularizer: serializeRegularizer(this.activityRegularizer),\n        kernelConstraint: serializeConstraint(this.kernelConstraint),\n        recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n        biasConstraint: serializeConstraint(this.biasConstraint),\n        dropout: this.dropout,\n        recurrentDropout: this.recurrentDropout,\n        implementation: this.implementation\n      };\n      return Object.assign(Object.assign({}, baseConfig), config);\n    }\n  }]);\n  return LSTMCell;\n}(RNNCell);\n/** @nocollapse */\nLSTMCell.className = 'LSTMCell';\nserialization.registerClass(LSTMCell);\nexport var LSTM = /*#__PURE__*/function (_RNN3) {\n  _inherits(LSTM, _RNN3);\n  var _super9 = _createSuper(LSTM);\n  function LSTM(args) {\n    _classCallCheck(this, LSTM);\n    if (args.implementation === 0) {\n      console.warn('`implementation=0` has been deprecated, and now defaults to ' + '`implementation=1`. Please update your layer call.');\n    }\n    args.cell = new LSTMCell(args);\n    return _super9.call(this, args); // TODO(cais): Add activityRegularizer.\n  }\n  _createClass(LSTM, [{\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this14 = this;\n      return tidy(function () {\n        if (_this14.cell.dropoutMask != null) {\n          tfc.dispose(_this14.cell.dropoutMask);\n          _this14.cell.dropoutMask = null;\n        }\n        if (_this14.cell.recurrentDropoutMask != null) {\n          tfc.dispose(_this14.cell.recurrentDropoutMask);\n          _this14.cell.recurrentDropoutMask = null;\n        }\n        var mask = kwargs == null ? null : kwargs['mask'];\n        var training = kwargs == null ? null : kwargs['training'];\n        var initialState = kwargs == null ? null : kwargs['initialState'];\n        return _get(_getPrototypeOf(LSTM.prototype), \"call\", _this14).call(_this14, inputs, {\n          mask: mask,\n          training: training,\n          initialState: initialState\n        });\n      });\n    }\n    /** @nocollapse */\n  }], [{\n    key: \"fromConfig\",\n    value: function fromConfig(cls, config) {\n      if (config['implmentation'] === 0) {\n        config['implementation'] = 1;\n      }\n      return new cls(config);\n    }\n  }]);\n  return LSTM;\n}(RNN);\n/** @nocollapse */\nLSTM.className = 'LSTM';\nserialization.registerClass(LSTM);\nexport var StackedRNNCells = /*#__PURE__*/function (_RNNCell4) {\n  _inherits(StackedRNNCells, _RNNCell4);\n  var _super10 = _createSuper(StackedRNNCells);\n  function StackedRNNCells(args) {\n    var _this15;\n    _classCallCheck(this, StackedRNNCells);\n    _this15 = _super10.call(this, args);\n    _this15.cells = args.cells;\n    return _this15;\n  }\n  _createClass(StackedRNNCells, [{\n    key: \"stateSize\",\n    get: function get() {\n      // States are a flat list in reverse order of the cell stack.\n      // This allows perserving the requirement `stack.statesize[0] ===\n      // outputDim`. E.g., states of a 2-layer LSTM would be `[h2, c2, h1, c1]`,\n      // assuming one LSTM has states `[h, c]`.\n      var stateSize = [];\n      var _iterator3 = _createForOfIteratorHelper(this.cells.slice().reverse()),\n        _step3;\n      try {\n        for (_iterator3.s(); !(_step3 = _iterator3.n()).done;) {\n          var cell = _step3.value;\n          if (Array.isArray(cell.stateSize)) {\n            stateSize.push.apply(stateSize, _toConsumableArray(cell.stateSize));\n          } else {\n            stateSize.push(cell.stateSize);\n          }\n        }\n      } catch (err) {\n        _iterator3.e(err);\n      } finally {\n        _iterator3.f();\n      }\n      return stateSize;\n    }\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this16 = this;\n      return tidy(function () {\n        inputs = inputs;\n        var states = inputs.slice(1);\n        // Recover per-cell states.\n        var nestedStates = [];\n        var _iterator4 = _createForOfIteratorHelper(_this16.cells.slice().reverse()),\n          _step4;\n        try {\n          for (_iterator4.s(); !(_step4 = _iterator4.n()).done;) {\n            var _cell = _step4.value;\n            if (Array.isArray(_cell.stateSize)) {\n              nestedStates.push(states.splice(0, _cell.stateSize.length));\n            } else {\n              nestedStates.push(states.splice(0, 1));\n            }\n          }\n        } catch (err) {\n          _iterator4.e(err);\n        } finally {\n          _iterator4.f();\n        }\n        nestedStates.reverse();\n        // Call the cells in order and store the returned states.\n        var newNestedStates = [];\n        var callInputs;\n        for (var i = 0; i < _this16.cells.length; ++i) {\n          var cell = _this16.cells[i];\n          states = nestedStates[i];\n          // TODO(cais): Take care of constants.\n          if (i === 0) {\n            callInputs = [inputs[0]].concat(states);\n          } else {\n            callInputs = [callInputs[0]].concat(states);\n          }\n          callInputs = cell.call(callInputs, kwargs);\n          newNestedStates.push(callInputs.slice(1));\n        }\n        // Format the new states as a flat list in reverse cell order.\n        states = [];\n        var _iterator5 = _createForOfIteratorHelper(newNestedStates.slice().reverse()),\n          _step5;\n        try {\n          for (_iterator5.s(); !(_step5 = _iterator5.n()).done;) {\n            var _states;\n            var cellStates = _step5.value;\n            (_states = states).push.apply(_states, _toConsumableArray(cellStates));\n          }\n        } catch (err) {\n          _iterator5.e(err);\n        } finally {\n          _iterator5.f();\n        }\n        return [callInputs[0]].concat(states);\n      });\n    }\n  }, {\n    key: \"build\",\n    value: function build(inputShape) {\n      if (isArrayOfShapes(inputShape)) {\n        // TODO(cais): Take care of input constants.\n        // const constantShape = inputShape.slice(1);\n        inputShape = inputShape[0];\n      }\n      inputShape = inputShape;\n      var outputDim;\n      this.cells.forEach(function (cell, i) {\n        nameScope(\"RNNCell_\".concat(i), function () {\n          // TODO(cais): Take care of input constants.\n          cell.build(inputShape);\n          if (Array.isArray(cell.stateSize)) {\n            outputDim = cell.stateSize[0];\n          } else {\n            outputDim = cell.stateSize;\n          }\n          inputShape = [inputShape[0], outputDim];\n        });\n      });\n      this.built = true;\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var baseConfig = _get(_getPrototypeOf(StackedRNNCells.prototype), \"getConfig\", this).call(this);\n      var getCellConfig = function getCellConfig(cell) {\n        return {\n          'className': cell.getClassName(),\n          'config': cell.getConfig()\n        };\n      };\n      var cellConfigs = this.cells.map(getCellConfig);\n      var config = {\n        'cells': cellConfigs\n      };\n      return Object.assign(Object.assign({}, baseConfig), config);\n    }\n    /** @nocollapse */\n  }, {\n    key: \"trainableWeights\",\n    get: function get() {\n      if (!this.trainable) {\n        return [];\n      }\n      var weights = [];\n      var _iterator6 = _createForOfIteratorHelper(this.cells),\n        _step6;\n      try {\n        for (_iterator6.s(); !(_step6 = _iterator6.n()).done;) {\n          var cell = _step6.value;\n          weights.push.apply(weights, _toConsumableArray(cell.trainableWeights));\n        }\n      } catch (err) {\n        _iterator6.e(err);\n      } finally {\n        _iterator6.f();\n      }\n      return weights;\n    }\n  }, {\n    key: \"nonTrainableWeights\",\n    get: function get() {\n      var weights = [];\n      var _iterator7 = _createForOfIteratorHelper(this.cells),\n        _step7;\n      try {\n        for (_iterator7.s(); !(_step7 = _iterator7.n()).done;) {\n          var _cell2 = _step7.value;\n          weights.push.apply(weights, _toConsumableArray(_cell2.nonTrainableWeights));\n        }\n      } catch (err) {\n        _iterator7.e(err);\n      } finally {\n        _iterator7.f();\n      }\n      if (!this.trainable) {\n        var trainableWeights = [];\n        var _iterator8 = _createForOfIteratorHelper(this.cells),\n          _step8;\n        try {\n          for (_iterator8.s(); !(_step8 = _iterator8.n()).done;) {\n            var cell = _step8.value;\n            trainableWeights.push.apply(trainableWeights, _toConsumableArray(cell.trainableWeights));\n          }\n        } catch (err) {\n          _iterator8.e(err);\n        } finally {\n          _iterator8.f();\n        }\n        return trainableWeights.concat(weights);\n      }\n      return weights;\n    }\n    /**\n     * Retrieve the weights of a the model.\n     *\n     * @returns A flat `Array` of `tf.Tensor`s.\n     */\n  }, {\n    key: \"getWeights\",\n    value: function getWeights() {\n      var weights = [];\n      var _iterator9 = _createForOfIteratorHelper(this.cells),\n        _step9;\n      try {\n        for (_iterator9.s(); !(_step9 = _iterator9.n()).done;) {\n          var cell = _step9.value;\n          weights.push.apply(weights, _toConsumableArray(cell.weights));\n        }\n      } catch (err) {\n        _iterator9.e(err);\n      } finally {\n        _iterator9.f();\n      }\n      return batchGetValue(weights);\n    }\n    /**\n     * Set the weights of the model.\n     *\n     * @param weights An `Array` of `tf.Tensor`s with shapes and types matching\n     *     the output of `getWeights()`.\n     */\n  }, {\n    key: \"setWeights\",\n    value: function setWeights(weights) {\n      var tuples = [];\n      var _iterator10 = _createForOfIteratorHelper(this.cells),\n        _step10;\n      try {\n        for (_iterator10.s(); !(_step10 = _iterator10.n()).done;) {\n          var cell = _step10.value;\n          var numParams = cell.weights.length;\n          var inputWeights = weights.splice(numParams);\n          for (var i = 0; i < cell.weights.length; ++i) {\n            tuples.push([cell.weights[i], inputWeights[i]]);\n          }\n        }\n      } catch (err) {\n        _iterator10.e(err);\n      } finally {\n        _iterator10.f();\n      }\n      batchSetValue(tuples);\n    }\n  }], [{\n    key: \"fromConfig\",\n    value: function fromConfig(cls, config) {\n      var customObjects = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n      var cells = [];\n      var _iterator11 = _createForOfIteratorHelper(config['cells']),\n        _step11;\n      try {\n        for (_iterator11.s(); !(_step11 = _iterator11.n()).done;) {\n          var cellConfig = _step11.value;\n          cells.push(deserialize(cellConfig, customObjects));\n        }\n      } catch (err) {\n        _iterator11.e(err);\n      } finally {\n        _iterator11.f();\n      }\n      return new cls({\n        cells: cells\n      });\n    }\n  }]);\n  return StackedRNNCells;\n}(RNNCell);\n/** @nocollapse */\nStackedRNNCells.className = 'StackedRNNCells';\nserialization.registerClass(StackedRNNCells);\nexport function generateDropoutMask(args) {\n  var ones = args.ones,\n    rate = args.rate,\n    _args$training = args.training,\n    training = _args$training === void 0 ? false : _args$training,\n    _args$count = args.count,\n    count = _args$count === void 0 ? 1 : _args$count,\n    dropoutFunc = args.dropoutFunc;\n  var droppedInputs = function droppedInputs() {\n    return dropoutFunc != null ? dropoutFunc(ones(), rate) : K.dropout(ones(), rate);\n  };\n  var createMask = function createMask() {\n    return K.inTrainPhase(droppedInputs, ones, training);\n  };\n  // just in case count is provided with null or undefined\n  if (!count || count <= 1) {\n    return tfc.keep(createMask().clone());\n  }\n  var masks = Array(count).fill(undefined).map(createMask);\n  return masks.map(function (m) {\n    return tfc.keep(m.clone());\n  });\n}","map":{"version":3,"names":["tfc","serialization","tidy","util","getActivation","serializeActivation","K","nameScope","getConstraint","serializeConstraint","InputSpec","SymbolicTensor","Layer","AttributeError","NotImplementedError","ValueError","getInitializer","Initializer","Ones","serializeInitializer","getRegularizer","serializeRegularizer","assertPositiveInteger","math_utils","getExactlyOneShape","getExactlyOneTensor","isArrayOfShapes","batchGetValue","batchSetValue","deserialize","standardizeArgs","inputs","initialState","constants","numConstants","Array","isArray","slice","length","toListOrNull","x","rnn","stepFunction","initialStates","goBackwards","arguments","undefined","mask","unroll","needPerStepOutputs","ndim","shape","concat","axes","range","transpose","console","warn","cast","rank","expandDims","reverse","perStepOutputs","lastOutput","states","timeSteps","perStepInputs","unstack","perStepMasks","_loop","t","currentInput","stepOutputs","maskedOutputs","stepMask","negStepMask","sub","onesLike","output","add","mul","newStates","map","state","i","push","outputs","axis","stack","RNN","_Layer","_inherits","_super","_createSuper","args","_this","_classCallCheck","call","cell","StackedRNNCells","cells","stateSize","returnSequences","returnState","_stateful","stateful","supportsMasking","inputSpec","stateSpec","states_","keptStates","_createClass","key","value","getStates","numStates","setStates","computeOutputShape","inputShape","outputDim","outputShape","stateShape","_iterator","_createForOfIteratorHelper","_step","s","n","done","dim","err","e","f","computeMask","_this2","outputMask","stateMask","get","set","build","constantShape","batchSize","inputDim","_toConsumableArray","stepInputShape","arraysEqual","spec","resetStates","_this3","training","zeros","dispose","name","index","expectedShape","keep","clone","apply","kwargs","standardized","additionalInputs","additionalSpecs","_iterator2","_step2","isTensor","fullInput","fullInputSpec","originalInputSpec","_get","_getPrototypeOf","prototype","_this4","getInitialState","cellCallKwargs","step","rnnOutputs","_this5","sum","tile","trainable","trainableWeights","weights","nonTrainableWeights","setFastWeightInitDuringBuild","getConfig","baseConfig","config","cellConfig","getClassName","className","Object","assign","fromConfig","cls","customObjects","registerClass","RNNCell","_Layer2","_super2","SimpleRNNCell","_RNNCell","_super3","_this6","DEFAULT_ACTIVATION","DEFAULT_KERNEL_INITIALIZER","DEFAULT_RECURRENT_INITIALIZER","DEFAULT_BIAS_INITIALIZER","units","activation","useBias","kernelInitializer","recurrentInitializer","biasInitializer","kernelRegularizer","recurrentRegularizer","biasRegularizer","kernelConstraint","recurrentConstraint","biasConstraint","dropout","min","max","recurrentDropout","dropoutFunc","dropoutMask","recurrentDropoutMask","kernel","addWeight","recurrentKernel","bias","built","_this7","prevOutput","generateDropoutMask","ones","rate","h","dpMask","recDpMask","dot","read","biasAdd","activityRegularizer","SimpleRNN","_RNN","_super4","_this8","GRUCell","_RNNCell2","_super5","_this9","DEFAULT_RECURRENT_ACTIVATION","resetAfter","recurrentActivation","implementation","_this10","hTMinus1","count","z","r","hh","matrixX","recurrentKernelValue","_tfc$split","split","_tfc$split2","_slicedToArray","rk1","rk2","matrixInner","_tfc$split3","_tfc$split4","xZ","xR","xH","_tfc$split5","_tfc$split6","recurrentZ","recurrentR","recurrentH","neg","GRU","_RNN2","_super6","_this11","LSTMCell","_RNNCell3","_super7","_this12","unitForgetBias","capturedBiasInit","capturedUnits","_a","_Initializer","CustomInit","_super8","dtype","bI","bF","bCAndH","concatAlongFirstAxis","_this13","cTMinus1","c","o","_tfc$split7","_tfc$split8","z0","z1","z2","z3","LSTM","_RNN3","_super9","_this14","_RNNCell4","_super10","_this15","_iterator3","_step3","_this16","nestedStates","_iterator4","_step4","splice","newNestedStates","callInputs","_iterator5","_step5","_states","cellStates","forEach","getCellConfig","cellConfigs","_iterator6","_step6","_iterator7","_step7","_iterator8","_step8","getWeights","_iterator9","_step9","setWeights","tuples","_iterator10","_step10","numParams","inputWeights","_iterator11","_step11","_args$training","_args$count","droppedInputs","createMask","inTrainPhase","masks","fill","m"],"sources":["C:\\Users\\vince\\OneDrive\\Documents\\GitHub\\tfjs-layers\\src\\layers\\recurrent.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * TensorFlow.js Layers: Recurrent Neural Network Layers.\n */\n\nimport * as tfc from '@tensorflow/tfjs-core';\nimport {DataType, serialization, Tensor, tidy, util} from '@tensorflow/tfjs-core';\n\nimport {Activation, getActivation, serializeActivation} from '../activations';\nimport * as K from '../backend/tfjs_backend';\nimport {nameScope} from '../common';\nimport {Constraint, ConstraintIdentifier, getConstraint, serializeConstraint} from '../constraints';\nimport {InputSpec, SymbolicTensor} from '../engine/topology';\nimport {Layer, LayerArgs} from '../engine/topology';\nimport {AttributeError, NotImplementedError, ValueError} from '../errors';\nimport {getInitializer, Initializer, InitializerIdentifier, Ones, serializeInitializer} from '../initializers';\nimport {ActivationIdentifier} from '../keras_format/activation_config';\nimport {Shape} from '../keras_format/common';\nimport {getRegularizer, Regularizer, RegularizerIdentifier, serializeRegularizer} from '../regularizers';\nimport {Kwargs, RnnStepFunction} from '../types';\nimport {assertPositiveInteger} from '../utils/generic_utils';\nimport * as math_utils from '../utils/math_utils';\nimport {getExactlyOneShape, getExactlyOneTensor, isArrayOfShapes} from '../utils/types_utils';\nimport {batchGetValue, batchSetValue, LayerVariable} from '../variables';\n\nimport {deserialize} from './serialization';\n\n/**\n * Standardize `apply()` args to a single list of tensor inputs.\n *\n * When running a model loaded from file, the input tensors `initialState` and\n * `constants` are passed to `RNN.apply()` as part of `inputs` instead of the\n * dedicated kwargs fields. `inputs` consists of\n * `[inputs, initialState0, initialState1, ..., constant0, constant1]` in this\n * case.\n * This method makes sure that arguments are\n * separated and that `initialState` and `constants` are `Array`s of tensors\n * (or None).\n *\n * @param inputs Tensor or `Array` of  tensors.\n * @param initialState Tensor or `Array` of tensors or `null`/`undefined`.\n * @param constants Tensor or `Array` of tensors or `null`/`undefined`.\n * @returns An object consisting of\n *   inputs: A tensor.\n *   initialState: `Array` of tensors or `null`.\n *   constants: `Array` of tensors or `null`.\n * @throws ValueError, if `inputs` is an `Array` but either `initialState` or\n *   `constants` is provided.\n */\nexport function standardizeArgs(\n    inputs: Tensor|Tensor[]|SymbolicTensor|SymbolicTensor[],\n    initialState: Tensor|Tensor[]|SymbolicTensor|SymbolicTensor[],\n    constants: Tensor|Tensor[]|SymbolicTensor|SymbolicTensor[],\n    numConstants?: number): {\n  inputs: Tensor|SymbolicTensor,\n  initialState: Tensor[]|SymbolicTensor[],\n  constants: Tensor[]|SymbolicTensor[]\n} {\n  if (Array.isArray(inputs)) {\n    if (initialState != null || constants != null) {\n      throw new ValueError(\n          'When inputs is an array, neither initialState or constants ' +\n          'should be provided');\n    }\n    if (numConstants != null) {\n      constants = inputs.slice(inputs.length - numConstants, inputs.length);\n      inputs = inputs.slice(0, inputs.length - numConstants);\n    }\n    if (inputs.length > 1) {\n      initialState = inputs.slice(1, inputs.length);\n    }\n    inputs = inputs[0];\n  }\n\n  function toListOrNull(x: Tensor|Tensor[]|SymbolicTensor|\n                        SymbolicTensor[]): Tensor[]|SymbolicTensor[] {\n    if (x == null || Array.isArray(x)) {\n      return x as Tensor[] | SymbolicTensor[];\n    } else {\n      return [x] as Tensor[] | SymbolicTensor[];\n    }\n  }\n\n  initialState = toListOrNull(initialState);\n  constants = toListOrNull(constants);\n\n  return {inputs, initialState, constants};\n}\n\n/**\n * Iterates over the time dimension of a tensor.\n *\n * @param stepFunction RNN step function.\n *   Parameters:\n *     inputs: tensor with shape `[samples, ...]` (no time dimension),\n *       representing input for the batch of samples at a certain time step.\n *     states: an Array of tensors.\n *   Returns:\n *     outputs: tensor with shape `[samples, outputDim]` (no time dimension).\n *     newStates: list of tensors, same length and shapes as `states`. The first\n *       state in the list must be the output tensor at the previous timestep.\n * @param inputs Tensor of temporal data of shape `[samples, time, ...]` (at\n *   least 3D).\n * @param initialStates Tensor with shape `[samples, outputDim]` (no time\n *   dimension), containing the initial values of the states used in the step\n *   function.\n * @param goBackwards If `true`, do the iteration over the time dimension in\n *   reverse order and return the reversed sequence.\n * @param mask Binary tensor with shape `[sample, time, 1]`, with a zero for\n *   every element that is masked.\n * @param constants An Array of constant values passed at each step.\n * @param unroll Whether to unroll the RNN or to use a symbolic loop. *Not*\n *   applicable to this imperative deeplearn.js backend. Its value is ignored.\n * @param needPerStepOutputs Whether the per-step outputs are to be\n *   concatenated into a single tensor and returned (as the second return\n *   value). Default: `false`. This arg is included so that the relatively\n *   expensive concatenation of the stepwise outputs can be omitted unless\n *   the stepwise outputs need to be kept (e.g., for an LSTM layer of which\n *   `returnSequence` is `true`.)\n * @returns An Array: `[lastOutput, outputs, newStates]`.\n *   lastOutput: the lastest output of the RNN, of shape `[samples, ...]`.\n *   outputs: tensor with shape `[samples, time, ...]` where each entry\n *     `output[s, t]` is the output of the step function at time `t` for sample\n *     `s`. This return value is provided if and only if the\n *     `needPerStepOutputs` is set as `true`. If it is set as `false`, this\n *     return value will be `undefined`.\n *   newStates: Array of tensors, latest states returned by the step function,\n *      of shape `(samples, ...)`.\n * @throws ValueError If input dimension is less than 3.\n *\n * TODO(nielsene): This needs to be tidy-ed.\n */\nexport function rnn(\n    stepFunction: RnnStepFunction, inputs: Tensor, initialStates: Tensor[],\n    goBackwards = false, mask?: Tensor, constants?: Tensor[], unroll = false,\n    needPerStepOutputs = false): [Tensor, Tensor, Tensor[]] {\n  return tfc.tidy(() => {\n    const ndim = inputs.shape.length;\n    if (ndim < 3) {\n      throw new ValueError(`Input should be at least 3D, but is ${ndim}D.`);\n    }\n\n    // Transpose to time-major, i.e., from [batch, time, ...] to [time, batch,\n    // ...].\n    const axes = [1, 0].concat(math_utils.range(2, ndim));\n    inputs = tfc.transpose(inputs, axes);\n\n    if (constants != null) {\n      throw new NotImplementedError(\n          'The rnn() functoin of the deeplearn.js backend does not support ' +\n          'constants yet.');\n    }\n\n    // Porting Note: the unroll option is ignored by the imperative backend.\n    if (unroll) {\n      console.warn(\n          'Backend rnn(): the unroll = true option is not applicable to the ' +\n          'imperative deeplearn.js backend.');\n    }\n\n    if (mask != null) {\n      mask = tfc.cast(tfc.cast(mask, 'bool'), 'float32');\n      if (mask.rank === ndim - 1) {\n        mask = tfc.expandDims(mask, -1);\n      }\n      mask = tfc.transpose(mask, axes);\n    }\n\n    if (goBackwards) {\n      inputs = tfc.reverse(inputs, 0);\n      if (mask != null) {\n        mask = tfc.reverse(mask, 0);\n      }\n    }\n\n    // Porting Note: PyKeras with TensorFlow backend uses a symbolic loop\n    //   (tf.while_loop). But for the imperative deeplearn.js backend, we just\n    //   use the usual TypeScript control flow to iterate over the time steps in\n    //   the inputs.\n    // Porting Note: PyKeras patches a \"_use_learning_phase\" attribute to\n    // outputs.\n    //   This is not idiomatic in TypeScript. The info regarding whether we are\n    //   in a learning (i.e., training) phase for RNN is passed in a different\n    //   way.\n\n    const perStepOutputs: Tensor[] = [];\n    let lastOutput: Tensor;\n    let states = initialStates;\n    const timeSteps = inputs.shape[0];\n    const perStepInputs = tfc.unstack(inputs);\n    let perStepMasks: Tensor[];\n    if (mask != null) {\n      perStepMasks = tfc.unstack(mask);\n    }\n\n    for (let t = 0; t < timeSteps; ++t) {\n      const currentInput = perStepInputs[t];\n      const stepOutputs = tfc.tidy(() => stepFunction(currentInput, states));\n\n      if (mask == null) {\n        lastOutput = stepOutputs[0];\n        states = stepOutputs[1];\n      } else {\n        const maskedOutputs = tfc.tidy(() => {\n          const stepMask = perStepMasks[t];\n          const negStepMask = tfc.sub(tfc.onesLike(stepMask), stepMask);\n          // TODO(cais): Would tfc.where() be better for performance?\n          const output = tfc.add(\n              tfc.mul(stepOutputs[0], stepMask),\n              tfc.mul(states[0], negStepMask));\n          const newStates = states.map((state, i) => {\n            return tfc.add(\n                tfc.mul(stepOutputs[1][i], stepMask),\n                tfc.mul(state, negStepMask));\n          });\n          return {output, newStates};\n        });\n        lastOutput = maskedOutputs.output;\n        states = maskedOutputs.newStates;\n      }\n\n      if (needPerStepOutputs) {\n        perStepOutputs.push(lastOutput);\n      }\n    }\n    let outputs: Tensor;\n    if (needPerStepOutputs) {\n      const axis = 1;\n      outputs = tfc.stack(perStepOutputs, axis);\n    }\n    return [lastOutput, outputs, states] as [Tensor, Tensor, Tensor[]];\n  });\n}\n\nexport declare interface BaseRNNLayerArgs extends LayerArgs {\n  /**\n   * A RNN cell instance. A RNN cell is a class that has:\n   *   - a `call()` method, which takes `[Tensor, Tensor]` as the\n   *     first input argument. The first item is the input at time t, and\n   *     second item is the cell state at time t.\n   *     The `call()` method returns `[outputAtT, statesAtTPlus1]`.\n   *     The `call()` method of the cell can also take the argument `constants`,\n   *     see section \"Note on passing external constants\" below.\n   *     Porting Node: PyKeras overrides the `call()` signature of RNN cells,\n   *       which are Layer subtypes, to accept two arguments. tfjs-layers does\n   *       not do such overriding. Instead we preseve the `call()` signature,\n   *       which due to its `Tensor|Tensor[]` argument and return value is\n   *       flexible enough to handle the inputs and states.\n   *   - a `stateSize` attribute. This can be a single integer (single state)\n   *     in which case it is the size of the recurrent state (which should be\n   *     the same as the size of the cell output). This can also be an Array of\n   *     integers (one size per state). In this case, the first entry\n   *     (`stateSize[0]`) should be the same as the size of the cell output.\n   * It is also possible for `cell` to be a list of RNN cell instances, in which\n   * case the cells get stacked on after the other in the RNN, implementing an\n   * efficient stacked RNN.\n   */\n  cell?: RNNCell|RNNCell[];\n\n  /**\n   * Whether to return the last output in the output sequence, or the full\n   * sequence.\n   */\n  returnSequences?: boolean;\n\n  /**\n   * Whether to return the last state in addition to the output.\n   */\n  returnState?: boolean;\n\n  /**\n   * If `true`, process the input sequence backwards and return the reversed\n   * sequence (default: `false`).\n   */\n  goBackwards?: boolean;\n\n  /**\n   * If `true`, the last state for each sample at index i in a batch will be\n   * used as initial state of the sample of index i in the following batch\n   * (default: `false`).\n   *\n   * You can set RNN layers to be \"stateful\", which means that the states\n   * computed for the samples in one batch will be reused as initial states\n   * for the samples in the next batch. This assumes a one-to-one mapping\n   * between samples in different successive batches.\n   *\n   * To enable \"statefulness\":\n   *   - specify `stateful: true` in the layer constructor.\n   *   - specify a fixed batch size for your model, by passing\n   *     - if sequential model:\n   *       `batchInputShape: [...]` to the first layer in your model.\n   *     - else for functional model with 1 or more Input layers:\n   *       `batchShape: [...]` to all the first layers in your model.\n   *     This is the expected shape of your inputs\n   *     *including the batch size*.\n   *     It should be a tuple of integers, e.g., `[32, 10, 100]`.\n   *   - specify `shuffle: false` when calling `LayersModel.fit()`.\n   *\n   * To reset the state of your model, call `resetStates()` on either the\n   * specific layer or on the entire model.\n   */\n  stateful?: boolean;\n  // TODO(cais): Explore whether we can warn users when they fail to set\n  //   `shuffle: false` when training a model consisting of stateful RNNs\n  //   and any stateful Layers in general.\n\n  /**\n   * If `true`, the network will be unrolled, else a symbolic loop will be\n   * used. Unrolling can speed up a RNN, although it tends to be more\n   * memory-intensive. Unrolling is only suitable for short sequences (default:\n   * `false`).\n   * Porting Note: tfjs-layers has an imperative backend. RNNs are executed with\n   *   normal TypeScript control flow. Hence this property is inapplicable and\n   *   ignored in tfjs-layers.\n   */\n  unroll?: boolean;\n\n  /**\n   * Dimensionality of the input (integer).\n   *   This option (or alternatively, the option `inputShape`) is required when\n   *   this layer is used as the first layer in a model.\n   */\n  inputDim?: number;\n\n  /**\n   * Length of the input sequences, to be specified when it is constant.\n   * This argument is required if you are going to connect `Flatten` then\n   * `Dense` layers upstream (without it, the shape of the dense outputs cannot\n   * be computed). Note that if the recurrent layer is not the first layer in\n   * your model, you would need to specify the input length at the level of the\n   * first layer (e.g., via the `inputShape` option).\n   */\n  inputLength?: number;\n}\n\nexport class RNN extends Layer {\n  /** @nocollapse */\n  static className = 'RNN';\n  public readonly cell: RNNCell;\n  public readonly returnSequences: boolean;\n  public readonly returnState: boolean;\n  public readonly goBackwards: boolean;\n  public readonly unroll: boolean;\n\n  public stateSpec: InputSpec[];\n  protected states_: Tensor[];\n\n  // NOTE(cais): For stateful RNNs, the old states cannot be disposed right\n  // away when new states are set, because the old states may need to be used\n  // later for backpropagation through time (BPTT) and other purposes. So we\n  // keep them here for final disposal when the state is reset completely\n  // (i.e., through no-arg call to `resetStates()`).\n  protected keptStates: Tensor[][];\n\n  private numConstants: number;\n\n  constructor(args: RNNLayerArgs) {\n    super(args);\n    let cell: RNNCell;\n    if (args.cell == null) {\n      throw new ValueError(\n          'cell property is missing for the constructor of RNN.');\n    } else if (Array.isArray(args.cell)) {\n      cell = new StackedRNNCells({cells: args.cell});\n    } else {\n      cell = args.cell;\n    }\n    if (cell.stateSize == null) {\n      throw new ValueError(\n          'The RNN cell should have an attribute `stateSize` (tuple of ' +\n          'integers, one integer per RNN state).');\n    }\n    this.cell = cell;\n    this.returnSequences =\n        args.returnSequences == null ? false : args.returnSequences;\n    this.returnState = args.returnState == null ? false : args.returnState;\n    this.goBackwards = args.goBackwards == null ? false : args.goBackwards;\n    this._stateful = args.stateful == null ? false : args.stateful;\n    this.unroll = args.unroll == null ? false : args.unroll;\n\n    this.supportsMasking = true;\n    this.inputSpec = [new InputSpec({ndim: 3})];\n    this.stateSpec = null;\n    this.states_ = null;\n    // TODO(cais): Add constantsSpec and numConstants.\n    this.numConstants = null;\n    // TODO(cais): Look into the use of initial_state in the kwargs of the\n    //   constructor.\n\n    this.keptStates = [];\n  }\n\n  // Porting Note: This is the equivalent of `RNN.states` property getter in\n  //   PyKeras.\n  getStates(): Tensor[] {\n    if (this.states_ == null) {\n      const numStates =\n          Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;\n      return math_utils.range(0, numStates).map(x => null);\n    } else {\n      return this.states_;\n    }\n  }\n\n  // Porting Note: This is the equivalent of the `RNN.states` property setter in\n  //   PyKeras.\n  setStates(states: Tensor[]): void {\n    this.states_ = states;\n  }\n\n  override computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    if (isArrayOfShapes(inputShape)) {\n      inputShape = (inputShape as Shape[])[0];\n    }\n    inputShape = inputShape as Shape;\n\n    // TODO(cais): Remove the casting once stacked RNN cells become supported.\n    let stateSize = this.cell.stateSize;\n    if (!Array.isArray(stateSize)) {\n      stateSize = [stateSize];\n    }\n    const outputDim = stateSize[0];\n    let outputShape: Shape|Shape[];\n    if (this.returnSequences) {\n      outputShape = [inputShape[0], inputShape[1], outputDim];\n    } else {\n      outputShape = [inputShape[0], outputDim];\n    }\n\n    if (this.returnState) {\n      const stateShape: Shape[] = [];\n      for (const dim of stateSize) {\n        stateShape.push([inputShape[0], dim]);\n      }\n      return [outputShape].concat(stateShape);\n    } else {\n      return outputShape;\n    }\n  }\n\n  override computeMask(inputs: Tensor|Tensor[], mask?: Tensor|Tensor[]): Tensor\n      |Tensor[] {\n    return tfc.tidy(() => {\n      if (Array.isArray(mask)) {\n        mask = mask[0];\n      }\n      const outputMask = this.returnSequences ? mask : null;\n\n      if (this.returnState) {\n        const stateMask = this.states.map(s => null);\n        return [outputMask].concat(stateMask);\n      } else {\n        return outputMask;\n      }\n    });\n  }\n\n  /**\n   * Get the current state tensors of the RNN.\n   *\n   * If the state hasn't been set, return an array of `null`s of the correct\n   * length.\n   */\n  get states(): Tensor[] {\n    if (this.states_ == null) {\n      const numStates =\n          Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;\n      const output: Tensor[] = [];\n      for (let i = 0; i < numStates; ++i) {\n        output.push(null);\n      }\n      return output;\n    } else {\n      return this.states_;\n    }\n  }\n\n  set states(s: Tensor[]) {\n    this.states_ = s;\n  }\n\n  public override build(inputShape: Shape|Shape[]): void {\n    // Note inputShape will be an Array of Shapes of initial states and\n    // constants if these are passed in apply().\n    const constantShape: Shape[] = null;\n    if (this.numConstants != null) {\n      throw new NotImplementedError(\n          'Constants support is not implemented in RNN yet.');\n    }\n\n    if (isArrayOfShapes(inputShape)) {\n      inputShape = (inputShape as Shape[])[0];\n    }\n    inputShape = inputShape as Shape;\n\n    const batchSize: number = this.stateful ? inputShape[0] : null;\n    const inputDim = inputShape.slice(2);\n    this.inputSpec[0] = new InputSpec({shape: [batchSize, null, ...inputDim]});\n\n    // Allow cell (if RNNCell Layer) to build before we set or validate\n    // stateSpec.\n    const stepInputShape = [inputShape[0]].concat(inputShape.slice(2));\n    if (constantShape != null) {\n      throw new NotImplementedError(\n          'Constants support is not implemented in RNN yet.');\n    } else {\n      this.cell.build(stepInputShape);\n    }\n\n    // Set or validate stateSpec.\n    let stateSize: number[];\n    if (Array.isArray(this.cell.stateSize)) {\n      stateSize = this.cell.stateSize;\n    } else {\n      stateSize = [this.cell.stateSize];\n    }\n\n    if (this.stateSpec != null) {\n      if (!util.arraysEqual(\n              this.stateSpec.map(spec => spec.shape[spec.shape.length - 1]),\n              stateSize)) {\n        throw new ValueError(\n            `An initialState was passed that is not compatible with ` +\n            `cell.stateSize. Received stateSpec=${this.stateSpec}; ` +\n            `However cell.stateSize is ${this.cell.stateSize}`);\n      }\n    } else {\n      this.stateSpec =\n          stateSize.map(dim => new InputSpec({shape: [null, dim]}));\n    }\n    if (this.stateful) {\n      this.resetStates();\n    }\n  }\n\n  /**\n   * Reset the state tensors of the RNN.\n   *\n   * If the `states` argument is `undefined` or `null`, will set the\n   * state tensor(s) of the RNN to all-zero tensors of the appropriate\n   * shape(s).\n   *\n   * If `states` is provided, will set the state tensors of the RNN to its\n   * value.\n   *\n   * @param states Optional externally-provided initial states.\n   * @param training Whether this call is done during training. For stateful\n   *   RNNs, this affects whether the old states are kept or discarded. In\n   *   particular, if `training` is `true`, the old states will be kept so\n   *   that subsequent backpropgataion through time (BPTT) may work properly.\n   *   Else, the old states will be discarded.\n   */\n  override resetStates(states?: Tensor|Tensor[], training = false): void {\n    tidy(() => {\n      if (!this.stateful) {\n        throw new AttributeError(\n            'Cannot call resetStates() on an RNN Layer that is not stateful.');\n      }\n      const batchSize = this.inputSpec[0].shape[0];\n      if (batchSize == null) {\n        throw new ValueError(\n            'If an RNN is stateful, it needs to know its batch size. Specify ' +\n            'the batch size of your input tensors: \\n' +\n            '- If using a Sequential model, specify the batch size by ' +\n            'passing a `batchInputShape` option to your first layer.\\n' +\n            '- If using the functional API, specify the batch size by ' +\n            'passing a `batchShape` option to your Input layer.');\n      }\n      // Initialize state if null.\n      if (this.states_ == null) {\n        if (Array.isArray(this.cell.stateSize)) {\n          this.states_ =\n              this.cell.stateSize.map(dim => tfc.zeros([batchSize, dim]));\n        } else {\n          this.states_ = [tfc.zeros([batchSize, this.cell.stateSize])];\n        }\n      } else if (states == null) {\n        // Dispose old state tensors.\n        tfc.dispose(this.states_);\n        // For stateful RNNs, fully dispose kept old states.\n        if (this.keptStates != null) {\n          tfc.dispose(this.keptStates);\n          this.keptStates = [];\n        }\n\n        if (Array.isArray(this.cell.stateSize)) {\n          this.states_ =\n              this.cell.stateSize.map(dim => tfc.zeros([batchSize, dim]));\n        } else {\n          this.states_[0] = tfc.zeros([batchSize, this.cell.stateSize]);\n        }\n      } else {\n        if (!Array.isArray(states)) {\n          states = [states];\n        }\n        if (states.length !== this.states_.length) {\n          throw new ValueError(\n              `Layer ${this.name} expects ${this.states_.length} state(s), ` +\n              `but it received ${states.length} state value(s). Input ` +\n              `received: ${states}`);\n        }\n\n        if (training === true) {\n          // Store old state tensors for complete disposal later, i.e., during\n          // the next no-arg call to this method. We do not dispose the old\n          // states immediately because that BPTT (among other things) require\n          // them.\n          this.keptStates.push(this.states_.slice());\n        } else {\n          tfc.dispose(this.states_);\n        }\n\n        for (let index = 0; index < this.states_.length; ++index) {\n          const value = states[index];\n          const dim = Array.isArray(this.cell.stateSize) ?\n              this.cell.stateSize[index] :\n              this.cell.stateSize;\n          const expectedShape = [batchSize, dim];\n          if (!util.arraysEqual(value.shape, expectedShape)) {\n            throw new ValueError(\n                `State ${index} is incompatible with layer ${this.name}: ` +\n                `expected shape=${expectedShape}, received shape=${\n                    value.shape}`);\n          }\n          this.states_[index] = value;\n        }\n      }\n      this.states_ = this.states_.map(state => tfc.keep(state.clone()));\n    });\n  }\n\n  override apply(\n      inputs: Tensor|Tensor[]|SymbolicTensor|SymbolicTensor[],\n      kwargs?: Kwargs): Tensor|Tensor[]|SymbolicTensor|SymbolicTensor[] {\n    // TODO(cais): Figure out whether initialState is in kwargs or inputs.\n    let initialState: Tensor[]|SymbolicTensor[] =\n        kwargs == null ? null : kwargs['initialState'];\n    let constants: Tensor[]|SymbolicTensor[] =\n        kwargs == null ? null : kwargs['constants'];\n    if (kwargs == null) {\n      kwargs = {};\n    }\n\n    const standardized =\n        standardizeArgs(inputs, initialState, constants, this.numConstants);\n    inputs = standardized.inputs;\n    initialState = standardized.initialState;\n    constants = standardized.constants;\n\n    // If any of `initial_state` or `constants` are specified and are\n    // `tf.SymbolicTensor`s, then add them to the inputs and temporarily modify\n    // the input_spec to include them.\n\n    let additionalInputs: Array<Tensor|SymbolicTensor> = [];\n    let additionalSpecs: InputSpec[] = [];\n    if (initialState != null) {\n      kwargs['initialState'] = initialState;\n      additionalInputs = additionalInputs.concat(initialState);\n      this.stateSpec = [];\n      for (const state of initialState) {\n        this.stateSpec.push(new InputSpec({shape: state.shape}));\n      }\n      // TODO(cais): Use the following instead.\n      // this.stateSpec = initialState.map(state => new InputSpec({shape:\n      // state.shape}));\n      additionalSpecs = additionalSpecs.concat(this.stateSpec);\n    }\n    if (constants != null) {\n      kwargs['constants'] = constants;\n      additionalInputs = additionalInputs.concat(constants);\n      // TODO(cais): Add this.constantsSpec.\n      this.numConstants = constants.length;\n    }\n\n    const isTensor = additionalInputs[0] instanceof SymbolicTensor;\n    if (isTensor) {\n      // Compute full input spec, including state and constants.\n      const fullInput =\n          [inputs].concat(additionalInputs) as Tensor[] | SymbolicTensor[];\n      const fullInputSpec = this.inputSpec.concat(additionalSpecs);\n      // Perform the call with temporarily replaced inputSpec.\n      const originalInputSpec = this.inputSpec;\n      this.inputSpec = fullInputSpec;\n      const output = super.apply(fullInput, kwargs);\n      this.inputSpec = originalInputSpec;\n      return output;\n    } else {\n      return super.apply(inputs, kwargs);\n    }\n  }\n\n  // tslint:disable-next-line:no-any\n  override call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    // Input shape: `[samples, time (padded with zeros), input_dim]`.\n    // Note that the .build() method of subclasses **must** define\n    // this.inputSpec and this.stateSpec owith complete input shapes.\n    return tidy(() => {\n      const mask = kwargs == null ? null : kwargs['mask'] as Tensor;\n      const training = kwargs == null ? null : kwargs['training'];\n      let initialState: Tensor[] =\n          kwargs == null ? null : kwargs['initialState'];\n\n      inputs = getExactlyOneTensor(inputs);\n      if (initialState == null) {\n        if (this.stateful) {\n          initialState = this.states_;\n        } else {\n          initialState = this.getInitialState(inputs);\n        }\n      }\n\n      const numStates =\n          Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;\n      if (initialState.length !== numStates) {\n        throw new ValueError(\n            `RNN Layer has ${numStates} state(s) but was passed ` +\n            `${initialState.length} initial state(s).`);\n      }\n      if (this.unroll) {\n        console.warn(\n            'Ignoring unroll = true for RNN layer, due to imperative backend.');\n      }\n\n      const cellCallKwargs: Kwargs = {training};\n\n      // TODO(cais): Add support for constants.\n      const step = (inputs: Tensor, states: Tensor[]) => {\n        // `inputs` and `states` are concatenated to form a single `Array` of\n        // `tf.Tensor`s as the input to `cell.call()`.\n        const outputs =\n            this.cell.call([inputs].concat(states), cellCallKwargs) as Tensor[];\n        // Marshall the return value into output and new states.\n        return [outputs[0], outputs.slice(1)] as [Tensor, Tensor[]];\n      };\n\n      // TODO(cais): Add support for constants.\n\n      const rnnOutputs =\n          rnn(step, inputs, initialState, this.goBackwards, mask, null,\n              this.unroll, this.returnSequences);\n      const lastOutput = rnnOutputs[0];\n      const outputs = rnnOutputs[1];\n      const states = rnnOutputs[2];\n\n      if (this.stateful) {\n        this.resetStates(states, training);\n      }\n\n      const output = this.returnSequences ? outputs : lastOutput;\n\n      // TODO(cais): Porperty set learning phase flag.\n\n      if (this.returnState) {\n        return [output].concat(states);\n      } else {\n        return output;\n      }\n    });\n  }\n\n  getInitialState(inputs: Tensor): Tensor[] {\n    return tidy(() => {\n      // Build an all-zero tensor of shape [samples, outputDim].\n      // [Samples, timeSteps, inputDim].\n      let initialState = tfc.zeros(inputs.shape);\n      // [Samples].\n      initialState = tfc.sum(initialState, [1, 2]);\n      initialState = K.expandDims(initialState);  // [Samples, 1].\n\n      if (Array.isArray(this.cell.stateSize)) {\n        return this.cell.stateSize.map(\n            dim => dim > 1 ? K.tile(initialState, [1, dim]) : initialState);\n      } else {\n        return this.cell.stateSize > 1 ?\n            [K.tile(initialState, [1, this.cell.stateSize])] :\n            [initialState];\n      }\n    });\n  }\n\n  override get trainableWeights(): LayerVariable[] {\n    if (!this.trainable) {\n      return [];\n    }\n    // Porting Note: In TypeScript, `this` is always an instance of `Layer`.\n    return this.cell.trainableWeights;\n  }\n\n  override get nonTrainableWeights(): LayerVariable[] {\n    // Porting Note: In TypeScript, `this` is always an instance of `Layer`.\n    if (!this.trainable) {\n      return this.cell.weights;\n    }\n    return this.cell.nonTrainableWeights;\n  }\n\n  override setFastWeightInitDuringBuild(value: boolean) {\n    super.setFastWeightInitDuringBuild(value);\n    if (this.cell != null) {\n      this.cell.setFastWeightInitDuringBuild(value);\n    }\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    const baseConfig = super.getConfig();\n\n    const config: serialization.ConfigDict = {\n      returnSequences: this.returnSequences,\n      returnState: this.returnState,\n      goBackwards: this.goBackwards,\n      stateful: this.stateful,\n      unroll: this.unroll,\n    };\n\n    if (this.numConstants != null) {\n      config['numConstants'] = this.numConstants;\n    }\n\n    const cellConfig = this.cell.getConfig();\n\n    if (this.getClassName() === RNN.className) {\n      config['cell'] = {\n        'className': this.cell.getClassName(),\n        'config': cellConfig,\n      } as serialization.ConfigDictValue;\n    }\n\n    // this order is necessary, to prevent cell name from replacing layer name\n    return {...cellConfig, ...baseConfig, ...config};\n  }\n\n  /** @nocollapse */\n  static override fromConfig<T extends serialization.Serializable>(\n      cls: serialization.SerializableConstructor<T>,\n      config: serialization.ConfigDict,\n      customObjects = {} as serialization.ConfigDict): T {\n    const cellConfig = config['cell'] as serialization.ConfigDict;\n    const cell = deserialize(cellConfig, customObjects) as RNNCell;\n    return new cls(Object.assign(config, {cell}));\n  }\n}\nserialization.registerClass(RNN);\n\n// Porting Note: This is a common parent class for RNN cells. There is no\n// equivalent of this in PyKeras. Having a common parent class forgoes the\n//  need for `has_attr(cell, ...)` checks or its TypeScript equivalent.\n/**\n * An RNNCell layer.\n *\n * @doc {heading: 'Layers', subheading: 'Classes'}\n */\nexport abstract class RNNCell extends Layer {\n  /**\n   * Size(s) of the states.\n   * For RNN cells with only a single state, this is a single integer.\n   */\n  // See\n  // https://www.typescriptlang.org/docs/handbook/release-notes/typescript-4-0.html#properties-overriding-accessors-and-vice-versa-is-an-error\n  public abstract stateSize: number|number[];\n  public dropoutMask: Tensor|Tensor[];\n  public recurrentDropoutMask: Tensor|Tensor[];\n}\n\nexport declare interface SimpleRNNCellLayerArgs extends LayerArgs {\n  /**\n   * units: Positive integer, dimensionality of the output space.\n   */\n  units: number;\n\n  /**\n   * Activation function to use.\n   * Default: hyperbolic tangent ('tanh').\n   * If you pass `null`,  'linear' activation will be applied.\n   */\n  activation?: ActivationIdentifier;\n\n  /**\n   * Whether the layer uses a bias vector.\n   */\n  useBias?: boolean;\n\n  /**\n   * Initializer for the `kernel` weights matrix, used for the linear\n   * transformation of the inputs.\n   */\n  kernelInitializer?: InitializerIdentifier|Initializer;\n\n  /**\n   * Initializer for the `recurrentKernel` weights matrix, used for\n   * linear transformation of the recurrent state.\n   */\n  recurrentInitializer?: InitializerIdentifier|Initializer;\n\n  /**\n   * Initializer for the bias vector.\n   */\n  biasInitializer?: InitializerIdentifier|Initializer;\n\n  /**\n   * Regularizer function applied to the `kernel` weights matrix.\n   */\n  kernelRegularizer?: RegularizerIdentifier|Regularizer;\n\n  /**\n   * Regularizer function applied to the `recurrent_kernel` weights matrix.\n   */\n  recurrentRegularizer?: RegularizerIdentifier|Regularizer;\n\n  /**\n   * Regularizer function applied to the bias vector.\n   */\n  biasRegularizer?: RegularizerIdentifier|Regularizer;\n\n  /**\n   * Constraint function applied to the `kernel` weights matrix.\n   */\n  kernelConstraint?: ConstraintIdentifier|Constraint;\n\n  /**\n   * Constraint function applied to the `recurrentKernel` weights matrix.\n   */\n  recurrentConstraint?: ConstraintIdentifier|Constraint;\n\n  /**\n   * Constraint function applied to the bias vector.\n   */\n  biasConstraint?: ConstraintIdentifier|Constraint;\n\n  /**\n   * Float number between 0 and 1. Fraction of the units to drop for the linear\n   * transformation of the inputs.\n   */\n  dropout?: number;\n\n  /**\n   * Float number between 0 and 1. Fraction of the units to drop for the linear\n   * transformation of the recurrent state.\n   */\n  recurrentDropout?: number;\n\n  /**\n   * This is added for test DI purpose.\n   */\n  dropoutFunc?: Function;\n}\n\nexport class SimpleRNNCell extends RNNCell {\n  /** @nocollapse */\n  static className = 'SimpleRNNCell';\n  readonly units: number;\n  readonly activation: Activation;\n  readonly useBias: boolean;\n\n  readonly kernelInitializer: Initializer;\n  readonly recurrentInitializer: Initializer;\n  readonly biasInitializer: Initializer;\n\n  readonly kernelConstraint: Constraint;\n  readonly recurrentConstraint: Constraint;\n  readonly biasConstraint: Constraint;\n\n  readonly kernelRegularizer: Regularizer;\n  readonly recurrentRegularizer: Regularizer;\n  readonly biasRegularizer: Regularizer;\n\n  readonly dropout: number;\n  readonly recurrentDropout: number;\n  readonly dropoutFunc: Function;\n\n  readonly stateSize: number;\n\n  kernel: LayerVariable;\n  recurrentKernel: LayerVariable;\n  bias: LayerVariable;\n\n  readonly DEFAULT_ACTIVATION = 'tanh';\n  readonly DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n  readonly DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n  readonly DEFAULT_BIAS_INITIALIZER: InitializerIdentifier = 'zeros';\n\n  constructor(args: SimpleRNNCellLayerArgs) {\n    super(args);\n    this.units = args.units;\n    assertPositiveInteger(this.units, `units`);\n    this.activation = getActivation(\n        args.activation == null ? this.DEFAULT_ACTIVATION : args.activation);\n    this.useBias = args.useBias == null ? true : args.useBias;\n\n    this.kernelInitializer = getInitializer(\n        args.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER);\n    this.recurrentInitializer = getInitializer(\n        args.recurrentInitializer || this.DEFAULT_RECURRENT_INITIALIZER);\n\n    this.biasInitializer =\n        getInitializer(args.biasInitializer || this.DEFAULT_BIAS_INITIALIZER);\n\n    this.kernelRegularizer = getRegularizer(args.kernelRegularizer);\n    this.recurrentRegularizer = getRegularizer(args.recurrentRegularizer);\n    this.biasRegularizer = getRegularizer(args.biasRegularizer);\n\n    this.kernelConstraint = getConstraint(args.kernelConstraint);\n    this.recurrentConstraint = getConstraint(args.recurrentConstraint);\n    this.biasConstraint = getConstraint(args.biasConstraint);\n\n    this.dropout = math_utils.min(\n        [1, math_utils.max([0, args.dropout == null ? 0 : args.dropout])]);\n    this.recurrentDropout = math_utils.min([\n      1,\n      math_utils.max(\n          [0, args.recurrentDropout == null ? 0 : args.recurrentDropout])\n    ]);\n    this.dropoutFunc = args.dropoutFunc;\n    this.stateSize = this.units;\n    this.dropoutMask = null;\n    this.recurrentDropoutMask = null;\n  }\n\n  override build(inputShape: Shape|Shape[]): void {\n    inputShape = getExactlyOneShape(inputShape);\n    // TODO(cais): Use regularizer.\n    this.kernel = this.addWeight(\n        'kernel', [inputShape[inputShape.length - 1], this.units], null,\n        this.kernelInitializer, this.kernelRegularizer, true,\n        this.kernelConstraint);\n    this.recurrentKernel = this.addWeight(\n        'recurrent_kernel', [this.units, this.units], null,\n        this.recurrentInitializer, this.recurrentRegularizer, true,\n        this.recurrentConstraint);\n    if (this.useBias) {\n      this.bias = this.addWeight(\n          'bias', [this.units], null, this.biasInitializer,\n          this.biasRegularizer, true, this.biasConstraint);\n    } else {\n      this.bias = null;\n    }\n    this.built = true;\n  }\n\n  // Porting Note: PyKeras' equivalent of this method takes two tensor inputs:\n  //   `inputs` and `states`. Here, the two tensors are combined into an\n  //   `Tensor[]` Array as the first input argument.\n  //   Similarly, PyKeras' equivalent of this method returns two values:\n  //    `output` and `[output]`. Here the two are combined into one length-2\n  //    `Tensor[]`, consisting of `output` repeated.\n  override call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      inputs = inputs as Tensor[];\n      if (inputs.length !== 2) {\n        throw new ValueError(\n            `SimpleRNNCell expects 2 input Tensors, got ${inputs.length}.`);\n      }\n      let prevOutput = inputs[1];\n      inputs = inputs[0];\n      const training = kwargs['training'] == null ? false : kwargs['training'];\n\n      if (0 < this.dropout && this.dropout < 1 && this.dropoutMask == null) {\n        this.dropoutMask = generateDropoutMask({\n                             ones: () => tfc.onesLike(inputs as Tensor),\n                             rate: this.dropout,\n                             training,\n                             dropoutFunc: this.dropoutFunc,\n                           }) as Tensor;\n      }\n      if (0 < this.recurrentDropout && this.recurrentDropout < 1 &&\n          this.recurrentDropoutMask == null) {\n        this.recurrentDropoutMask = generateDropoutMask({\n                                      ones: () => tfc.onesLike(prevOutput),\n                                      rate: this.recurrentDropout,\n                                      training,\n                                      dropoutFunc: this.dropoutFunc,\n                                    }) as Tensor;\n      }\n      let h: Tensor;\n      const dpMask: Tensor = this.dropoutMask as Tensor;\n      const recDpMask: Tensor = this.recurrentDropoutMask as Tensor;\n      if (dpMask != null) {\n        h = K.dot(tfc.mul(inputs, dpMask), this.kernel.read());\n      } else {\n        h = K.dot(inputs, this.kernel.read());\n      }\n      if (this.bias != null) {\n        h = K.biasAdd(h, this.bias.read());\n      }\n      if (recDpMask != null) {\n        prevOutput = tfc.mul(prevOutput, recDpMask);\n      }\n      let output = tfc.add(h, K.dot(prevOutput, this.recurrentKernel.read()));\n      if (this.activation != null) {\n        output = this.activation.apply(output);\n      }\n\n      // TODO(cais): Properly set learning phase on output tensor?\n      return [output, output];\n    });\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    const baseConfig = super.getConfig();\n\n    const config: serialization.ConfigDict = {\n      units: this.units,\n      activation: serializeActivation(this.activation),\n      useBias: this.useBias,\n      kernelInitializer: serializeInitializer(this.kernelInitializer),\n      recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n      biasInitializer: serializeInitializer(this.biasInitializer),\n      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n      recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n      biasRegularizer: serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: serializeConstraint(this.kernelConstraint),\n      recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n      biasConstraint: serializeConstraint(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout,\n    };\n\n    return {...baseConfig, ...config};\n  }\n}\nserialization.registerClass(SimpleRNNCell);\n\nexport declare interface SimpleRNNLayerArgs extends BaseRNNLayerArgs {\n  /**\n   * Positive integer, dimensionality of the output space.\n   */\n  units: number;\n\n  /**\n   * Activation function to use.\n   *\n   * Defaults to  hyperbolic tangent (`tanh`)\n   *\n   * If you pass `null`, no activation will be applied.\n   */\n  activation?: ActivationIdentifier;\n\n  /**\n   * Whether the layer uses a bias vector.\n   */\n  useBias?: boolean;\n\n  /**\n   * Initializer for the `kernel` weights matrix, used for the linear\n   * transformation of the inputs.\n   */\n  kernelInitializer?: InitializerIdentifier|Initializer;\n\n  /**\n   * Initializer for the `recurrentKernel` weights matrix, used for\n   * linear transformation of the recurrent state.\n   */\n  recurrentInitializer?: InitializerIdentifier|Initializer;\n\n  /**\n   * Initializer for the bias vector.\n   */\n  biasInitializer?: InitializerIdentifier|Initializer;\n\n  /**\n   * Regularizer function applied to the kernel weights matrix.\n   */\n  kernelRegularizer?: RegularizerIdentifier|Regularizer;\n\n  /**\n   * Regularizer function applied to the recurrentKernel weights matrix.\n   */\n  recurrentRegularizer?: RegularizerIdentifier|Regularizer;\n\n  /**\n   * Regularizer function applied to the bias vector.\n   */\n  biasRegularizer?: RegularizerIdentifier|Regularizer;\n\n  /**\n   * Constraint function applied to the kernel weights matrix.\n   */\n  kernelConstraint?: ConstraintIdentifier|Constraint;\n\n  /**\n   * Constraint function applied to the recurrentKernel weights matrix.\n   */\n  recurrentConstraint?: ConstraintIdentifier|Constraint;\n\n  /**\n   * Constraint function applied to the bias vector.\n   */\n  biasConstraint?: ConstraintIdentifier|Constraint;\n\n  /**\n   * Number between 0 and 1. Fraction of the units to drop for the linear\n   * transformation of the inputs.\n   */\n  dropout?: number;\n\n  /**\n   * Number between 0 and 1. Fraction of the units to drop for the linear\n   * transformation of the recurrent state.\n   */\n  recurrentDropout?: number;\n\n  /**\n   * This is added for test DI purpose.\n   */\n  dropoutFunc?: Function;\n}\n\n/**\n * RNNLayerConfig is identical to BaseRNNLayerConfig, except it makes the\n * `cell` property required. This interface is to be used with constructors\n * of concrete RNN layer subtypes.\n */\nexport declare interface RNNLayerArgs extends BaseRNNLayerArgs {\n  cell: RNNCell|RNNCell[];\n}\n\nexport class SimpleRNN extends RNN {\n  /** @nocollapse */\n  static override className = 'SimpleRNN';\n  constructor(args: SimpleRNNLayerArgs) {\n    args.cell = new SimpleRNNCell(args);\n    super(args as RNNLayerArgs);\n    // TODO(cais): Add activityRegularizer.\n  }\n\n  override call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      if (this.cell.dropoutMask != null) {\n        tfc.dispose(this.cell.dropoutMask);\n        this.cell.dropoutMask = null;\n      }\n      if (this.cell.recurrentDropoutMask != null) {\n        tfc.dispose(this.cell.recurrentDropoutMask);\n        this.cell.recurrentDropoutMask = null;\n      }\n      const mask = kwargs == null ? null : kwargs['mask'];\n      const training = kwargs == null ? null : kwargs['training'];\n      const initialState: Tensor[] =\n          kwargs == null ? null : kwargs['initialState'];\n      return super.call(inputs, {mask, training, initialState});\n    });\n  }\n\n  /** @nocollapse */\n  static override fromConfig<T extends serialization.Serializable>(\n      cls: serialization.SerializableConstructor<T>,\n      config: serialization.ConfigDict): T {\n    return new cls(config);\n  }\n}\nserialization.registerClass(SimpleRNN);\n\n// Porting Note: Since this is a superset of SimpleRNNLayerConfig, we extend\n//   that interface instead of repeating the fields.\nexport declare interface GRUCellLayerArgs extends SimpleRNNCellLayerArgs {\n  /**\n   * Activation function to use for the recurrent step.\n   *\n   * Defaults to hard sigmoid (`hardSigmoid`).\n   *\n   * If `null`, no activation is applied.\n   */\n  recurrentActivation?: ActivationIdentifier;\n\n  /**\n   * Implementation mode, either 1 or 2.\n   *\n   * Mode 1 will structure its operations as a larger number of\n   *   smaller dot products and additions.\n   *\n   * Mode 2 will batch them into fewer, larger operations. These modes will\n   * have different performance profiles on different hardware and\n   * for different applications.\n   *\n   * Note: For superior performance, TensorFlow.js always uses implementation\n   * 2, regardless of the actual value of this configuration field.\n   */\n  implementation?: number;\n\n  /**\n   * GRU convention (whether to apply reset gate after or before matrix\n   * multiplication). false = \"before\", true = \"after\" (only false is\n   * supported).\n   */\n  resetAfter?: boolean;\n}\n\nexport class GRUCell extends RNNCell {\n  /** @nocollapse */\n  static className = 'GRUCell';\n  readonly units: number;\n  readonly activation: Activation;\n  readonly recurrentActivation: Activation;\n  readonly useBias: boolean;\n\n  readonly kernelInitializer: Initializer;\n  readonly recurrentInitializer: Initializer;\n  readonly biasInitializer: Initializer;\n\n  readonly kernelRegularizer: Regularizer;\n  readonly recurrentRegularizer: Regularizer;\n  readonly biasRegularizer: Regularizer;\n\n  readonly kernelConstraint: Constraint;\n  readonly recurrentConstraint: Constraint;\n  readonly biasConstraint: Constraint;\n\n  readonly dropout: number;\n  readonly recurrentDropout: number;\n  readonly dropoutFunc: Function;\n\n  readonly stateSize: number;\n  readonly implementation: number;\n\n  readonly DEFAULT_ACTIVATION = 'tanh';\n  readonly DEFAULT_RECURRENT_ACTIVATION: ActivationIdentifier = 'hardSigmoid';\n\n  readonly DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n  readonly DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n  readonly DEFAULT_BIAS_INITIALIZER: InitializerIdentifier = 'zeros';\n\n  kernel: LayerVariable;\n  recurrentKernel: LayerVariable;\n  bias: LayerVariable;\n\n  constructor(args: GRUCellLayerArgs) {\n    super(args);\n    if (args.resetAfter) {\n      throw new ValueError(\n          `GRUCell does not support reset_after parameter set to true.`);\n    }\n    this.units = args.units;\n    assertPositiveInteger(this.units, 'units');\n    this.activation = getActivation(\n        args.activation === undefined ? this.DEFAULT_ACTIVATION :\n                                        args.activation);\n    this.recurrentActivation = getActivation(\n        args.recurrentActivation === undefined ?\n            this.DEFAULT_RECURRENT_ACTIVATION :\n            args.recurrentActivation);\n    this.useBias = args.useBias == null ? true : args.useBias;\n\n    this.kernelInitializer = getInitializer(\n        args.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER);\n    this.recurrentInitializer = getInitializer(\n        args.recurrentInitializer || this.DEFAULT_RECURRENT_INITIALIZER);\n\n    this.biasInitializer =\n        getInitializer(args.biasInitializer || this.DEFAULT_BIAS_INITIALIZER);\n\n    this.kernelRegularizer = getRegularizer(args.kernelRegularizer);\n    this.recurrentRegularizer = getRegularizer(args.recurrentRegularizer);\n    this.biasRegularizer = getRegularizer(args.biasRegularizer);\n\n    this.kernelConstraint = getConstraint(args.kernelConstraint);\n    this.recurrentConstraint = getConstraint(args.recurrentConstraint);\n    this.biasConstraint = getConstraint(args.biasConstraint);\n\n    this.dropout = math_utils.min(\n        [1, math_utils.max([0, args.dropout == null ? 0 : args.dropout])]);\n    this.recurrentDropout = math_utils.min([\n      1,\n      math_utils.max(\n          [0, args.recurrentDropout == null ? 0 : args.recurrentDropout])\n    ]);\n    this.dropoutFunc = args.dropoutFunc;\n    this.implementation = args.implementation;\n    this.stateSize = this.units;\n    this.dropoutMask = null;\n    this.recurrentDropoutMask = null;\n  }\n\n  public override build(inputShape: Shape|Shape[]): void {\n    inputShape = getExactlyOneShape(inputShape);\n    const inputDim = inputShape[inputShape.length - 1];\n    this.kernel = this.addWeight(\n        'kernel', [inputDim, this.units * 3], null, this.kernelInitializer,\n        this.kernelRegularizer, true, this.kernelConstraint);\n    this.recurrentKernel = this.addWeight(\n        'recurrent_kernel', [this.units, this.units * 3], null,\n        this.recurrentInitializer, this.recurrentRegularizer, true,\n        this.recurrentConstraint);\n    if (this.useBias) {\n      this.bias = this.addWeight(\n          'bias', [this.units * 3], null, this.biasInitializer,\n          this.biasRegularizer, true, this.biasConstraint);\n    } else {\n      this.bias = null;\n    }\n    // Porting Notes: Unlike the PyKeras implementation, we perform slicing\n    //   of the weights and bias in the call() method, at execution time.\n    this.built = true;\n  }\n\n  override call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      inputs = inputs as Tensor[];\n      if (inputs.length !== 2) {\n        throw new ValueError(\n            `GRUCell expects 2 input Tensors (inputs, h, c), got ` +\n            `${inputs.length}.`);\n      }\n\n      const training = kwargs['training'] == null ? false : kwargs['training'];\n      let hTMinus1 = inputs[1];  // Previous memory state.\n      inputs = inputs[0];\n\n      // Note: For superior performance, TensorFlow.js always uses\n      // implementation 2, regardless of the actual value of\n      // config.implementation.\n      if (0 < this.dropout && this.dropout < 1 && this.dropoutMask == null) {\n        this.dropoutMask = generateDropoutMask({\n                             ones: () => tfc.onesLike(inputs as Tensor),\n                             rate: this.dropout,\n                             training,\n                             count: 3,\n                             dropoutFunc: this.dropoutFunc,\n                           }) as Tensor[];\n      }\n      if (0 < this.recurrentDropout && this.recurrentDropout < 1 &&\n          this.recurrentDropoutMask == null) {\n        this.recurrentDropoutMask = generateDropoutMask({\n                                      ones: () => tfc.onesLike(hTMinus1),\n                                      rate: this.recurrentDropout,\n                                      training,\n                                      count: 3,\n                                      dropoutFunc: this.dropoutFunc,\n                                    }) as Tensor[];\n      }\n      const dpMask = this.dropoutMask as [Tensor, Tensor, Tensor];\n      const recDpMask = this.recurrentDropoutMask as [Tensor, Tensor, Tensor];\n      let z: Tensor;\n      let r: Tensor;\n      let hh: Tensor;\n\n      if (0 < this.dropout && this.dropout < 1) {\n        inputs = tfc.mul(inputs, dpMask[0]);\n      }\n      let matrixX = K.dot(inputs, this.kernel.read());\n      if (this.useBias) {\n        matrixX = K.biasAdd(matrixX, this.bias.read());\n      }\n      if (0 < this.recurrentDropout && this.recurrentDropout < 1) {\n        hTMinus1 = tfc.mul(hTMinus1, recDpMask[0]);\n      }\n\n      const recurrentKernelValue = this.recurrentKernel.read();\n      const [rk1, rk2] = tfc.split(\n          recurrentKernelValue, [2 * this.units, this.units],\n          recurrentKernelValue.rank - 1);\n      const matrixInner = K.dot(hTMinus1, rk1);\n\n      const [xZ, xR, xH] = tfc.split(matrixX, 3, matrixX.rank - 1);\n      const [recurrentZ, recurrentR] =\n          tfc.split(matrixInner, 2, matrixInner.rank - 1);\n      z = this.recurrentActivation.apply(tfc.add(xZ, recurrentZ));\n      r = this.recurrentActivation.apply(tfc.add(xR, recurrentR));\n\n      const recurrentH = K.dot(tfc.mul(r, hTMinus1), rk2);\n      hh = this.activation.apply(tfc.add(xH, recurrentH));\n\n      const h =\n          tfc.add(tfc.mul(z, hTMinus1), tfc.mul(tfc.add(1, tfc.neg(z)), hh));\n      // TODO(cais): Add use_learning_phase flag properly.\n      return [h, h];\n    });\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    const baseConfig = super.getConfig();\n\n    const config: serialization.ConfigDict = {\n      units: this.units,\n      activation: serializeActivation(this.activation),\n      recurrentActivation: serializeActivation(this.recurrentActivation),\n      useBias: this.useBias,\n      kernelInitializer: serializeInitializer(this.kernelInitializer),\n      recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n      biasInitializer: serializeInitializer(this.biasInitializer),\n      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n      recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n      biasRegularizer: serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: serializeConstraint(this.kernelConstraint),\n      recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n      biasConstraint: serializeConstraint(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout,\n      implementation: this.implementation,\n      resetAfter: false\n    };\n\n    return {...baseConfig, ...config};\n  }\n}\nserialization.registerClass(GRUCell);\n\n// Porting Note: Since this is a superset of SimpleRNNLayerConfig, we inherit\n//   from that interface instead of repeating the fields here.\nexport declare interface GRULayerArgs extends SimpleRNNLayerArgs {\n  /**\n   * Activation function to use for the recurrent step.\n   *\n   * Defaults to hard sigmoid (`hardSigmoid`).\n   *\n   * If `null`, no activation is applied.\n   */\n  recurrentActivation?: ActivationIdentifier;\n\n  /**\n   * Implementation mode, either 1 or 2.\n   *\n   * Mode 1 will structure its operations as a larger number of\n   * smaller dot products and additions.\n   *\n   * Mode 2 will batch them into fewer, larger operations. These modes will\n   * have different performance profiles on different hardware and\n   * for different applications.\n   *\n   * Note: For superior performance, TensorFlow.js always uses implementation\n   * 2, regardless of the actual value of this configuration field.\n   */\n  implementation?: number;\n}\n\nexport class GRU extends RNN {\n  /** @nocollapse */\n  static override className = 'GRU';\n  constructor(args: GRULayerArgs) {\n    if (args.implementation === 0) {\n      console.warn(\n          '`implementation=0` has been deprecated, and now defaults to ' +\n          '`implementation=1`. Please update your layer call.');\n    }\n    args.cell = new GRUCell(args);\n    super(args as RNNLayerArgs);\n    // TODO(cais): Add activityRegularizer.\n  }\n\n  override call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      if (this.cell.dropoutMask != null) {\n        tfc.dispose(this.cell.dropoutMask);\n        this.cell.dropoutMask = null;\n      }\n      if (this.cell.recurrentDropoutMask != null) {\n        tfc.dispose(this.cell.recurrentDropoutMask);\n        this.cell.recurrentDropoutMask = null;\n      }\n      const mask = kwargs == null ? null : kwargs['mask'];\n      const training = kwargs == null ? null : kwargs['training'];\n      const initialState: Tensor[] =\n          kwargs == null ? null : kwargs['initialState'];\n      return super.call(inputs, {mask, training, initialState});\n    });\n  }\n\n  /** @nocollapse */\n  static override fromConfig<T extends serialization.Serializable>(\n      cls: serialization.SerializableConstructor<T>,\n      config: serialization.ConfigDict): T {\n    if (config['implmentation'] === 0) {\n      config['implementation'] = 1;\n    }\n    return new cls(config);\n  }\n}\nserialization.registerClass(GRU);\n\n// Porting Note: Since this is a superset of SimpleRNNLayerConfig, we extend\n//   that interface instead of repeating the fields.\nexport declare interface LSTMCellLayerArgs extends SimpleRNNCellLayerArgs {\n  /**\n   * Activation function to use for the recurrent step.\n   *\n   * Defaults to hard sigmoid (`hardSigmoid`).\n   *\n   * If `null`, no activation is applied.\n   */\n  recurrentActivation?: ActivationIdentifier;\n\n  /**\n   * If `true`, add 1 to the bias of the forget gate at initialization.\n   * Setting it to `true` will also force `biasInitializer = 'zeros'`.\n   * This is recommended in\n   * [Jozefowicz et\n   * al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n   */\n  unitForgetBias?: boolean;\n\n  /**\n   * Implementation mode, either 1 or 2.\n   *\n   * Mode 1 will structure its operations as a larger number of\n   *   smaller dot products and additions.\n   *\n   * Mode 2 will batch them into fewer, larger operations. These modes will\n   * have different performance profiles on different hardware and\n   * for different applications.\n   *\n   * Note: For superior performance, TensorFlow.js always uses implementation\n   * 2, regardless of the actual value of this configuration field.\n   */\n  implementation?: number;\n}\n\nexport class LSTMCell extends RNNCell {\n  /** @nocollapse */\n  static className = 'LSTMCell';\n  readonly units: number;\n  readonly activation: Activation;\n  readonly recurrentActivation: Activation;\n  readonly useBias: boolean;\n\n  readonly kernelInitializer: Initializer;\n  readonly recurrentInitializer: Initializer;\n  readonly biasInitializer: Initializer;\n  readonly unitForgetBias: boolean;\n\n  readonly kernelConstraint: Constraint;\n  readonly recurrentConstraint: Constraint;\n  readonly biasConstraint: Constraint;\n\n  readonly kernelRegularizer: Regularizer;\n  readonly recurrentRegularizer: Regularizer;\n  readonly biasRegularizer: Regularizer;\n\n  readonly dropout: number;\n  readonly recurrentDropout: number;\n  readonly dropoutFunc: Function;\n\n  readonly stateSize: number[];\n  readonly implementation: number;\n\n  readonly DEFAULT_ACTIVATION = 'tanh';\n  readonly DEFAULT_RECURRENT_ACTIVATION = 'hardSigmoid';\n  readonly DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n  readonly DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n\n  readonly DEFAULT_BIAS_INITIALIZER = 'zeros';\n\n  kernel: LayerVariable;\n  recurrentKernel: LayerVariable;\n  bias: LayerVariable;\n\n  constructor(args: LSTMCellLayerArgs) {\n    super(args);\n\n    this.units = args.units;\n    assertPositiveInteger(this.units, 'units');\n    this.activation = getActivation(\n        args.activation === undefined ? this.DEFAULT_ACTIVATION :\n                                        args.activation);\n    this.recurrentActivation = getActivation(\n        args.recurrentActivation === undefined ?\n            this.DEFAULT_RECURRENT_ACTIVATION :\n            args.recurrentActivation);\n    this.useBias = args.useBias == null ? true : args.useBias;\n\n    this.kernelInitializer = getInitializer(\n        args.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER);\n    this.recurrentInitializer = getInitializer(\n        args.recurrentInitializer || this.DEFAULT_RECURRENT_INITIALIZER);\n\n    this.biasInitializer =\n        getInitializer(args.biasInitializer || this.DEFAULT_BIAS_INITIALIZER);\n    this.unitForgetBias = args.unitForgetBias;\n\n    this.kernelRegularizer = getRegularizer(args.kernelRegularizer);\n    this.recurrentRegularizer = getRegularizer(args.recurrentRegularizer);\n    this.biasRegularizer = getRegularizer(args.biasRegularizer);\n\n    this.kernelConstraint = getConstraint(args.kernelConstraint);\n    this.recurrentConstraint = getConstraint(args.recurrentConstraint);\n    this.biasConstraint = getConstraint(args.biasConstraint);\n\n    this.dropout = math_utils.min(\n        [1, math_utils.max([0, args.dropout == null ? 0 : args.dropout])]);\n    this.recurrentDropout = math_utils.min([\n      1,\n      math_utils.max(\n          [0, args.recurrentDropout == null ? 0 : args.recurrentDropout])\n    ]);\n    this.dropoutFunc = args.dropoutFunc;\n    this.implementation = args.implementation;\n    this.stateSize = [this.units, this.units];\n    this.dropoutMask = null;\n    this.recurrentDropoutMask = null;\n  }\n\n  public override build(inputShape: Shape|Shape[]): void {\n    inputShape = getExactlyOneShape(inputShape);\n    const inputDim = inputShape[inputShape.length - 1];\n    this.kernel = this.addWeight(\n        'kernel', [inputDim, this.units * 4], null, this.kernelInitializer,\n        this.kernelRegularizer, true, this.kernelConstraint);\n    this.recurrentKernel = this.addWeight(\n        'recurrent_kernel', [this.units, this.units * 4], null,\n        this.recurrentInitializer, this.recurrentRegularizer, true,\n        this.recurrentConstraint);\n    let biasInitializer: Initializer;\n    if (this.useBias) {\n      if (this.unitForgetBias) {\n        const capturedBiasInit = this.biasInitializer;\n        const capturedUnits = this.units;\n        biasInitializer = new (class CustomInit extends Initializer {\n          /** @nocollapse */\n          static className = 'CustomInit';\n\n          apply(shape: Shape, dtype?: DataType): Tensor {\n            // TODO(cais): More informative variable names?\n            const bI = capturedBiasInit.apply([capturedUnits]);\n            const bF = (new Ones()).apply([capturedUnits]);\n            const bCAndH = capturedBiasInit.apply([capturedUnits * 2]);\n            return K.concatAlongFirstAxis(\n                K.concatAlongFirstAxis(bI, bF), bCAndH);\n          }\n        })();\n      } else {\n        biasInitializer = this.biasInitializer;\n      }\n      this.bias = this.addWeight(\n          'bias', [this.units * 4], null, biasInitializer, this.biasRegularizer,\n          true, this.biasConstraint);\n    } else {\n      this.bias = null;\n    }\n    // Porting Notes: Unlike the PyKeras implementation, we perform slicing\n    //   of the weights and bias in the call() method, at execution time.\n    this.built = true;\n  }\n\n  override call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      const training = kwargs['training'] == null ? false : kwargs['training'];\n      inputs = inputs as Tensor[];\n      if (inputs.length !== 3) {\n        throw new ValueError(\n            `LSTMCell expects 3 input Tensors (inputs, h, c), got ` +\n            `${inputs.length}.`);\n      }\n      let hTMinus1 = inputs[1];    // Previous memory state.\n      const cTMinus1 = inputs[2];  // Previous carry state.\n      inputs = inputs[0];\n      if (0 < this.dropout && this.dropout < 1 && this.dropoutMask == null) {\n        this.dropoutMask = generateDropoutMask({\n                             ones: () => tfc.onesLike(inputs as Tensor),\n                             rate: this.dropout,\n                             training,\n                             count: 4,\n                             dropoutFunc: this.dropoutFunc\n                           }) as Tensor[];\n      }\n      if (0 < this.recurrentDropout && this.recurrentDropout < 1 &&\n          this.recurrentDropoutMask == null) {\n        this.recurrentDropoutMask = generateDropoutMask({\n                                      ones: () => tfc.onesLike(hTMinus1),\n                                      rate: this.recurrentDropout,\n                                      training,\n                                      count: 4,\n                                      dropoutFunc: this.dropoutFunc\n                                    }) as Tensor[];\n      }\n      const dpMask = this.dropoutMask as [Tensor, Tensor, Tensor, Tensor];\n      const recDpMask =\n          this.recurrentDropoutMask as [Tensor, Tensor, Tensor, Tensor];\n\n      // Note: For superior performance, TensorFlow.js always uses\n      // implementation 2 regardless of the actual value of\n      // config.implementation.\n      let i: Tensor;\n      let f: Tensor;\n      let c: Tensor;\n      let o: Tensor;\n      if (0 < this.dropout && this.dropout < 1) {\n        inputs = tfc.mul(inputs, dpMask[0]);\n      }\n      let z = K.dot(inputs, this.kernel.read());\n      if (0 < this.recurrentDropout && this.recurrentDropout < 1) {\n        hTMinus1 = tfc.mul(hTMinus1, recDpMask[0]);\n      }\n      z = tfc.add(z, K.dot(hTMinus1, this.recurrentKernel.read()));\n      if (this.useBias) {\n        z = K.biasAdd(z, this.bias.read());\n      }\n\n      const [z0, z1, z2, z3] = tfc.split(z, 4, z.rank - 1);\n\n      i = this.recurrentActivation.apply(z0);\n      f = this.recurrentActivation.apply(z1);\n      c = tfc.add(tfc.mul(f, cTMinus1), tfc.mul(i, this.activation.apply(z2)));\n      o = this.recurrentActivation.apply(z3);\n\n      const h = tfc.mul(o, this.activation.apply(c));\n      // TODO(cais): Add use_learning_phase flag properly.\n      return [h, h, c];\n    });\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    const baseConfig = super.getConfig();\n\n    const config: serialization.ConfigDict = {\n      units: this.units,\n      activation: serializeActivation(this.activation),\n      recurrentActivation: serializeActivation(this.recurrentActivation),\n      useBias: this.useBias,\n      kernelInitializer: serializeInitializer(this.kernelInitializer),\n      recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n      biasInitializer: serializeInitializer(this.biasInitializer),\n      unitForgetBias: this.unitForgetBias,\n      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n      recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n      biasRegularizer: serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: serializeConstraint(this.kernelConstraint),\n      recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n      biasConstraint: serializeConstraint(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout,\n      implementation: this.implementation,\n    };\n\n    return {...baseConfig, ...config};\n  }\n}\nserialization.registerClass(LSTMCell);\n\n// Porting Note: Since this is a superset of SimpleRNNLayerConfig, we inherit\n//   from that interface instead of repeating the fields here.\nexport declare interface LSTMLayerArgs extends SimpleRNNLayerArgs {\n  /**\n   * Activation function to use for the recurrent step.\n   *\n   * Defaults to hard sigmoid (`hardSigmoid`).\n   *\n   * If `null`, no activation is applied.\n   */\n  recurrentActivation?: ActivationIdentifier;\n\n  /**\n   * If `true`, add 1 to the bias of the forget gate at initialization.\n   * Setting it to `true` will also force `biasInitializer = 'zeros'`.\n   * This is recommended in\n   * [Jozefowicz et\n   * al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n   */\n  unitForgetBias?: boolean;\n\n  /**\n   * Implementation mode, either 1 or 2.\n   *   Mode 1 will structure its operations as a larger number of\n   *   smaller dot products and additions, whereas mode 2 will\n   *   batch them into fewer, larger operations. These modes will\n   *   have different performance profiles on different hardware and\n   *   for different applications.\n   *\n   * Note: For superior performance, TensorFlow.js always uses implementation\n   * 2, regardless of the actual value of this config field.\n   */\n  implementation?: number;\n}\n\nexport class LSTM extends RNN {\n  /** @nocollapse */\n  static override className = 'LSTM';\n  constructor(args: LSTMLayerArgs) {\n    if (args.implementation === 0) {\n      console.warn(\n          '`implementation=0` has been deprecated, and now defaults to ' +\n          '`implementation=1`. Please update your layer call.');\n    }\n    args.cell = new LSTMCell(args);\n    super(args as RNNLayerArgs);\n    // TODO(cais): Add activityRegularizer.\n  }\n\n  override call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      if (this.cell.dropoutMask != null) {\n        tfc.dispose(this.cell.dropoutMask);\n        this.cell.dropoutMask = null;\n      }\n      if (this.cell.recurrentDropoutMask != null) {\n        tfc.dispose(this.cell.recurrentDropoutMask);\n        this.cell.recurrentDropoutMask = null;\n      }\n      const mask = kwargs == null ? null : kwargs['mask'];\n      const training = kwargs == null ? null : kwargs['training'];\n      const initialState: Tensor[] =\n          kwargs == null ? null : kwargs['initialState'];\n      return super.call(inputs, {mask, training, initialState});\n    });\n  }\n\n  /** @nocollapse */\n  static override fromConfig<T extends serialization.Serializable>(\n      cls: serialization.SerializableConstructor<T>,\n      config: serialization.ConfigDict): T {\n    if (config['implmentation'] === 0) {\n      config['implementation'] = 1;\n    }\n    return new cls(config);\n  }\n}\nserialization.registerClass(LSTM);\n\nexport declare interface StackedRNNCellsArgs extends LayerArgs {\n  /**\n   * An `Array` of `RNNCell` instances.\n   */\n  cells: RNNCell[];\n}\n\nexport class StackedRNNCells extends RNNCell {\n  /** @nocollapse */\n  static className = 'StackedRNNCells';\n  protected cells: RNNCell[];\n\n  constructor(args: StackedRNNCellsArgs) {\n    super(args);\n    this.cells = args.cells;\n  }\n\n  get stateSize(): number[] {\n    // States are a flat list in reverse order of the cell stack.\n    // This allows perserving the requirement `stack.statesize[0] ===\n    // outputDim`. E.g., states of a 2-layer LSTM would be `[h2, c2, h1, c1]`,\n    // assuming one LSTM has states `[h, c]`.\n    const stateSize: number[] = [];\n    for (const cell of this.cells.slice().reverse()) {\n      if (Array.isArray(cell.stateSize)) {\n        stateSize.push(...cell.stateSize);\n      } else {\n        stateSize.push(cell.stateSize);\n      }\n    }\n    return stateSize;\n  }\n\n  override call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      inputs = inputs as Tensor[];\n      let states = inputs.slice(1);\n\n      // Recover per-cell states.\n      const nestedStates: Tensor[][] = [];\n      for (const cell of this.cells.slice().reverse()) {\n        if (Array.isArray(cell.stateSize)) {\n          nestedStates.push(states.splice(0, cell.stateSize.length));\n        } else {\n          nestedStates.push(states.splice(0, 1));\n        }\n      }\n      nestedStates.reverse();\n\n      // Call the cells in order and store the returned states.\n      const newNestedStates: Tensor[][] = [];\n      let callInputs: Tensor[];\n      for (let i = 0; i < this.cells.length; ++i) {\n        const cell = this.cells[i];\n        states = nestedStates[i];\n        // TODO(cais): Take care of constants.\n        if (i === 0) {\n          callInputs = [inputs[0]].concat(states);\n        } else {\n          callInputs = [callInputs[0]].concat(states);\n        }\n        callInputs = cell.call(callInputs, kwargs) as Tensor[];\n        newNestedStates.push(callInputs.slice(1));\n      }\n\n      // Format the new states as a flat list in reverse cell order.\n      states = [];\n      for (const cellStates of newNestedStates.slice().reverse()) {\n        states.push(...cellStates);\n      }\n      return [callInputs[0]].concat(states);\n    });\n  }\n\n  public override build(inputShape: Shape|Shape[]): void {\n    if (isArrayOfShapes(inputShape)) {\n      // TODO(cais): Take care of input constants.\n      // const constantShape = inputShape.slice(1);\n      inputShape = (inputShape as Shape[])[0];\n    }\n    inputShape = inputShape as Shape;\n    let outputDim: number;\n    this.cells.forEach((cell, i) => {\n      nameScope(`RNNCell_${i}`, () => {\n        // TODO(cais): Take care of input constants.\n\n        cell.build(inputShape);\n        if (Array.isArray(cell.stateSize)) {\n          outputDim = cell.stateSize[0];\n        } else {\n          outputDim = cell.stateSize;\n        }\n        inputShape = [inputShape[0], outputDim] as Shape;\n      });\n    });\n    this.built = true;\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    const baseConfig = super.getConfig();\n\n    const getCellConfig = (cell: RNNCell) => {\n      return {\n        'className': cell.getClassName(),\n        'config': cell.getConfig(),\n      };\n    };\n\n    const cellConfigs = this.cells.map(getCellConfig);\n\n    const config = {'cells': cellConfigs};\n\n    return {...baseConfig, ...config};\n  }\n\n  /** @nocollapse */\n  static override fromConfig<T extends serialization.Serializable>(\n      cls: serialization.SerializableConstructor<T>,\n      config: serialization.ConfigDict,\n      customObjects = {} as serialization.ConfigDict): T {\n    const cells: RNNCell[] = [];\n    for (const cellConfig of (config['cells'] as serialization.ConfigDict[])) {\n      cells.push(deserialize(cellConfig, customObjects) as RNNCell);\n    }\n    return new cls({cells});\n  }\n\n  override get trainableWeights(): LayerVariable[] {\n    if (!this.trainable) {\n      return [];\n    }\n    const weights: LayerVariable[] = [];\n    for (const cell of this.cells) {\n      weights.push(...cell.trainableWeights);\n    }\n    return weights;\n  }\n\n  override get nonTrainableWeights(): LayerVariable[] {\n    const weights: LayerVariable[] = [];\n    for (const cell of this.cells) {\n      weights.push(...cell.nonTrainableWeights);\n    }\n    if (!this.trainable) {\n      const trainableWeights: LayerVariable[] = [];\n      for (const cell of this.cells) {\n        trainableWeights.push(...cell.trainableWeights);\n      }\n      return trainableWeights.concat(weights);\n    }\n    return weights;\n  }\n\n  /**\n   * Retrieve the weights of a the model.\n   *\n   * @returns A flat `Array` of `tf.Tensor`s.\n   */\n  override getWeights(): Tensor[] {\n    const weights: LayerVariable[] = [];\n    for (const cell of this.cells) {\n      weights.push(...cell.weights);\n    }\n    return batchGetValue(weights);\n  }\n\n  /**\n   * Set the weights of the model.\n   *\n   * @param weights An `Array` of `tf.Tensor`s with shapes and types matching\n   *     the output of `getWeights()`.\n   */\n  override setWeights(weights: Tensor[]): void {\n    const tuples: Array<[LayerVariable, Tensor]> = [];\n    for (const cell of this.cells) {\n      const numParams = cell.weights.length;\n      const inputWeights = weights.splice(numParams);\n      for (let i = 0; i < cell.weights.length; ++i) {\n        tuples.push([cell.weights[i], inputWeights[i]]);\n      }\n    }\n    batchSetValue(tuples);\n  }\n\n  // TODO(cais): Maybe implemnt `losses` and `getLossesFor`.\n}\nserialization.registerClass(StackedRNNCells);\n\nexport function generateDropoutMask(args: {\n  ones: () => tfc.Tensor,\n  rate: number,\n  training?: boolean,\n  count?: number,\n  dropoutFunc?: Function,\n}): tfc.Tensor|tfc.Tensor[] {\n  const {ones, rate, training = false, count = 1, dropoutFunc} = args;\n\n  const droppedInputs = () =>\n      dropoutFunc != null ? dropoutFunc(ones(), rate) : K.dropout(ones(), rate);\n\n  const createMask = () => K.inTrainPhase(droppedInputs, ones, training);\n\n  // just in case count is provided with null or undefined\n  if (!count || count <= 1) {\n    return tfc.keep(createMask().clone());\n  }\n\n  const masks = Array(count).fill(undefined).map(createMask);\n\n  return masks.map(m => tfc.keep(m.clone()));\n}\n"],"mappings":";;;;;;;;;AAAA;;;;;;;;;AAUA;;;AAIA,OAAO,KAAKA,GAAG,MAAM,uBAAuB;AAC5C,SAAkBC,aAAa,EAAUC,IAAI,EAAEC,IAAI,QAAO,uBAAuB;AAEjF,SAAoBC,aAAa,EAAEC,mBAAmB,QAAO,gBAAgB;AAC7E,OAAO,KAAKC,CAAC,MAAM,yBAAyB;AAC5C,SAAQC,SAAS,QAAO,WAAW;AACnC,SAA0CC,aAAa,EAAEC,mBAAmB,QAAO,gBAAgB;AACnG,SAAQC,SAAS,EAAEC,cAAc,QAAO,oBAAoB;AAC5D,SAAQC,KAAK,QAAkB,oBAAoB;AACnD,SAAQC,cAAc,EAAEC,mBAAmB,EAAEC,UAAU,QAAO,WAAW;AACzE,SAAQC,cAAc,EAAEC,WAAW,EAAyBC,IAAI,EAAEC,oBAAoB,QAAO,iBAAiB;AAG9G,SAAQC,cAAc,EAAsCC,oBAAoB,QAAO,iBAAiB;AAExG,SAAQC,qBAAqB,QAAO,wBAAwB;AAC5D,OAAO,KAAKC,UAAU,MAAM,qBAAqB;AACjD,SAAQC,kBAAkB,EAAEC,mBAAmB,EAAEC,eAAe,QAAO,sBAAsB;AAC7F,SAAQC,aAAa,EAAEC,aAAa,QAAsB,cAAc;AAExE,SAAQC,WAAW,QAAO,iBAAiB;AAE3C;;;;;;;;;;;;;;;;;;;;;;AAsBA,OAAM,SAAUC,eAAeA,CAC3BC,MAAuD,EACvDC,YAA6D,EAC7DC,SAA0D,EAC1DC,YAAqB;EAKvB,IAAIC,KAAK,CAACC,OAAO,CAACL,MAAM,CAAC,EAAE;IACzB,IAAIC,YAAY,IAAI,IAAI,IAAIC,SAAS,IAAI,IAAI,EAAE;MAC7C,MAAM,IAAIlB,UAAU,CAChB,6DAA6D,GAC7D,oBAAoB,CAAC;;IAE3B,IAAImB,YAAY,IAAI,IAAI,EAAE;MACxBD,SAAS,GAAGF,MAAM,CAACM,KAAK,CAACN,MAAM,CAACO,MAAM,GAAGJ,YAAY,EAAEH,MAAM,CAACO,MAAM,CAAC;MACrEP,MAAM,GAAGA,MAAM,CAACM,KAAK,CAAC,CAAC,EAAEN,MAAM,CAACO,MAAM,GAAGJ,YAAY,CAAC;;IAExD,IAAIH,MAAM,CAACO,MAAM,GAAG,CAAC,EAAE;MACrBN,YAAY,GAAGD,MAAM,CAACM,KAAK,CAAC,CAAC,EAAEN,MAAM,CAACO,MAAM,CAAC;;IAE/CP,MAAM,GAAGA,MAAM,CAAC,CAAC,CAAC;;EAGpB,SAASQ,YAAYA,CAACC,CACgB;IACpC,IAAIA,CAAC,IAAI,IAAI,IAAIL,KAAK,CAACC,OAAO,CAACI,CAAC,CAAC,EAAE;MACjC,OAAOA,CAAgC;KACxC,MAAM;MACL,OAAO,CAACA,CAAC,CAAgC;;EAE7C;EAEAR,YAAY,GAAGO,YAAY,CAACP,YAAY,CAAC;EACzCC,SAAS,GAAGM,YAAY,CAACN,SAAS,CAAC;EAEnC,OAAO;IAACF,MAAM,EAANA,MAAM;IAAEC,YAAY,EAAZA,YAAY;IAAEC,SAAS,EAATA;EAAS,CAAC;AAC1C;AAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA2CA,OAAM,SAAUQ,GAAGA,CACfC,YAA6B,EAAEX,MAAc,EAAEY,aAAuB,EAE5C;EAAA,IAD1BC,WAAW,GAAAC,SAAA,CAAAP,MAAA,QAAAO,SAAA,QAAAC,SAAA,GAAAD,SAAA,MAAG,KAAK;EAAA,IAAEE,IAAa,GAAAF,SAAA,CAAAP,MAAA,OAAAO,SAAA,MAAAC,SAAA;EAAA,IAAEb,SAAoB,GAAAY,SAAA,CAAAP,MAAA,OAAAO,SAAA,MAAAC,SAAA;EAAA,IAAEE,MAAM,GAAAH,SAAA,CAAAP,MAAA,QAAAO,SAAA,QAAAC,SAAA,GAAAD,SAAA,MAAG,KAAK;EAAA,IACxEI,kBAAkB,GAAAJ,SAAA,CAAAP,MAAA,QAAAO,SAAA,QAAAC,SAAA,GAAAD,SAAA,MAAG,KAAK;EAC5B,OAAO7C,GAAG,CAACE,IAAI,CAAC,YAAK;IACnB,IAAMgD,IAAI,GAAGnB,MAAM,CAACoB,KAAK,CAACb,MAAM;IAChC,IAAIY,IAAI,GAAG,CAAC,EAAE;MACZ,MAAM,IAAInC,UAAU,wCAAAqC,MAAA,CAAwCF,IAAI,QAAK;;IAGvE;IACA;IACA,IAAMG,IAAI,GAAG,CAAC,CAAC,EAAE,CAAC,CAAC,CAACD,MAAM,CAAC7B,UAAU,CAAC+B,KAAK,CAAC,CAAC,EAAEJ,IAAI,CAAC,CAAC;IACrDnB,MAAM,GAAG/B,GAAG,CAACuD,SAAS,CAACxB,MAAM,EAAEsB,IAAI,CAAC;IAEpC,IAAIpB,SAAS,IAAI,IAAI,EAAE;MACrB,MAAM,IAAInB,mBAAmB,CACzB,kEAAkE,GAClE,gBAAgB,CAAC;;IAGvB;IACA,IAAIkC,MAAM,EAAE;MACVQ,OAAO,CAACC,IAAI,CACR,mEAAmE,GACnE,kCAAkC,CAAC;;IAGzC,IAAIV,IAAI,IAAI,IAAI,EAAE;MAChBA,IAAI,GAAG/C,GAAG,CAAC0D,IAAI,CAAC1D,GAAG,CAAC0D,IAAI,CAACX,IAAI,EAAE,MAAM,CAAC,EAAE,SAAS,CAAC;MAClD,IAAIA,IAAI,CAACY,IAAI,KAAKT,IAAI,GAAG,CAAC,EAAE;QAC1BH,IAAI,GAAG/C,GAAG,CAAC4D,UAAU,CAACb,IAAI,EAAE,CAAC,CAAC,CAAC;;MAEjCA,IAAI,GAAG/C,GAAG,CAACuD,SAAS,CAACR,IAAI,EAAEM,IAAI,CAAC;;IAGlC,IAAIT,WAAW,EAAE;MACfb,MAAM,GAAG/B,GAAG,CAAC6D,OAAO,CAAC9B,MAAM,EAAE,CAAC,CAAC;MAC/B,IAAIgB,IAAI,IAAI,IAAI,EAAE;QAChBA,IAAI,GAAG/C,GAAG,CAAC6D,OAAO,CAACd,IAAI,EAAE,CAAC,CAAC;;;IAI/B;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IAEA,IAAMe,cAAc,GAAa,EAAE;IACnC,IAAIC,UAAkB;IACtB,IAAIC,MAAM,GAAGrB,aAAa;IAC1B,IAAMsB,SAAS,GAAGlC,MAAM,CAACoB,KAAK,CAAC,CAAC,CAAC;IACjC,IAAMe,aAAa,GAAGlE,GAAG,CAACmE,OAAO,CAACpC,MAAM,CAAC;IACzC,IAAIqC,YAAsB;IAC1B,IAAIrB,IAAI,IAAI,IAAI,EAAE;MAChBqB,YAAY,GAAGpE,GAAG,CAACmE,OAAO,CAACpB,IAAI,CAAC;;IACjC,IAAAsB,KAAA,YAAAA,MAAAC,CAAA,EAEmC;MAClC,IAAMC,YAAY,GAAGL,aAAa,CAACI,CAAC,CAAC;MACrC,IAAME,WAAW,GAAGxE,GAAG,CAACE,IAAI,CAAC;QAAA,OAAMwC,YAAY,CAAC6B,YAAY,EAAEP,MAAM,CAAC;MAAA,EAAC;MAEtE,IAAIjB,IAAI,IAAI,IAAI,EAAE;QAChBgB,UAAU,GAAGS,WAAW,CAAC,CAAC,CAAC;QAC3BR,MAAM,GAAGQ,WAAW,CAAC,CAAC,CAAC;OACxB,MAAM;QACL,IAAMC,aAAa,GAAGzE,GAAG,CAACE,IAAI,CAAC,YAAK;UAClC,IAAMwE,QAAQ,GAAGN,YAAY,CAACE,CAAC,CAAC;UAChC,IAAMK,WAAW,GAAG3E,GAAG,CAAC4E,GAAG,CAAC5E,GAAG,CAAC6E,QAAQ,CAACH,QAAQ,CAAC,EAAEA,QAAQ,CAAC;UAC7D;UACA,IAAMI,MAAM,GAAG9E,GAAG,CAAC+E,GAAG,CAClB/E,GAAG,CAACgF,GAAG,CAACR,WAAW,CAAC,CAAC,CAAC,EAAEE,QAAQ,CAAC,EACjC1E,GAAG,CAACgF,GAAG,CAAChB,MAAM,CAAC,CAAC,CAAC,EAAEW,WAAW,CAAC,CAAC;UACpC,IAAMM,SAAS,GAAGjB,MAAM,CAACkB,GAAG,CAAC,UAACC,KAAK,EAAEC,CAAC,EAAI;YACxC,OAAOpF,GAAG,CAAC+E,GAAG,CACV/E,GAAG,CAACgF,GAAG,CAACR,WAAW,CAAC,CAAC,CAAC,CAACY,CAAC,CAAC,EAAEV,QAAQ,CAAC,EACpC1E,GAAG,CAACgF,GAAG,CAACG,KAAK,EAAER,WAAW,CAAC,CAAC;UAClC,CAAC,CAAC;UACF,OAAO;YAACG,MAAM,EAANA,MAAM;YAAEG,SAAS,EAATA;UAAS,CAAC;QAC5B,CAAC,CAAC;QACFlB,UAAU,GAAGU,aAAa,CAACK,MAAM;QACjCd,MAAM,GAAGS,aAAa,CAACQ,SAAS;;MAGlC,IAAIhC,kBAAkB,EAAE;QACtBa,cAAc,CAACuB,IAAI,CAACtB,UAAU,CAAC;;KAElC;IA7BD,KAAK,IAAIO,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGL,SAAS,EAAE,EAAEK,CAAC;MAAAD,KAAA,CAAAC,CAAA;IAAA;IA8BlC,IAAIgB,OAAe;IACnB,IAAIrC,kBAAkB,EAAE;MACtB,IAAMsC,IAAI,GAAG,CAAC;MACdD,OAAO,GAAGtF,GAAG,CAACwF,KAAK,CAAC1B,cAAc,EAAEyB,IAAI,CAAC;;IAE3C,OAAO,CAACxB,UAAU,EAAEuB,OAAO,EAAEtB,MAAM,CAA+B;EACpE,CAAC,CAAC;AACJ;AAuGA,WAAayB,GAAI,0BAAAC,MAAA;EAAAC,SAAA,CAAAF,GAAA,EAAAC,MAAA;EAAA,IAAAE,MAAA,GAAAC,YAAA,CAAAJ,GAAA;EAqBf,SAAAA,IAAYK,IAAkB;IAAA,IAAAC,KAAA;IAAAC,eAAA,OAAAP,GAAA;IAC5BM,KAAA,GAAAH,MAAA,CAAAK,IAAA,OAAMH,IAAI;IACV,IAAII,IAAa;IACjB,IAAIJ,IAAI,CAACI,IAAI,IAAI,IAAI,EAAE;MACrB,MAAM,IAAInF,UAAU,CAChB,sDAAsD,CAAC;KAC5D,MAAM,IAAIoB,KAAK,CAACC,OAAO,CAAC0D,IAAI,CAACI,IAAI,CAAC,EAAE;MACnCA,IAAI,GAAG,IAAIC,eAAe,CAAC;QAACC,KAAK,EAAEN,IAAI,CAACI;MAAI,CAAC,CAAC;KAC/C,MAAM;MACLA,IAAI,GAAGJ,IAAI,CAACI,IAAI;;IAElB,IAAIA,IAAI,CAACG,SAAS,IAAI,IAAI,EAAE;MAC1B,MAAM,IAAItF,UAAU,CAChB,8DAA8D,GAC9D,uCAAuC,CAAC;;IAE9CgF,KAAA,CAAKG,IAAI,GAAGA,IAAI;IAChBH,KAAA,CAAKO,eAAe,GAChBR,IAAI,CAACQ,eAAe,IAAI,IAAI,GAAG,KAAK,GAAGR,IAAI,CAACQ,eAAe;IAC/DP,KAAA,CAAKQ,WAAW,GAAGT,IAAI,CAACS,WAAW,IAAI,IAAI,GAAG,KAAK,GAAGT,IAAI,CAACS,WAAW;IACtER,KAAA,CAAKnD,WAAW,GAAGkD,IAAI,CAAClD,WAAW,IAAI,IAAI,GAAG,KAAK,GAAGkD,IAAI,CAAClD,WAAW;IACtEmD,KAAA,CAAKS,SAAS,GAAGV,IAAI,CAACW,QAAQ,IAAI,IAAI,GAAG,KAAK,GAAGX,IAAI,CAACW,QAAQ;IAC9DV,KAAA,CAAK/C,MAAM,GAAG8C,IAAI,CAAC9C,MAAM,IAAI,IAAI,GAAG,KAAK,GAAG8C,IAAI,CAAC9C,MAAM;IAEvD+C,KAAA,CAAKW,eAAe,GAAG,IAAI;IAC3BX,KAAA,CAAKY,SAAS,GAAG,CAAC,IAAIjG,SAAS,CAAC;MAACwC,IAAI,EAAE;IAAC,CAAC,CAAC,CAAC;IAC3C6C,KAAA,CAAKa,SAAS,GAAG,IAAI;IACrBb,KAAA,CAAKc,OAAO,GAAG,IAAI;IACnB;IACAd,KAAA,CAAK7D,YAAY,GAAG,IAAI;IACxB;IACA;IAEA6D,KAAA,CAAKe,UAAU,GAAG,EAAE;IAAC,OAAAf,KAAA;EACvB;EAEA;EACA;EAAAgB,YAAA,CAAAtB,GAAA;IAAAuB,GAAA;IAAAC,KAAA,EACA,SAAAC,UAAA,EAAS;MACP,IAAI,IAAI,CAACL,OAAO,IAAI,IAAI,EAAE;QACxB,IAAMM,SAAS,GACXhF,KAAK,CAACC,OAAO,CAAC,IAAI,CAAC8D,IAAI,CAACG,SAAS,CAAC,GAAG,IAAI,CAACH,IAAI,CAACG,SAAS,CAAC/D,MAAM,GAAG,CAAC;QACvE,OAAOf,UAAU,CAAC+B,KAAK,CAAC,CAAC,EAAE6D,SAAS,CAAC,CAACjC,GAAG,CAAC,UAAA1C,CAAC;UAAA,OAAI,IAAI;QAAA,EAAC;OACrD,MAAM;QACL,OAAO,IAAI,CAACqE,OAAO;;IAEvB;IAEA;IACA;EAAA;IAAAG,GAAA;IAAAC,KAAA,EACA,SAAAG,UAAUpD,MAAgB;MACxB,IAAI,CAAC6C,OAAO,GAAG7C,MAAM;IACvB;EAAC;IAAAgD,GAAA;IAAAC,KAAA,EAEQ,SAAAI,mBAAmBC,UAAyB;MACnD,IAAI5F,eAAe,CAAC4F,UAAU,CAAC,EAAE;QAC/BA,UAAU,GAAIA,UAAsB,CAAC,CAAC,CAAC;;MAEzCA,UAAU,GAAGA,UAAmB;MAEhC;MACA,IAAIjB,SAAS,GAAG,IAAI,CAACH,IAAI,CAACG,SAAS;MACnC,IAAI,CAAClE,KAAK,CAACC,OAAO,CAACiE,SAAS,CAAC,EAAE;QAC7BA,SAAS,GAAG,CAACA,SAAS,CAAC;;MAEzB,IAAMkB,SAAS,GAAGlB,SAAS,CAAC,CAAC,CAAC;MAC9B,IAAImB,WAA0B;MAC9B,IAAI,IAAI,CAAClB,eAAe,EAAE;QACxBkB,WAAW,GAAG,CAACF,UAAU,CAAC,CAAC,CAAC,EAAEA,UAAU,CAAC,CAAC,CAAC,EAAEC,SAAS,CAAC;OACxD,MAAM;QACLC,WAAW,GAAG,CAACF,UAAU,CAAC,CAAC,CAAC,EAAEC,SAAS,CAAC;;MAG1C,IAAI,IAAI,CAAChB,WAAW,EAAE;QACpB,IAAMkB,UAAU,GAAY,EAAE;QAAC,IAAAC,SAAA,GAAAC,0BAAA,CACbtB,SAAS;UAAAuB,KAAA;QAAA;UAA3B,KAAAF,SAAA,CAAAG,CAAA,MAAAD,KAAA,GAAAF,SAAA,CAAAI,CAAA,IAAAC,IAAA,GAA6B;YAAA,IAAlBC,GAAG,GAAAJ,KAAA,CAAAX,KAAA;YACZQ,UAAU,CAACpC,IAAI,CAAC,CAACiC,UAAU,CAAC,CAAC,CAAC,EAAEU,GAAG,CAAC,CAAC;;QACtC,SAAAC,GAAA;UAAAP,SAAA,CAAAQ,CAAA,CAAAD,GAAA;QAAA;UAAAP,SAAA,CAAAS,CAAA;QAAA;QACD,OAAO,CAACX,WAAW,CAAC,CAACpE,MAAM,CAACqE,UAAU,CAAC;OACxC,MAAM;QACL,OAAOD,WAAW;;IAEtB;EAAC;IAAAR,GAAA;IAAAC,KAAA,EAEQ,SAAAmB,YAAYrG,MAAuB,EAAEgB,IAAsB;MAAA,IAAAsF,MAAA;MAElE,OAAOrI,GAAG,CAACE,IAAI,CAAC,YAAK;QACnB,IAAIiC,KAAK,CAACC,OAAO,CAACW,IAAI,CAAC,EAAE;UACvBA,IAAI,GAAGA,IAAI,CAAC,CAAC,CAAC;;QAEhB,IAAMuF,UAAU,GAAGD,MAAI,CAAC/B,eAAe,GAAGvD,IAAI,GAAG,IAAI;QAErD,IAAIsF,MAAI,CAAC9B,WAAW,EAAE;UACpB,IAAMgC,SAAS,GAAGF,MAAI,CAACrE,MAAM,CAACkB,GAAG,CAAC,UAAA2C,CAAC;YAAA,OAAI,IAAI;UAAA,EAAC;UAC5C,OAAO,CAACS,UAAU,CAAC,CAAClF,MAAM,CAACmF,SAAS,CAAC;SACtC,MAAM;UACL,OAAOD,UAAU;;MAErB,CAAC,CAAC;IACJ;IAEA;;;;;;EAAA;IAAAtB,GAAA;IAAAwB,GAAA,EAMA,SAAAA,IAAA,EAAU;MACR,IAAI,IAAI,CAAC3B,OAAO,IAAI,IAAI,EAAE;QACxB,IAAMM,SAAS,GACXhF,KAAK,CAACC,OAAO,CAAC,IAAI,CAAC8D,IAAI,CAACG,SAAS,CAAC,GAAG,IAAI,CAACH,IAAI,CAACG,SAAS,CAAC/D,MAAM,GAAG,CAAC;QACvE,IAAMwC,MAAM,GAAa,EAAE;QAC3B,KAAK,IAAIM,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG+B,SAAS,EAAE,EAAE/B,CAAC,EAAE;UAClCN,MAAM,CAACO,IAAI,CAAC,IAAI,CAAC;;QAEnB,OAAOP,MAAM;OACd,MAAM;QACL,OAAO,IAAI,CAAC+B,OAAO;;IAEvB,CAAC;IAAA4B,GAAA,EAED,SAAAA,IAAWZ,CAAW;MACpB,IAAI,CAAChB,OAAO,GAAGgB,CAAC;IAClB;EAAC;IAAAb,GAAA;IAAAC,KAAA,EAEe,SAAAyB,MAAMpB,UAAyB;MAC7C;MACA;MACA,IAAMqB,aAAa,GAAY,IAAI;MACnC,IAAI,IAAI,CAACzG,YAAY,IAAI,IAAI,EAAE;QAC7B,MAAM,IAAIpB,mBAAmB,CACzB,kDAAkD,CAAC;;MAGzD,IAAIY,eAAe,CAAC4F,UAAU,CAAC,EAAE;QAC/BA,UAAU,GAAIA,UAAsB,CAAC,CAAC,CAAC;;MAEzCA,UAAU,GAAGA,UAAmB;MAEhC,IAAMsB,SAAS,GAAW,IAAI,CAACnC,QAAQ,GAAGa,UAAU,CAAC,CAAC,CAAC,GAAG,IAAI;MAC9D,IAAMuB,QAAQ,GAAGvB,UAAU,CAACjF,KAAK,CAAC,CAAC,CAAC;MACpC,IAAI,CAACsE,SAAS,CAAC,CAAC,CAAC,GAAG,IAAIjG,SAAS,CAAC;QAACyC,KAAK,GAAGyF,SAAS,EAAE,IAAI,EAAAxF,MAAA,CAAA0F,kBAAA,CAAKD,QAAQ;MAAC,CAAC,CAAC;MAE1E;MACA;MACA,IAAME,cAAc,GAAG,CAACzB,UAAU,CAAC,CAAC,CAAC,CAAC,CAAClE,MAAM,CAACkE,UAAU,CAACjF,KAAK,CAAC,CAAC,CAAC,CAAC;MAClE,IAAIsG,aAAa,IAAI,IAAI,EAAE;QACzB,MAAM,IAAI7H,mBAAmB,CACzB,kDAAkD,CAAC;OACxD,MAAM;QACL,IAAI,CAACoF,IAAI,CAACwC,KAAK,CAACK,cAAc,CAAC;;MAGjC;MACA,IAAI1C,SAAmB;MACvB,IAAIlE,KAAK,CAACC,OAAO,CAAC,IAAI,CAAC8D,IAAI,CAACG,SAAS,CAAC,EAAE;QACtCA,SAAS,GAAG,IAAI,CAACH,IAAI,CAACG,SAAS;OAChC,MAAM;QACLA,SAAS,GAAG,CAAC,IAAI,CAACH,IAAI,CAACG,SAAS,CAAC;;MAGnC,IAAI,IAAI,CAACO,SAAS,IAAI,IAAI,EAAE;QAC1B,IAAI,CAACzG,IAAI,CAAC6I,WAAW,CACb,IAAI,CAACpC,SAAS,CAAC1B,GAAG,CAAC,UAAA+D,IAAI;UAAA,OAAIA,IAAI,CAAC9F,KAAK,CAAC8F,IAAI,CAAC9F,KAAK,CAACb,MAAM,GAAG,CAAC,CAAC;QAAA,EAAC,EAC7D+D,SAAS,CAAC,EAAE;UAClB,MAAM,IAAItF,UAAU,CAChB,kGAAAqC,MAAA,CACsC,IAAI,CAACwD,SAAS,OAAI,gCAAAxD,MAAA,CAC3B,IAAI,CAAC8C,IAAI,CAACG,SAAS,CAAE,CAAC;;OAE1D,MAAM;QACL,IAAI,CAACO,SAAS,GACVP,SAAS,CAACnB,GAAG,CAAC,UAAA8C,GAAG;UAAA,OAAI,IAAItH,SAAS,CAAC;YAACyC,KAAK,EAAE,CAAC,IAAI,EAAE6E,GAAG;UAAC,CAAC,CAAC;QAAA,EAAC;;MAE/D,IAAI,IAAI,CAACvB,QAAQ,EAAE;QACjB,IAAI,CAACyC,WAAW,EAAE;;IAEtB;IAEA;;;;;;;;;;;;;;;;;EAAA;IAAAlC,GAAA;IAAAC,KAAA,EAiBS,SAAAiC,YAAYlF,MAAwB,EAAkB;MAAA,IAAAmF,MAAA;MAAA,IAAhBC,QAAQ,GAAAvG,SAAA,CAAAP,MAAA,QAAAO,SAAA,QAAAC,SAAA,GAAAD,SAAA,MAAG,KAAK;MAC7D3C,IAAI,CAAC,YAAK;QACR,IAAI,CAACiJ,MAAI,CAAC1C,QAAQ,EAAE;UAClB,MAAM,IAAI5F,cAAc,CACpB,iEAAiE,CAAC;;QAExE,IAAM+H,SAAS,GAAGO,MAAI,CAACxC,SAAS,CAAC,CAAC,CAAC,CAACxD,KAAK,CAAC,CAAC,CAAC;QAC5C,IAAIyF,SAAS,IAAI,IAAI,EAAE;UACrB,MAAM,IAAI7H,UAAU,CAChB,kEAAkE,GAClE,0CAA0C,GAC1C,2DAA2D,GAC3D,2DAA2D,GAC3D,2DAA2D,GAC3D,oDAAoD,CAAC;;QAE3D;QACA,IAAIoI,MAAI,CAACtC,OAAO,IAAI,IAAI,EAAE;UACxB,IAAI1E,KAAK,CAACC,OAAO,CAAC+G,MAAI,CAACjD,IAAI,CAACG,SAAS,CAAC,EAAE;YACtC8C,MAAI,CAACtC,OAAO,GACRsC,MAAI,CAACjD,IAAI,CAACG,SAAS,CAACnB,GAAG,CAAC,UAAA8C,GAAG;cAAA,OAAIhI,GAAG,CAACqJ,KAAK,CAAC,CAACT,SAAS,EAAEZ,GAAG,CAAC,CAAC;YAAA,EAAC;WAChE,MAAM;YACLmB,MAAI,CAACtC,OAAO,GAAG,CAAC7G,GAAG,CAACqJ,KAAK,CAAC,CAACT,SAAS,EAAEO,MAAI,CAACjD,IAAI,CAACG,SAAS,CAAC,CAAC,CAAC;;SAE/D,MAAM,IAAIrC,MAAM,IAAI,IAAI,EAAE;UACzB;UACAhE,GAAG,CAACsJ,OAAO,CAACH,MAAI,CAACtC,OAAO,CAAC;UACzB;UACA,IAAIsC,MAAI,CAACrC,UAAU,IAAI,IAAI,EAAE;YAC3B9G,GAAG,CAACsJ,OAAO,CAACH,MAAI,CAACrC,UAAU,CAAC;YAC5BqC,MAAI,CAACrC,UAAU,GAAG,EAAE;;UAGtB,IAAI3E,KAAK,CAACC,OAAO,CAAC+G,MAAI,CAACjD,IAAI,CAACG,SAAS,CAAC,EAAE;YACtC8C,MAAI,CAACtC,OAAO,GACRsC,MAAI,CAACjD,IAAI,CAACG,SAAS,CAACnB,GAAG,CAAC,UAAA8C,GAAG;cAAA,OAAIhI,GAAG,CAACqJ,KAAK,CAAC,CAACT,SAAS,EAAEZ,GAAG,CAAC,CAAC;YAAA,EAAC;WAChE,MAAM;YACLmB,MAAI,CAACtC,OAAO,CAAC,CAAC,CAAC,GAAG7G,GAAG,CAACqJ,KAAK,CAAC,CAACT,SAAS,EAAEO,MAAI,CAACjD,IAAI,CAACG,SAAS,CAAC,CAAC;;SAEhE,MAAM;UACL,IAAI,CAAClE,KAAK,CAACC,OAAO,CAAC4B,MAAM,CAAC,EAAE;YAC1BA,MAAM,GAAG,CAACA,MAAM,CAAC;;UAEnB,IAAIA,MAAM,CAAC1B,MAAM,KAAK6G,MAAI,CAACtC,OAAO,CAACvE,MAAM,EAAE;YACzC,MAAM,IAAIvB,UAAU,CAChB,SAAAqC,MAAA,CAAS+F,MAAI,CAACI,IAAI,eAAAnG,MAAA,CAAY+F,MAAI,CAACtC,OAAO,CAACvE,MAAM,sCAAAc,MAAA,CAC9BY,MAAM,CAAC1B,MAAM,4BAAyB,gBAAAc,MAAA,CAC5CY,MAAM,CAAE,CAAC;;UAG5B,IAAIoF,QAAQ,KAAK,IAAI,EAAE;YACrB;YACA;YACA;YACA;YACAD,MAAI,CAACrC,UAAU,CAACzB,IAAI,CAAC8D,MAAI,CAACtC,OAAO,CAACxE,KAAK,EAAE,CAAC;WAC3C,MAAM;YACLrC,GAAG,CAACsJ,OAAO,CAACH,MAAI,CAACtC,OAAO,CAAC;;UAG3B,KAAK,IAAI2C,KAAK,GAAG,CAAC,EAAEA,KAAK,GAAGL,MAAI,CAACtC,OAAO,CAACvE,MAAM,EAAE,EAAEkH,KAAK,EAAE;YACxD,IAAMvC,KAAK,GAAGjD,MAAM,CAACwF,KAAK,CAAC;YAC3B,IAAMxB,GAAG,GAAG7F,KAAK,CAACC,OAAO,CAAC+G,MAAI,CAACjD,IAAI,CAACG,SAAS,CAAC,GAC1C8C,MAAI,CAACjD,IAAI,CAACG,SAAS,CAACmD,KAAK,CAAC,GAC1BL,MAAI,CAACjD,IAAI,CAACG,SAAS;YACvB,IAAMoD,aAAa,GAAG,CAACb,SAAS,EAAEZ,GAAG,CAAC;YACtC,IAAI,CAAC7H,IAAI,CAAC6I,WAAW,CAAC/B,KAAK,CAAC9D,KAAK,EAAEsG,aAAa,CAAC,EAAE;cACjD,MAAM,IAAI1I,UAAU,CAChB,SAAAqC,MAAA,CAASoG,KAAK,kCAAApG,MAAA,CAA+B+F,MAAI,CAACI,IAAI,4BAAAnG,MAAA,CACpCqG,aAAa,uBAAArG,MAAA,CAC3B6D,KAAK,CAAC9D,KAAK,CAAE,CAAC;;YAExBgG,MAAI,CAACtC,OAAO,CAAC2C,KAAK,CAAC,GAAGvC,KAAK;;;QAG/BkC,MAAI,CAACtC,OAAO,GAAGsC,MAAI,CAACtC,OAAO,CAAC3B,GAAG,CAAC,UAAAC,KAAK;UAAA,OAAInF,GAAG,CAAC0J,IAAI,CAACvE,KAAK,CAACwE,KAAK,EAAE,CAAC;QAAA,EAAC;MACnE,CAAC,CAAC;IACJ;EAAC;IAAA3C,GAAA;IAAAC,KAAA,EAEQ,SAAA2C,MACL7H,MAAuD,EACvD8H,MAAe;MACjB;MACA,IAAI7H,YAAY,GACZ6H,MAAM,IAAI,IAAI,GAAG,IAAI,GAAGA,MAAM,CAAC,cAAc,CAAC;MAClD,IAAI5H,SAAS,GACT4H,MAAM,IAAI,IAAI,GAAG,IAAI,GAAGA,MAAM,CAAC,WAAW,CAAC;MAC/C,IAAIA,MAAM,IAAI,IAAI,EAAE;QAClBA,MAAM,GAAG,EAAE;;MAGb,IAAMC,YAAY,GACdhI,eAAe,CAACC,MAAM,EAAEC,YAAY,EAAEC,SAAS,EAAE,IAAI,CAACC,YAAY,CAAC;MACvEH,MAAM,GAAG+H,YAAY,CAAC/H,MAAM;MAC5BC,YAAY,GAAG8H,YAAY,CAAC9H,YAAY;MACxCC,SAAS,GAAG6H,YAAY,CAAC7H,SAAS;MAElC;MACA;MACA;MAEA,IAAI8H,gBAAgB,GAAiC,EAAE;MACvD,IAAIC,eAAe,GAAgB,EAAE;MACrC,IAAIhI,YAAY,IAAI,IAAI,EAAE;QACxB6H,MAAM,CAAC,cAAc,CAAC,GAAG7H,YAAY;QACrC+H,gBAAgB,GAAGA,gBAAgB,CAAC3G,MAAM,CAACpB,YAAY,CAAC;QACxD,IAAI,CAAC4E,SAAS,GAAG,EAAE;QAAC,IAAAqD,UAAA,GAAAtC,0BAAA,CACA3F,YAAY;UAAAkI,MAAA;QAAA;UAAhC,KAAAD,UAAA,CAAApC,CAAA,MAAAqC,MAAA,GAAAD,UAAA,CAAAnC,CAAA,IAAAC,IAAA,GAAkC;YAAA,IAAvB5C,KAAK,GAAA+E,MAAA,CAAAjD,KAAA;YACd,IAAI,CAACL,SAAS,CAACvB,IAAI,CAAC,IAAI3E,SAAS,CAAC;cAACyC,KAAK,EAAEgC,KAAK,CAAChC;YAAK,CAAC,CAAC,CAAC;;UAE1D;UACA;UACA;QAAA,SAAA8E,GAAA;UAAAgC,UAAA,CAAA/B,CAAA,CAAAD,GAAA;QAAA;UAAAgC,UAAA,CAAA9B,CAAA;QAAA;QACA6B,eAAe,GAAGA,eAAe,CAAC5G,MAAM,CAAC,IAAI,CAACwD,SAAS,CAAC;;MAE1D,IAAI3E,SAAS,IAAI,IAAI,EAAE;QACrB4H,MAAM,CAAC,WAAW,CAAC,GAAG5H,SAAS;QAC/B8H,gBAAgB,GAAGA,gBAAgB,CAAC3G,MAAM,CAACnB,SAAS,CAAC;QACrD;QACA,IAAI,CAACC,YAAY,GAAGD,SAAS,CAACK,MAAM;;MAGtC,IAAM6H,QAAQ,GAAGJ,gBAAgB,CAAC,CAAC,CAAC,YAAYpJ,cAAc;MAC9D,IAAIwJ,QAAQ,EAAE;QACZ;QACA,IAAMC,SAAS,GACX,CAACrI,MAAM,CAAC,CAACqB,MAAM,CAAC2G,gBAAgB,CAAgC;QACpE,IAAMM,aAAa,GAAG,IAAI,CAAC1D,SAAS,CAACvD,MAAM,CAAC4G,eAAe,CAAC;QAC5D;QACA,IAAMM,iBAAiB,GAAG,IAAI,CAAC3D,SAAS;QACxC,IAAI,CAACA,SAAS,GAAG0D,aAAa;QAC9B,IAAMvF,MAAM,GAAAyF,IAAA,CAAAC,eAAA,CAAA/E,GAAA,CAAAgF,SAAA,kBAAAxE,IAAA,OAAemE,SAAS,EAAEP,MAAM,CAAC;QAC7C,IAAI,CAAClD,SAAS,GAAG2D,iBAAiB;QAClC,OAAOxF,MAAM;OACd,MAAM;QACL,OAAAyF,IAAA,CAAAC,eAAA,CAAA/E,GAAA,CAAAgF,SAAA,kBAAAxE,IAAA,OAAmBlE,MAAM,EAAE8H,MAAM;;IAErC;IAEA;EAAA;IAAA7C,GAAA;IAAAC,KAAA,EACS,SAAAhB,KAAKlE,MAAuB,EAAE8H,MAAc;MAAA,IAAAa,MAAA;MACnD;MACA;MACA;MACA,OAAOxK,IAAI,CAAC,YAAK;QACf,IAAM6C,IAAI,GAAG8G,MAAM,IAAI,IAAI,GAAG,IAAI,GAAGA,MAAM,CAAC,MAAM,CAAW;QAC7D,IAAMT,QAAQ,GAAGS,MAAM,IAAI,IAAI,GAAG,IAAI,GAAGA,MAAM,CAAC,UAAU,CAAC;QAC3D,IAAI7H,YAAY,GACZ6H,MAAM,IAAI,IAAI,GAAG,IAAI,GAAGA,MAAM,CAAC,cAAc,CAAC;QAElD9H,MAAM,GAAGN,mBAAmB,CAACM,MAAM,CAAC;QACpC,IAAIC,YAAY,IAAI,IAAI,EAAE;UACxB,IAAI0I,MAAI,CAACjE,QAAQ,EAAE;YACjBzE,YAAY,GAAG0I,MAAI,CAAC7D,OAAO;WAC5B,MAAM;YACL7E,YAAY,GAAG0I,MAAI,CAACC,eAAe,CAAC5I,MAAM,CAAC;;;QAI/C,IAAMoF,SAAS,GACXhF,KAAK,CAACC,OAAO,CAACsI,MAAI,CAACxE,IAAI,CAACG,SAAS,CAAC,GAAGqE,MAAI,CAACxE,IAAI,CAACG,SAAS,CAAC/D,MAAM,GAAG,CAAC;QACvE,IAAIN,YAAY,CAACM,MAAM,KAAK6E,SAAS,EAAE;UACrC,MAAM,IAAIpG,UAAU,CAChB,iBAAAqC,MAAA,CAAiB+D,SAAS,oCAAA/D,MAAA,CACvBpB,YAAY,CAACM,MAAM,uBAAoB,CAAC;;QAEjD,IAAIoI,MAAI,CAAC1H,MAAM,EAAE;UACfQ,OAAO,CAACC,IAAI,CACR,kEAAkE,CAAC;;QAGzE,IAAMmH,cAAc,GAAW;UAACxB,QAAQ,EAARA;QAAQ,CAAC;QAEzC;QACA,IAAMyB,IAAI,GAAG,SAAPA,IAAIA,CAAI9I,MAAc,EAAEiC,MAAgB,EAAI;UAChD;UACA;UACA,IAAMsB,OAAO,GACToF,MAAI,CAACxE,IAAI,CAACD,IAAI,CAAC,CAAClE,MAAM,CAAC,CAACqB,MAAM,CAACY,MAAM,CAAC,EAAE4G,cAAc,CAAa;UACvE;UACA,OAAO,CAACtF,OAAO,CAAC,CAAC,CAAC,EAAEA,OAAO,CAACjD,KAAK,CAAC,CAAC,CAAC,CAAuB;QAC7D,CAAC;QAED;QAEA,IAAMyI,UAAU,GACZrI,GAAG,CAACoI,IAAI,EAAE9I,MAAM,EAAEC,YAAY,EAAE0I,MAAI,CAAC9H,WAAW,EAAEG,IAAI,EAAE,IAAI,EACxD2H,MAAI,CAAC1H,MAAM,EAAE0H,MAAI,CAACpE,eAAe,CAAC;QAC1C,IAAMvC,UAAU,GAAG+G,UAAU,CAAC,CAAC,CAAC;QAChC,IAAMxF,OAAO,GAAGwF,UAAU,CAAC,CAAC,CAAC;QAC7B,IAAM9G,MAAM,GAAG8G,UAAU,CAAC,CAAC,CAAC;QAE5B,IAAIJ,MAAI,CAACjE,QAAQ,EAAE;UACjBiE,MAAI,CAACxB,WAAW,CAAClF,MAAM,EAAEoF,QAAQ,CAAC;;QAGpC,IAAMtE,MAAM,GAAG4F,MAAI,CAACpE,eAAe,GAAGhB,OAAO,GAAGvB,UAAU;QAE1D;QAEA,IAAI2G,MAAI,CAACnE,WAAW,EAAE;UACpB,OAAO,CAACzB,MAAM,CAAC,CAAC1B,MAAM,CAACY,MAAM,CAAC;SAC/B,MAAM;UACL,OAAOc,MAAM;;MAEjB,CAAC,CAAC;IACJ;EAAC;IAAAkC,GAAA;IAAAC,KAAA,EAED,SAAA0D,gBAAgB5I,MAAc;MAAA,IAAAgJ,MAAA;MAC5B,OAAO7K,IAAI,CAAC,YAAK;QACf;QACA;QACA,IAAI8B,YAAY,GAAGhC,GAAG,CAACqJ,KAAK,CAACtH,MAAM,CAACoB,KAAK,CAAC;QAC1C;QACAnB,YAAY,GAAGhC,GAAG,CAACgL,GAAG,CAAChJ,YAAY,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;QAC5CA,YAAY,GAAG1B,CAAC,CAACsD,UAAU,CAAC5B,YAAY,CAAC,CAAC,CAAE;QAE5C,IAAIG,KAAK,CAACC,OAAO,CAAC2I,MAAI,CAAC7E,IAAI,CAACG,SAAS,CAAC,EAAE;UACtC,OAAO0E,MAAI,CAAC7E,IAAI,CAACG,SAAS,CAACnB,GAAG,CAC1B,UAAA8C,GAAG;YAAA,OAAIA,GAAG,GAAG,CAAC,GAAG1H,CAAC,CAAC2K,IAAI,CAACjJ,YAAY,EAAE,CAAC,CAAC,EAAEgG,GAAG,CAAC,CAAC,GAAGhG,YAAY;UAAA,EAAC;SACpE,MAAM;UACL,OAAO+I,MAAI,CAAC7E,IAAI,CAACG,SAAS,GAAG,CAAC,GAC1B,CAAC/F,CAAC,CAAC2K,IAAI,CAACjJ,YAAY,EAAE,CAAC,CAAC,EAAE+I,MAAI,CAAC7E,IAAI,CAACG,SAAS,CAAC,CAAC,CAAC,GAChD,CAACrE,YAAY,CAAC;;MAEtB,CAAC,CAAC;IACJ;EAAC;IAAAgF,GAAA;IAAAwB,GAAA,EAED,SAAAA,IAAA,EAA6B;MAC3B,IAAI,CAAC,IAAI,CAAC0C,SAAS,EAAE;QACnB,OAAO,EAAE;;MAEX;MACA,OAAO,IAAI,CAAChF,IAAI,CAACiF,gBAAgB;IACnC;EAAC;IAAAnE,GAAA;IAAAwB,GAAA,EAED,SAAAA,IAAA,EAAgC;MAC9B;MACA,IAAI,CAAC,IAAI,CAAC0C,SAAS,EAAE;QACnB,OAAO,IAAI,CAAChF,IAAI,CAACkF,OAAO;;MAE1B,OAAO,IAAI,CAAClF,IAAI,CAACmF,mBAAmB;IACtC;EAAC;IAAArE,GAAA;IAAAC,KAAA,EAEQ,SAAAqE,6BAA6BrE,KAAc;MAClDsD,IAAA,CAAAC,eAAA,CAAA/E,GAAA,CAAAgF,SAAA,yCAAAxE,IAAA,OAAmCgB,KAAK;MACxC,IAAI,IAAI,CAACf,IAAI,IAAI,IAAI,EAAE;QACrB,IAAI,CAACA,IAAI,CAACoF,4BAA4B,CAACrE,KAAK,CAAC;;IAEjD;EAAC;IAAAD,GAAA;IAAAC,KAAA,EAEQ,SAAAsE,UAAA,EAAS;MAChB,IAAMC,UAAU,GAAAjB,IAAA,CAAAC,eAAA,CAAA/E,GAAA,CAAAgF,SAAA,sBAAAxE,IAAA,MAAoB;MAEpC,IAAMwF,MAAM,GAA6B;QACvCnF,eAAe,EAAE,IAAI,CAACA,eAAe;QACrCC,WAAW,EAAE,IAAI,CAACA,WAAW;QAC7B3D,WAAW,EAAE,IAAI,CAACA,WAAW;QAC7B6D,QAAQ,EAAE,IAAI,CAACA,QAAQ;QACvBzD,MAAM,EAAE,IAAI,CAACA;OACd;MAED,IAAI,IAAI,CAACd,YAAY,IAAI,IAAI,EAAE;QAC7BuJ,MAAM,CAAC,cAAc,CAAC,GAAG,IAAI,CAACvJ,YAAY;;MAG5C,IAAMwJ,UAAU,GAAG,IAAI,CAACxF,IAAI,CAACqF,SAAS,EAAE;MAExC,IAAI,IAAI,CAACI,YAAY,EAAE,KAAKlG,GAAG,CAACmG,SAAS,EAAE;QACzCH,MAAM,CAAC,MAAM,CAAC,GAAG;UACf,WAAW,EAAE,IAAI,CAACvF,IAAI,CAACyF,YAAY,EAAE;UACrC,QAAQ,EAAED;SACsB;;MAGpC;MACA,OAAAG,MAAA,CAAAC,MAAA,CAAAD,MAAA,CAAAC,MAAA,CAAAD,MAAA,CAAAC,MAAA,KAAWJ,UAAU,GAAKF,UAAU,GAAKC,MAAM;IACjD;IAEA;EAAA;IAAAzE,GAAA;IAAAC,KAAA,EACA,SAAA8E,WACIC,GAA6C,EAC7CP,MAAgC,EACc;MAAA,IAA9CQ,aAAA,GAAApJ,SAAA,CAAAP,MAAA,QAAAO,SAAA,QAAAC,SAAA,GAAAD,SAAA,MAAgB,EAA8B;MAChD,IAAM6I,UAAU,GAAGD,MAAM,CAAC,MAAM,CAA6B;MAC7D,IAAMvF,IAAI,GAAGrE,WAAW,CAAC6J,UAAU,EAAEO,aAAa,CAAY;MAC9D,OAAO,IAAID,GAAG,CAACH,MAAM,CAACC,MAAM,CAACL,MAAM,EAAE;QAACvF,IAAI,EAAJA;MAAI,CAAC,CAAC,CAAC;IAC/C;EAAC;EAAA,OAAAT,GAAA;AAAA,EAxfsB7E,KAAK;AAC5B;AACO6E,GAAA,CAAAmG,SAAS,GAAG,KAAK;AAwf1B3L,aAAa,CAACiM,aAAa,CAACzG,GAAG,CAAC;AAEhC;AACA;AACA;AACA;;;;;AAKA,WAAsB0G,OAAQ,0BAAAC,OAAA;EAAAzG,SAAA,CAAAwG,OAAA,EAAAC,OAAA;EAAA,IAAAC,OAAA,GAAAxG,YAAA,CAAAsG,OAAA;EAAA,SAAAA,QAAA;IAAAnG,eAAA,OAAAmG,OAAA;IAAA,OAAAE,OAAA,CAAAzC,KAAA,OAAA/G,SAAA;EAAA;EAAA,OAAAkE,YAAA,CAAAoF,OAAA;AAAA,EAAQvL,KAAK;AA+F3C,WAAa0L,aAAc,0BAAAC,QAAA;EAAA5G,SAAA,CAAA2G,aAAA,EAAAC,QAAA;EAAA,IAAAC,OAAA,GAAA3G,YAAA,CAAAyG,aAAA;EAkCzB,SAAAA,cAAYxG,IAA4B;IAAA,IAAA2G,MAAA;IAAAzG,eAAA,OAAAsG,aAAA;IACtCG,MAAA,GAAAD,OAAA,CAAAvG,IAAA,OAAMH,IAAI;IANH2G,MAAA,CAAAC,kBAAkB,GAAG,MAAM;IAC3BD,MAAA,CAAAE,0BAA0B,GAAG,cAAc;IAC3CF,MAAA,CAAAG,6BAA6B,GAAG,YAAY;IAC5CH,MAAA,CAAAI,wBAAwB,GAA0B,OAAO;IAIhEJ,MAAA,CAAKK,KAAK,GAAGhH,IAAI,CAACgH,KAAK;IACvBxL,qBAAqB,CAACmL,MAAA,CAAKK,KAAK,UAAU;IAC1CL,MAAA,CAAKM,UAAU,GAAG3M,aAAa,CAC3B0F,IAAI,CAACiH,UAAU,IAAI,IAAI,GAAGN,MAAA,CAAKC,kBAAkB,GAAG5G,IAAI,CAACiH,UAAU,CAAC;IACxEN,MAAA,CAAKO,OAAO,GAAGlH,IAAI,CAACkH,OAAO,IAAI,IAAI,GAAG,IAAI,GAAGlH,IAAI,CAACkH,OAAO;IAEzDP,MAAA,CAAKQ,iBAAiB,GAAGjM,cAAc,CACnC8E,IAAI,CAACmH,iBAAiB,IAAIR,MAAA,CAAKE,0BAA0B,CAAC;IAC9DF,MAAA,CAAKS,oBAAoB,GAAGlM,cAAc,CACtC8E,IAAI,CAACoH,oBAAoB,IAAIT,MAAA,CAAKG,6BAA6B,CAAC;IAEpEH,MAAA,CAAKU,eAAe,GAChBnM,cAAc,CAAC8E,IAAI,CAACqH,eAAe,IAAIV,MAAA,CAAKI,wBAAwB,CAAC;IAEzEJ,MAAA,CAAKW,iBAAiB,GAAGhM,cAAc,CAAC0E,IAAI,CAACsH,iBAAiB,CAAC;IAC/DX,MAAA,CAAKY,oBAAoB,GAAGjM,cAAc,CAAC0E,IAAI,CAACuH,oBAAoB,CAAC;IACrEZ,MAAA,CAAKa,eAAe,GAAGlM,cAAc,CAAC0E,IAAI,CAACwH,eAAe,CAAC;IAE3Db,MAAA,CAAKc,gBAAgB,GAAG/M,aAAa,CAACsF,IAAI,CAACyH,gBAAgB,CAAC;IAC5Dd,MAAA,CAAKe,mBAAmB,GAAGhN,aAAa,CAACsF,IAAI,CAAC0H,mBAAmB,CAAC;IAClEf,MAAA,CAAKgB,cAAc,GAAGjN,aAAa,CAACsF,IAAI,CAAC2H,cAAc,CAAC;IAExDhB,MAAA,CAAKiB,OAAO,GAAGnM,UAAU,CAACoM,GAAG,CACzB,CAAC,CAAC,EAAEpM,UAAU,CAACqM,GAAG,CAAC,CAAC,CAAC,EAAE9H,IAAI,CAAC4H,OAAO,IAAI,IAAI,GAAG,CAAC,GAAG5H,IAAI,CAAC4H,OAAO,CAAC,CAAC,CAAC,CAAC;IACtEjB,MAAA,CAAKoB,gBAAgB,GAAGtM,UAAU,CAACoM,GAAG,CAAC,CACrC,CAAC,EACDpM,UAAU,CAACqM,GAAG,CACV,CAAC,CAAC,EAAE9H,IAAI,CAAC+H,gBAAgB,IAAI,IAAI,GAAG,CAAC,GAAG/H,IAAI,CAAC+H,gBAAgB,CAAC,CAAC,CACpE,CAAC;IACFpB,MAAA,CAAKqB,WAAW,GAAGhI,IAAI,CAACgI,WAAW;IACnCrB,MAAA,CAAKpG,SAAS,GAAGoG,MAAA,CAAKK,KAAK;IAC3BL,MAAA,CAAKsB,WAAW,GAAG,IAAI;IACvBtB,MAAA,CAAKuB,oBAAoB,GAAG,IAAI;IAAC,OAAAvB,MAAA;EACnC;EAAC1F,YAAA,CAAAuF,aAAA;IAAAtF,GAAA;IAAAC,KAAA,EAEQ,SAAAyB,MAAMpB,UAAyB;MACtCA,UAAU,GAAG9F,kBAAkB,CAAC8F,UAAU,CAAC;MAC3C;MACA,IAAI,CAAC2G,MAAM,GAAG,IAAI,CAACC,SAAS,CACxB,QAAQ,EAAE,CAAC5G,UAAU,CAACA,UAAU,CAAChF,MAAM,GAAG,CAAC,CAAC,EAAE,IAAI,CAACwK,KAAK,CAAC,EAAE,IAAI,EAC/D,IAAI,CAACG,iBAAiB,EAAE,IAAI,CAACG,iBAAiB,EAAE,IAAI,EACpD,IAAI,CAACG,gBAAgB,CAAC;MAC1B,IAAI,CAACY,eAAe,GAAG,IAAI,CAACD,SAAS,CACjC,kBAAkB,EAAE,CAAC,IAAI,CAACpB,KAAK,EAAE,IAAI,CAACA,KAAK,CAAC,EAAE,IAAI,EAClD,IAAI,CAACI,oBAAoB,EAAE,IAAI,CAACG,oBAAoB,EAAE,IAAI,EAC1D,IAAI,CAACG,mBAAmB,CAAC;MAC7B,IAAI,IAAI,CAACR,OAAO,EAAE;QAChB,IAAI,CAACoB,IAAI,GAAG,IAAI,CAACF,SAAS,CACtB,MAAM,EAAE,CAAC,IAAI,CAACpB,KAAK,CAAC,EAAE,IAAI,EAAE,IAAI,CAACK,eAAe,EAChD,IAAI,CAACG,eAAe,EAAE,IAAI,EAAE,IAAI,CAACG,cAAc,CAAC;OACrD,MAAM;QACL,IAAI,CAACW,IAAI,GAAG,IAAI;;MAElB,IAAI,CAACC,KAAK,GAAG,IAAI;IACnB;IAEA;IACA;IACA;IACA;IACA;IACA;EAAA;IAAArH,GAAA;IAAAC,KAAA,EACS,SAAAhB,KAAKlE,MAAuB,EAAE8H,MAAc;MAAA,IAAAyE,MAAA;MACnD,OAAOpO,IAAI,CAAC,YAAK;QACf6B,MAAM,GAAGA,MAAkB;QAC3B,IAAIA,MAAM,CAACO,MAAM,KAAK,CAAC,EAAE;UACvB,MAAM,IAAIvB,UAAU,+CAAAqC,MAAA,CAC8BrB,MAAM,CAACO,MAAM,OAAI;;QAErE,IAAIiM,UAAU,GAAGxM,MAAM,CAAC,CAAC,CAAC;QAC1BA,MAAM,GAAGA,MAAM,CAAC,CAAC,CAAC;QAClB,IAAMqH,QAAQ,GAAGS,MAAM,CAAC,UAAU,CAAC,IAAI,IAAI,GAAG,KAAK,GAAGA,MAAM,CAAC,UAAU,CAAC;QAExE,IAAI,CAAC,GAAGyE,MAAI,CAACZ,OAAO,IAAIY,MAAI,CAACZ,OAAO,GAAG,CAAC,IAAIY,MAAI,CAACP,WAAW,IAAI,IAAI,EAAE;UACpEO,MAAI,CAACP,WAAW,GAAGS,mBAAmB,CAAC;YAClBC,IAAI,EAAE,SAAAA,KAAA;cAAA,OAAMzO,GAAG,CAAC6E,QAAQ,CAAC9C,MAAgB,CAAC;YAAA;YAC1C2M,IAAI,EAAEJ,MAAI,CAACZ,OAAO;YAClBtE,QAAQ,EAARA,QAAQ;YACR0E,WAAW,EAAEQ,MAAI,CAACR;WACnB,CAAW;;QAEjC,IAAI,CAAC,GAAGQ,MAAI,CAACT,gBAAgB,IAAIS,MAAI,CAACT,gBAAgB,GAAG,CAAC,IACtDS,MAAI,CAACN,oBAAoB,IAAI,IAAI,EAAE;UACrCM,MAAI,CAACN,oBAAoB,GAAGQ,mBAAmB,CAAC;YAClBC,IAAI,EAAE,SAAAA,KAAA;cAAA,OAAMzO,GAAG,CAAC6E,QAAQ,CAAC0J,UAAU,CAAC;YAAA;YACpCG,IAAI,EAAEJ,MAAI,CAACT,gBAAgB;YAC3BzE,QAAQ,EAARA,QAAQ;YACR0E,WAAW,EAAEQ,MAAI,CAACR;WACnB,CAAW;;QAE1C,IAAIa,CAAS;QACb,IAAMC,MAAM,GAAWN,MAAI,CAACP,WAAqB;QACjD,IAAMc,SAAS,GAAWP,MAAI,CAACN,oBAA8B;QAC7D,IAAIY,MAAM,IAAI,IAAI,EAAE;UAClBD,CAAC,GAAGrO,CAAC,CAACwO,GAAG,CAAC9O,GAAG,CAACgF,GAAG,CAACjD,MAAM,EAAE6M,MAAM,CAAC,EAAEN,MAAI,CAACL,MAAM,CAACc,IAAI,EAAE,CAAC;SACvD,MAAM;UACLJ,CAAC,GAAGrO,CAAC,CAACwO,GAAG,CAAC/M,MAAM,EAAEuM,MAAI,CAACL,MAAM,CAACc,IAAI,EAAE,CAAC;;QAEvC,IAAIT,MAAI,CAACF,IAAI,IAAI,IAAI,EAAE;UACrBO,CAAC,GAAGrO,CAAC,CAAC0O,OAAO,CAACL,CAAC,EAAEL,MAAI,CAACF,IAAI,CAACW,IAAI,EAAE,CAAC;;QAEpC,IAAIF,SAAS,IAAI,IAAI,EAAE;UACrBN,UAAU,GAAGvO,GAAG,CAACgF,GAAG,CAACuJ,UAAU,EAAEM,SAAS,CAAC;;QAE7C,IAAI/J,MAAM,GAAG9E,GAAG,CAAC+E,GAAG,CAAC4J,CAAC,EAAErO,CAAC,CAACwO,GAAG,CAACP,UAAU,EAAED,MAAI,CAACH,eAAe,CAACY,IAAI,EAAE,CAAC,CAAC;QACvE,IAAIT,MAAI,CAACvB,UAAU,IAAI,IAAI,EAAE;UAC3BjI,MAAM,GAAGwJ,MAAI,CAACvB,UAAU,CAACnD,KAAK,CAAC9E,MAAM,CAAC;;QAGxC;QACA,OAAO,CAACA,MAAM,EAAEA,MAAM,CAAC;MACzB,CAAC,CAAC;IACJ;EAAC;IAAAkC,GAAA;IAAAC,KAAA,EAEQ,SAAAsE,UAAA,EAAS;MAChB,IAAMC,UAAU,GAAAjB,IAAA,CAAAC,eAAA,CAAA8B,aAAA,CAAA7B,SAAA,sBAAAxE,IAAA,MAAoB;MAEpC,IAAMwF,MAAM,GAA6B;QACvCqB,KAAK,EAAE,IAAI,CAACA,KAAK;QACjBC,UAAU,EAAE1M,mBAAmB,CAAC,IAAI,CAAC0M,UAAU,CAAC;QAChDC,OAAO,EAAE,IAAI,CAACA,OAAO;QACrBC,iBAAiB,EAAE9L,oBAAoB,CAAC,IAAI,CAAC8L,iBAAiB,CAAC;QAC/DC,oBAAoB,EAAE/L,oBAAoB,CAAC,IAAI,CAAC+L,oBAAoB,CAAC;QACrEC,eAAe,EAAEhM,oBAAoB,CAAC,IAAI,CAACgM,eAAe,CAAC;QAC3DC,iBAAiB,EAAE/L,oBAAoB,CAAC,IAAI,CAAC+L,iBAAiB,CAAC;QAC/DC,oBAAoB,EAAEhM,oBAAoB,CAAC,IAAI,CAACgM,oBAAoB,CAAC;QACrEC,eAAe,EAAEjM,oBAAoB,CAAC,IAAI,CAACiM,eAAe,CAAC;QAC3D2B,mBAAmB,EAAE5N,oBAAoB,CAAC,IAAI,CAAC4N,mBAAmB,CAAC;QACnE1B,gBAAgB,EAAE9M,mBAAmB,CAAC,IAAI,CAAC8M,gBAAgB,CAAC;QAC5DC,mBAAmB,EAAE/M,mBAAmB,CAAC,IAAI,CAAC+M,mBAAmB,CAAC;QAClEC,cAAc,EAAEhN,mBAAmB,CAAC,IAAI,CAACgN,cAAc,CAAC;QACxDC,OAAO,EAAE,IAAI,CAACA,OAAO;QACrBG,gBAAgB,EAAE,IAAI,CAACA;OACxB;MAED,OAAAhC,MAAA,CAAAC,MAAA,CAAAD,MAAA,CAAAC,MAAA,KAAWN,UAAU,GAAKC,MAAM;IAClC;EAAC;EAAA,OAAAa,aAAA;AAAA,EA5KgCH,OAAO;AACxC;AACOG,aAAA,CAAAV,SAAS,GAAG,eAAe;AA4KpC3L,aAAa,CAACiM,aAAa,CAACI,aAAa,CAAC;AAgG1C,WAAa4C,SAAU,0BAAAC,IAAA;EAAAxJ,SAAA,CAAAuJ,SAAA,EAAAC,IAAA;EAAA,IAAAC,OAAA,GAAAvJ,YAAA,CAAAqJ,SAAA;EAGrB,SAAAA,UAAYpJ,IAAwB;IAAAE,eAAA,OAAAkJ,SAAA;IAClCpJ,IAAI,CAACI,IAAI,GAAG,IAAIoG,aAAa,CAACxG,IAAI,CAAC;IAAC,OAAAsJ,OAAA,CAAAnJ,IAAA,OAC9BH,IAAoB,GAC1B;EACF;EAACiB,YAAA,CAAAmI,SAAA;IAAAlI,GAAA;IAAAC,KAAA,EAEQ,SAAAhB,KAAKlE,MAAuB,EAAE8H,MAAc;MAAA,IAAAwF,MAAA;MACnD,OAAOnP,IAAI,CAAC,YAAK;QACf,IAAImP,MAAI,CAACnJ,IAAI,CAAC6H,WAAW,IAAI,IAAI,EAAE;UACjC/N,GAAG,CAACsJ,OAAO,CAAC+F,MAAI,CAACnJ,IAAI,CAAC6H,WAAW,CAAC;UAClCsB,MAAI,CAACnJ,IAAI,CAAC6H,WAAW,GAAG,IAAI;;QAE9B,IAAIsB,MAAI,CAACnJ,IAAI,CAAC8H,oBAAoB,IAAI,IAAI,EAAE;UAC1ChO,GAAG,CAACsJ,OAAO,CAAC+F,MAAI,CAACnJ,IAAI,CAAC8H,oBAAoB,CAAC;UAC3CqB,MAAI,CAACnJ,IAAI,CAAC8H,oBAAoB,GAAG,IAAI;;QAEvC,IAAMjL,IAAI,GAAG8G,MAAM,IAAI,IAAI,GAAG,IAAI,GAAGA,MAAM,CAAC,MAAM,CAAC;QACnD,IAAMT,QAAQ,GAAGS,MAAM,IAAI,IAAI,GAAG,IAAI,GAAGA,MAAM,CAAC,UAAU,CAAC;QAC3D,IAAM7H,YAAY,GACd6H,MAAM,IAAI,IAAI,GAAG,IAAI,GAAGA,MAAM,CAAC,cAAc,CAAC;QAClD,OAAAU,IAAA,CAAAC,eAAA,CAAA0E,SAAA,CAAAzE,SAAA,WAAA4E,MAAA,EAAApJ,IAAA,CAAAoJ,MAAA,EAAkBtN,MAAM,EAAE;UAACgB,IAAI,EAAJA,IAAI;UAAEqG,QAAQ,EAARA,QAAQ;UAAEpH,YAAY,EAAZA;QAAY,CAAC;MAC1D,CAAC,CAAC;IACJ;IAEA;EAAA;IAAAgF,GAAA;IAAAC,KAAA,EACA,SAAA8E,WACIC,GAA6C,EAC7CP,MAAgC;MAClC,OAAO,IAAIO,GAAG,CAACP,MAAM,CAAC;IACxB;EAAC;EAAA,OAAAyD,SAAA;AAAA,EAhC4BzJ,GAAG;AAChC;AACgByJ,SAAA,CAAAtD,SAAS,GAAG,WAAW;AAgCzC3L,aAAa,CAACiM,aAAa,CAACgD,SAAS,CAAC;AAqCtC,WAAaI,OAAQ,0BAAAC,SAAA;EAAA5J,SAAA,CAAA2J,OAAA,EAAAC,SAAA;EAAA,IAAAC,OAAA,GAAA3J,YAAA,CAAAyJ,OAAA;EAsCnB,SAAAA,QAAYxJ,IAAsB;IAAA,IAAA2J,MAAA;IAAAzJ,eAAA,OAAAsJ,OAAA;IAChCG,MAAA,GAAAD,OAAA,CAAAvJ,IAAA,OAAMH,IAAI;IAZH2J,MAAA,CAAA/C,kBAAkB,GAAG,MAAM;IAC3B+C,MAAA,CAAAC,4BAA4B,GAAyB,aAAa;IAElED,MAAA,CAAA9C,0BAA0B,GAAG,cAAc;IAC3C8C,MAAA,CAAA7C,6BAA6B,GAAG,YAAY;IAC5C6C,MAAA,CAAA5C,wBAAwB,GAA0B,OAAO;IAQhE,IAAI/G,IAAI,CAAC6J,UAAU,EAAE;MACnB,MAAM,IAAI5O,UAAU,+DAC8C;;IAEpE0O,MAAA,CAAK3C,KAAK,GAAGhH,IAAI,CAACgH,KAAK;IACvBxL,qBAAqB,CAACmO,MAAA,CAAK3C,KAAK,EAAE,OAAO,CAAC;IAC1C2C,MAAA,CAAK1C,UAAU,GAAG3M,aAAa,CAC3B0F,IAAI,CAACiH,UAAU,KAAKjK,SAAS,GAAG2M,MAAA,CAAK/C,kBAAkB,GACvB5G,IAAI,CAACiH,UAAU,CAAC;IACpD0C,MAAA,CAAKG,mBAAmB,GAAGxP,aAAa,CACpC0F,IAAI,CAAC8J,mBAAmB,KAAK9M,SAAS,GAClC2M,MAAA,CAAKC,4BAA4B,GACjC5J,IAAI,CAAC8J,mBAAmB,CAAC;IACjCH,MAAA,CAAKzC,OAAO,GAAGlH,IAAI,CAACkH,OAAO,IAAI,IAAI,GAAG,IAAI,GAAGlH,IAAI,CAACkH,OAAO;IAEzDyC,MAAA,CAAKxC,iBAAiB,GAAGjM,cAAc,CACnC8E,IAAI,CAACmH,iBAAiB,IAAIwC,MAAA,CAAK9C,0BAA0B,CAAC;IAC9D8C,MAAA,CAAKvC,oBAAoB,GAAGlM,cAAc,CACtC8E,IAAI,CAACoH,oBAAoB,IAAIuC,MAAA,CAAK7C,6BAA6B,CAAC;IAEpE6C,MAAA,CAAKtC,eAAe,GAChBnM,cAAc,CAAC8E,IAAI,CAACqH,eAAe,IAAIsC,MAAA,CAAK5C,wBAAwB,CAAC;IAEzE4C,MAAA,CAAKrC,iBAAiB,GAAGhM,cAAc,CAAC0E,IAAI,CAACsH,iBAAiB,CAAC;IAC/DqC,MAAA,CAAKpC,oBAAoB,GAAGjM,cAAc,CAAC0E,IAAI,CAACuH,oBAAoB,CAAC;IACrEoC,MAAA,CAAKnC,eAAe,GAAGlM,cAAc,CAAC0E,IAAI,CAACwH,eAAe,CAAC;IAE3DmC,MAAA,CAAKlC,gBAAgB,GAAG/M,aAAa,CAACsF,IAAI,CAACyH,gBAAgB,CAAC;IAC5DkC,MAAA,CAAKjC,mBAAmB,GAAGhN,aAAa,CAACsF,IAAI,CAAC0H,mBAAmB,CAAC;IAClEiC,MAAA,CAAKhC,cAAc,GAAGjN,aAAa,CAACsF,IAAI,CAAC2H,cAAc,CAAC;IAExDgC,MAAA,CAAK/B,OAAO,GAAGnM,UAAU,CAACoM,GAAG,CACzB,CAAC,CAAC,EAAEpM,UAAU,CAACqM,GAAG,CAAC,CAAC,CAAC,EAAE9H,IAAI,CAAC4H,OAAO,IAAI,IAAI,GAAG,CAAC,GAAG5H,IAAI,CAAC4H,OAAO,CAAC,CAAC,CAAC,CAAC;IACtE+B,MAAA,CAAK5B,gBAAgB,GAAGtM,UAAU,CAACoM,GAAG,CAAC,CACrC,CAAC,EACDpM,UAAU,CAACqM,GAAG,CACV,CAAC,CAAC,EAAE9H,IAAI,CAAC+H,gBAAgB,IAAI,IAAI,GAAG,CAAC,GAAG/H,IAAI,CAAC+H,gBAAgB,CAAC,CAAC,CACpE,CAAC;IACF4B,MAAA,CAAK3B,WAAW,GAAGhI,IAAI,CAACgI,WAAW;IACnC2B,MAAA,CAAKI,cAAc,GAAG/J,IAAI,CAAC+J,cAAc;IACzCJ,MAAA,CAAKpJ,SAAS,GAAGoJ,MAAA,CAAK3C,KAAK;IAC3B2C,MAAA,CAAK1B,WAAW,GAAG,IAAI;IACvB0B,MAAA,CAAKzB,oBAAoB,GAAG,IAAI;IAAC,OAAAyB,MAAA;EACnC;EAAC1I,YAAA,CAAAuI,OAAA;IAAAtI,GAAA;IAAAC,KAAA,EAEe,SAAAyB,MAAMpB,UAAyB;MAC7CA,UAAU,GAAG9F,kBAAkB,CAAC8F,UAAU,CAAC;MAC3C,IAAMuB,QAAQ,GAAGvB,UAAU,CAACA,UAAU,CAAChF,MAAM,GAAG,CAAC,CAAC;MAClD,IAAI,CAAC2L,MAAM,GAAG,IAAI,CAACC,SAAS,CACxB,QAAQ,EAAE,CAACrF,QAAQ,EAAE,IAAI,CAACiE,KAAK,GAAG,CAAC,CAAC,EAAE,IAAI,EAAE,IAAI,CAACG,iBAAiB,EAClE,IAAI,CAACG,iBAAiB,EAAE,IAAI,EAAE,IAAI,CAACG,gBAAgB,CAAC;MACxD,IAAI,CAACY,eAAe,GAAG,IAAI,CAACD,SAAS,CACjC,kBAAkB,EAAE,CAAC,IAAI,CAACpB,KAAK,EAAE,IAAI,CAACA,KAAK,GAAG,CAAC,CAAC,EAAE,IAAI,EACtD,IAAI,CAACI,oBAAoB,EAAE,IAAI,CAACG,oBAAoB,EAAE,IAAI,EAC1D,IAAI,CAACG,mBAAmB,CAAC;MAC7B,IAAI,IAAI,CAACR,OAAO,EAAE;QAChB,IAAI,CAACoB,IAAI,GAAG,IAAI,CAACF,SAAS,CACtB,MAAM,EAAE,CAAC,IAAI,CAACpB,KAAK,GAAG,CAAC,CAAC,EAAE,IAAI,EAAE,IAAI,CAACK,eAAe,EACpD,IAAI,CAACG,eAAe,EAAE,IAAI,EAAE,IAAI,CAACG,cAAc,CAAC;OACrD,MAAM;QACL,IAAI,CAACW,IAAI,GAAG,IAAI;;MAElB;MACA;MACA,IAAI,CAACC,KAAK,GAAG,IAAI;IACnB;EAAC;IAAArH,GAAA;IAAAC,KAAA,EAEQ,SAAAhB,KAAKlE,MAAuB,EAAE8H,MAAc;MAAA,IAAAiG,OAAA;MACnD,OAAO5P,IAAI,CAAC,YAAK;QACf6B,MAAM,GAAGA,MAAkB;QAC3B,IAAIA,MAAM,CAACO,MAAM,KAAK,CAAC,EAAE;UACvB,MAAM,IAAIvB,UAAU,CAChB,4DAAAqC,MAAA,CACGrB,MAAM,CAACO,MAAM,MAAG,CAAC;;QAG1B,IAAM8G,QAAQ,GAAGS,MAAM,CAAC,UAAU,CAAC,IAAI,IAAI,GAAG,KAAK,GAAGA,MAAM,CAAC,UAAU,CAAC;QACxE,IAAIkG,QAAQ,GAAGhO,MAAM,CAAC,CAAC,CAAC,CAAC,CAAE;QAC3BA,MAAM,GAAGA,MAAM,CAAC,CAAC,CAAC;QAElB;QACA;QACA;QACA,IAAI,CAAC,GAAG+N,OAAI,CAACpC,OAAO,IAAIoC,OAAI,CAACpC,OAAO,GAAG,CAAC,IAAIoC,OAAI,CAAC/B,WAAW,IAAI,IAAI,EAAE;UACpE+B,OAAI,CAAC/B,WAAW,GAAGS,mBAAmB,CAAC;YAClBC,IAAI,EAAE,SAAAA,KAAA;cAAA,OAAMzO,GAAG,CAAC6E,QAAQ,CAAC9C,MAAgB,CAAC;YAAA;YAC1C2M,IAAI,EAAEoB,OAAI,CAACpC,OAAO;YAClBtE,QAAQ,EAARA,QAAQ;YACR4G,KAAK,EAAE,CAAC;YACRlC,WAAW,EAAEgC,OAAI,CAAChC;WACnB,CAAa;;QAEnC,IAAI,CAAC,GAAGgC,OAAI,CAACjC,gBAAgB,IAAIiC,OAAI,CAACjC,gBAAgB,GAAG,CAAC,IACtDiC,OAAI,CAAC9B,oBAAoB,IAAI,IAAI,EAAE;UACrC8B,OAAI,CAAC9B,oBAAoB,GAAGQ,mBAAmB,CAAC;YAClBC,IAAI,EAAE,SAAAA,KAAA;cAAA,OAAMzO,GAAG,CAAC6E,QAAQ,CAACkL,QAAQ,CAAC;YAAA;YAClCrB,IAAI,EAAEoB,OAAI,CAACjC,gBAAgB;YAC3BzE,QAAQ,EAARA,QAAQ;YACR4G,KAAK,EAAE,CAAC;YACRlC,WAAW,EAAEgC,OAAI,CAAChC;WACnB,CAAa;;QAE5C,IAAMc,MAAM,GAAGkB,OAAI,CAAC/B,WAAuC;QAC3D,IAAMc,SAAS,GAAGiB,OAAI,CAAC9B,oBAAgD;QACvE,IAAIiC,CAAS;QACb,IAAIC,CAAS;QACb,IAAIC,EAAU;QAEd,IAAI,CAAC,GAAGL,OAAI,CAACpC,OAAO,IAAIoC,OAAI,CAACpC,OAAO,GAAG,CAAC,EAAE;UACxC3L,MAAM,GAAG/B,GAAG,CAACgF,GAAG,CAACjD,MAAM,EAAE6M,MAAM,CAAC,CAAC,CAAC,CAAC;;QAErC,IAAIwB,OAAO,GAAG9P,CAAC,CAACwO,GAAG,CAAC/M,MAAM,EAAE+N,OAAI,CAAC7B,MAAM,CAACc,IAAI,EAAE,CAAC;QAC/C,IAAIe,OAAI,CAAC9C,OAAO,EAAE;UAChBoD,OAAO,GAAG9P,CAAC,CAAC0O,OAAO,CAACoB,OAAO,EAAEN,OAAI,CAAC1B,IAAI,CAACW,IAAI,EAAE,CAAC;;QAEhD,IAAI,CAAC,GAAGe,OAAI,CAACjC,gBAAgB,IAAIiC,OAAI,CAACjC,gBAAgB,GAAG,CAAC,EAAE;UAC1DkC,QAAQ,GAAG/P,GAAG,CAACgF,GAAG,CAAC+K,QAAQ,EAAElB,SAAS,CAAC,CAAC,CAAC,CAAC;;QAG5C,IAAMwB,oBAAoB,GAAGP,OAAI,CAAC3B,eAAe,CAACY,IAAI,EAAE;QACxD,IAAAuB,UAAA,GAAmBtQ,GAAG,CAACuQ,KAAK,CACxBF,oBAAoB,EAAE,CAAC,CAAC,GAAGP,OAAI,CAAChD,KAAK,EAAEgD,OAAI,CAAChD,KAAK,CAAC,EAClDuD,oBAAoB,CAAC1M,IAAI,GAAG,CAAC,CAAC;UAAA6M,WAAA,GAAAC,cAAA,CAAAH,UAAA;UAF3BI,GAAG,GAAAF,WAAA;UAAEG,GAAG,GAAAH,WAAA;QAGf,IAAMI,WAAW,GAAGtQ,CAAC,CAACwO,GAAG,CAACiB,QAAQ,EAAEW,GAAG,CAAC;QAExC,IAAAG,WAAA,GAAqB7Q,GAAG,CAACuQ,KAAK,CAACH,OAAO,EAAE,CAAC,EAAEA,OAAO,CAACzM,IAAI,GAAG,CAAC,CAAC;UAAAmN,WAAA,GAAAL,cAAA,CAAAI,WAAA;UAArDE,EAAE,GAAAD,WAAA;UAAEE,EAAE,GAAAF,WAAA;UAAEG,EAAE,GAAAH,WAAA;QACjB,IAAAI,WAAA,GACIlR,GAAG,CAACuQ,KAAK,CAACK,WAAW,EAAE,CAAC,EAAEA,WAAW,CAACjN,IAAI,GAAG,CAAC,CAAC;UAAAwN,WAAA,GAAAV,cAAA,CAAAS,WAAA;UAD5CE,UAAU,GAAAD,WAAA;UAAEE,UAAU,GAAAF,WAAA;QAE7BlB,CAAC,GAAGH,OAAI,CAACF,mBAAmB,CAAChG,KAAK,CAAC5J,GAAG,CAAC+E,GAAG,CAACgM,EAAE,EAAEK,UAAU,CAAC,CAAC;QAC3DlB,CAAC,GAAGJ,OAAI,CAACF,mBAAmB,CAAChG,KAAK,CAAC5J,GAAG,CAAC+E,GAAG,CAACiM,EAAE,EAAEK,UAAU,CAAC,CAAC;QAE3D,IAAMC,UAAU,GAAGhR,CAAC,CAACwO,GAAG,CAAC9O,GAAG,CAACgF,GAAG,CAACkL,CAAC,EAAEH,QAAQ,CAAC,EAAEY,GAAG,CAAC;QACnDR,EAAE,GAAGL,OAAI,CAAC/C,UAAU,CAACnD,KAAK,CAAC5J,GAAG,CAAC+E,GAAG,CAACkM,EAAE,EAAEK,UAAU,CAAC,CAAC;QAEnD,IAAM3C,CAAC,GACH3O,GAAG,CAAC+E,GAAG,CAAC/E,GAAG,CAACgF,GAAG,CAACiL,CAAC,EAAEF,QAAQ,CAAC,EAAE/P,GAAG,CAACgF,GAAG,CAAChF,GAAG,CAAC+E,GAAG,CAAC,CAAC,EAAE/E,GAAG,CAACuR,GAAG,CAACtB,CAAC,CAAC,CAAC,EAAEE,EAAE,CAAC,CAAC;QACtE;QACA,OAAO,CAACxB,CAAC,EAAEA,CAAC,CAAC;MACf,CAAC,CAAC;IACJ;EAAC;IAAA3H,GAAA;IAAAC,KAAA,EAEQ,SAAAsE,UAAA,EAAS;MAChB,IAAMC,UAAU,GAAAjB,IAAA,CAAAC,eAAA,CAAA8E,OAAA,CAAA7E,SAAA,sBAAAxE,IAAA,MAAoB;MAEpC,IAAMwF,MAAM,GAA6B;QACvCqB,KAAK,EAAE,IAAI,CAACA,KAAK;QACjBC,UAAU,EAAE1M,mBAAmB,CAAC,IAAI,CAAC0M,UAAU,CAAC;QAChD6C,mBAAmB,EAAEvP,mBAAmB,CAAC,IAAI,CAACuP,mBAAmB,CAAC;QAClE5C,OAAO,EAAE,IAAI,CAACA,OAAO;QACrBC,iBAAiB,EAAE9L,oBAAoB,CAAC,IAAI,CAAC8L,iBAAiB,CAAC;QAC/DC,oBAAoB,EAAE/L,oBAAoB,CAAC,IAAI,CAAC+L,oBAAoB,CAAC;QACrEC,eAAe,EAAEhM,oBAAoB,CAAC,IAAI,CAACgM,eAAe,CAAC;QAC3DC,iBAAiB,EAAE/L,oBAAoB,CAAC,IAAI,CAAC+L,iBAAiB,CAAC;QAC/DC,oBAAoB,EAAEhM,oBAAoB,CAAC,IAAI,CAACgM,oBAAoB,CAAC;QACrEC,eAAe,EAAEjM,oBAAoB,CAAC,IAAI,CAACiM,eAAe,CAAC;QAC3D2B,mBAAmB,EAAE5N,oBAAoB,CAAC,IAAI,CAAC4N,mBAAmB,CAAC;QACnE1B,gBAAgB,EAAE9M,mBAAmB,CAAC,IAAI,CAAC8M,gBAAgB,CAAC;QAC5DC,mBAAmB,EAAE/M,mBAAmB,CAAC,IAAI,CAAC+M,mBAAmB,CAAC;QAClEC,cAAc,EAAEhN,mBAAmB,CAAC,IAAI,CAACgN,cAAc,CAAC;QACxDC,OAAO,EAAE,IAAI,CAACA,OAAO;QACrBG,gBAAgB,EAAE,IAAI,CAACA,gBAAgB;QACvCgC,cAAc,EAAE,IAAI,CAACA,cAAc;QACnCF,UAAU,EAAE;OACb;MAED,OAAA9D,MAAA,CAAAC,MAAA,CAAAD,MAAA,CAAAC,MAAA,KAAWN,UAAU,GAAKC,MAAM;IAClC;EAAC;EAAA,OAAA6D,OAAA;AAAA,EA9M0BnD,OAAO;AAClC;AACOmD,OAAA,CAAA1D,SAAS,GAAG,SAAS;AA8M9B3L,aAAa,CAACiM,aAAa,CAACoD,OAAO,CAAC;AA8BpC,WAAakC,GAAI,0BAAAC,KAAA;EAAA9L,SAAA,CAAA6L,GAAA,EAAAC,KAAA;EAAA,IAAAC,OAAA,GAAA7L,YAAA,CAAA2L,GAAA;EAGf,SAAAA,IAAY1L,IAAkB;IAAAE,eAAA,OAAAwL,GAAA;IAC5B,IAAI1L,IAAI,CAAC+J,cAAc,KAAK,CAAC,EAAE;MAC7BrM,OAAO,CAACC,IAAI,CACR,8DAA8D,GAC9D,oDAAoD,CAAC;;IAE3DqC,IAAI,CAACI,IAAI,GAAG,IAAIoJ,OAAO,CAACxJ,IAAI,CAAC;IAAC,OAAA4L,OAAA,CAAAzL,IAAA,OACxBH,IAAoB,GAC1B;EACF;EAACiB,YAAA,CAAAyK,GAAA;IAAAxK,GAAA;IAAAC,KAAA,EAEQ,SAAAhB,KAAKlE,MAAuB,EAAE8H,MAAc;MAAA,IAAA8H,OAAA;MACnD,OAAOzR,IAAI,CAAC,YAAK;QACf,IAAIyR,OAAI,CAACzL,IAAI,CAAC6H,WAAW,IAAI,IAAI,EAAE;UACjC/N,GAAG,CAACsJ,OAAO,CAACqI,OAAI,CAACzL,IAAI,CAAC6H,WAAW,CAAC;UAClC4D,OAAI,CAACzL,IAAI,CAAC6H,WAAW,GAAG,IAAI;;QAE9B,IAAI4D,OAAI,CAACzL,IAAI,CAAC8H,oBAAoB,IAAI,IAAI,EAAE;UAC1ChO,GAAG,CAACsJ,OAAO,CAACqI,OAAI,CAACzL,IAAI,CAAC8H,oBAAoB,CAAC;UAC3C2D,OAAI,CAACzL,IAAI,CAAC8H,oBAAoB,GAAG,IAAI;;QAEvC,IAAMjL,IAAI,GAAG8G,MAAM,IAAI,IAAI,GAAG,IAAI,GAAGA,MAAM,CAAC,MAAM,CAAC;QACnD,IAAMT,QAAQ,GAAGS,MAAM,IAAI,IAAI,GAAG,IAAI,GAAGA,MAAM,CAAC,UAAU,CAAC;QAC3D,IAAM7H,YAAY,GACd6H,MAAM,IAAI,IAAI,GAAG,IAAI,GAAGA,MAAM,CAAC,cAAc,CAAC;QAClD,OAAAU,IAAA,CAAAC,eAAA,CAAAgH,GAAA,CAAA/G,SAAA,WAAAkH,OAAA,EAAA1L,IAAA,CAAA0L,OAAA,EAAkB5P,MAAM,EAAE;UAACgB,IAAI,EAAJA,IAAI;UAAEqG,QAAQ,EAARA,QAAQ;UAAEpH,YAAY,EAAZA;QAAY,CAAC;MAC1D,CAAC,CAAC;IACJ;IAEA;EAAA;IAAAgF,GAAA;IAAAC,KAAA,EACA,SAAA8E,WACIC,GAA6C,EAC7CP,MAAgC;MAClC,IAAIA,MAAM,CAAC,eAAe,CAAC,KAAK,CAAC,EAAE;QACjCA,MAAM,CAAC,gBAAgB,CAAC,GAAG,CAAC;;MAE9B,OAAO,IAAIO,GAAG,CAACP,MAAM,CAAC;IACxB;EAAC;EAAA,OAAA+F,GAAA;AAAA,EAxCsB/L,GAAG;AAC1B;AACgB+L,GAAA,CAAA5F,SAAS,GAAG,KAAK;AAwCnC3L,aAAa,CAACiM,aAAa,CAACsF,GAAG,CAAC;AAuChC,WAAaI,QAAS,0BAAAC,SAAA;EAAAlM,SAAA,CAAAiM,QAAA,EAAAC,SAAA;EAAA,IAAAC,OAAA,GAAAjM,YAAA,CAAA+L,QAAA;EAuCpB,SAAAA,SAAY9L,IAAuB;IAAA,IAAAiM,OAAA;IAAA/L,eAAA,OAAA4L,QAAA;IACjCG,OAAA,GAAAD,OAAA,CAAA7L,IAAA,OAAMH,IAAI;IAZHiM,OAAA,CAAArF,kBAAkB,GAAG,MAAM;IAC3BqF,OAAA,CAAArC,4BAA4B,GAAG,aAAa;IAC5CqC,OAAA,CAAApF,0BAA0B,GAAG,cAAc;IAC3CoF,OAAA,CAAAnF,6BAA6B,GAAG,YAAY;IAE5CmF,OAAA,CAAAlF,wBAAwB,GAAG,OAAO;IASzCkF,OAAA,CAAKjF,KAAK,GAAGhH,IAAI,CAACgH,KAAK;IACvBxL,qBAAqB,CAACyQ,OAAA,CAAKjF,KAAK,EAAE,OAAO,CAAC;IAC1CiF,OAAA,CAAKhF,UAAU,GAAG3M,aAAa,CAC3B0F,IAAI,CAACiH,UAAU,KAAKjK,SAAS,GAAGiP,OAAA,CAAKrF,kBAAkB,GACvB5G,IAAI,CAACiH,UAAU,CAAC;IACpDgF,OAAA,CAAKnC,mBAAmB,GAAGxP,aAAa,CACpC0F,IAAI,CAAC8J,mBAAmB,KAAK9M,SAAS,GAClCiP,OAAA,CAAKrC,4BAA4B,GACjC5J,IAAI,CAAC8J,mBAAmB,CAAC;IACjCmC,OAAA,CAAK/E,OAAO,GAAGlH,IAAI,CAACkH,OAAO,IAAI,IAAI,GAAG,IAAI,GAAGlH,IAAI,CAACkH,OAAO;IAEzD+E,OAAA,CAAK9E,iBAAiB,GAAGjM,cAAc,CACnC8E,IAAI,CAACmH,iBAAiB,IAAI8E,OAAA,CAAKpF,0BAA0B,CAAC;IAC9DoF,OAAA,CAAK7E,oBAAoB,GAAGlM,cAAc,CACtC8E,IAAI,CAACoH,oBAAoB,IAAI6E,OAAA,CAAKnF,6BAA6B,CAAC;IAEpEmF,OAAA,CAAK5E,eAAe,GAChBnM,cAAc,CAAC8E,IAAI,CAACqH,eAAe,IAAI4E,OAAA,CAAKlF,wBAAwB,CAAC;IACzEkF,OAAA,CAAKC,cAAc,GAAGlM,IAAI,CAACkM,cAAc;IAEzCD,OAAA,CAAK3E,iBAAiB,GAAGhM,cAAc,CAAC0E,IAAI,CAACsH,iBAAiB,CAAC;IAC/D2E,OAAA,CAAK1E,oBAAoB,GAAGjM,cAAc,CAAC0E,IAAI,CAACuH,oBAAoB,CAAC;IACrE0E,OAAA,CAAKzE,eAAe,GAAGlM,cAAc,CAAC0E,IAAI,CAACwH,eAAe,CAAC;IAE3DyE,OAAA,CAAKxE,gBAAgB,GAAG/M,aAAa,CAACsF,IAAI,CAACyH,gBAAgB,CAAC;IAC5DwE,OAAA,CAAKvE,mBAAmB,GAAGhN,aAAa,CAACsF,IAAI,CAAC0H,mBAAmB,CAAC;IAClEuE,OAAA,CAAKtE,cAAc,GAAGjN,aAAa,CAACsF,IAAI,CAAC2H,cAAc,CAAC;IAExDsE,OAAA,CAAKrE,OAAO,GAAGnM,UAAU,CAACoM,GAAG,CACzB,CAAC,CAAC,EAAEpM,UAAU,CAACqM,GAAG,CAAC,CAAC,CAAC,EAAE9H,IAAI,CAAC4H,OAAO,IAAI,IAAI,GAAG,CAAC,GAAG5H,IAAI,CAAC4H,OAAO,CAAC,CAAC,CAAC,CAAC;IACtEqE,OAAA,CAAKlE,gBAAgB,GAAGtM,UAAU,CAACoM,GAAG,CAAC,CACrC,CAAC,EACDpM,UAAU,CAACqM,GAAG,CACV,CAAC,CAAC,EAAE9H,IAAI,CAAC+H,gBAAgB,IAAI,IAAI,GAAG,CAAC,GAAG/H,IAAI,CAAC+H,gBAAgB,CAAC,CAAC,CACpE,CAAC;IACFkE,OAAA,CAAKjE,WAAW,GAAGhI,IAAI,CAACgI,WAAW;IACnCiE,OAAA,CAAKlC,cAAc,GAAG/J,IAAI,CAAC+J,cAAc;IACzCkC,OAAA,CAAK1L,SAAS,GAAG,CAAC0L,OAAA,CAAKjF,KAAK,EAAEiF,OAAA,CAAKjF,KAAK,CAAC;IACzCiF,OAAA,CAAKhE,WAAW,GAAG,IAAI;IACvBgE,OAAA,CAAK/D,oBAAoB,GAAG,IAAI;IAAC,OAAA+D,OAAA;EACnC;EAAChL,YAAA,CAAA6K,QAAA;IAAA5K,GAAA;IAAAC,KAAA,EAEe,SAAAyB,MAAMpB,UAAyB;;MAC7CA,UAAU,GAAG9F,kBAAkB,CAAC8F,UAAU,CAAC;MAC3C,IAAMuB,QAAQ,GAAGvB,UAAU,CAACA,UAAU,CAAChF,MAAM,GAAG,CAAC,CAAC;MAClD,IAAI,CAAC2L,MAAM,GAAG,IAAI,CAACC,SAAS,CACxB,QAAQ,EAAE,CAACrF,QAAQ,EAAE,IAAI,CAACiE,KAAK,GAAG,CAAC,CAAC,EAAE,IAAI,EAAE,IAAI,CAACG,iBAAiB,EAClE,IAAI,CAACG,iBAAiB,EAAE,IAAI,EAAE,IAAI,CAACG,gBAAgB,CAAC;MACxD,IAAI,CAACY,eAAe,GAAG,IAAI,CAACD,SAAS,CACjC,kBAAkB,EAAE,CAAC,IAAI,CAACpB,KAAK,EAAE,IAAI,CAACA,KAAK,GAAG,CAAC,CAAC,EAAE,IAAI,EACtD,IAAI,CAACI,oBAAoB,EAAE,IAAI,CAACG,oBAAoB,EAAE,IAAI,EAC1D,IAAI,CAACG,mBAAmB,CAAC;MAC7B,IAAIL,eAA4B;MAChC,IAAI,IAAI,CAACH,OAAO,EAAE;QAChB,IAAI,IAAI,CAACgF,cAAc,EAAE;UACvB,IAAMC,gBAAgB,GAAG,IAAI,CAAC9E,eAAe;UAC7C,IAAM+E,aAAa,GAAG,IAAI,CAACpF,KAAK;UAChCK,eAAe,GAAG,KAAIgF,EAAA,0BAAAC,YAAA;YAAAzM,SAAA,CAAA0M,UAAA,EAAAD,YAAA;YAAA,IAAAE,OAAA,GAAAzM,YAAA,CAAAwM,UAAA;YAAA,SAAAA,WAAA;cAAArM,eAAA,OAAAqM,UAAA;cAAA,OAAAC,OAAA,CAAA1I,KAAA,OAAA/G,SAAA;YAAA;YAAAkE,YAAA,CAAAsL,UAAA;cAAArL,GAAA;cAAAC,KAAA,EAIpB,SAAA2C,MAAMzG,KAAY,EAAEoP,KAAgB;gBAClC;gBACA,IAAMC,EAAE,GAAGP,gBAAgB,CAACrI,KAAK,CAAC,CAACsI,aAAa,CAAC,CAAC;gBAClD,IAAMO,EAAE,GAAI,IAAIvR,IAAI,EAAE,CAAE0I,KAAK,CAAC,CAACsI,aAAa,CAAC,CAAC;gBAC9C,IAAMQ,MAAM,GAAGT,gBAAgB,CAACrI,KAAK,CAAC,CAACsI,aAAa,GAAG,CAAC,CAAC,CAAC;gBAC1D,OAAO5R,CAAC,CAACqS,oBAAoB,CACzBrS,CAAC,CAACqS,oBAAoB,CAACH,EAAE,EAAEC,EAAE,CAAC,EAAEC,MAAM,CAAC;cAC7C;YAAC;YAAA,OAAAL,UAAA;UAAA,EAX6CpR,WAAW,CAY1D,EAXC;UACOkR,EAAA,CAAAvG,SAAS,GAAG,YAAa,E,KAU9B;SACL,MAAM;UACLuB,eAAe,GAAG,IAAI,CAACA,eAAe;;QAExC,IAAI,CAACiB,IAAI,GAAG,IAAI,CAACF,SAAS,CACtB,MAAM,EAAE,CAAC,IAAI,CAACpB,KAAK,GAAG,CAAC,CAAC,EAAE,IAAI,EAAEK,eAAe,EAAE,IAAI,CAACG,eAAe,EACrE,IAAI,EAAE,IAAI,CAACG,cAAc,CAAC;OAC/B,MAAM;QACL,IAAI,CAACW,IAAI,GAAG,IAAI;;MAElB;MACA;MACA,IAAI,CAACC,KAAK,GAAG,IAAI;IACnB;EAAC;IAAArH,GAAA;IAAAC,KAAA,EAEQ,SAAAhB,KAAKlE,MAAuB,EAAE8H,MAAc;MAAA,IAAA+I,OAAA;MACnD,OAAO1S,IAAI,CAAC,YAAK;QACf,IAAMkJ,QAAQ,GAAGS,MAAM,CAAC,UAAU,CAAC,IAAI,IAAI,GAAG,KAAK,GAAGA,MAAM,CAAC,UAAU,CAAC;QACxE9H,MAAM,GAAGA,MAAkB;QAC3B,IAAIA,MAAM,CAACO,MAAM,KAAK,CAAC,EAAE;UACvB,MAAM,IAAIvB,UAAU,CAChB,6DAAAqC,MAAA,CACGrB,MAAM,CAACO,MAAM,MAAG,CAAC;;QAE1B,IAAIyN,QAAQ,GAAGhO,MAAM,CAAC,CAAC,CAAC,CAAC,CAAI;QAC7B,IAAM8Q,QAAQ,GAAG9Q,MAAM,CAAC,CAAC,CAAC,CAAC,CAAE;QAC7BA,MAAM,GAAGA,MAAM,CAAC,CAAC,CAAC;QAClB,IAAI,CAAC,GAAG6Q,OAAI,CAAClF,OAAO,IAAIkF,OAAI,CAAClF,OAAO,GAAG,CAAC,IAAIkF,OAAI,CAAC7E,WAAW,IAAI,IAAI,EAAE;UACpE6E,OAAI,CAAC7E,WAAW,GAAGS,mBAAmB,CAAC;YAClBC,IAAI,EAAE,SAAAA,KAAA;cAAA,OAAMzO,GAAG,CAAC6E,QAAQ,CAAC9C,MAAgB,CAAC;YAAA;YAC1C2M,IAAI,EAAEkE,OAAI,CAAClF,OAAO;YAClBtE,QAAQ,EAARA,QAAQ;YACR4G,KAAK,EAAE,CAAC;YACRlC,WAAW,EAAE8E,OAAI,CAAC9E;WACnB,CAAa;;QAEnC,IAAI,CAAC,GAAG8E,OAAI,CAAC/E,gBAAgB,IAAI+E,OAAI,CAAC/E,gBAAgB,GAAG,CAAC,IACtD+E,OAAI,CAAC5E,oBAAoB,IAAI,IAAI,EAAE;UACrC4E,OAAI,CAAC5E,oBAAoB,GAAGQ,mBAAmB,CAAC;YAClBC,IAAI,EAAE,SAAAA,KAAA;cAAA,OAAMzO,GAAG,CAAC6E,QAAQ,CAACkL,QAAQ,CAAC;YAAA;YAClCrB,IAAI,EAAEkE,OAAI,CAAC/E,gBAAgB;YAC3BzE,QAAQ,EAARA,QAAQ;YACR4G,KAAK,EAAE,CAAC;YACRlC,WAAW,EAAE8E,OAAI,CAAC9E;WACnB,CAAa;;QAE5C,IAAMc,MAAM,GAAGgE,OAAI,CAAC7E,WAA+C;QACnE,IAAMc,SAAS,GACX+D,OAAI,CAAC5E,oBAAwD;QAEjE;QACA;QACA;QACA,IAAI5I,CAAS;QACb,IAAI+C,CAAS;QACb,IAAI2K,CAAS;QACb,IAAIC,CAAS;QACb,IAAI,CAAC,GAAGH,OAAI,CAAClF,OAAO,IAAIkF,OAAI,CAAClF,OAAO,GAAG,CAAC,EAAE;UACxC3L,MAAM,GAAG/B,GAAG,CAACgF,GAAG,CAACjD,MAAM,EAAE6M,MAAM,CAAC,CAAC,CAAC,CAAC;;QAErC,IAAIqB,CAAC,GAAG3P,CAAC,CAACwO,GAAG,CAAC/M,MAAM,EAAE6Q,OAAI,CAAC3E,MAAM,CAACc,IAAI,EAAE,CAAC;QACzC,IAAI,CAAC,GAAG6D,OAAI,CAAC/E,gBAAgB,IAAI+E,OAAI,CAAC/E,gBAAgB,GAAG,CAAC,EAAE;UAC1DkC,QAAQ,GAAG/P,GAAG,CAACgF,GAAG,CAAC+K,QAAQ,EAAElB,SAAS,CAAC,CAAC,CAAC,CAAC;;QAE5CoB,CAAC,GAAGjQ,GAAG,CAAC+E,GAAG,CAACkL,CAAC,EAAE3P,CAAC,CAACwO,GAAG,CAACiB,QAAQ,EAAE6C,OAAI,CAACzE,eAAe,CAACY,IAAI,EAAE,CAAC,CAAC;QAC5D,IAAI6D,OAAI,CAAC5F,OAAO,EAAE;UAChBiD,CAAC,GAAG3P,CAAC,CAAC0O,OAAO,CAACiB,CAAC,EAAE2C,OAAI,CAACxE,IAAI,CAACW,IAAI,EAAE,CAAC;;QAGpC,IAAAiE,WAAA,GAAyBhT,GAAG,CAACuQ,KAAK,CAACN,CAAC,EAAE,CAAC,EAAEA,CAAC,CAACtM,IAAI,GAAG,CAAC,CAAC;UAAAsP,WAAA,GAAAxC,cAAA,CAAAuC,WAAA;UAA7CE,EAAE,GAAAD,WAAA;UAAEE,EAAE,GAAAF,WAAA;UAAEG,EAAE,GAAAH,WAAA;UAAEI,EAAE,GAAAJ,WAAA;QAErB7N,CAAC,GAAGwN,OAAI,CAAChD,mBAAmB,CAAChG,KAAK,CAACsJ,EAAE,CAAC;QACtC/K,CAAC,GAAGyK,OAAI,CAAChD,mBAAmB,CAAChG,KAAK,CAACuJ,EAAE,CAAC;QACtCL,CAAC,GAAG9S,GAAG,CAAC+E,GAAG,CAAC/E,GAAG,CAACgF,GAAG,CAACmD,CAAC,EAAE0K,QAAQ,CAAC,EAAE7S,GAAG,CAACgF,GAAG,CAACI,CAAC,EAAEwN,OAAI,CAAC7F,UAAU,CAACnD,KAAK,CAACwJ,EAAE,CAAC,CAAC,CAAC;QACxEL,CAAC,GAAGH,OAAI,CAAChD,mBAAmB,CAAChG,KAAK,CAACyJ,EAAE,CAAC;QAEtC,IAAM1E,CAAC,GAAG3O,GAAG,CAACgF,GAAG,CAAC+N,CAAC,EAAEH,OAAI,CAAC7F,UAAU,CAACnD,KAAK,CAACkJ,CAAC,CAAC,CAAC;QAC9C;QACA,OAAO,CAACnE,CAAC,EAAEA,CAAC,EAAEmE,CAAC,CAAC;MAClB,CAAC,CAAC;IACJ;EAAC;IAAA9L,GAAA;IAAAC,KAAA,EAEQ,SAAAsE,UAAA,EAAS;MAChB,IAAMC,UAAU,GAAAjB,IAAA,CAAAC,eAAA,CAAAoH,QAAA,CAAAnH,SAAA,sBAAAxE,IAAA,MAAoB;MAEpC,IAAMwF,MAAM,GAA6B;QACvCqB,KAAK,EAAE,IAAI,CAACA,KAAK;QACjBC,UAAU,EAAE1M,mBAAmB,CAAC,IAAI,CAAC0M,UAAU,CAAC;QAChD6C,mBAAmB,EAAEvP,mBAAmB,CAAC,IAAI,CAACuP,mBAAmB,CAAC;QAClE5C,OAAO,EAAE,IAAI,CAACA,OAAO;QACrBC,iBAAiB,EAAE9L,oBAAoB,CAAC,IAAI,CAAC8L,iBAAiB,CAAC;QAC/DC,oBAAoB,EAAE/L,oBAAoB,CAAC,IAAI,CAAC+L,oBAAoB,CAAC;QACrEC,eAAe,EAAEhM,oBAAoB,CAAC,IAAI,CAACgM,eAAe,CAAC;QAC3D6E,cAAc,EAAE,IAAI,CAACA,cAAc;QACnC5E,iBAAiB,EAAE/L,oBAAoB,CAAC,IAAI,CAAC+L,iBAAiB,CAAC;QAC/DC,oBAAoB,EAAEhM,oBAAoB,CAAC,IAAI,CAACgM,oBAAoB,CAAC;QACrEC,eAAe,EAAEjM,oBAAoB,CAAC,IAAI,CAACiM,eAAe,CAAC;QAC3D2B,mBAAmB,EAAE5N,oBAAoB,CAAC,IAAI,CAAC4N,mBAAmB,CAAC;QACnE1B,gBAAgB,EAAE9M,mBAAmB,CAAC,IAAI,CAAC8M,gBAAgB,CAAC;QAC5DC,mBAAmB,EAAE/M,mBAAmB,CAAC,IAAI,CAAC+M,mBAAmB,CAAC;QAClEC,cAAc,EAAEhN,mBAAmB,CAAC,IAAI,CAACgN,cAAc,CAAC;QACxDC,OAAO,EAAE,IAAI,CAACA,OAAO;QACrBG,gBAAgB,EAAE,IAAI,CAACA,gBAAgB;QACvCgC,cAAc,EAAE,IAAI,CAACA;OACtB;MAED,OAAAhE,MAAA,CAAAC,MAAA,CAAAD,MAAA,CAAAC,MAAA,KAAWN,UAAU,GAAKC,MAAM;IAClC;EAAC;EAAA,OAAAmG,QAAA;AAAA,EA1N2BzF,OAAO;AACnC;AACOyF,QAAA,CAAAhG,SAAS,GAAG,UAAU;AA0N/B3L,aAAa,CAACiM,aAAa,CAAC0F,QAAQ,CAAC;AAqCrC,WAAa0B,IAAK,0BAAAC,KAAA;EAAA5N,SAAA,CAAA2N,IAAA,EAAAC,KAAA;EAAA,IAAAC,OAAA,GAAA3N,YAAA,CAAAyN,IAAA;EAGhB,SAAAA,KAAYxN,IAAmB;IAAAE,eAAA,OAAAsN,IAAA;IAC7B,IAAIxN,IAAI,CAAC+J,cAAc,KAAK,CAAC,EAAE;MAC7BrM,OAAO,CAACC,IAAI,CACR,8DAA8D,GAC9D,oDAAoD,CAAC;;IAE3DqC,IAAI,CAACI,IAAI,GAAG,IAAI0L,QAAQ,CAAC9L,IAAI,CAAC;IAAC,OAAA0N,OAAA,CAAAvN,IAAA,OACzBH,IAAoB,GAC1B;EACF;EAACiB,YAAA,CAAAuM,IAAA;IAAAtM,GAAA;IAAAC,KAAA,EAEQ,SAAAhB,KAAKlE,MAAuB,EAAE8H,MAAc;MAAA,IAAA4J,OAAA;MACnD,OAAOvT,IAAI,CAAC,YAAK;QACf,IAAIuT,OAAI,CAACvN,IAAI,CAAC6H,WAAW,IAAI,IAAI,EAAE;UACjC/N,GAAG,CAACsJ,OAAO,CAACmK,OAAI,CAACvN,IAAI,CAAC6H,WAAW,CAAC;UAClC0F,OAAI,CAACvN,IAAI,CAAC6H,WAAW,GAAG,IAAI;;QAE9B,IAAI0F,OAAI,CAACvN,IAAI,CAAC8H,oBAAoB,IAAI,IAAI,EAAE;UAC1ChO,GAAG,CAACsJ,OAAO,CAACmK,OAAI,CAACvN,IAAI,CAAC8H,oBAAoB,CAAC;UAC3CyF,OAAI,CAACvN,IAAI,CAAC8H,oBAAoB,GAAG,IAAI;;QAEvC,IAAMjL,IAAI,GAAG8G,MAAM,IAAI,IAAI,GAAG,IAAI,GAAGA,MAAM,CAAC,MAAM,CAAC;QACnD,IAAMT,QAAQ,GAAGS,MAAM,IAAI,IAAI,GAAG,IAAI,GAAGA,MAAM,CAAC,UAAU,CAAC;QAC3D,IAAM7H,YAAY,GACd6H,MAAM,IAAI,IAAI,GAAG,IAAI,GAAGA,MAAM,CAAC,cAAc,CAAC;QAClD,OAAAU,IAAA,CAAAC,eAAA,CAAA8I,IAAA,CAAA7I,SAAA,WAAAgJ,OAAA,EAAAxN,IAAA,CAAAwN,OAAA,EAAkB1R,MAAM,EAAE;UAACgB,IAAI,EAAJA,IAAI;UAAEqG,QAAQ,EAARA,QAAQ;UAAEpH,YAAY,EAAZA;QAAY,CAAC;MAC1D,CAAC,CAAC;IACJ;IAEA;EAAA;IAAAgF,GAAA;IAAAC,KAAA,EACA,SAAA8E,WACIC,GAA6C,EAC7CP,MAAgC;MAClC,IAAIA,MAAM,CAAC,eAAe,CAAC,KAAK,CAAC,EAAE;QACjCA,MAAM,CAAC,gBAAgB,CAAC,GAAG,CAAC;;MAE9B,OAAO,IAAIO,GAAG,CAACP,MAAM,CAAC;IACxB;EAAC;EAAA,OAAA6H,IAAA;AAAA,EAxCuB7N,GAAG;AAC3B;AACgB6N,IAAA,CAAA1H,SAAS,GAAG,MAAM;AAwCpC3L,aAAa,CAACiM,aAAa,CAACoH,IAAI,CAAC;AASjC,WAAanN,eAAgB,0BAAAuN,SAAA;EAAA/N,SAAA,CAAAQ,eAAA,EAAAuN,SAAA;EAAA,IAAAC,QAAA,GAAA9N,YAAA,CAAAM,eAAA;EAK3B,SAAAA,gBAAYL,IAAyB;IAAA,IAAA8N,OAAA;IAAA5N,eAAA,OAAAG,eAAA;IACnCyN,OAAA,GAAAD,QAAA,CAAA1N,IAAA,OAAMH,IAAI;IACV8N,OAAA,CAAKxN,KAAK,GAAGN,IAAI,CAACM,KAAK;IAAC,OAAAwN,OAAA;EAC1B;EAAC7M,YAAA,CAAAZ,eAAA;IAAAa,GAAA;IAAAwB,GAAA,EAED,SAAAA,IAAA,EAAa;MACX;MACA;MACA;MACA;MACA,IAAMnC,SAAS,GAAa,EAAE;MAAC,IAAAwN,UAAA,GAAAlM,0BAAA,CACZ,IAAI,CAACvB,KAAK,CAAC/D,KAAK,EAAE,CAACwB,OAAO,EAAE;QAAAiQ,MAAA;MAAA;QAA/C,KAAAD,UAAA,CAAAhM,CAAA,MAAAiM,MAAA,GAAAD,UAAA,CAAA/L,CAAA,IAAAC,IAAA,GAAiD;UAAA,IAAtC7B,IAAI,GAAA4N,MAAA,CAAA7M,KAAA;UACb,IAAI9E,KAAK,CAACC,OAAO,CAAC8D,IAAI,CAACG,SAAS,CAAC,EAAE;YACjCA,SAAS,CAAChB,IAAI,CAAAuE,KAAA,CAAdvD,SAAS,EAAAyC,kBAAA,CAAS5C,IAAI,CAACG,SAAS,EAAC;WAClC,MAAM;YACLA,SAAS,CAAChB,IAAI,CAACa,IAAI,CAACG,SAAS,CAAC;;;MAEjC,SAAA4B,GAAA;QAAA4L,UAAA,CAAA3L,CAAA,CAAAD,GAAA;MAAA;QAAA4L,UAAA,CAAA1L,CAAA;MAAA;MACD,OAAO9B,SAAS;IAClB;EAAC;IAAAW,GAAA;IAAAC,KAAA,EAEQ,SAAAhB,KAAKlE,MAAuB,EAAE8H,MAAc;MAAA,IAAAkK,OAAA;MACnD,OAAO7T,IAAI,CAAC,YAAK;QACf6B,MAAM,GAAGA,MAAkB;QAC3B,IAAIiC,MAAM,GAAGjC,MAAM,CAACM,KAAK,CAAC,CAAC,CAAC;QAE5B;QACA,IAAM2R,YAAY,GAAe,EAAE;QAAC,IAAAC,UAAA,GAAAtM,0BAAA,CACjBoM,OAAI,CAAC3N,KAAK,CAAC/D,KAAK,EAAE,CAACwB,OAAO,EAAE;UAAAqQ,MAAA;QAAA;UAA/C,KAAAD,UAAA,CAAApM,CAAA,MAAAqM,MAAA,GAAAD,UAAA,CAAAnM,CAAA,IAAAC,IAAA,GAAiD;YAAA,IAAtC7B,KAAI,GAAAgO,MAAA,CAAAjN,KAAA;YACb,IAAI9E,KAAK,CAACC,OAAO,CAAC8D,KAAI,CAACG,SAAS,CAAC,EAAE;cACjC2N,YAAY,CAAC3O,IAAI,CAACrB,MAAM,CAACmQ,MAAM,CAAC,CAAC,EAAEjO,KAAI,CAACG,SAAS,CAAC/D,MAAM,CAAC,CAAC;aAC3D,MAAM;cACL0R,YAAY,CAAC3O,IAAI,CAACrB,MAAM,CAACmQ,MAAM,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;;;QAEzC,SAAAlM,GAAA;UAAAgM,UAAA,CAAA/L,CAAA,CAAAD,GAAA;QAAA;UAAAgM,UAAA,CAAA9L,CAAA;QAAA;QACD6L,YAAY,CAACnQ,OAAO,EAAE;QAEtB;QACA,IAAMuQ,eAAe,GAAe,EAAE;QACtC,IAAIC,UAAoB;QACxB,KAAK,IAAIjP,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG2O,OAAI,CAAC3N,KAAK,CAAC9D,MAAM,EAAE,EAAE8C,CAAC,EAAE;UAC1C,IAAMc,IAAI,GAAG6N,OAAI,CAAC3N,KAAK,CAAChB,CAAC,CAAC;UAC1BpB,MAAM,GAAGgQ,YAAY,CAAC5O,CAAC,CAAC;UACxB;UACA,IAAIA,CAAC,KAAK,CAAC,EAAE;YACXiP,UAAU,GAAG,CAACtS,MAAM,CAAC,CAAC,CAAC,CAAC,CAACqB,MAAM,CAACY,MAAM,CAAC;WACxC,MAAM;YACLqQ,UAAU,GAAG,CAACA,UAAU,CAAC,CAAC,CAAC,CAAC,CAACjR,MAAM,CAACY,MAAM,CAAC;;UAE7CqQ,UAAU,GAAGnO,IAAI,CAACD,IAAI,CAACoO,UAAU,EAAExK,MAAM,CAAa;UACtDuK,eAAe,CAAC/O,IAAI,CAACgP,UAAU,CAAChS,KAAK,CAAC,CAAC,CAAC,CAAC;;QAG3C;QACA2B,MAAM,GAAG,EAAE;QAAC,IAAAsQ,UAAA,GAAA3M,0BAAA,CACayM,eAAe,CAAC/R,KAAK,EAAE,CAACwB,OAAO,EAAE;UAAA0Q,MAAA;QAAA;UAA1D,KAAAD,UAAA,CAAAzM,CAAA,MAAA0M,MAAA,GAAAD,UAAA,CAAAxM,CAAA,IAAAC,IAAA,GAA4D;YAAA,IAAAyM,OAAA;YAAA,IAAjDC,UAAU,GAAAF,MAAA,CAAAtN,KAAA;YACnB,CAAAuN,OAAA,GAAAxQ,MAAM,EAACqB,IAAI,CAAAuE,KAAA,CAAA4K,OAAA,EAAA1L,kBAAA,CAAI2L,UAAU,EAAC;;QAC3B,SAAAxM,GAAA;UAAAqM,UAAA,CAAApM,CAAA,CAAAD,GAAA;QAAA;UAAAqM,UAAA,CAAAnM,CAAA;QAAA;QACD,OAAO,CAACkM,UAAU,CAAC,CAAC,CAAC,CAAC,CAACjR,MAAM,CAACY,MAAM,CAAC;MACvC,CAAC,CAAC;IACJ;EAAC;IAAAgD,GAAA;IAAAC,KAAA,EAEe,SAAAyB,MAAMpB,UAAyB;MAC7C,IAAI5F,eAAe,CAAC4F,UAAU,CAAC,EAAE;QAC/B;QACA;QACAA,UAAU,GAAIA,UAAsB,CAAC,CAAC,CAAC;;MAEzCA,UAAU,GAAGA,UAAmB;MAChC,IAAIC,SAAiB;MACrB,IAAI,CAACnB,KAAK,CAACsO,OAAO,CAAC,UAACxO,IAAI,EAAEd,CAAC,EAAI;QAC7B7E,SAAS,YAAA6C,MAAA,CAAYgC,CAAC,GAAI,YAAK;UAC7B;UAEAc,IAAI,CAACwC,KAAK,CAACpB,UAAU,CAAC;UACtB,IAAInF,KAAK,CAACC,OAAO,CAAC8D,IAAI,CAACG,SAAS,CAAC,EAAE;YACjCkB,SAAS,GAAGrB,IAAI,CAACG,SAAS,CAAC,CAAC,CAAC;WAC9B,MAAM;YACLkB,SAAS,GAAGrB,IAAI,CAACG,SAAS;;UAE5BiB,UAAU,GAAG,CAACA,UAAU,CAAC,CAAC,CAAC,EAAEC,SAAS,CAAU;QAClD,CAAC,CAAC;MACJ,CAAC,CAAC;MACF,IAAI,CAAC8G,KAAK,GAAG,IAAI;IACnB;EAAC;IAAArH,GAAA;IAAAC,KAAA,EAEQ,SAAAsE,UAAA,EAAS;MAChB,IAAMC,UAAU,GAAAjB,IAAA,CAAAC,eAAA,CAAArE,eAAA,CAAAsE,SAAA,sBAAAxE,IAAA,MAAoB;MAEpC,IAAM0O,aAAa,GAAG,SAAhBA,aAAaA,CAAIzO,IAAa,EAAI;QACtC,OAAO;UACL,WAAW,EAAEA,IAAI,CAACyF,YAAY,EAAE;UAChC,QAAQ,EAAEzF,IAAI,CAACqF,SAAS;SACzB;MACH,CAAC;MAED,IAAMqJ,WAAW,GAAG,IAAI,CAACxO,KAAK,CAAClB,GAAG,CAACyP,aAAa,CAAC;MAEjD,IAAMlJ,MAAM,GAAG;QAAC,OAAO,EAAEmJ;MAAW,CAAC;MAErC,OAAA/I,MAAA,CAAAC,MAAA,CAAAD,MAAA,CAAAC,MAAA,KAAWN,UAAU,GAAKC,MAAM;IAClC;IAEA;EAAA;IAAAzE,GAAA;IAAAwB,GAAA,EAYA,SAAAA,IAAA,EAA6B;MAC3B,IAAI,CAAC,IAAI,CAAC0C,SAAS,EAAE;QACnB,OAAO,EAAE;;MAEX,IAAME,OAAO,GAAoB,EAAE;MAAC,IAAAyJ,UAAA,GAAAlN,0BAAA,CACjB,IAAI,CAACvB,KAAK;QAAA0O,MAAA;MAAA;QAA7B,KAAAD,UAAA,CAAAhN,CAAA,MAAAiN,MAAA,GAAAD,UAAA,CAAA/M,CAAA,IAAAC,IAAA,GAA+B;UAAA,IAApB7B,IAAI,GAAA4O,MAAA,CAAA7N,KAAA;UACbmE,OAAO,CAAC/F,IAAI,CAAAuE,KAAA,CAAZwB,OAAO,EAAAtC,kBAAA,CAAS5C,IAAI,CAACiF,gBAAgB,EAAC;;MACvC,SAAAlD,GAAA;QAAA4M,UAAA,CAAA3M,CAAA,CAAAD,GAAA;MAAA;QAAA4M,UAAA,CAAA1M,CAAA;MAAA;MACD,OAAOiD,OAAO;IAChB;EAAC;IAAApE,GAAA;IAAAwB,GAAA,EAED,SAAAA,IAAA,EAAgC;MAC9B,IAAM4C,OAAO,GAAoB,EAAE;MAAC,IAAA2J,UAAA,GAAApN,0BAAA,CACjB,IAAI,CAACvB,KAAK;QAAA4O,MAAA;MAAA;QAA7B,KAAAD,UAAA,CAAAlN,CAAA,MAAAmN,MAAA,GAAAD,UAAA,CAAAjN,CAAA,IAAAC,IAAA,GAA+B;UAAA,IAApB7B,MAAI,GAAA8O,MAAA,CAAA/N,KAAA;UACbmE,OAAO,CAAC/F,IAAI,CAAAuE,KAAA,CAAZwB,OAAO,EAAAtC,kBAAA,CAAS5C,MAAI,CAACmF,mBAAmB,EAAC;;MAC1C,SAAApD,GAAA;QAAA8M,UAAA,CAAA7M,CAAA,CAAAD,GAAA;MAAA;QAAA8M,UAAA,CAAA5M,CAAA;MAAA;MACD,IAAI,CAAC,IAAI,CAAC+C,SAAS,EAAE;QACnB,IAAMC,gBAAgB,GAAoB,EAAE;QAAC,IAAA8J,UAAA,GAAAtN,0BAAA,CAC1B,IAAI,CAACvB,KAAK;UAAA8O,MAAA;QAAA;UAA7B,KAAAD,UAAA,CAAApN,CAAA,MAAAqN,MAAA,GAAAD,UAAA,CAAAnN,CAAA,IAAAC,IAAA,GAA+B;YAAA,IAApB7B,IAAI,GAAAgP,MAAA,CAAAjO,KAAA;YACbkE,gBAAgB,CAAC9F,IAAI,CAAAuE,KAAA,CAArBuB,gBAAgB,EAAArC,kBAAA,CAAS5C,IAAI,CAACiF,gBAAgB,EAAC;;QAChD,SAAAlD,GAAA;UAAAgN,UAAA,CAAA/M,CAAA,CAAAD,GAAA;QAAA;UAAAgN,UAAA,CAAA9M,CAAA;QAAA;QACD,OAAOgD,gBAAgB,CAAC/H,MAAM,CAACgI,OAAO,CAAC;;MAEzC,OAAOA,OAAO;IAChB;IAEA;;;;;EAAA;IAAApE,GAAA;IAAAC,KAAA,EAKS,SAAAkO,WAAA,EAAU;MACjB,IAAM/J,OAAO,GAAoB,EAAE;MAAC,IAAAgK,UAAA,GAAAzN,0BAAA,CACjB,IAAI,CAACvB,KAAK;QAAAiP,MAAA;MAAA;QAA7B,KAAAD,UAAA,CAAAvN,CAAA,MAAAwN,MAAA,GAAAD,UAAA,CAAAtN,CAAA,IAAAC,IAAA,GAA+B;UAAA,IAApB7B,IAAI,GAAAmP,MAAA,CAAApO,KAAA;UACbmE,OAAO,CAAC/F,IAAI,CAAAuE,KAAA,CAAZwB,OAAO,EAAAtC,kBAAA,CAAS5C,IAAI,CAACkF,OAAO,EAAC;;MAC9B,SAAAnD,GAAA;QAAAmN,UAAA,CAAAlN,CAAA,CAAAD,GAAA;MAAA;QAAAmN,UAAA,CAAAjN,CAAA;MAAA;MACD,OAAOxG,aAAa,CAACyJ,OAAO,CAAC;IAC/B;IAEA;;;;;;EAAA;IAAApE,GAAA;IAAAC,KAAA,EAMS,SAAAqO,WAAWlK,OAAiB;MACnC,IAAMmK,MAAM,GAAmC,EAAE;MAAC,IAAAC,WAAA,GAAA7N,0BAAA,CAC/B,IAAI,CAACvB,KAAK;QAAAqP,OAAA;MAAA;QAA7B,KAAAD,WAAA,CAAA3N,CAAA,MAAA4N,OAAA,GAAAD,WAAA,CAAA1N,CAAA,IAAAC,IAAA,GAA+B;UAAA,IAApB7B,IAAI,GAAAuP,OAAA,CAAAxO,KAAA;UACb,IAAMyO,SAAS,GAAGxP,IAAI,CAACkF,OAAO,CAAC9I,MAAM;UACrC,IAAMqT,YAAY,GAAGvK,OAAO,CAAC+I,MAAM,CAACuB,SAAS,CAAC;UAC9C,KAAK,IAAItQ,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGc,IAAI,CAACkF,OAAO,CAAC9I,MAAM,EAAE,EAAE8C,CAAC,EAAE;YAC5CmQ,MAAM,CAAClQ,IAAI,CAAC,CAACa,IAAI,CAACkF,OAAO,CAAChG,CAAC,CAAC,EAAEuQ,YAAY,CAACvQ,CAAC,CAAC,CAAC,CAAC;;;MAElD,SAAA6C,GAAA;QAAAuN,WAAA,CAAAtN,CAAA,CAAAD,GAAA;MAAA;QAAAuN,WAAA,CAAArN,CAAA;MAAA;MACDvG,aAAa,CAAC2T,MAAM,CAAC;IACvB;EAAC;IAAAvO,GAAA;IAAAC,KAAA,EAlED,SAAA8E,WACIC,GAA6C,EAC7CP,MAAgC,EACc;MAAA,IAA9CQ,aAAA,GAAApJ,SAAA,CAAAP,MAAA,QAAAO,SAAA,QAAAC,SAAA,GAAAD,SAAA,MAAgB,EAA8B;MAChD,IAAMuD,KAAK,GAAc,EAAE;MAAC,IAAAwP,WAAA,GAAAjO,0BAAA,CACF8D,MAAM,CAAC,OAAO,CAAgC;QAAAoK,OAAA;MAAA;QAAxE,KAAAD,WAAA,CAAA/N,CAAA,MAAAgO,OAAA,GAAAD,WAAA,CAAA9N,CAAA,IAAAC,IAAA,GAA0E;UAAA,IAA/D2D,UAAU,GAAAmK,OAAA,CAAA5O,KAAA;UACnBb,KAAK,CAACf,IAAI,CAACxD,WAAW,CAAC6J,UAAU,EAAEO,aAAa,CAAY,CAAC;;MAC9D,SAAAhE,GAAA;QAAA2N,WAAA,CAAA1N,CAAA,CAAAD,GAAA;MAAA;QAAA2N,WAAA,CAAAzN,CAAA;MAAA;MACD,OAAO,IAAI6D,GAAG,CAAC;QAAC5F,KAAK,EAALA;MAAK,CAAC,CAAC;IACzB;EAAC;EAAA,OAAAD,eAAA;AAAA,EAtHkCgG,OAAO;AAC1C;AACOhG,eAAA,CAAAyF,SAAS,GAAG,iBAAiB;AAiLtC3L,aAAa,CAACiM,aAAa,CAAC/F,eAAe,CAAC;AAE5C,OAAM,SAAUqI,mBAAmBA,CAAC1I,IAMnC;EACC,IAAO2I,IAAI,GAAoD3I,IAAI,CAA5D2I,IAAI;IAAEC,IAAI,GAA8C5I,IAAI,CAAtD4I,IAAI;IAAAoH,cAAA,GAA8ChQ,IAAI,CAAhDsD,QAAQ;IAARA,QAAQ,GAAA0M,cAAA,cAAG,KAAK,GAAAA,cAAA;IAAAC,WAAA,GAA4BjQ,IAAI,CAA9BkK,KAAK;IAALA,KAAK,GAAA+F,WAAA,cAAG,CAAC,GAAAA,WAAA;IAAEjI,WAAW,GAAIhI,IAAI,CAAnBgI,WAAW;EAE3D,IAAMkI,aAAa,GAAG,SAAhBA,aAAaA,CAAA;IAAA,OACflI,WAAW,IAAI,IAAI,GAAGA,WAAW,CAACW,IAAI,EAAE,EAAEC,IAAI,CAAC,GAAGpO,CAAC,CAACoN,OAAO,CAACe,IAAI,EAAE,EAAEC,IAAI,CAAC;EAAA;EAE7E,IAAMuH,UAAU,GAAG,SAAbA,UAAUA,CAAA;IAAA,OAAS3V,CAAC,CAAC4V,YAAY,CAACF,aAAa,EAAEvH,IAAI,EAAErF,QAAQ,CAAC;EAAA;EAEtE;EACA,IAAI,CAAC4G,KAAK,IAAIA,KAAK,IAAI,CAAC,EAAE;IACxB,OAAOhQ,GAAG,CAAC0J,IAAI,CAACuM,UAAU,EAAE,CAACtM,KAAK,EAAE,CAAC;;EAGvC,IAAMwM,KAAK,GAAGhU,KAAK,CAAC6N,KAAK,CAAC,CAACoG,IAAI,CAACtT,SAAS,CAAC,CAACoC,GAAG,CAAC+Q,UAAU,CAAC;EAE1D,OAAOE,KAAK,CAACjR,GAAG,CAAC,UAAAmR,CAAC;IAAA,OAAIrW,GAAG,CAAC0J,IAAI,CAAC2M,CAAC,CAAC1M,KAAK,EAAE,CAAC;EAAA,EAAC;AAC5C"},"metadata":{},"sourceType":"module","externalDependencies":[]}