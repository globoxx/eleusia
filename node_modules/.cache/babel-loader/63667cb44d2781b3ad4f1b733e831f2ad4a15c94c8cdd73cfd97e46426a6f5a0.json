{"ast":null,"code":"import _regeneratorRuntime from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/regeneratorRuntime.js\";\nimport _asyncToGenerator from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/asyncToGenerator.js\";\nimport _createForOfIteratorHelper from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/createForOfIteratorHelper.js\";\nimport _slicedToArray from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/slicedToArray.js\";\nimport _toConsumableArray from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/toConsumableArray.js\";\nimport _classCallCheck from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/classCallCheck.js\";\nimport _createClass from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/createClass.js\";\n/**\r\n * @license\r\n * Copyright 2018 Google LLC. All Rights Reserved.\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n * http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n * =============================================================================\r\n */\nimport { env, keep, tidy, util } from '@tensorflow/tfjs-core';\nimport { getNodeNameAndIndex, getParamValue, getTensor, getTensorsForCurrentContext, parseNodeName } from '../operations/executors/utils';\nimport { executeOp } from '../operations/operation_executor';\nimport { ExecutionContext } from './execution_context';\nimport { getExecutionSubgraph, getNodeLiveUntilMap, getNodesInTopologicalOrder, isControlFlow } from './model_analysis';\nexport var GraphExecutor = /*#__PURE__*/function () {\n  /**\r\n   *\r\n   * @param graph Graph the model or function graph to be executed.\r\n   * @param parent When building function exector you need to set the parent\r\n   * executor. Since the weights and function executor maps are set at parant\r\n   * level, that function executor can access the function maps and weight maps\r\n   * through the parent.\r\n   */\n  function GraphExecutor(graph, parent) {\n    var _this = this;\n    _classCallCheck(this, GraphExecutor);\n    this.graph = graph;\n    this.parent = parent;\n    this.compiledMap = new Map();\n    this.parseNodeNameCache = new Map();\n    this._weightMap = {};\n    this.SEPARATOR = ',';\n    this._functions = {};\n    this._functionExecutorMap = {};\n    this.keepIntermediateTensors = false;\n    this._outputs = graph.outputs;\n    this._inputs = graph.inputs;\n    this._initNodes = graph.initNodes;\n    this._signature = graph.signature;\n    this._functions = graph.functions;\n    // create sub-graph executors\n    if (graph.functions != null) {\n      Object.keys(graph.functions).forEach(function (name) {\n        _this._functionExecutorMap[name] = new GraphExecutor(graph.functions[name], _this);\n      });\n    }\n  }\n  _createClass(GraphExecutor, [{\n    key: \"weightIds\",\n    get: function get() {\n      return this.parent ? this.parent.weightIds : this._weightIds;\n    }\n  }, {\n    key: \"functionExecutorMap\",\n    get: function get() {\n      return this.parent ? this.parent.functionExecutorMap : this._functionExecutorMap;\n    }\n  }, {\n    key: \"weightMap\",\n    get: function get() {\n      return this.parent ? this.parent.weightMap : this._weightMap;\n    },\n    set: function set(weightMap) {\n      var _ref;\n      var weightIds = Object.keys(weightMap).map(function (key) {\n        return weightMap[key].map(function (tensor) {\n          return tensor.id;\n        });\n      });\n      this._weightIds = (_ref = []).concat.apply(_ref, _toConsumableArray(weightIds));\n      this._weightMap = weightMap;\n    }\n    /**\r\n     * Set `ResourceManager` shared by executors of a model.\r\n     * @param resourceManager: `ResourceManager` of the `GraphModel`.\r\n     */\n  }, {\n    key: \"resourceManager\",\n    set: function set(resourceManager) {\n      this._resourceManager = resourceManager;\n    }\n  }, {\n    key: \"inputs\",\n    get: function get() {\n      return this._inputs.map(function (node) {\n        return {\n          name: node.name,\n          shape: node.attrParams['shape'] ? node.attrParams['shape'].value : undefined,\n          dtype: node.attrParams['dtype'] ? node.attrParams['dtype'].value : undefined\n        };\n      });\n    }\n  }, {\n    key: \"outputs\",\n    get: function get() {\n      return this._outputs.map(function (node) {\n        return {\n          name: node.name,\n          shape: node.attrParams['shape'] ? node.attrParams['shape'].value : undefined,\n          dtype: node.attrParams['dtype'] ? node.attrParams['dtype'].value : undefined\n        };\n      });\n    }\n  }, {\n    key: \"inputNodes\",\n    get: function get() {\n      return this._inputs.map(function (node) {\n        return node.signatureKey || node.name;\n      });\n    }\n  }, {\n    key: \"outputNodes\",\n    get: function get() {\n      return this._outputs.map(function (node) {\n        var name = node.signatureKey || node.name;\n        return node.defaultOutput ? \"\".concat(name, \":\").concat(node.defaultOutput) : name;\n      });\n    }\n  }, {\n    key: \"functions\",\n    get: function get() {\n      var _this2 = this;\n      return Object.keys(this._functions).reduce(function (map, key) {\n        map[key] = _this2._functions[key].signature;\n        return map;\n      }, {});\n    }\n  }, {\n    key: \"getCompilationKey\",\n    value: function getCompilationKey(inputs, outputs) {\n      var sortedInputs = inputs.map(function (node) {\n        return node.name;\n      }).sort();\n      var sortedOutputs = outputs.map(function (node) {\n        return node.name;\n      }).sort();\n      return sortedInputs.join(this.SEPARATOR) + '--' + sortedOutputs.join(this.SEPARATOR);\n    }\n    /**\r\n     * Compiles the inference graph and returns the minimal set of nodes that are\r\n     * required for execution, in the correct execution order.\r\n     * @returns {Object} compilation The compile result.\r\n     * @returns {Node[]} compilation.orderedNodes Nodes in the correct execution\r\n     *     order.\r\n     * @returns {Map<string, Node[]>} compilation.nodeLiveUntilMap A map from node\r\n     *     to disposable nodes after its execution. That is, for a node `x`,\r\n     *     `nodeLiveUntilMap[x]` indicates all nodes whose intermediate\r\n     *     tensors should be disposed after `x` is executed.\r\n     */\n  }, {\n    key: \"compile\",\n    value: function compile(inputs, outputs) {\n      var executionInfo = getExecutionSubgraph(inputs, outputs, this.weightMap, this._initNodes);\n      var missingInputs = executionInfo.missingInputs,\n        dynamicNode = executionInfo.dynamicNode,\n        syncInputs = executionInfo.syncInputs;\n      if (dynamicNode != null) {\n        throw new Error(\"This execution contains the node '\".concat(dynamicNode.name, \"', which has \") + \"the dynamic op '\".concat(dynamicNode.op, \"'. Please use \") + \"model.executeAsync() instead. Alternatively, to avoid the \" + \"dynamic ops, specify the inputs [\".concat(syncInputs, \"]\"));\n      }\n      if (missingInputs.length > 0) {\n        var outNames = outputs.map(function (n) {\n          return n.name;\n        });\n        var inNames = Object.keys(inputs);\n        throw new Error(\"Cannot compute the outputs [\".concat(outNames, \"] from the provided inputs \") + \"[\".concat(inNames, \"]. Missing the following inputs: [\").concat(missingInputs, \"]\"));\n      }\n      var orderedNodes = getNodesInTopologicalOrder(this.graph, executionInfo);\n      var nodeLiveUntilMap = getNodeLiveUntilMap(orderedNodes);\n      return {\n        orderedNodes: orderedNodes,\n        nodeLiveUntilMap: nodeLiveUntilMap\n      };\n    }\n  }, {\n    key: \"cloneAndKeepTensor\",\n    value: function cloneAndKeepTensor(tensor) {\n      if (tensor == null) {\n        return null;\n      }\n      var clone = tensor.clone();\n      // Keep the clone because`model.execute()` may be called within\n      // a `tidy()`, but the user may inspect these tensors after the\n      // tidy.\n      keep(clone);\n      return clone;\n    }\n  }, {\n    key: \"cloneTensorList\",\n    value: function cloneTensorList(tensors) {\n      var _this3 = this;\n      if (!tensors) {\n        return null;\n      }\n      var clonedTensor = tensors.map(function (tensor) {\n        return _this3.cloneAndKeepTensor(tensor);\n      });\n      return clonedTensor;\n    }\n  }, {\n    key: \"cloneTensorMap\",\n    value: function cloneTensorMap(tensorsMap) {\n      var _this4 = this;\n      return Object.fromEntries(Object.entries(tensorsMap).map(function (_ref2) {\n        var _ref3 = _slicedToArray(_ref2, 2),\n          name = _ref3[0],\n          tensorsList = _ref3[1];\n        return [name, _this4.cloneTensorList(tensorsList)];\n      }));\n    }\n    /**\r\n     * Executes the inference for given input tensors.\r\n     * @param inputs Tensor map for the model inputs, keyed by the input node\r\n     * names.\r\n     * @param outputs Optional. output node name from the Tensorflow model, if\r\n     * no outputs are specified, the default outputs of the model would be used.\r\n     * You can inspect intermediate nodes of the model by adding them to the\r\n     * outputs array.\r\n     */\n  }, {\n    key: \"execute\",\n    value: function execute(inputs, outputs) {\n      var _this5 = this;\n      // Dispose any tensors from a prior run to avoid leaking them.\n      this.disposeIntermediateTensors();\n      inputs = this.mapInputs(inputs);\n      var names = Object.keys(inputs).sort();\n      this.checkInputs(inputs);\n      this.checkInputShapeAndType(inputs);\n      outputs = this.mapOutputs(outputs);\n      this.checkOutputs(outputs);\n      var inputNodes = names.map(function (name) {\n        return _this5.graph.nodes[parseNodeName(name)[0]];\n      });\n      var outputNodeNames = outputs.map(function (name) {\n        return parseNodeName(name)[0];\n      });\n      var outputNodeNameSet = new Set(outputNodeNames);\n      var outputNodes = outputNodeNames.map(function (name) {\n        return _this5.graph.nodes[name];\n      });\n      // If no outputs are specified, then use the default outputs of the model.\n      if (outputNodes.length === 0) {\n        outputNodes = this._outputs;\n      }\n      var compilationKey = this.getCompilationKey(inputNodes, outputNodes);\n      // Do nothing if the compiled graph cache contains the input.\n      var compilation = this.compiledMap.get(compilationKey);\n      if (compilation == null) {\n        compilation = this.compile(inputs, outputNodes);\n        this.compiledMap.set(compilationKey, compilation);\n      }\n      // Keep tensors if KEEP_INTERMEDIATE_TENSORS is on.\n      try {\n        this.keepIntermediateTensors = env().getBool('KEEP_INTERMEDIATE_TENSORS');\n      } catch (e) {\n        this.keepIntermediateTensors = false;\n        console.warn(e.message);\n      }\n      var tensorArrayMap = {};\n      var tensorListMap = {};\n      return tidy(function () {\n        var context = new ExecutionContext(_this5.weightMap, tensorArrayMap, tensorListMap, _this5.functionExecutorMap, _this5.parseNodeNameCache);\n        var tensorsMap = Object.assign({}, _this5.weightMap);\n        if (_this5.keepIntermediateTensors) {\n          _this5.clonedTensorsMap = _this5.cloneTensorMap(_this5.weightMap);\n        }\n        Object.keys(inputs).forEach(function (name) {\n          var _parseNodeName = parseNodeName(name, context),\n            _parseNodeName2 = _slicedToArray(_parseNodeName, 2),\n            nodeName = _parseNodeName2[0],\n            index = _parseNodeName2[1];\n          var tensors = [];\n          tensors[index] = inputs[name];\n          tensorsMap[nodeName] = tensors;\n          if (_this5.keepIntermediateTensors) {\n            _this5.clonedTensorsMap[nodeName] = _this5.cloneTensorList(tensors);\n          }\n        });\n        var tensorsToKeep = _this5.getFrozenTensorIds(tensorsMap);\n        var _compilation = compilation,\n          orderedNodes = _compilation.orderedNodes,\n          nodeLiveUntilMap = _compilation.nodeLiveUntilMap;\n        var _iterator = _createForOfIteratorHelper(orderedNodes),\n          _step;\n        try {\n          for (_iterator.s(); !(_step = _iterator.n()).done;) {\n            var node = _step.value;\n            if (tensorsMap[node.name]) {\n              continue;\n            }\n            var tensors = executeOp(node, tensorsMap, context, _this5._resourceManager);\n            if (util.isPromise(tensors)) {\n              throw new Error(\"The execution of the op '\".concat(node.op, \"' returned a promise. \") + \"Please use model.executeAsync() instead.\");\n            }\n            tensorsMap[node.name] = tensors;\n            if (_this5.keepIntermediateTensors) {\n              _this5.clonedTensorsMap[node.name] = _this5.cloneTensorList(tensors);\n            }\n            _this5.checkTensorForDisposalWithNodeLiveUntilInfo(node, tensorsMap, context, tensorsToKeep, outputNodeNameSet, nodeLiveUntilMap.get(node.name));\n          }\n          // dispose the context for the root executor\n        } catch (err) {\n          _iterator.e(err);\n        } finally {\n          _iterator.f();\n        }\n        if (_this5.parent == null) {\n          context.dispose(tensorsToKeep);\n        }\n        return outputs.map(function (name) {\n          return getTensor(name, tensorsMap, context);\n        });\n      });\n    }\n  }, {\n    key: \"getFrozenTensorIds\",\n    value: function getFrozenTensorIds(tensorMap) {\n      var ids = [].concat.apply([], Object.keys(tensorMap).map(function (key) {\n        return tensorMap[key];\n      }).map(function (tensors) {\n        return tensors.map(function (tensor) {\n          return tensor.id;\n        });\n      }));\n      return new Set(ids);\n    }\n  }, {\n    key: \"checkTensorForDisposal\",\n    value: function checkTensorForDisposal(nodeName, node, tensorMap, context, tensorsToKeep, outputNodeNameSet, intermediateTensorConsumerCount) {\n      // Skip output nodes and any control flow nodes, since its dependency is\n      // tricky to track correctly.\n      if (isControlFlow(node) || outputNodeNameSet.has(nodeName)) {\n        return;\n      }\n      var _iterator2 = _createForOfIteratorHelper(tensorMap[nodeName]),\n        _step2;\n      try {\n        for (_iterator2.s(); !(_step2 = _iterator2.n()).done;) {\n          var tensor = _step2.value;\n          if (tensor == null) {\n            continue;\n          }\n          intermediateTensorConsumerCount[tensor.id] = (intermediateTensorConsumerCount[tensor.id] || 0) + node.children.length;\n        }\n      } catch (err) {\n        _iterator2.e(err);\n      } finally {\n        _iterator2.f();\n      }\n      var _iterator3 = _createForOfIteratorHelper(node.inputs),\n        _step3;\n      try {\n        for (_iterator3.s(); !(_step3 = _iterator3.n()).done;) {\n          var input = _step3.value;\n          // Skip any control flow nodes, since its dependency is tricky to track\n          // correctly.\n          if (isControlFlow(input)) {\n            continue;\n          }\n          var tensors = getTensorsForCurrentContext(input.name, tensorMap, context);\n          if (tensors == null) {\n            continue;\n          }\n          var _iterator4 = _createForOfIteratorHelper(tensors),\n            _step4;\n          try {\n            for (_iterator4.s(); !(_step4 = _iterator4.n()).done;) {\n              var _tensor = _step4.value;\n              if (!_tensor || _tensor.kept || tensorsToKeep.has(_tensor.id)) {\n                continue;\n              }\n              // Only intermediate nodes' tensors have counts set, not marked as\n              // kept, and not in `tensorsToKeep`.\n              // Input and weight nodes' tensors should exist in `tensorsToKeep`.\n              // Output and control flow nodes' tensors should never have count set.\n              var count = intermediateTensorConsumerCount[_tensor.id];\n              if (count === 1) {\n                _tensor.dispose();\n                delete intermediateTensorConsumerCount[_tensor.id];\n              } else if (count != null) {\n                intermediateTensorConsumerCount[_tensor.id]--;\n              }\n            }\n          } catch (err) {\n            _iterator4.e(err);\n          } finally {\n            _iterator4.f();\n          }\n        }\n      } catch (err) {\n        _iterator3.e(err);\n      } finally {\n        _iterator3.f();\n      }\n    }\n  }, {\n    key: \"checkTensorForDisposalWithNodeLiveUntilInfo\",\n    value: function checkTensorForDisposalWithNodeLiveUntilInfo(node, tensorMap, context, tensorsToKeep, outputNodeNameSet, liveUntilNodes) {\n      function isNonDisposableNode(node) {\n        // Skip output nodes and any control flow nodes, since its dependency is\n        // tricky to track correctly.\n        return isControlFlow(node) || outputNodeNameSet.has(node.name);\n      }\n      if (isControlFlow(node) || liveUntilNodes == null) {\n        return;\n      }\n      var _iterator5 = _createForOfIteratorHelper(liveUntilNodes),\n        _step5;\n      try {\n        for (_iterator5.s(); !(_step5 = _iterator5.n()).done;) {\n          var nodeToDispose = _step5.value;\n          if (isNonDisposableNode(nodeToDispose)) {\n            continue;\n          }\n          var tensors = getTensorsForCurrentContext(nodeToDispose.name, tensorMap, context);\n          var _iterator6 = _createForOfIteratorHelper(tensors),\n            _step6;\n          try {\n            for (_iterator6.s(); !(_step6 = _iterator6.n()).done;) {\n              var tensor = _step6.value;\n              if (!tensor || tensor.kept || tensorsToKeep.has(tensor.id)) {\n                continue;\n              }\n              tensor.dispose();\n            }\n          } catch (err) {\n            _iterator6.e(err);\n          } finally {\n            _iterator6.f();\n          }\n        }\n      } catch (err) {\n        _iterator5.e(err);\n      } finally {\n        _iterator5.f();\n      }\n    }\n    /**\r\n     * Executes the inference for given input tensors in Async fashion.\r\n     * @param inputs Tensor map for the model inputs, keyed by the input node\r\n     * names.\r\n     * @param outputs output node name from the Tensorflow model, if no outputs\r\n     * are specified, the default outputs of the model would be used. You can\r\n     * inspect intermediate nodes of the model by adding them to the outputs\r\n     * array.\r\n     */\n  }, {\n    key: \"executeAsync\",\n    value: function () {\n      var _executeAsync2 = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee(inputs, outputs) {\n        return _regeneratorRuntime().wrap(function _callee$(_context) {\n          while (1) switch (_context.prev = _context.next) {\n            case 0:\n              return _context.abrupt(\"return\", this._executeAsync(inputs, outputs));\n            case 1:\n            case \"end\":\n              return _context.stop();\n          }\n        }, _callee, this);\n      }));\n      function executeAsync(_x, _x2) {\n        return _executeAsync2.apply(this, arguments);\n      }\n      return executeAsync;\n    }()\n  }, {\n    key: \"disposeIntermediateTensors\",\n    value: function disposeIntermediateTensors() {\n      if (!this.clonedTensorsMap) {\n        return;\n      }\n      Object.values(this.clonedTensorsMap).forEach(function (tensorsList) {\n        var _iterator7 = _createForOfIteratorHelper(tensorsList),\n          _step7;\n        try {\n          for (_iterator7.s(); !(_step7 = _iterator7.n()).done;) {\n            var tensor = _step7.value;\n            if (tensor && !tensor.isDisposed) {\n              tensor.dispose();\n            }\n          }\n        } catch (err) {\n          _iterator7.e(err);\n        } finally {\n          _iterator7.f();\n        }\n      });\n      this.clonedTensorsMap = null;\n    }\n  }, {\n    key: \"getIntermediateTensors\",\n    value: function getIntermediateTensors() {\n      return this.clonedTensorsMap;\n    }\n    /**\r\n     * Executes the inference for given input tensors in Async fashion.\r\n     * @param inputs Tensor map for the model inputs, keyed by the input node\r\n     * names.\r\n     * @param outputs Optional. output node name from the Tensorflow model,\r\n     * if no outputs are specified, the default outputs of the model would be\r\n     * used. You can inspect intermediate nodes of the model by adding them to\r\n     * the outputs array.\r\n     * @param isFunctionExecution Optional. Flag for executing a function.\r\n     * @param tensorArrayMap Optional, global TensorArray map by id. Used for\r\n     * function execution.\r\n     * @param tensorArrayMap Optinal global TensorList map by id. Used for\r\n     * function execution.\r\n     */\n  }, {\n    key: \"_executeAsync\",\n    value: function () {\n      var _executeAsync3 = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee2(inputs, outputs) {\n        var isFunctionExecution,\n          tensorArrayMap,\n          tensorListMap,\n          context,\n          tensorsMap,\n          results,\n          outputIds,\n          inputIds,\n          keepIds,\n          _args2 = arguments;\n        return _regeneratorRuntime().wrap(function _callee2$(_context2) {\n          while (1) switch (_context2.prev = _context2.next) {\n            case 0:\n              isFunctionExecution = _args2.length > 2 && _args2[2] !== undefined ? _args2[2] : false;\n              tensorArrayMap = _args2.length > 3 && _args2[3] !== undefined ? _args2[3] : {};\n              tensorListMap = _args2.length > 4 && _args2[4] !== undefined ? _args2[4] : {};\n              // Dispose any tensors from a prior run to avoid leaking them.\n              this.disposeIntermediateTensors();\n              if (!isFunctionExecution) {\n                inputs = this.mapInputs(inputs);\n                this.checkInputs(inputs);\n                this.checkInputShapeAndType(inputs);\n                outputs = this.mapOutputs(outputs);\n                this.checkOutputs(outputs);\n              }\n              // Keep tensors if KEEP_INTERMEDIATE_TENSORS is on.\n              try {\n                this.keepIntermediateTensors = env().getBool('KEEP_INTERMEDIATE_TENSORS');\n              } catch (e) {\n                this.keepIntermediateTensors = false;\n                console.warn(e.message);\n              }\n              context = new ExecutionContext(this.weightMap, tensorArrayMap, tensorListMap, this.functionExecutorMap, this.parseNodeNameCache);\n              if (this.keepIntermediateTensors) {\n                this.clonedTensorsMap = this.cloneTensorMap(this.weightMap);\n              }\n              // Graph with control flow op requires runtime evaluation of the execution\n              // order, while without control flow the execution order is pre-determined\n              // in the compile method.\n              _context2.next = 10;\n              return this.executeWithControlFlow(inputs, context, outputs, isFunctionExecution);\n            case 10:\n              tensorsMap = _context2.sent;\n              results = outputs.map(function (name) {\n                return getTensor(name, tensorsMap, context);\n              }); // dispose all the intermediate tensors\n              outputIds = results.map(function (t) {\n                return t.id;\n              });\n              inputIds = Object.keys(inputs).map(function (name) {\n                return inputs[name].id;\n              });\n              keepIds = new Set([].concat(_toConsumableArray(outputIds), _toConsumableArray(inputIds), _toConsumableArray(this.weightIds)));\n              Object.values(tensorsMap).forEach(function (tensorsList) {\n                tensorsList.forEach(function (tensor) {\n                  if (tensor && !tensor.isDisposed && !keepIds.has(tensor.id)) {\n                    tensor.dispose();\n                  }\n                });\n              });\n              // dispose the context for the root executor\n              if (this.parent == null) {\n                context.dispose(keepIds);\n              }\n              return _context2.abrupt(\"return\", results);\n            case 18:\n            case \"end\":\n              return _context2.stop();\n          }\n        }, _callee2, this);\n      }));\n      function _executeAsync(_x3, _x4) {\n        return _executeAsync3.apply(this, arguments);\n      }\n      return _executeAsync;\n    }()\n  }, {\n    key: \"executeFunctionAsync\",\n    value: function () {\n      var _executeFunctionAsync = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee3(inputs, tensorArrayMap, tensorListMap) {\n        var _this6 = this;\n        var mappedInputs;\n        return _regeneratorRuntime().wrap(function _callee3$(_context3) {\n          while (1) switch (_context3.prev = _context3.next) {\n            case 0:\n              mappedInputs = inputs.reduce(function (map, tensor, index) {\n                map[_this6.inputs[index].name] = tensor;\n                return map;\n              }, {});\n              return _context3.abrupt(\"return\", this._executeAsync(mappedInputs, this.outputNodes, true, tensorArrayMap, tensorListMap));\n            case 2:\n            case \"end\":\n              return _context3.stop();\n          }\n        }, _callee3, this);\n      }));\n      function executeFunctionAsync(_x5, _x6, _x7) {\n        return _executeFunctionAsync.apply(this, arguments);\n      }\n      return executeFunctionAsync;\n    }()\n    /**\r\n     * When there are control flow nodes in the graph, the graph execution use\r\n     * ExecutionContext to keep track of the frames and loop iterators.\r\n     * @param inputs placeholder tensors for the graph.\r\n     * @param context the execution context object for current execution.\r\n     * @param outputNames Optional. output node name from the Tensorflow model,\r\n     * if no outputs are specified, the default outputs of the model would be\r\n     * used. You can inspect intermediate nodes of the model by adding them to\r\n     * the outputs array.\r\n     * @param isFunctionExecution Flag for executing a function.\r\n     */\n  }, {\n    key: \"executeWithControlFlow\",\n    value: function () {\n      var _executeWithControlFlow = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee4(inputs, context, outputNames, isFunctionExecution) {\n        var _this7 = this;\n        var names, inputNodes, outputNodeNames, outputNodeNameSet, outputNodes, _getExecutionSubgraph, usedNodes, missingInputs, dynamicNode, syncInputs, stack, tensorsMap, intermediateTensorConsumerCount, tensorsToKeep, added, promises, missingOutputs, alternativeMsg;\n        return _regeneratorRuntime().wrap(function _callee4$(_context4) {\n          while (1) switch (_context4.prev = _context4.next) {\n            case 0:\n              names = Object.keys(inputs);\n              inputNodes = names.map(function (name) {\n                return _this7.graph.nodes[parseNodeName(name)[0]];\n              });\n              outputNodeNames = outputNames.map(function (name) {\n                return parseNodeName(name)[0];\n              });\n              outputNodeNameSet = new Set(outputNodeNames);\n              outputNodes = outputNodeNames.map(function (name) {\n                return _this7.graph.nodes[name];\n              }); // If no outputs are specified, then use the default outputs of the model.\n              if (outputNodes.length === 0) {\n                outputNodes = this._outputs;\n              }\n              _getExecutionSubgraph = getExecutionSubgraph(inputs, outputNodes, this.weightMap, this._initNodes), usedNodes = _getExecutionSubgraph.usedNodes, missingInputs = _getExecutionSubgraph.missingInputs, dynamicNode = _getExecutionSubgraph.dynamicNode, syncInputs = _getExecutionSubgraph.syncInputs; // First nodes to execute include inputNodes, weights, and initNodes.\n              stack = [].concat(_toConsumableArray(inputNodes), _toConsumableArray(this.graph.weights), _toConsumableArray(this._initNodes || [])).map(function (node) {\n                return {\n                  node: node,\n                  contexts: context.currentContext\n                };\n              });\n              tensorsMap = Object.assign({}, this.weightMap);\n              Object.keys(inputs).forEach(function (name) {\n                var _parseNodeName3 = parseNodeName(name),\n                  _parseNodeName4 = _slicedToArray(_parseNodeName3, 2),\n                  nodeName = _parseNodeName4[0],\n                  index = _parseNodeName4[1];\n                var tensors = [];\n                tensors[index] = inputs[name];\n                tensorsMap[nodeName] = tensors;\n              });\n              intermediateTensorConsumerCount = {};\n              tensorsToKeep = this.getFrozenTensorIds(tensorsMap);\n              added = {};\n            case 13:\n              if (!(stack.length > 0)) {\n                _context4.next = 19;\n                break;\n              }\n              promises = this.processStack(inputNodes, stack, context, tensorsMap, added, tensorsToKeep, outputNodeNameSet, intermediateTensorConsumerCount, usedNodes);\n              _context4.next = 17;\n              return Promise.all(promises);\n            case 17:\n              _context4.next = 13;\n              break;\n            case 19:\n              if (dynamicNode == null && !isFunctionExecution) {\n                console.warn(\"This model execution did not contain any nodes with control flow \" + \"or dynamic output shapes. You can use model.execute() instead.\");\n              }\n              missingOutputs = outputNodes.filter(function (node) {\n                return !isControlFlow(node) && !getTensor(node.name, tensorsMap, context);\n              }).map(function (node) {\n                return node.name;\n              });\n              if (!(missingOutputs.length > 0)) {\n                _context4.next = 25;\n                break;\n              }\n              alternativeMsg = '';\n              if (dynamicNode != null) {\n                alternativeMsg = \"Alternatively, to avoid the dynamic ops, use model.execute() \" + \"and specify the inputs [\".concat(syncInputs, \"]\");\n              }\n              throw new Error(\"Cannot compute the outputs [\".concat(missingOutputs, \"] from the provided \") + \"inputs [\".concat(names, \"]. Consider providing the following inputs: \") + \"[\".concat(missingInputs, \"]. \").concat(alternativeMsg));\n            case 25:\n              return _context4.abrupt(\"return\", tensorsMap);\n            case 26:\n            case \"end\":\n              return _context4.stop();\n          }\n        }, _callee4, this);\n      }));\n      function executeWithControlFlow(_x8, _x9, _x10, _x11) {\n        return _executeWithControlFlow.apply(this, arguments);\n      }\n      return executeWithControlFlow;\n    }()\n  }, {\n    key: \"processStack\",\n    value: function processStack(inputNodes, stack, context, tensorMap, added, tensorsToKeep, outputNodeNameSet, intermediateTensorConsumerCount, usedNodes) {\n      var _this8 = this;\n      var promises = [];\n      var _loop = function _loop() {\n        var item = stack.pop();\n        context.currentContext = item.contexts;\n        var nodeName = '';\n        // The tensor of the Enter op with isConstant set should be set\n        // in the parent scope, so it will be available as constant for the\n        // whole loop.\n        if (item.node.op === 'Enter' && getParamValue('isConstant', item.node, tensorMap, context)) {\n          var _getNodeNameAndIndex = getNodeNameAndIndex(item.node.name, context);\n          var _getNodeNameAndIndex2 = _slicedToArray(_getNodeNameAndIndex, 1);\n          nodeName = _getNodeNameAndIndex2[0];\n        }\n        // only process nodes that are not in the tensorMap yet, this include\n        // inputNodes and internal initNodes.\n        if (tensorMap[item.node.name] == null) {\n          var tensors = executeOp(item.node, tensorMap, context, _this8._resourceManager);\n          if (!nodeName) {\n            var _getNodeNameAndIndex3 = getNodeNameAndIndex(item.node.name, context);\n            var _getNodeNameAndIndex4 = _slicedToArray(_getNodeNameAndIndex3, 1);\n            nodeName = _getNodeNameAndIndex4[0];\n          }\n          var currentContext = context.currentContext;\n          if (util.isPromise(tensors)) {\n            promises.push(tensors.then(function (t) {\n              tensorMap[nodeName] = t;\n              if (_this8.keepIntermediateTensors) {\n                _this8.clonedTensorsMap[nodeName] = _this8.cloneTensorList(t);\n              }\n              context.currentContext = currentContext;\n              _this8.checkTensorForDisposal(nodeName, item.node, tensorMap, context, tensorsToKeep, outputNodeNameSet, intermediateTensorConsumerCount);\n              _this8.processChildNodes(item.node, stack, context, tensorMap, added, usedNodes);\n              return t;\n            }));\n          } else {\n            tensorMap[nodeName] = tensors;\n            if (_this8.keepIntermediateTensors) {\n              _this8.clonedTensorsMap[nodeName] = _this8.cloneTensorList(tensors);\n            }\n            _this8.checkTensorForDisposal(nodeName, item.node, tensorMap, context, tensorsToKeep, outputNodeNameSet, intermediateTensorConsumerCount);\n            _this8.processChildNodes(item.node, stack, context, tensorMap, added, usedNodes);\n          }\n        } else {\n          _this8.processChildNodes(item.node, stack, context, tensorMap, added, usedNodes);\n        }\n      };\n      while (stack.length > 0) {\n        _loop();\n      }\n      return promises;\n    }\n  }, {\n    key: \"processChildNodes\",\n    value: function processChildNodes(node, stack, context, tensorMap, added, usedNodes) {\n      node.children.forEach(function (childNode) {\n        var _getNodeNameAndIndex5 = getNodeNameAndIndex(childNode.name, context),\n          _getNodeNameAndIndex6 = _slicedToArray(_getNodeNameAndIndex5, 1),\n          nodeName = _getNodeNameAndIndex6[0];\n        if (added[nodeName] || !usedNodes.has(childNode.name)) {\n          return;\n        }\n        // Merge op can be pushed if any of its inputs has value.\n        if (childNode.op === 'Merge') {\n          if (childNode.inputNames.some(function (name) {\n            return !!getTensor(name, tensorMap, context);\n          })) {\n            added[nodeName] = true;\n            stack.push({\n              contexts: context.currentContext,\n              node: childNode\n            });\n          }\n        } else\n          // Otherwise all inputs must to have value.\n          if (childNode.inputNames.every(function (name) {\n            return !!getTensor(name, tensorMap, context);\n          })) {\n            added[nodeName] = true;\n            stack.push({\n              contexts: context.currentContext,\n              node: childNode\n            });\n          }\n      });\n    }\n    /**\r\n     * Releases the memory used by the weight tensors.\r\n     */\n  }, {\n    key: \"dispose\",\n    value: function dispose() {\n      var _this9 = this;\n      Object.keys(this.weightMap).forEach(function (key) {\n        return _this9.weightMap[key].forEach(function (tensor) {\n          return tensor.dispose();\n        });\n      });\n    }\n  }, {\n    key: \"checkInputShapeAndType\",\n    value: function checkInputShapeAndType(inputs) {\n      var _this10 = this;\n      Object.keys(inputs).forEach(function (name) {\n        var input = inputs[name];\n        var _parseNodeName5 = parseNodeName(name),\n          _parseNodeName6 = _slicedToArray(_parseNodeName5, 1),\n          nodeName = _parseNodeName6[0];\n        var node = _this10.graph.nodes[nodeName];\n        if (node.attrParams['shape'] && node.attrParams['shape'].value) {\n          var shape = node.attrParams['shape'].value;\n          var match = shape.length === input.shape.length && input.shape.every(function (dim, index) {\n            return shape[index] === -1 || shape[index] === dim;\n          });\n          util.assert(match, function () {\n            return \"The shape of dict['\".concat(node.name, \"'] provided in \") + \"model.execute(dict) must be [\".concat(shape, \"], but was \") + \"[\".concat(input.shape, \"]\");\n          });\n        }\n        if (node.attrParams['dtype'] && node.attrParams['dtype'].value) {\n          util.assert(input.dtype === node.attrParams['dtype'].value, function () {\n            return \"The dtype of dict['\".concat(node.name, \"'] provided in \") + \"model.execute(dict) must be \" + \"\".concat(node.attrParams['dtype'].value, \", but was \").concat(input.dtype);\n          });\n        }\n      });\n    }\n  }, {\n    key: \"mapInputs\",\n    value: function mapInputs(inputs) {\n      var _a, _b;\n      var result = {};\n      for (var inputName in inputs) {\n        var tensor = (_b = (_a = this._signature) === null || _a === void 0 ? void 0 : _a.inputs) === null || _b === void 0 ? void 0 : _b[inputName];\n        if (tensor != null) {\n          result[tensor.name] = inputs[inputName];\n        } else {\n          result[inputName] = inputs[inputName];\n        }\n      }\n      return result;\n    }\n  }, {\n    key: \"checkInputs\",\n    value: function checkInputs(inputs) {\n      var _this11 = this;\n      var notInGraph = Object.keys(inputs).filter(function (name) {\n        var _parseNodeName7 = parseNodeName(name),\n          _parseNodeName8 = _slicedToArray(_parseNodeName7, 1),\n          nodeName = _parseNodeName8[0];\n        return _this11.graph.nodes[nodeName] == null;\n      });\n      if (notInGraph.length > 0) {\n        throw new Error(\"The dict provided in model.execute(dict) has \" + \"keys: [\".concat(notInGraph, \"] that are not part of graph\"));\n      }\n    }\n  }, {\n    key: \"mapOutputs\",\n    value: function mapOutputs(outputs) {\n      var _this12 = this;\n      return outputs.map(function (name) {\n        var _a, _b;\n        var tensor = (_b = (_a = _this12._signature) === null || _a === void 0 ? void 0 : _a.outputs) === null || _b === void 0 ? void 0 : _b[name];\n        if (tensor != null) {\n          return tensor.name;\n        }\n        return name;\n      }, {});\n    }\n  }, {\n    key: \"checkOutputs\",\n    value: function checkOutputs(outputs) {\n      var _this13 = this;\n      outputs.forEach(function (name) {\n        var _parseNodeName9 = parseNodeName(name),\n          _parseNodeName10 = _slicedToArray(_parseNodeName9, 1),\n          normalizedName = _parseNodeName10[0];\n        if (!_this13.graph.nodes[normalizedName]) {\n          throw new Error(\"The output '\".concat(name, \"' is not found in the graph\"));\n        }\n      });\n    }\n  }]);\n  return GraphExecutor;\n}();","map":{"version":3,"names":["env","keep","tidy","util","getNodeNameAndIndex","getParamValue","getTensor","getTensorsForCurrentContext","parseNodeName","executeOp","ExecutionContext","getExecutionSubgraph","getNodeLiveUntilMap","getNodesInTopologicalOrder","isControlFlow","GraphExecutor","graph","parent","_this","_classCallCheck","compiledMap","Map","parseNodeNameCache","_weightMap","SEPARATOR","_functions","_functionExecutorMap","keepIntermediateTensors","_outputs","outputs","_inputs","inputs","_initNodes","initNodes","_signature","signature","functions","Object","keys","forEach","name","_createClass","key","get","weightIds","_weightIds","functionExecutorMap","weightMap","set","_ref","map","tensor","id","concat","apply","_toConsumableArray","resourceManager","_resourceManager","node","shape","attrParams","value","undefined","dtype","signatureKey","defaultOutput","_this2","reduce","getCompilationKey","sortedInputs","sort","sortedOutputs","join","compile","executionInfo","missingInputs","dynamicNode","syncInputs","Error","op","length","outNames","n","inNames","orderedNodes","nodeLiveUntilMap","cloneAndKeepTensor","clone","cloneTensorList","tensors","_this3","clonedTensor","cloneTensorMap","tensorsMap","_this4","fromEntries","entries","_ref2","_ref3","_slicedToArray","tensorsList","execute","_this5","disposeIntermediateTensors","mapInputs","names","checkInputs","checkInputShapeAndType","mapOutputs","checkOutputs","inputNodes","nodes","outputNodeNames","outputNodeNameSet","Set","outputNodes","compilationKey","compilation","getBool","e","console","warn","message","tensorArrayMap","tensorListMap","context","assign","clonedTensorsMap","_parseNodeName","_parseNodeName2","nodeName","index","tensorsToKeep","getFrozenTensorIds","_compilation","_iterator","_createForOfIteratorHelper","_step","s","done","isPromise","checkTensorForDisposalWithNodeLiveUntilInfo","err","f","dispose","tensorMap","ids","checkTensorForDisposal","intermediateTensorConsumerCount","has","_iterator2","_step2","children","_iterator3","_step3","input","_iterator4","_step4","kept","count","liveUntilNodes","isNonDisposableNode","_iterator5","_step5","nodeToDispose","_iterator6","_step6","_executeAsync2","_asyncToGenerator","_regeneratorRuntime","mark","_callee","wrap","_callee$","_context","prev","next","abrupt","_executeAsync","stop","executeAsync","_x","_x2","arguments","values","_iterator7","_step7","isDisposed","getIntermediateTensors","_executeAsync3","_callee2","isFunctionExecution","results","outputIds","inputIds","keepIds","_args2","_callee2$","_context2","executeWithControlFlow","sent","t","_x3","_x4","_executeFunctionAsync","_callee3","_this6","mappedInputs","_callee3$","_context3","executeFunctionAsync","_x5","_x6","_x7","_executeWithControlFlow","_callee4","outputNames","_this7","_getExecutionSubgraph","usedNodes","stack","added","promises","missingOutputs","alternativeMsg","_callee4$","_context4","weights","contexts","currentContext","_parseNodeName3","_parseNodeName4","processStack","Promise","all","filter","_x8","_x9","_x10","_x11","_this8","_loop","item","pop","_getNodeNameAndIndex","_getNodeNameAndIndex2","_getNodeNameAndIndex3","_getNodeNameAndIndex4","push","then","processChildNodes","childNode","_getNodeNameAndIndex5","_getNodeNameAndIndex6","inputNames","some","every","_this9","_this10","_parseNodeName5","_parseNodeName6","match","dim","assert","result","inputName","_b","_a","_this11","notInGraph","_parseNodeName7","_parseNodeName8","_this12","_this13","_parseNodeName9","_parseNodeName10","normalizedName"],"sources":["C:\\Users\\vince\\OneDrive\\Documents\\GitHub\\tfjs-converter\\src\\executor\\graph_executor.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {DataType, env, keep, NamedTensorMap, Tensor, tidy, util} from '@tensorflow/tfjs-core';\n\nimport {ISignatureDef} from '../data/compiled_api';\nimport {NamedTensorsMap, TensorArrayMap, TensorInfo, TensorListMap} from '../data/types';\nimport {getNodeNameAndIndex, getParamValue, getTensor, getTensorsForCurrentContext, parseNodeName} from '../operations/executors/utils';\nimport {executeOp} from '../operations/operation_executor';\nimport {Graph, Node} from '../operations/types';\n\nimport {ExecutionContext, ExecutionContextInfo} from './execution_context';\nimport {getExecutionSubgraph, getNodeLiveUntilMap, getNodesInTopologicalOrder, isControlFlow} from './model_analysis';\nimport {ResourceManager} from './resource_manager';\nimport {FunctionExecutor} from './types';\n\ninterface NodeWithContexts {\n  contexts: ExecutionContextInfo[];\n  node: Node;\n}\n\nexport class GraphExecutor implements FunctionExecutor {\n  private compiledMap = new Map<string, ReturnType<typeof this.compile>>();\n  private parseNodeNameCache = new Map<string, [string, number, string?]>();\n  private _weightMap: NamedTensorsMap = {};\n  private _weightIds: number[];\n  private _signature: ISignatureDef;\n  private _inputs: Node[];\n  private _outputs: Node[];\n  private _initNodes: Node[];  // Internal init nodes to start initialization.\n  private SEPARATOR = ',';\n  private _functions: {[key: string]: Graph} = {};\n  private _functionExecutorMap: {[key: string]: FunctionExecutor} = {};\n  private _resourceManager: ResourceManager;\n  private clonedTensorsMap: NamedTensorsMap;\n  private keepIntermediateTensors = false;\n\n  get weightIds(): number[] {\n    return this.parent ? this.parent.weightIds : this._weightIds;\n  }\n\n  get functionExecutorMap(): {[key: string]: FunctionExecutor} {\n    return this.parent ? this.parent.functionExecutorMap :\n                         this._functionExecutorMap;\n  }\n\n  get weightMap(): NamedTensorsMap {\n    return this.parent ? this.parent.weightMap : this._weightMap;\n  }\n\n  set weightMap(weightMap: NamedTensorsMap) {\n    const weightIds = Object.keys(weightMap).map(\n        key => weightMap[key].map(tensor => tensor.id));\n    this._weightIds = [].concat(...weightIds);\n    this._weightMap = weightMap;\n  }\n\n  /**\n   * Set `ResourceManager` shared by executors of a model.\n   * @param resourceManager: `ResourceManager` of the `GraphModel`.\n   */\n  set resourceManager(resourceManager: ResourceManager) {\n    this._resourceManager = resourceManager;\n  }\n\n  get inputs(): TensorInfo[] {\n    return this._inputs.map(node => {\n      return {\n        name: node.name,\n        shape: node.attrParams['shape'] ?\n            node.attrParams['shape'].value as number[] :\n            undefined,\n        dtype: node.attrParams['dtype'] ?\n            node.attrParams['dtype'].value as DataType :\n            undefined\n      };\n    });\n  }\n\n  get outputs(): TensorInfo[] {\n    return this._outputs.map(node => {\n      return {\n        name: node.name,\n        shape: node.attrParams['shape'] ?\n            node.attrParams['shape'].value as number[] :\n            undefined,\n        dtype: node.attrParams['dtype'] ?\n            node.attrParams['dtype'].value as DataType :\n            undefined\n      };\n    });\n  }\n\n  get inputNodes(): string[] {\n    return this._inputs.map(node => node.signatureKey || node.name);\n  }\n\n  get outputNodes(): string[] {\n    return this._outputs.map((node) => {\n      const name = node.signatureKey || node.name;\n      return node.defaultOutput ? (`${name}:${node.defaultOutput}`) : name;\n    });\n  }\n\n  get functions(): {[key: string]: ISignatureDef} {\n    return Object.keys(this._functions).reduce((map, key) => {\n      map[key] = this._functions[key].signature;\n      return map;\n    }, {} as {[key: string]: ISignatureDef});\n  }\n\n  /**\n   *\n   * @param graph Graph the model or function graph to be executed.\n   * @param parent When building function exector you need to set the parent\n   * executor. Since the weights and function executor maps are set at parant\n   * level, that function executor can access the function maps and weight maps\n   * through the parent.\n   */\n  constructor(private graph: Graph, private parent?: GraphExecutor) {\n    this._outputs = graph.outputs;\n    this._inputs = graph.inputs;\n    this._initNodes = graph.initNodes;\n    this._signature = graph.signature;\n    this._functions = graph.functions;\n    // create sub-graph executors\n    if (graph.functions != null) {\n      Object.keys(graph.functions).forEach(name => {\n        this._functionExecutorMap[name] =\n            new GraphExecutor(graph.functions[name], this);\n      });\n    }\n  }\n\n  private getCompilationKey(inputs: Node[], outputs: Node[]): string {\n    const sortedInputs = inputs.map(node => node.name).sort();\n    const sortedOutputs = outputs.map(node => node.name).sort();\n    return sortedInputs.join(this.SEPARATOR) + '--' +\n        sortedOutputs.join(this.SEPARATOR);\n  }\n\n  /**\n   * Compiles the inference graph and returns the minimal set of nodes that are\n   * required for execution, in the correct execution order.\n   * @returns {Object} compilation The compile result.\n   * @returns {Node[]} compilation.orderedNodes Nodes in the correct execution\n   *     order.\n   * @returns {Map<string, Node[]>} compilation.nodeLiveUntilMap A map from node\n   *     to disposable nodes after its execution. That is, for a node `x`,\n   *     `nodeLiveUntilMap[x]` indicates all nodes whose intermediate\n   *     tensors should be disposed after `x` is executed.\n   */\n  private compile(inputs: NamedTensorMap, outputs: Node[]):\n      {orderedNodes: Node[], nodeLiveUntilMap: Map<string, Node[]>} {\n    const executionInfo =\n        getExecutionSubgraph(inputs, outputs, this.weightMap, this._initNodes);\n    const {missingInputs, dynamicNode, syncInputs} = executionInfo;\n    if (dynamicNode != null) {\n      throw new Error(\n          `This execution contains the node '${dynamicNode.name}', which has ` +\n          `the dynamic op '${dynamicNode.op}'. Please use ` +\n          `model.executeAsync() instead. Alternatively, to avoid the ` +\n          `dynamic ops, specify the inputs [${syncInputs}]`);\n    }\n\n    if (missingInputs.length > 0) {\n      const outNames = outputs.map(n => n.name);\n      const inNames = Object.keys(inputs);\n      throw new Error(\n          `Cannot compute the outputs [${outNames}] from the provided inputs ` +\n          `[${inNames}]. Missing the following inputs: [${missingInputs}]`);\n    }\n\n    const orderedNodes = getNodesInTopologicalOrder(this.graph, executionInfo);\n    const nodeLiveUntilMap = getNodeLiveUntilMap(orderedNodes);\n    return {orderedNodes, nodeLiveUntilMap};\n  }\n\n  private cloneAndKeepTensor(tensor: Tensor) {\n    if (tensor == null) {\n      return null;\n    }\n    const clone = tensor.clone();\n    // Keep the clone because`model.execute()` may be called within\n    // a `tidy()`, but the user may inspect these tensors after the\n    // tidy.\n    keep(clone);\n    return clone;\n  }\n\n  private cloneTensorList(tensors: Tensor[]) {\n    if (!tensors) {\n      return null;\n    }\n    const clonedTensor = tensors.map(tensor => {\n      return this.cloneAndKeepTensor(tensor);\n    });\n    return clonedTensor;\n  }\n\n  private cloneTensorMap(tensorsMap: NamedTensorsMap): NamedTensorsMap {\n    return Object.fromEntries(\n        Object.entries(tensorsMap).map(([name, tensorsList]) => {\n          return [name, this.cloneTensorList(tensorsList)];\n        }));\n  }\n\n  /**\n   * Executes the inference for given input tensors.\n   * @param inputs Tensor map for the model inputs, keyed by the input node\n   * names.\n   * @param outputs Optional. output node name from the Tensorflow model, if\n   * no outputs are specified, the default outputs of the model would be used.\n   * You can inspect intermediate nodes of the model by adding them to the\n   * outputs array.\n   */\n  execute(inputs: NamedTensorMap, outputs?: string[]): Tensor[] {\n    // Dispose any tensors from a prior run to avoid leaking them.\n    this.disposeIntermediateTensors();\n    inputs = this.mapInputs(inputs);\n    const names = Object.keys(inputs).sort();\n    this.checkInputs(inputs);\n    this.checkInputShapeAndType(inputs);\n    outputs = this.mapOutputs(outputs);\n    this.checkOutputs(outputs);\n    const inputNodes =\n        names.map(name => this.graph.nodes[parseNodeName(name)[0]]);\n    const outputNodeNames = outputs.map(name => parseNodeName(name)[0]);\n    const outputNodeNameSet = new Set(outputNodeNames);\n    let outputNodes = outputNodeNames.map(name => this.graph.nodes[name]);\n    // If no outputs are specified, then use the default outputs of the model.\n    if (outputNodes.length === 0) {\n      outputNodes = this._outputs;\n    }\n\n    const compilationKey = this.getCompilationKey(inputNodes, outputNodes);\n\n    // Do nothing if the compiled graph cache contains the input.\n    let compilation = this.compiledMap.get(compilationKey);\n    if (compilation == null) {\n      compilation = this.compile(inputs, outputNodes);\n      this.compiledMap.set(compilationKey, compilation);\n    }\n\n    // Keep tensors if KEEP_INTERMEDIATE_TENSORS is on.\n    try {\n      this.keepIntermediateTensors = env().getBool('KEEP_INTERMEDIATE_TENSORS');\n    } catch (e) {\n      this.keepIntermediateTensors = false;\n      console.warn(e.message);\n    }\n    const tensorArrayMap: TensorArrayMap = {};\n    const tensorListMap: TensorListMap = {};\n\n    return tidy(() => {\n      const context = new ExecutionContext(\n          this.weightMap, tensorArrayMap, tensorListMap,\n          this.functionExecutorMap, this.parseNodeNameCache);\n      const tensorsMap: NamedTensorsMap = {...this.weightMap};\n      if (this.keepIntermediateTensors) {\n        this.clonedTensorsMap = this.cloneTensorMap(this.weightMap);\n      }\n\n      Object.keys(inputs).forEach(name => {\n        const [nodeName, index] = parseNodeName(name, context);\n        const tensors: Tensor[] = [];\n        tensors[index] = inputs[name];\n        tensorsMap[nodeName] = tensors;\n        if (this.keepIntermediateTensors) {\n          this.clonedTensorsMap[nodeName] = this.cloneTensorList(tensors);\n        }\n      });\n\n      const tensorsToKeep = this.getFrozenTensorIds(tensorsMap);\n      const {orderedNodes, nodeLiveUntilMap} = compilation;\n      for (const node of orderedNodes) {\n        if (tensorsMap[node.name]) {\n          continue;\n        }\n        const tensors =\n            executeOp(node, tensorsMap, context, this._resourceManager) as\n            Tensor[];\n        if (util.isPromise(tensors)) {\n          throw new Error(\n              `The execution of the op '${node.op}' returned a promise. ` +\n              `Please use model.executeAsync() instead.`);\n        }\n        tensorsMap[node.name] = tensors;\n        if (this.keepIntermediateTensors) {\n          this.clonedTensorsMap[node.name] = this.cloneTensorList(tensors);\n        }\n        this.checkTensorForDisposalWithNodeLiveUntilInfo(\n            node, tensorsMap, context, tensorsToKeep, outputNodeNameSet,\n            nodeLiveUntilMap.get(node.name));\n      }\n\n      // dispose the context for the root executor\n      if (this.parent == null) {\n        context.dispose(tensorsToKeep);\n      }\n\n      return outputs.map(name => getTensor(name, tensorsMap, context));\n    });\n  }\n\n  private getFrozenTensorIds(tensorMap: NamedTensorsMap): Set<number> {\n    const ids = [].concat.apply(\n        [],\n        Object.keys(tensorMap)\n            .map(key => tensorMap[key])\n            .map(tensors => tensors.map(tensor => tensor.id)));\n    return new Set(ids);\n  }\n\n  private checkTensorForDisposal(\n      nodeName: string, node: Node, tensorMap: NamedTensorsMap,\n      context: ExecutionContext, tensorsToKeep: Set<number>,\n      outputNodeNameSet: Set<string>,\n      intermediateTensorConsumerCount: {[key: string]: number}) {\n    // Skip output nodes and any control flow nodes, since its dependency is\n    // tricky to track correctly.\n    if (isControlFlow(node) || outputNodeNameSet.has(nodeName)) {\n      return;\n    }\n\n    for (const tensor of tensorMap[nodeName]) {\n      if (tensor == null) {\n        continue;\n      }\n      intermediateTensorConsumerCount[tensor.id] =\n          (intermediateTensorConsumerCount[tensor.id] || 0) +\n          node.children.length;\n    }\n\n    for (const input of node.inputs) {\n      // Skip any control flow nodes, since its dependency is tricky to track\n      // correctly.\n      if (isControlFlow(input)) {\n        continue;\n      }\n\n      const tensors =\n          getTensorsForCurrentContext(input.name, tensorMap, context);\n      if (tensors == null) {\n        continue;\n      }\n\n      for (const tensor of tensors) {\n        if (!tensor || tensor.kept || tensorsToKeep.has(tensor.id)) {\n          continue;\n        }\n\n        // Only intermediate nodes' tensors have counts set, not marked as\n        // kept, and not in `tensorsToKeep`.\n        // Input and weight nodes' tensors should exist in `tensorsToKeep`.\n        // Output and control flow nodes' tensors should never have count set.\n        const count = intermediateTensorConsumerCount[tensor.id];\n        if (count === 1) {\n          tensor.dispose();\n          delete intermediateTensorConsumerCount[tensor.id];\n        } else if (count != null) {\n          intermediateTensorConsumerCount[tensor.id]--;\n        }\n      }\n    }\n  }\n\n  private checkTensorForDisposalWithNodeLiveUntilInfo(\n      node: Node, tensorMap: NamedTensorsMap, context: ExecutionContext,\n      tensorsToKeep: Set<number>, outputNodeNameSet: Set<string>,\n      liveUntilNodes?: Node[]) {\n    function isNonDisposableNode(node: Node) {\n      // Skip output nodes and any control flow nodes, since its dependency is\n      // tricky to track correctly.\n      return isControlFlow(node) || outputNodeNameSet.has(node.name);\n    }\n\n    if (isControlFlow(node) || liveUntilNodes == null) {\n      return;\n    }\n\n    for (const nodeToDispose of liveUntilNodes) {\n      if (isNonDisposableNode(nodeToDispose)) {\n        continue;\n      }\n      const tensors = getTensorsForCurrentContext(\n          nodeToDispose.name, tensorMap, context);\n      for (const tensor of tensors) {\n        if (!tensor || tensor.kept || tensorsToKeep.has(tensor.id)) {\n          continue;\n        }\n        tensor.dispose();\n      }\n    }\n  }\n\n  /**\n   * Executes the inference for given input tensors in Async fashion.\n   * @param inputs Tensor map for the model inputs, keyed by the input node\n   * names.\n   * @param outputs output node name from the Tensorflow model, if no outputs\n   * are specified, the default outputs of the model would be used. You can\n   * inspect intermediate nodes of the model by adding them to the outputs\n   * array.\n   */\n  async executeAsync(inputs: NamedTensorMap, outputs?: string[]):\n      Promise<Tensor[]> {\n    return this._executeAsync(inputs, outputs);\n  }\n\n  disposeIntermediateTensors() {\n    if (!this.clonedTensorsMap) {\n      return;\n    }\n    Object.values(this.clonedTensorsMap).forEach(tensorsList => {\n      for (const tensor of tensorsList) {\n        if (tensor && !tensor.isDisposed) {\n          tensor.dispose();\n        }\n      }\n    });\n\n    this.clonedTensorsMap = null;\n  }\n\n  getIntermediateTensors(): NamedTensorsMap {\n    return this.clonedTensorsMap;\n  }\n\n  /**\n   * Executes the inference for given input tensors in Async fashion.\n   * @param inputs Tensor map for the model inputs, keyed by the input node\n   * names.\n   * @param outputs Optional. output node name from the Tensorflow model,\n   * if no outputs are specified, the default outputs of the model would be\n   * used. You can inspect intermediate nodes of the model by adding them to\n   * the outputs array.\n   * @param isFunctionExecution Optional. Flag for executing a function.\n   * @param tensorArrayMap Optional, global TensorArray map by id. Used for\n   * function execution.\n   * @param tensorArrayMap Optinal global TensorList map by id. Used for\n   * function execution.\n   */\n  private async _executeAsync(\n      inputs: NamedTensorMap, outputs?: string[], isFunctionExecution = false,\n      tensorArrayMap: TensorArrayMap = {},\n      tensorListMap: TensorListMap = {}): Promise<Tensor[]> {\n    // Dispose any tensors from a prior run to avoid leaking them.\n    this.disposeIntermediateTensors();\n    if (!isFunctionExecution) {\n      inputs = this.mapInputs(inputs);\n      this.checkInputs(inputs);\n      this.checkInputShapeAndType(inputs);\n      outputs = this.mapOutputs(outputs);\n      this.checkOutputs(outputs);\n    }\n\n    // Keep tensors if KEEP_INTERMEDIATE_TENSORS is on.\n    try {\n      this.keepIntermediateTensors = env().getBool('KEEP_INTERMEDIATE_TENSORS');\n    } catch (e) {\n      this.keepIntermediateTensors = false;\n      console.warn(e.message);\n    }\n\n    const context = new ExecutionContext(\n        this.weightMap, tensorArrayMap, tensorListMap, this.functionExecutorMap,\n        this.parseNodeNameCache);\n\n    if (this.keepIntermediateTensors) {\n      this.clonedTensorsMap = this.cloneTensorMap(this.weightMap);\n    }\n\n    // Graph with control flow op requires runtime evaluation of the execution\n    // order, while without control flow the execution order is pre-determined\n    // in the compile method.\n    const tensorsMap = await this.executeWithControlFlow(\n        inputs, context, outputs, isFunctionExecution);\n    const results = outputs.map(name => getTensor(name, tensorsMap, context));\n\n    // dispose all the intermediate tensors\n    const outputIds = results.map(t => t.id);\n    const inputIds = Object.keys(inputs).map(name => inputs[name].id);\n    const keepIds =\n        new Set<number>([...outputIds, ...inputIds, ...this.weightIds]);\n\n    Object.values(tensorsMap).forEach(tensorsList => {\n      tensorsList.forEach(tensor => {\n        if (tensor && !tensor.isDisposed && !keepIds.has(tensor.id)) {\n          tensor.dispose();\n        }\n      });\n    });\n\n    // dispose the context for the root executor\n    if (this.parent == null) {\n      context.dispose(keepIds);\n    }\n\n    return results;\n  }\n\n  async executeFunctionAsync(\n      inputs: Tensor[], tensorArrayMap: TensorArrayMap,\n      tensorListMap: TensorListMap): Promise<Tensor[]> {\n    const mappedInputs = inputs.reduce((map, tensor, index) => {\n      map[this.inputs[index].name] = tensor;\n      return map;\n    }, {} as NamedTensorMap);\n\n    return this._executeAsync(\n        mappedInputs, this.outputNodes, true, tensorArrayMap, tensorListMap);\n  }\n\n  /**\n   * When there are control flow nodes in the graph, the graph execution use\n   * ExecutionContext to keep track of the frames and loop iterators.\n   * @param inputs placeholder tensors for the graph.\n   * @param context the execution context object for current execution.\n   * @param outputNames Optional. output node name from the Tensorflow model,\n   * if no outputs are specified, the default outputs of the model would be\n   * used. You can inspect intermediate nodes of the model by adding them to\n   * the outputs array.\n   * @param isFunctionExecution Flag for executing a function.\n   */\n  private async executeWithControlFlow(\n      inputs: NamedTensorMap, context: ExecutionContext, outputNames?: string[],\n      isFunctionExecution?: boolean): Promise<NamedTensorsMap> {\n    const names = Object.keys(inputs);\n    const inputNodes =\n        names.map(name => this.graph.nodes[parseNodeName(name)[0]]);\n    const outputNodeNames = outputNames.map(name => parseNodeName(name)[0]);\n    const outputNodeNameSet = new Set(outputNodeNames);\n    let outputNodes = outputNodeNames.map(name => this.graph.nodes[name]);\n\n    // If no outputs are specified, then use the default outputs of the model.\n    if (outputNodes.length === 0) {\n      outputNodes = this._outputs;\n    }\n\n    const {usedNodes, missingInputs, dynamicNode, syncInputs} =\n        getExecutionSubgraph(\n            inputs, outputNodes, this.weightMap, this._initNodes);\n\n    // First nodes to execute include inputNodes, weights, and initNodes.\n    const stack: NodeWithContexts[] = [\n      ...inputNodes, ...this.graph.weights, ...(this._initNodes || [])\n    ].map(node => {\n      return {node, contexts: context.currentContext};\n    });\n    const tensorsMap: NamedTensorsMap = {...this.weightMap};\n    Object.keys(inputs).forEach(name => {\n      const [nodeName, index] = parseNodeName(name);\n      const tensors: Tensor[] = [];\n      tensors[index] = inputs[name];\n      tensorsMap[nodeName] = tensors;\n    });\n    const intermediateTensorConsumerCount: {[key: number]: number} = {};\n    const tensorsToKeep = this.getFrozenTensorIds(tensorsMap);\n    const added: {[key: string]: boolean} = {};\n    while (stack.length > 0) {\n      const promises = this.processStack(\n          inputNodes, stack, context, tensorsMap, added, tensorsToKeep,\n          outputNodeNameSet, intermediateTensorConsumerCount, usedNodes);\n      await Promise.all(promises);\n    }\n    if (dynamicNode == null && !isFunctionExecution) {\n      console.warn(\n          `This model execution did not contain any nodes with control flow ` +\n          `or dynamic output shapes. You can use model.execute() instead.`);\n    }\n    const missingOutputs =\n        outputNodes\n            .filter(\n                node => !isControlFlow(node) &&\n                    !getTensor(node.name, tensorsMap, context))\n            .map(node => node.name);\n    if (missingOutputs.length > 0) {\n      let alternativeMsg = '';\n      if (dynamicNode != null) {\n        alternativeMsg =\n            `Alternatively, to avoid the dynamic ops, use model.execute() ` +\n            `and specify the inputs [${syncInputs}]`;\n      }\n      throw new Error(\n          `Cannot compute the outputs [${missingOutputs}] from the provided ` +\n          `inputs [${names}]. Consider providing the following inputs: ` +\n          `[${missingInputs}]. ${alternativeMsg}`);\n    }\n    return tensorsMap;\n  }\n\n  private processStack(\n      inputNodes: Node[], stack: NodeWithContexts[], context: ExecutionContext,\n      tensorMap: NamedTensorsMap, added: {[key: string]: boolean},\n      tensorsToKeep: Set<number>, outputNodeNameSet: Set<string>,\n      intermediateTensorConsumerCount: {[key: number]: number},\n      usedNodes: Set<string>) {\n    const promises: Array<Promise<Tensor[]>> = [];\n    while (stack.length > 0) {\n      const item = stack.pop();\n      context.currentContext = item.contexts;\n      let nodeName = '';\n      // The tensor of the Enter op with isConstant set should be set\n      // in the parent scope, so it will be available as constant for the\n      // whole loop.\n      if (item.node.op === 'Enter' &&\n          getParamValue('isConstant', item.node, tensorMap, context)) {\n        [nodeName] = getNodeNameAndIndex(item.node.name, context);\n      }\n\n      // only process nodes that are not in the tensorMap yet, this include\n      // inputNodes and internal initNodes.\n      if (tensorMap[item.node.name] == null) {\n        const tensors =\n            executeOp(item.node, tensorMap, context, this._resourceManager);\n        if (!nodeName) {\n          [nodeName] = getNodeNameAndIndex(item.node.name, context);\n        }\n        const currentContext = context.currentContext;\n        if (util.isPromise(tensors)) {\n          promises.push(tensors.then(t => {\n            tensorMap[nodeName] = t;\n            if (this.keepIntermediateTensors) {\n              this.clonedTensorsMap[nodeName] = this.cloneTensorList(t);\n            }\n            context.currentContext = currentContext;\n            this.checkTensorForDisposal(\n                nodeName, item.node, tensorMap, context, tensorsToKeep,\n                outputNodeNameSet, intermediateTensorConsumerCount);\n            this.processChildNodes(\n                item.node, stack, context, tensorMap, added, usedNodes);\n            return t;\n          }));\n        } else {\n          tensorMap[nodeName] = tensors;\n          if (this.keepIntermediateTensors) {\n            this.clonedTensorsMap[nodeName] = this.cloneTensorList(tensors);\n          }\n          this.checkTensorForDisposal(\n              nodeName, item.node, tensorMap, context, tensorsToKeep,\n              outputNodeNameSet, intermediateTensorConsumerCount);\n          this.processChildNodes(\n              item.node, stack, context, tensorMap, added, usedNodes);\n        }\n      } else {\n        this.processChildNodes(\n            item.node, stack, context, tensorMap, added, usedNodes);\n      }\n    }\n    return promises;\n  }\n\n  private processChildNodes(\n      node: Node, stack: NodeWithContexts[], context: ExecutionContext,\n      tensorMap: NamedTensorsMap, added: {[key: string]: boolean},\n      usedNodes: Set<string>) {\n    node.children.forEach((childNode) => {\n      const [nodeName, ] = getNodeNameAndIndex(childNode.name, context);\n      if (added[nodeName] || !usedNodes.has(childNode.name)) {\n        return;\n      }\n      // Merge op can be pushed if any of its inputs has value.\n      if (childNode.op === 'Merge') {\n        if (childNode.inputNames.some(name => {\n              return !!getTensor(name, tensorMap, context);\n            })) {\n          added[nodeName] = true;\n          stack.push({contexts: context.currentContext, node: childNode});\n        }\n      } else  // Otherwise all inputs must to have value.\n          if (childNode.inputNames.every(name => {\n                return !!getTensor(name, tensorMap, context);\n              })) {\n        added[nodeName] = true;\n        stack.push({contexts: context.currentContext, node: childNode});\n      }\n    });\n  }\n\n  /**\n   * Releases the memory used by the weight tensors.\n   */\n  dispose() {\n    Object.keys(this.weightMap)\n        .forEach(\n            key => this.weightMap[key].forEach(tensor => tensor.dispose()));\n  }\n\n  private checkInputShapeAndType(inputs: NamedTensorMap) {\n    Object.keys(inputs).forEach(name => {\n      const input = inputs[name];\n      const [nodeName, ] = parseNodeName(name);\n      const node = this.graph.nodes[nodeName];\n      if (node.attrParams['shape'] && node.attrParams['shape'].value) {\n        const shape = node.attrParams['shape'].value as number[];\n        const match = shape.length === input.shape.length &&\n            input.shape.every(\n                (dim, index) => shape[index] === -1 || shape[index] === dim);\n        util.assert(\n            match,\n            () => `The shape of dict['${node.name}'] provided in ` +\n                `model.execute(dict) must be [${shape}], but was ` +\n                `[${input.shape}]`);\n      }\n      if (node.attrParams['dtype'] && node.attrParams['dtype'].value) {\n        util.assert(\n            input.dtype === node.attrParams['dtype'].value as string,\n            () => `The dtype of dict['${node.name}'] provided in ` +\n                `model.execute(dict) must be ` +\n                `${node.attrParams['dtype'].value}, but was ${input.dtype}`);\n      }\n    });\n  }\n\n  private mapInputs(inputs: NamedTensorMap) {\n    const result: NamedTensorMap = {};\n    for (const inputName in inputs) {\n      const tensor = this._signature ?.inputs ?.[inputName];\n      if (tensor != null) {\n        result[tensor.name] = inputs[inputName];\n      } else {\n        result[inputName] = inputs[inputName];\n      }\n    }\n    return result;\n  }\n\n  private checkInputs(inputs: NamedTensorMap) {\n    const notInGraph = Object.keys(inputs).filter(name => {\n      const [nodeName] = parseNodeName(name);\n      return this.graph.nodes[nodeName] == null;\n    });\n    if (notInGraph.length > 0) {\n      throw new Error(\n          `The dict provided in model.execute(dict) has ` +\n          `keys: [${notInGraph}] that are not part of graph`);\n    }\n  }\n\n  private mapOutputs(outputs: string[]) {\n    return outputs.map(name => {\n      const tensor = this._signature ?.outputs ?.[name];\n      if (tensor != null) {\n        return tensor.name;\n      }\n      return name;\n    }, {});\n  }\n\n  private checkOutputs(outputs: string[]): void {\n    outputs.forEach(name => {\n      const [normalizedName] = parseNodeName(name);\n      if (!this.graph.nodes[normalizedName]) {\n        throw new Error(`The output '${name}' is not found in the graph`);\n      }\n    });\n  }\n}\n"],"mappings":";;;;;;;AAAA;;;;;;;;;;;;;;;;AAiBA,SAAkBA,GAAG,EAAEC,IAAI,EAA0BC,IAAI,EAAEC,IAAI,QAAO,uBAAuB;AAI7F,SAAQC,mBAAmB,EAAEC,aAAa,EAAEC,SAAS,EAAEC,2BAA2B,EAAEC,aAAa,QAAO,+BAA+B;AACvI,SAAQC,SAAS,QAAO,kCAAkC;AAG1D,SAAQC,gBAAgB,QAA6B,qBAAqB;AAC1E,SAAQC,oBAAoB,EAAEC,mBAAmB,EAAEC,0BAA0B,EAAEC,aAAa,QAAO,kBAAkB;AASrH,WAAaC,aAAa;EA0FxB;;;;;;;;EAQA,SAAAA,cAAoBC,KAAY,EAAUC,MAAsB;IAAA,IAAAC,KAAA;IAAAC,eAAA,OAAAJ,aAAA;IAA5C,KAAAC,KAAK,GAALA,KAAK;IAAiB,KAAAC,MAAM,GAANA,MAAM;IAjGxC,KAAAG,WAAW,GAAG,IAAIC,GAAG,EAA2C;IAChE,KAAAC,kBAAkB,GAAG,IAAID,GAAG,EAAqC;IACjE,KAAAE,UAAU,GAAoB,EAAE;IAMhC,KAAAC,SAAS,GAAG,GAAG;IACf,KAAAC,UAAU,GAA2B,EAAE;IACvC,KAAAC,oBAAoB,GAAsC,EAAE;IAG5D,KAAAC,uBAAuB,GAAG,KAAK;IAqFrC,IAAI,CAACC,QAAQ,GAAGZ,KAAK,CAACa,OAAO;IAC7B,IAAI,CAACC,OAAO,GAAGd,KAAK,CAACe,MAAM;IAC3B,IAAI,CAACC,UAAU,GAAGhB,KAAK,CAACiB,SAAS;IACjC,IAAI,CAACC,UAAU,GAAGlB,KAAK,CAACmB,SAAS;IACjC,IAAI,CAACV,UAAU,GAAGT,KAAK,CAACoB,SAAS;IACjC;IACA,IAAIpB,KAAK,CAACoB,SAAS,IAAI,IAAI,EAAE;MAC3BC,MAAM,CAACC,IAAI,CAACtB,KAAK,CAACoB,SAAS,CAAC,CAACG,OAAO,CAAC,UAAAC,IAAI,EAAG;QAC1CtB,KAAI,CAACQ,oBAAoB,CAACc,IAAI,CAAC,GAC3B,IAAIzB,aAAa,CAACC,KAAK,CAACoB,SAAS,CAACI,IAAI,CAAC,EAAEtB,KAAI,CAAC;MACpD,CAAC,CAAC;;EAEN;EAACuB,YAAA,CAAA1B,aAAA;IAAA2B,GAAA;IAAAC,GAAA,EA/FD,SAAAA,IAAA,EAAa;MACX,OAAO,IAAI,CAAC1B,MAAM,GAAG,IAAI,CAACA,MAAM,CAAC2B,SAAS,GAAG,IAAI,CAACC,UAAU;IAC9D;EAAC;IAAAH,GAAA;IAAAC,GAAA,EAED,SAAAA,IAAA,EAAuB;MACrB,OAAO,IAAI,CAAC1B,MAAM,GAAG,IAAI,CAACA,MAAM,CAAC6B,mBAAmB,GAC/B,IAAI,CAACpB,oBAAoB;IAChD;EAAC;IAAAgB,GAAA;IAAAC,GAAA,EAED,SAAAA,IAAA,EAAa;MACX,OAAO,IAAI,CAAC1B,MAAM,GAAG,IAAI,CAACA,MAAM,CAAC8B,SAAS,GAAG,IAAI,CAACxB,UAAU;IAC9D,CAAC;IAAAyB,GAAA,EAED,SAAAA,IAAcD,SAA0B;MAAA,IAAAE,IAAA;MACtC,IAAML,SAAS,GAAGP,MAAM,CAACC,IAAI,CAACS,SAAS,CAAC,CAACG,GAAG,CACxC,UAAAR,GAAG;QAAA,OAAIK,SAAS,CAACL,GAAG,CAAC,CAACQ,GAAG,CAAC,UAAAC,MAAM;UAAA,OAAIA,MAAM,CAACC,EAAE;QAAA,EAAC;MAAA,EAAC;MACnD,IAAI,CAACP,UAAU,GAAG,CAAAI,IAAA,KAAE,EAACI,MAAM,CAAAC,KAAA,CAAAL,IAAA,EAAAM,kBAAA,CAAIX,SAAS,EAAC;MACzC,IAAI,CAACrB,UAAU,GAAGwB,SAAS;IAC7B;IAEA;;;;EAAA;IAAAL,GAAA;IAAAM,GAAA,EAIA,SAAAA,IAAoBQ,eAAgC;MAClD,IAAI,CAACC,gBAAgB,GAAGD,eAAe;IACzC;EAAC;IAAAd,GAAA;IAAAC,GAAA,EAED,SAAAA,IAAA,EAAU;MACR,OAAO,IAAI,CAACb,OAAO,CAACoB,GAAG,CAAC,UAAAQ,IAAI,EAAG;QAC7B,OAAO;UACLlB,IAAI,EAAEkB,IAAI,CAAClB,IAAI;UACfmB,KAAK,EAAED,IAAI,CAACE,UAAU,CAAC,OAAO,CAAC,GAC3BF,IAAI,CAACE,UAAU,CAAC,OAAO,CAAC,CAACC,KAAiB,GAC1CC,SAAS;UACbC,KAAK,EAAEL,IAAI,CAACE,UAAU,CAAC,OAAO,CAAC,GAC3BF,IAAI,CAACE,UAAU,CAAC,OAAO,CAAC,CAACC,KAAiB,GAC1CC;SACL;MACH,CAAC,CAAC;IACJ;EAAC;IAAApB,GAAA;IAAAC,GAAA,EAED,SAAAA,IAAA,EAAW;MACT,OAAO,IAAI,CAACf,QAAQ,CAACsB,GAAG,CAAC,UAAAQ,IAAI,EAAG;QAC9B,OAAO;UACLlB,IAAI,EAAEkB,IAAI,CAAClB,IAAI;UACfmB,KAAK,EAAED,IAAI,CAACE,UAAU,CAAC,OAAO,CAAC,GAC3BF,IAAI,CAACE,UAAU,CAAC,OAAO,CAAC,CAACC,KAAiB,GAC1CC,SAAS;UACbC,KAAK,EAAEL,IAAI,CAACE,UAAU,CAAC,OAAO,CAAC,GAC3BF,IAAI,CAACE,UAAU,CAAC,OAAO,CAAC,CAACC,KAAiB,GAC1CC;SACL;MACH,CAAC,CAAC;IACJ;EAAC;IAAApB,GAAA;IAAAC,GAAA,EAED,SAAAA,IAAA,EAAc;MACZ,OAAO,IAAI,CAACb,OAAO,CAACoB,GAAG,CAAC,UAAAQ,IAAI;QAAA,OAAIA,IAAI,CAACM,YAAY,IAAIN,IAAI,CAAClB,IAAI;MAAA,EAAC;IACjE;EAAC;IAAAE,GAAA;IAAAC,GAAA,EAED,SAAAA,IAAA,EAAe;MACb,OAAO,IAAI,CAACf,QAAQ,CAACsB,GAAG,CAAC,UAACQ,IAAI,EAAI;QAChC,IAAMlB,IAAI,GAAGkB,IAAI,CAACM,YAAY,IAAIN,IAAI,CAAClB,IAAI;QAC3C,OAAOkB,IAAI,CAACO,aAAa,MAAAZ,MAAA,CAAOb,IAAI,OAAAa,MAAA,CAAIK,IAAI,CAACO,aAAa,IAAMzB,IAAI;MACtE,CAAC,CAAC;IACJ;EAAC;IAAAE,GAAA;IAAAC,GAAA,EAED,SAAAA,IAAA,EAAa;MAAA,IAAAuB,MAAA;MACX,OAAO7B,MAAM,CAACC,IAAI,CAAC,IAAI,CAACb,UAAU,CAAC,CAAC0C,MAAM,CAAC,UAACjB,GAAG,EAAER,GAAG,EAAI;QACtDQ,GAAG,CAACR,GAAG,CAAC,GAAGwB,MAAI,CAACzC,UAAU,CAACiB,GAAG,CAAC,CAACP,SAAS;QACzC,OAAOe,GAAG;MACZ,CAAC,EAAE,EAAoC,CAAC;IAC1C;EAAC;IAAAR,GAAA;IAAAmB,KAAA,EAyBO,SAAAO,kBAAkBrC,MAAc,EAAEF,OAAe;MACvD,IAAMwC,YAAY,GAAGtC,MAAM,CAACmB,GAAG,CAAC,UAAAQ,IAAI;QAAA,OAAIA,IAAI,CAAClB,IAAI;MAAA,EAAC,CAAC8B,IAAI,EAAE;MACzD,IAAMC,aAAa,GAAG1C,OAAO,CAACqB,GAAG,CAAC,UAAAQ,IAAI;QAAA,OAAIA,IAAI,CAAClB,IAAI;MAAA,EAAC,CAAC8B,IAAI,EAAE;MAC3D,OAAOD,YAAY,CAACG,IAAI,CAAC,IAAI,CAAChD,SAAS,CAAC,GAAG,IAAI,GAC3C+C,aAAa,CAACC,IAAI,CAAC,IAAI,CAAChD,SAAS,CAAC;IACxC;IAEA;;;;;;;;;;;EAAA;IAAAkB,GAAA;IAAAmB,KAAA,EAWQ,SAAAY,QAAQ1C,MAAsB,EAAEF,OAAe;MAErD,IAAM6C,aAAa,GACf/D,oBAAoB,CAACoB,MAAM,EAAEF,OAAO,EAAE,IAAI,CAACkB,SAAS,EAAE,IAAI,CAACf,UAAU,CAAC;MAC1E,IAAO2C,aAAa,GAA6BD,aAAa,CAAvDC,aAAa;QAAEC,WAAW,GAAgBF,aAAa,CAAxCE,WAAW;QAAEC,UAAU,GAAIH,aAAa,CAA3BG,UAAU;MAC7C,IAAID,WAAW,IAAI,IAAI,EAAE;QACvB,MAAM,IAAIE,KAAK,CACX,qCAAAzB,MAAA,CAAqCuB,WAAW,CAACpC,IAAI,wCAAAa,MAAA,CAClCuB,WAAW,CAACG,EAAE,mBAAgB,+DACW,uCAAA1B,MAAA,CACxBwB,UAAU,MAAG,CAAC;;MAGxD,IAAIF,aAAa,CAACK,MAAM,GAAG,CAAC,EAAE;QAC5B,IAAMC,QAAQ,GAAGpD,OAAO,CAACqB,GAAG,CAAC,UAAAgC,CAAC;UAAA,OAAIA,CAAC,CAAC1C,IAAI;QAAA,EAAC;QACzC,IAAM2C,OAAO,GAAG9C,MAAM,CAACC,IAAI,CAACP,MAAM,CAAC;QACnC,MAAM,IAAI+C,KAAK,CACX,+BAAAzB,MAAA,CAA+B4B,QAAQ,uCAAA5B,MAAA,CACnC8B,OAAO,wCAAA9B,MAAA,CAAqCsB,aAAa,MAAG,CAAC;;MAGvE,IAAMS,YAAY,GAAGvE,0BAA0B,CAAC,IAAI,CAACG,KAAK,EAAE0D,aAAa,CAAC;MAC1E,IAAMW,gBAAgB,GAAGzE,mBAAmB,CAACwE,YAAY,CAAC;MAC1D,OAAO;QAACA,YAAY,EAAZA,YAAY;QAAEC,gBAAgB,EAAhBA;MAAgB,CAAC;IACzC;EAAC;IAAA3C,GAAA;IAAAmB,KAAA,EAEO,SAAAyB,mBAAmBnC,MAAc;MACvC,IAAIA,MAAM,IAAI,IAAI,EAAE;QAClB,OAAO,IAAI;;MAEb,IAAMoC,KAAK,GAAGpC,MAAM,CAACoC,KAAK,EAAE;MAC5B;MACA;MACA;MACAtF,IAAI,CAACsF,KAAK,CAAC;MACX,OAAOA,KAAK;IACd;EAAC;IAAA7C,GAAA;IAAAmB,KAAA,EAEO,SAAA2B,gBAAgBC,OAAiB;MAAA,IAAAC,MAAA;MACvC,IAAI,CAACD,OAAO,EAAE;QACZ,OAAO,IAAI;;MAEb,IAAME,YAAY,GAAGF,OAAO,CAACvC,GAAG,CAAC,UAAAC,MAAM,EAAG;QACxC,OAAOuC,MAAI,CAACJ,kBAAkB,CAACnC,MAAM,CAAC;MACxC,CAAC,CAAC;MACF,OAAOwC,YAAY;IACrB;EAAC;IAAAjD,GAAA;IAAAmB,KAAA,EAEO,SAAA+B,eAAeC,UAA2B;MAAA,IAAAC,MAAA;MAChD,OAAOzD,MAAM,CAAC0D,WAAW,CACrB1D,MAAM,CAAC2D,OAAO,CAACH,UAAU,CAAC,CAAC3C,GAAG,CAAC,UAAA+C,KAAA,EAAwB;QAAA,IAAAC,KAAA,GAAAC,cAAA,CAAAF,KAAA;UAAtBzD,IAAI,GAAA0D,KAAA;UAAEE,WAAW,GAAAF,KAAA;QAChD,OAAO,CAAC1D,IAAI,EAAEsD,MAAI,CAACN,eAAe,CAACY,WAAW,CAAC,CAAC;MAClD,CAAC,CAAC,CAAC;IACT;IAEA;;;;;;;;;EAAA;IAAA1D,GAAA;IAAAmB,KAAA,EASA,SAAAwC,QAAQtE,MAAsB,EAAEF,OAAkB;MAAA,IAAAyE,MAAA;MAChD;MACA,IAAI,CAACC,0BAA0B,EAAE;MACjCxE,MAAM,GAAG,IAAI,CAACyE,SAAS,CAACzE,MAAM,CAAC;MAC/B,IAAM0E,KAAK,GAAGpE,MAAM,CAACC,IAAI,CAACP,MAAM,CAAC,CAACuC,IAAI,EAAE;MACxC,IAAI,CAACoC,WAAW,CAAC3E,MAAM,CAAC;MACxB,IAAI,CAAC4E,sBAAsB,CAAC5E,MAAM,CAAC;MACnCF,OAAO,GAAG,IAAI,CAAC+E,UAAU,CAAC/E,OAAO,CAAC;MAClC,IAAI,CAACgF,YAAY,CAAChF,OAAO,CAAC;MAC1B,IAAMiF,UAAU,GACZL,KAAK,CAACvD,GAAG,CAAC,UAAAV,IAAI;QAAA,OAAI8D,MAAI,CAACtF,KAAK,CAAC+F,KAAK,CAACvG,aAAa,CAACgC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC;MAAA,EAAC;MAC/D,IAAMwE,eAAe,GAAGnF,OAAO,CAACqB,GAAG,CAAC,UAAAV,IAAI;QAAA,OAAIhC,aAAa,CAACgC,IAAI,CAAC,CAAC,CAAC,CAAC;MAAA,EAAC;MACnE,IAAMyE,iBAAiB,GAAG,IAAIC,GAAG,CAACF,eAAe,CAAC;MAClD,IAAIG,WAAW,GAAGH,eAAe,CAAC9D,GAAG,CAAC,UAAAV,IAAI;QAAA,OAAI8D,MAAI,CAACtF,KAAK,CAAC+F,KAAK,CAACvE,IAAI,CAAC;MAAA,EAAC;MACrE;MACA,IAAI2E,WAAW,CAACnC,MAAM,KAAK,CAAC,EAAE;QAC5BmC,WAAW,GAAG,IAAI,CAACvF,QAAQ;;MAG7B,IAAMwF,cAAc,GAAG,IAAI,CAAChD,iBAAiB,CAAC0C,UAAU,EAAEK,WAAW,CAAC;MAEtE;MACA,IAAIE,WAAW,GAAG,IAAI,CAACjG,WAAW,CAACuB,GAAG,CAACyE,cAAc,CAAC;MACtD,IAAIC,WAAW,IAAI,IAAI,EAAE;QACvBA,WAAW,GAAG,IAAI,CAAC5C,OAAO,CAAC1C,MAAM,EAAEoF,WAAW,CAAC;QAC/C,IAAI,CAAC/F,WAAW,CAAC4B,GAAG,CAACoE,cAAc,EAAEC,WAAW,CAAC;;MAGnD;MACA,IAAI;QACF,IAAI,CAAC1F,uBAAuB,GAAG3B,GAAG,EAAE,CAACsH,OAAO,CAAC,2BAA2B,CAAC;OAC1E,CAAC,OAAOC,CAAC,EAAE;QACV,IAAI,CAAC5F,uBAAuB,GAAG,KAAK;QACpC6F,OAAO,CAACC,IAAI,CAACF,CAAC,CAACG,OAAO,CAAC;;MAEzB,IAAMC,cAAc,GAAmB,EAAE;MACzC,IAAMC,aAAa,GAAkB,EAAE;MAEvC,OAAO1H,IAAI,CAAC,YAAK;QACf,IAAM2H,OAAO,GAAG,IAAInH,gBAAgB,CAChC4F,MAAI,CAACvD,SAAS,EAAE4E,cAAc,EAAEC,aAAa,EAC7CtB,MAAI,CAACxD,mBAAmB,EAAEwD,MAAI,CAAChF,kBAAkB,CAAC;QACtD,IAAMuE,UAAU,GAAAxD,MAAA,CAAAyF,MAAA,KAAwBxB,MAAI,CAACvD,SAAS,CAAC;QACvD,IAAIuD,MAAI,CAAC3E,uBAAuB,EAAE;UAChC2E,MAAI,CAACyB,gBAAgB,GAAGzB,MAAI,CAACV,cAAc,CAACU,MAAI,CAACvD,SAAS,CAAC;;QAG7DV,MAAM,CAACC,IAAI,CAACP,MAAM,CAAC,CAACQ,OAAO,CAAC,UAAAC,IAAI,EAAG;UACjC,IAAAwF,cAAA,GAA0BxH,aAAa,CAACgC,IAAI,EAAEqF,OAAO,CAAC;YAAAI,eAAA,GAAA9B,cAAA,CAAA6B,cAAA;YAA/CE,QAAQ,GAAAD,eAAA;YAAEE,KAAK,GAAAF,eAAA;UACtB,IAAMxC,OAAO,GAAa,EAAE;UAC5BA,OAAO,CAAC0C,KAAK,CAAC,GAAGpG,MAAM,CAACS,IAAI,CAAC;UAC7BqD,UAAU,CAACqC,QAAQ,CAAC,GAAGzC,OAAO;UAC9B,IAAIa,MAAI,CAAC3E,uBAAuB,EAAE;YAChC2E,MAAI,CAACyB,gBAAgB,CAACG,QAAQ,CAAC,GAAG5B,MAAI,CAACd,eAAe,CAACC,OAAO,CAAC;;QAEnE,CAAC,CAAC;QAEF,IAAM2C,aAAa,GAAG9B,MAAI,CAAC+B,kBAAkB,CAACxC,UAAU,CAAC;QACzD,IAAAyC,YAAA,GAAyCjB,WAAW;UAA7CjC,YAAY,GAAAkD,YAAA,CAAZlD,YAAY;UAAEC,gBAAgB,GAAAiD,YAAA,CAAhBjD,gBAAgB;QAAgB,IAAAkD,SAAA,GAAAC,0BAAA,CAClCpD,YAAY;UAAAqD,KAAA;QAAA;UAA/B,KAAAF,SAAA,CAAAG,CAAA,MAAAD,KAAA,GAAAF,SAAA,CAAArD,CAAA,IAAAyD,IAAA,GAAiC;YAAA,IAAtBjF,IAAI,GAAA+E,KAAA,CAAA5E,KAAA;YACb,IAAIgC,UAAU,CAACnC,IAAI,CAAClB,IAAI,CAAC,EAAE;cACzB;;YAEF,IAAMiD,OAAO,GACThF,SAAS,CAACiD,IAAI,EAAEmC,UAAU,EAAEgC,OAAO,EAAEvB,MAAI,CAAC7C,gBAAgB,CAClD;YACZ,IAAItD,IAAI,CAACyI,SAAS,CAACnD,OAAO,CAAC,EAAE;cAC3B,MAAM,IAAIX,KAAK,CACX,4BAAAzB,MAAA,CAA4BK,IAAI,CAACqB,EAAE,wEACO,CAAC;;YAEjDc,UAAU,CAACnC,IAAI,CAAClB,IAAI,CAAC,GAAGiD,OAAO;YAC/B,IAAIa,MAAI,CAAC3E,uBAAuB,EAAE;cAChC2E,MAAI,CAACyB,gBAAgB,CAACrE,IAAI,CAAClB,IAAI,CAAC,GAAG8D,MAAI,CAACd,eAAe,CAACC,OAAO,CAAC;;YAElEa,MAAI,CAACuC,2CAA2C,CAC5CnF,IAAI,EAAEmC,UAAU,EAAEgC,OAAO,EAAEO,aAAa,EAAEnB,iBAAiB,EAC3D5B,gBAAgB,CAAC1C,GAAG,CAACe,IAAI,CAAClB,IAAI,CAAC,CAAC;;UAGtC;QAAA,SAAAsG,GAAA;UAAAP,SAAA,CAAAhB,CAAA,CAAAuB,GAAA;QAAA;UAAAP,SAAA,CAAAQ,CAAA;QAAA;QACA,IAAIzC,MAAI,CAACrF,MAAM,IAAI,IAAI,EAAE;UACvB4G,OAAO,CAACmB,OAAO,CAACZ,aAAa,CAAC;;QAGhC,OAAOvG,OAAO,CAACqB,GAAG,CAAC,UAAAV,IAAI;UAAA,OAAIlC,SAAS,CAACkC,IAAI,EAAEqD,UAAU,EAAEgC,OAAO,CAAC;QAAA,EAAC;MAClE,CAAC,CAAC;IACJ;EAAC;IAAAnF,GAAA;IAAAmB,KAAA,EAEO,SAAAwE,mBAAmBY,SAA0B;MACnD,IAAMC,GAAG,GAAG,EAAE,CAAC7F,MAAM,CAACC,KAAK,CACvB,EAAE,EACFjB,MAAM,CAACC,IAAI,CAAC2G,SAAS,CAAC,CACjB/F,GAAG,CAAC,UAAAR,GAAG;QAAA,OAAIuG,SAAS,CAACvG,GAAG,CAAC;MAAA,EAAC,CAC1BQ,GAAG,CAAC,UAAAuC,OAAO;QAAA,OAAIA,OAAO,CAACvC,GAAG,CAAC,UAAAC,MAAM;UAAA,OAAIA,MAAM,CAACC,EAAE;QAAA,EAAC;MAAA,EAAC,CAAC;MAC1D,OAAO,IAAI8D,GAAG,CAACgC,GAAG,CAAC;IACrB;EAAC;IAAAxG,GAAA;IAAAmB,KAAA,EAEO,SAAAsF,uBACJjB,QAAgB,EAAExE,IAAU,EAAEuF,SAA0B,EACxDpB,OAAyB,EAAEO,aAA0B,EACrDnB,iBAA8B,EAC9BmC,+BAAwD;MAC1D;MACA;MACA,IAAItI,aAAa,CAAC4C,IAAI,CAAC,IAAIuD,iBAAiB,CAACoC,GAAG,CAACnB,QAAQ,CAAC,EAAE;QAC1D;;MACD,IAAAoB,UAAA,GAAAd,0BAAA,CAEoBS,SAAS,CAACf,QAAQ,CAAC;QAAAqB,MAAA;MAAA;QAAxC,KAAAD,UAAA,CAAAZ,CAAA,MAAAa,MAAA,GAAAD,UAAA,CAAApE,CAAA,IAAAyD,IAAA,GAA0C;UAAA,IAA/BxF,MAAM,GAAAoG,MAAA,CAAA1F,KAAA;UACf,IAAIV,MAAM,IAAI,IAAI,EAAE;YAClB;;UAEFiG,+BAA+B,CAACjG,MAAM,CAACC,EAAE,CAAC,GACtC,CAACgG,+BAA+B,CAACjG,MAAM,CAACC,EAAE,CAAC,IAAI,CAAC,IAChDM,IAAI,CAAC8F,QAAQ,CAACxE,MAAM;;MACzB,SAAA8D,GAAA;QAAAQ,UAAA,CAAA/B,CAAA,CAAAuB,GAAA;MAAA;QAAAQ,UAAA,CAAAP,CAAA;MAAA;MAAA,IAAAU,UAAA,GAAAjB,0BAAA,CAEmB9E,IAAI,CAAC3B,MAAM;QAAA2H,MAAA;MAAA;QAA/B,KAAAD,UAAA,CAAAf,CAAA,MAAAgB,MAAA,GAAAD,UAAA,CAAAvE,CAAA,IAAAyD,IAAA,GAAiC;UAAA,IAAtBgB,KAAK,GAAAD,MAAA,CAAA7F,KAAA;UACd;UACA;UACA,IAAI/C,aAAa,CAAC6I,KAAK,CAAC,EAAE;YACxB;;UAGF,IAAMlE,OAAO,GACTlF,2BAA2B,CAACoJ,KAAK,CAACnH,IAAI,EAAEyG,SAAS,EAAEpB,OAAO,CAAC;UAC/D,IAAIpC,OAAO,IAAI,IAAI,EAAE;YACnB;;UACD,IAAAmE,UAAA,GAAApB,0BAAA,CAEoB/C,OAAO;YAAAoE,MAAA;UAAA;YAA5B,KAAAD,UAAA,CAAAlB,CAAA,MAAAmB,MAAA,GAAAD,UAAA,CAAA1E,CAAA,IAAAyD,IAAA,GAA8B;cAAA,IAAnBxF,OAAM,GAAA0G,MAAA,CAAAhG,KAAA;cACf,IAAI,CAACV,OAAM,IAAIA,OAAM,CAAC2G,IAAI,IAAI1B,aAAa,CAACiB,GAAG,CAAClG,OAAM,CAACC,EAAE,CAAC,EAAE;gBAC1D;;cAGF;cACA;cACA;cACA;cACA,IAAM2G,KAAK,GAAGX,+BAA+B,CAACjG,OAAM,CAACC,EAAE,CAAC;cACxD,IAAI2G,KAAK,KAAK,CAAC,EAAE;gBACf5G,OAAM,CAAC6F,OAAO,EAAE;gBAChB,OAAOI,+BAA+B,CAACjG,OAAM,CAACC,EAAE,CAAC;eAClD,MAAM,IAAI2G,KAAK,IAAI,IAAI,EAAE;gBACxBX,+BAA+B,CAACjG,OAAM,CAACC,EAAE,CAAC,EAAE;;;UAE/C,SAAA0F,GAAA;YAAAc,UAAA,CAAArC,CAAA,CAAAuB,GAAA;UAAA;YAAAc,UAAA,CAAAb,CAAA;UAAA;;MACF,SAAAD,GAAA;QAAAW,UAAA,CAAAlC,CAAA,CAAAuB,GAAA;MAAA;QAAAW,UAAA,CAAAV,CAAA;MAAA;IACH;EAAC;IAAArG,GAAA;IAAAmB,KAAA,EAEO,SAAAgF,4CACJnF,IAAU,EAAEuF,SAA0B,EAAEpB,OAAyB,EACjEO,aAA0B,EAAEnB,iBAA8B,EAC1D+C,cAAuB;MACzB,SAASC,mBAAmBA,CAACvG,IAAU;QACrC;QACA;QACA,OAAO5C,aAAa,CAAC4C,IAAI,CAAC,IAAIuD,iBAAiB,CAACoC,GAAG,CAAC3F,IAAI,CAAClB,IAAI,CAAC;MAChE;MAEA,IAAI1B,aAAa,CAAC4C,IAAI,CAAC,IAAIsG,cAAc,IAAI,IAAI,EAAE;QACjD;;MACD,IAAAE,UAAA,GAAA1B,0BAAA,CAE2BwB,cAAc;QAAAG,MAAA;MAAA;QAA1C,KAAAD,UAAA,CAAAxB,CAAA,MAAAyB,MAAA,GAAAD,UAAA,CAAAhF,CAAA,IAAAyD,IAAA,GAA4C;UAAA,IAAjCyB,aAAa,GAAAD,MAAA,CAAAtG,KAAA;UACtB,IAAIoG,mBAAmB,CAACG,aAAa,CAAC,EAAE;YACtC;;UAEF,IAAM3E,OAAO,GAAGlF,2BAA2B,CACvC6J,aAAa,CAAC5H,IAAI,EAAEyG,SAAS,EAAEpB,OAAO,CAAC;UAAC,IAAAwC,UAAA,GAAA7B,0BAAA,CACvB/C,OAAO;YAAA6E,MAAA;UAAA;YAA5B,KAAAD,UAAA,CAAA3B,CAAA,MAAA4B,MAAA,GAAAD,UAAA,CAAAnF,CAAA,IAAAyD,IAAA,GAA8B;cAAA,IAAnBxF,MAAM,GAAAmH,MAAA,CAAAzG,KAAA;cACf,IAAI,CAACV,MAAM,IAAIA,MAAM,CAAC2G,IAAI,IAAI1B,aAAa,CAACiB,GAAG,CAAClG,MAAM,CAACC,EAAE,CAAC,EAAE;gBAC1D;;cAEFD,MAAM,CAAC6F,OAAO,EAAE;;UACjB,SAAAF,GAAA;YAAAuB,UAAA,CAAA9C,CAAA,CAAAuB,GAAA;UAAA;YAAAuB,UAAA,CAAAtB,CAAA;UAAA;;MACF,SAAAD,GAAA;QAAAoB,UAAA,CAAA3C,CAAA,CAAAuB,GAAA;MAAA;QAAAoB,UAAA,CAAAnB,CAAA;MAAA;IACH;IAEA;;;;;;;;;EAAA;IAAArG,GAAA;IAAAmB,KAAA;MAAA,IAAA0G,cAAA,GAAAC,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CASA,SAAAC,QAAmB5I,MAAsB,EAAEF,OAAkB;QAAA,OAAA4I,mBAAA,GAAAG,IAAA,UAAAC,SAAAC,QAAA;UAAA,kBAAAA,QAAA,CAAAC,IAAA,GAAAD,QAAA,CAAAE,IAAA;YAAA;cAAA,OAAAF,QAAA,CAAAG,MAAA,WAEpD,IAAI,CAACC,aAAa,CAACnJ,MAAM,EAAEF,OAAO,CAAC;YAAA;YAAA;cAAA,OAAAiJ,QAAA,CAAAK,IAAA;UAAA;QAAA,GAAAR,OAAA;MAAA,CAC3C;MAAA,SAAAS,aAAAC,EAAA,EAAAC,GAAA;QAAA,OAAAf,cAAA,CAAAjH,KAAA,OAAAiI,SAAA;MAAA;MAAA,OAAAH,YAAA;IAAA;EAAA;IAAA1I,GAAA;IAAAmB,KAAA,EAED,SAAA0C,2BAAA,EAA0B;MACxB,IAAI,CAAC,IAAI,CAACwB,gBAAgB,EAAE;QAC1B;;MAEF1F,MAAM,CAACmJ,MAAM,CAAC,IAAI,CAACzD,gBAAgB,CAAC,CAACxF,OAAO,CAAC,UAAA6D,WAAW,EAAG;QAAA,IAAAqF,UAAA,GAAAjD,0BAAA,CACpCpC,WAAW;UAAAsF,MAAA;QAAA;UAAhC,KAAAD,UAAA,CAAA/C,CAAA,MAAAgD,MAAA,GAAAD,UAAA,CAAAvG,CAAA,IAAAyD,IAAA,GAAkC;YAAA,IAAvBxF,MAAM,GAAAuI,MAAA,CAAA7H,KAAA;YACf,IAAIV,MAAM,IAAI,CAACA,MAAM,CAACwI,UAAU,EAAE;cAChCxI,MAAM,CAAC6F,OAAO,EAAE;;;QAEnB,SAAAF,GAAA;UAAA2C,UAAA,CAAAlE,CAAA,CAAAuB,GAAA;QAAA;UAAA2C,UAAA,CAAA1C,CAAA;QAAA;MACH,CAAC,CAAC;MAEF,IAAI,CAAChB,gBAAgB,GAAG,IAAI;IAC9B;EAAC;IAAArF,GAAA;IAAAmB,KAAA,EAED,SAAA+H,uBAAA,EAAsB;MACpB,OAAO,IAAI,CAAC7D,gBAAgB;IAC9B;IAEA;;;;;;;;;;;;;;EAAA;IAAArF,GAAA;IAAAmB,KAAA;MAAA,IAAAgI,cAAA,GAAArB,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CAcQ,SAAAoB,SACJ/J,MAAsB,EAAEF,OAAkB;QAAA,IAAAkK,mBAAA;UAAApE,cAAA;UAAAC,aAAA;UAAAC,OAAA;UAAAhC,UAAA;UAAAmG,OAAA;UAAAC,SAAA;UAAAC,QAAA;UAAAC,OAAA;UAAAC,MAAA,GAAAb,SAAA;QAAA,OAAAd,mBAAA,GAAAG,IAAA,UAAAyB,UAAAC,SAAA;UAAA,kBAAAA,SAAA,CAAAvB,IAAA,GAAAuB,SAAA,CAAAtB,IAAA;YAAA;cAAEe,mBAAmB,GAAAK,MAAA,CAAApH,MAAA,QAAAoH,MAAA,QAAAtI,SAAA,GAAAsI,MAAA,MAAG,KAAK;cACvEzE,cAAA,GAAAyE,MAAA,CAAApH,MAAA,QAAAoH,MAAA,QAAAtI,SAAA,GAAAsI,MAAA,MAAiC,EAAE;cACnCxE,aAAA,GAAAwE,MAAA,CAAApH,MAAA,QAAAoH,MAAA,QAAAtI,SAAA,GAAAsI,MAAA,MAA+B,EAAE;cACnC;cACA,IAAI,CAAC7F,0BAA0B,EAAE;cACjC,IAAI,CAACwF,mBAAmB,EAAE;gBACxBhK,MAAM,GAAG,IAAI,CAACyE,SAAS,CAACzE,MAAM,CAAC;gBAC/B,IAAI,CAAC2E,WAAW,CAAC3E,MAAM,CAAC;gBACxB,IAAI,CAAC4E,sBAAsB,CAAC5E,MAAM,CAAC;gBACnCF,OAAO,GAAG,IAAI,CAAC+E,UAAU,CAAC/E,OAAO,CAAC;gBAClC,IAAI,CAACgF,YAAY,CAAChF,OAAO,CAAC;;cAG5B;cACA,IAAI;gBACF,IAAI,CAACF,uBAAuB,GAAG3B,GAAG,EAAE,CAACsH,OAAO,CAAC,2BAA2B,CAAC;eAC1E,CAAC,OAAOC,CAAC,EAAE;gBACV,IAAI,CAAC5F,uBAAuB,GAAG,KAAK;gBACpC6F,OAAO,CAACC,IAAI,CAACF,CAAC,CAACG,OAAO,CAAC;;cAGnBG,OAAO,GAAG,IAAInH,gBAAgB,CAChC,IAAI,CAACqC,SAAS,EAAE4E,cAAc,EAAEC,aAAa,EAAE,IAAI,CAAC9E,mBAAmB,EACvE,IAAI,CAACxB,kBAAkB,CAAC;cAE5B,IAAI,IAAI,CAACK,uBAAuB,EAAE;gBAChC,IAAI,CAACoG,gBAAgB,GAAG,IAAI,CAACnC,cAAc,CAAC,IAAI,CAAC7C,SAAS,CAAC;;cAG7D;cACA;cACA;cAAAuJ,SAAA,CAAAtB,IAAA;cAAA,OACyB,IAAI,CAACuB,sBAAsB,CAChDxK,MAAM,EAAE8F,OAAO,EAAEhG,OAAO,EAAEkK,mBAAmB,CAAC;YAAA;cAD5ClG,UAAU,GAAAyG,SAAA,CAAAE,IAAA;cAEVR,OAAO,GAAGnK,OAAO,CAACqB,GAAG,CAAC,UAAAV,IAAI;gBAAA,OAAIlC,SAAS,CAACkC,IAAI,EAAEqD,UAAU,EAAEgC,OAAO,CAAC;cAAA,EAAC,EAEzE;cACMoE,SAAS,GAAGD,OAAO,CAAC9I,GAAG,CAAC,UAAAuJ,CAAC;gBAAA,OAAIA,CAAC,CAACrJ,EAAE;cAAA,EAAC;cAClC8I,QAAQ,GAAG7J,MAAM,CAACC,IAAI,CAACP,MAAM,CAAC,CAACmB,GAAG,CAAC,UAAAV,IAAI;gBAAA,OAAIT,MAAM,CAACS,IAAI,CAAC,CAACY,EAAE;cAAA,EAAC;cAC3D+I,OAAO,GACT,IAAIjF,GAAG,IAAA7D,MAAA,CAAAE,kBAAA,CAAa0I,SAAS,GAAA1I,kBAAA,CAAK2I,QAAQ,GAAA3I,kBAAA,CAAK,IAAI,CAACX,SAAS,GAAE;cAEnEP,MAAM,CAACmJ,MAAM,CAAC3F,UAAU,CAAC,CAACtD,OAAO,CAAC,UAAA6D,WAAW,EAAG;gBAC9CA,WAAW,CAAC7D,OAAO,CAAC,UAAAY,MAAM,EAAG;kBAC3B,IAAIA,MAAM,IAAI,CAACA,MAAM,CAACwI,UAAU,IAAI,CAACQ,OAAO,CAAC9C,GAAG,CAAClG,MAAM,CAACC,EAAE,CAAC,EAAE;oBAC3DD,MAAM,CAAC6F,OAAO,EAAE;;gBAEpB,CAAC,CAAC;cACJ,CAAC,CAAC;cAEF;cACA,IAAI,IAAI,CAAC/H,MAAM,IAAI,IAAI,EAAE;gBACvB4G,OAAO,CAACmB,OAAO,CAACmD,OAAO,CAAC;;cACzB,OAAAG,SAAA,CAAArB,MAAA,WAEMe,OAAO;YAAA;YAAA;cAAA,OAAAM,SAAA,CAAAnB,IAAA;UAAA;QAAA,GAAAW,QAAA;MAAA,CACf;MAAA,SAAAZ,cAAAwB,GAAA,EAAAC,GAAA;QAAA,OAAAd,cAAA,CAAAvI,KAAA,OAAAiI,SAAA;MAAA;MAAA,OAAAL,aAAA;IAAA;EAAA;IAAAxI,GAAA;IAAAmB,KAAA;MAAA,IAAA+I,qBAAA,GAAApC,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CAED,SAAAmC,SACI9K,MAAgB,EAAE4F,cAA8B,EAChDC,aAA4B;QAAA,IAAAkF,MAAA;QAAA,IAAAC,YAAA;QAAA,OAAAtC,mBAAA,GAAAG,IAAA,UAAAoC,UAAAC,SAAA;UAAA,kBAAAA,SAAA,CAAAlC,IAAA,GAAAkC,SAAA,CAAAjC,IAAA;YAAA;cACxB+B,YAAY,GAAGhL,MAAM,CAACoC,MAAM,CAAC,UAACjB,GAAG,EAAEC,MAAM,EAAEgF,KAAK,EAAI;gBACxDjF,GAAG,CAAC4J,MAAI,CAAC/K,MAAM,CAACoG,KAAK,CAAC,CAAC3F,IAAI,CAAC,GAAGW,MAAM;gBACrC,OAAOD,GAAG;cACZ,CAAC,EAAE,EAAoB,CAAC;cAAA,OAAA+J,SAAA,CAAAhC,MAAA,WAEjB,IAAI,CAACC,aAAa,CACrB6B,YAAY,EAAE,IAAI,CAAC5F,WAAW,EAAE,IAAI,EAAEQ,cAAc,EAAEC,aAAa,CAAC;YAAA;YAAA;cAAA,OAAAqF,SAAA,CAAA9B,IAAA;UAAA;QAAA,GAAA0B,QAAA;MAAA,CACzE;MAAA,SAAAK,qBAAAC,GAAA,EAAAC,GAAA,EAAAC,GAAA;QAAA,OAAAT,qBAAA,CAAAtJ,KAAA,OAAAiI,SAAA;MAAA;MAAA,OAAA2B,oBAAA;IAAA;IAED;;;;;;;;;;;EAAA;IAAAxK,GAAA;IAAAmB,KAAA;MAAA,IAAAyJ,uBAAA,GAAA9C,iBAAA,eAAAC,mBAAA,GAAAC,IAAA,CAWQ,SAAA6C,SACJxL,MAAsB,EAAE8F,OAAyB,EAAE2F,WAAsB,EACzEzB,mBAA6B;QAAA,IAAA0B,MAAA;QAAA,IAAAhH,KAAA,EAAAK,UAAA,EAAAE,eAAA,EAAAC,iBAAA,EAAAE,WAAA,EAAAuG,qBAAA,EAAAC,SAAA,EAAAhJ,aAAA,EAAAC,WAAA,EAAAC,UAAA,EAAA+I,KAAA,EAAA/H,UAAA,EAAAuD,+BAAA,EAAAhB,aAAA,EAAAyF,KAAA,EAAAC,QAAA,EAAAC,cAAA,EAAAC,cAAA;QAAA,OAAAvD,mBAAA,GAAAG,IAAA,UAAAqD,UAAAC,SAAA;UAAA,kBAAAA,SAAA,CAAAnD,IAAA,GAAAmD,SAAA,CAAAlD,IAAA;YAAA;cACzBvE,KAAK,GAAGpE,MAAM,CAACC,IAAI,CAACP,MAAM,CAAC;cAC3B+E,UAAU,GACZL,KAAK,CAACvD,GAAG,CAAC,UAAAV,IAAI;gBAAA,OAAIiL,MAAI,CAACzM,KAAK,CAAC+F,KAAK,CAACvG,aAAa,CAACgC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC;cAAA,EAAC;cACzDwE,eAAe,GAAGwG,WAAW,CAACtK,GAAG,CAAC,UAAAV,IAAI;gBAAA,OAAIhC,aAAa,CAACgC,IAAI,CAAC,CAAC,CAAC,CAAC;cAAA,EAAC;cACjEyE,iBAAiB,GAAG,IAAIC,GAAG,CAACF,eAAe,CAAC;cAC9CG,WAAW,GAAGH,eAAe,CAAC9D,GAAG,CAAC,UAAAV,IAAI;gBAAA,OAAIiL,MAAI,CAACzM,KAAK,CAAC+F,KAAK,CAACvE,IAAI,CAAC;cAAA,EAAC,EAErE;cACA,IAAI2E,WAAW,CAACnC,MAAM,KAAK,CAAC,EAAE;gBAC5BmC,WAAW,GAAG,IAAI,CAACvF,QAAQ;;cAC5B8L,qBAAA,GAGG/M,oBAAoB,CAChBoB,MAAM,EAAEoF,WAAW,EAAE,IAAI,CAACpE,SAAS,EAAE,IAAI,CAACf,UAAU,CAAC,EAFtD2L,SAAS,GAAAD,qBAAA,CAATC,SAAS,EAAEhJ,aAAa,GAAA+I,qBAAA,CAAb/I,aAAa,EAAEC,WAAW,GAAA8I,qBAAA,CAAX9I,WAAW,EAAEC,UAAU,GAAA6I,qBAAA,CAAV7I,UAAU,EAIxD;cACM+I,KAAK,GAAuB,GAAAvK,MAAA,CAAAE,kBAAA,CAC7BuD,UAAU,GAAAvD,kBAAA,CAAK,IAAI,CAACvC,KAAK,CAACmN,OAAO,GAAA5K,kBAAA,CAAM,IAAI,CAACvB,UAAU,IAAI,EAAE,GAC/DkB,GAAG,CAAC,UAAAQ,IAAI,EAAG;gBACX,OAAO;kBAACA,IAAI,EAAJA,IAAI;kBAAE0K,QAAQ,EAAEvG,OAAO,CAACwG;gBAAc,CAAC;cACjD,CAAC,CAAC;cACIxI,UAAU,GAAAxD,MAAA,CAAAyF,MAAA,KAAwB,IAAI,CAAC/E,SAAS,CAAC;cACvDV,MAAM,CAACC,IAAI,CAACP,MAAM,CAAC,CAACQ,OAAO,CAAC,UAAAC,IAAI,EAAG;gBACjC,IAAA8L,eAAA,GAA0B9N,aAAa,CAACgC,IAAI,CAAC;kBAAA+L,eAAA,GAAApI,cAAA,CAAAmI,eAAA;kBAAtCpG,QAAQ,GAAAqG,eAAA;kBAAEpG,KAAK,GAAAoG,eAAA;gBACtB,IAAM9I,OAAO,GAAa,EAAE;gBAC5BA,OAAO,CAAC0C,KAAK,CAAC,GAAGpG,MAAM,CAACS,IAAI,CAAC;gBAC7BqD,UAAU,CAACqC,QAAQ,CAAC,GAAGzC,OAAO;cAChC,CAAC,CAAC;cACI2D,+BAA+B,GAA4B,EAAE;cAC7DhB,aAAa,GAAG,IAAI,CAACC,kBAAkB,CAACxC,UAAU,CAAC;cACnDgI,KAAK,GAA6B,EAAE;YAAA;cAAA,MACnCD,KAAK,CAAC5I,MAAM,GAAG,CAAC;gBAAAkJ,SAAA,CAAAlD,IAAA;gBAAA;cAAA;cACf8C,QAAQ,GAAG,IAAI,CAACU,YAAY,CAC9B1H,UAAU,EAAE8G,KAAK,EAAE/F,OAAO,EAAEhC,UAAU,EAAEgI,KAAK,EAAEzF,aAAa,EAC5DnB,iBAAiB,EAAEmC,+BAA+B,EAAEuE,SAAS,CAAC;cAAAO,SAAA,CAAAlD,IAAA;cAAA,OAC5DyD,OAAO,CAACC,GAAG,CAACZ,QAAQ,CAAC;YAAA;cAAAI,SAAA,CAAAlD,IAAA;cAAA;YAAA;cAE7B,IAAIpG,WAAW,IAAI,IAAI,IAAI,CAACmH,mBAAmB,EAAE;gBAC/CvE,OAAO,CAACC,IAAI,CACR,sIACgE,CAAC;;cAEjEsG,cAAc,GAChB5G,WAAW,CACNwH,MAAM,CACH,UAAAjL,IAAI;gBAAA,OAAI,CAAC5C,aAAa,CAAC4C,IAAI,CAAC,IACxB,CAACpD,SAAS,CAACoD,IAAI,CAAClB,IAAI,EAAEqD,UAAU,EAAEgC,OAAO,CAAC;cAAA,EAAC,CAClD3E,GAAG,CAAC,UAAAQ,IAAI;gBAAA,OAAIA,IAAI,CAAClB,IAAI;cAAA,EAAC;cAAA,MAC3BuL,cAAc,CAAC/I,MAAM,GAAG,CAAC;gBAAAkJ,SAAA,CAAAlD,IAAA;gBAAA;cAAA;cACvBgD,cAAc,GAAG,EAAE;cACvB,IAAIpJ,WAAW,IAAI,IAAI,EAAE;gBACvBoJ,cAAc,GACV,6FAAA3K,MAAA,CAC2BwB,UAAU,MAAG;;cAC7C,MACK,IAAIC,KAAK,CACX,+BAAAzB,MAAA,CAA+B0K,cAAc,uCAAA1K,MAAA,CAClCoD,KAAK,iDAA8C,OAAApD,MAAA,CAC1DsB,aAAa,SAAAtB,MAAA,CAAM2K,cAAc,CAAE,CAAC;YAAA;cAAA,OAAAE,SAAA,CAAAjD,MAAA,WAEvCpF,UAAU;YAAA;YAAA;cAAA,OAAAqI,SAAA,CAAA/C,IAAA;UAAA;QAAA,GAAAoC,QAAA;MAAA,CAClB;MAAA,SAAAhB,uBAAAqC,GAAA,EAAAC,GAAA,EAAAC,IAAA,EAAAC,IAAA;QAAA,OAAAzB,uBAAA,CAAAhK,KAAA,OAAAiI,SAAA;MAAA;MAAA,OAAAgB,sBAAA;IAAA;EAAA;IAAA7J,GAAA;IAAAmB,KAAA,EAEO,SAAA2K,aACJ1H,UAAkB,EAAE8G,KAAyB,EAAE/F,OAAyB,EACxEoB,SAA0B,EAAE4E,KAA+B,EAC3DzF,aAA0B,EAAEnB,iBAA8B,EAC1DmC,+BAAwD,EACxDuE,SAAsB;MAAA,IAAAqB,MAAA;MACxB,IAAMlB,QAAQ,GAA6B,EAAE;MAAC,IAAAmB,KAAA,YAAAA,MAAA,EACrB;QACvB,IAAMC,IAAI,GAAGtB,KAAK,CAACuB,GAAG,EAAE;QACxBtH,OAAO,CAACwG,cAAc,GAAGa,IAAI,CAACd,QAAQ;QACtC,IAAIlG,QAAQ,GAAG,EAAE;QACjB;QACA;QACA;QACA,IAAIgH,IAAI,CAACxL,IAAI,CAACqB,EAAE,KAAK,OAAO,IACxB1E,aAAa,CAAC,YAAY,EAAE6O,IAAI,CAACxL,IAAI,EAAEuF,SAAS,EAAEpB,OAAO,CAAC,EAAE;UAAA,IAAAuH,oBAAA,GACjDhP,mBAAmB,CAAC8O,IAAI,CAACxL,IAAI,CAAClB,IAAI,EAAEqF,OAAO,CAAC;UAAA,IAAAwH,qBAAA,GAAAlJ,cAAA,CAAAiJ,oBAAA;UAAxDlH,QAAQ,GAAAmH,qBAAA;;QAGX;QACA;QACA,IAAIpG,SAAS,CAACiG,IAAI,CAACxL,IAAI,CAAClB,IAAI,CAAC,IAAI,IAAI,EAAE;UACrC,IAAMiD,OAAO,GACThF,SAAS,CAACyO,IAAI,CAACxL,IAAI,EAAEuF,SAAS,EAAEpB,OAAO,EAAEmH,MAAI,CAACvL,gBAAgB,CAAC;UACnE,IAAI,CAACyE,QAAQ,EAAE;YAAA,IAAAoH,qBAAA,GACAlP,mBAAmB,CAAC8O,IAAI,CAACxL,IAAI,CAAClB,IAAI,EAAEqF,OAAO,CAAC;YAAA,IAAA0H,qBAAA,GAAApJ,cAAA,CAAAmJ,qBAAA;YAAxDpH,QAAQ,GAAAqH,qBAAA;;UAEX,IAAMlB,cAAc,GAAGxG,OAAO,CAACwG,cAAc;UAC7C,IAAIlO,IAAI,CAACyI,SAAS,CAACnD,OAAO,CAAC,EAAE;YAC3BqI,QAAQ,CAAC0B,IAAI,CAAC/J,OAAO,CAACgK,IAAI,CAAC,UAAAhD,CAAC,EAAG;cAC7BxD,SAAS,CAACf,QAAQ,CAAC,GAAGuE,CAAC;cACvB,IAAIuC,MAAI,CAACrN,uBAAuB,EAAE;gBAChCqN,MAAI,CAACjH,gBAAgB,CAACG,QAAQ,CAAC,GAAG8G,MAAI,CAACxJ,eAAe,CAACiH,CAAC,CAAC;;cAE3D5E,OAAO,CAACwG,cAAc,GAAGA,cAAc;cACvCW,MAAI,CAAC7F,sBAAsB,CACvBjB,QAAQ,EAAEgH,IAAI,CAACxL,IAAI,EAAEuF,SAAS,EAAEpB,OAAO,EAAEO,aAAa,EACtDnB,iBAAiB,EAAEmC,+BAA+B,CAAC;cACvD4F,MAAI,CAACU,iBAAiB,CAClBR,IAAI,CAACxL,IAAI,EAAEkK,KAAK,EAAE/F,OAAO,EAAEoB,SAAS,EAAE4E,KAAK,EAAEF,SAAS,CAAC;cAC3D,OAAOlB,CAAC;YACV,CAAC,CAAC,CAAC;WACJ,MAAM;YACLxD,SAAS,CAACf,QAAQ,CAAC,GAAGzC,OAAO;YAC7B,IAAIuJ,MAAI,CAACrN,uBAAuB,EAAE;cAChCqN,MAAI,CAACjH,gBAAgB,CAACG,QAAQ,CAAC,GAAG8G,MAAI,CAACxJ,eAAe,CAACC,OAAO,CAAC;;YAEjEuJ,MAAI,CAAC7F,sBAAsB,CACvBjB,QAAQ,EAAEgH,IAAI,CAACxL,IAAI,EAAEuF,SAAS,EAAEpB,OAAO,EAAEO,aAAa,EACtDnB,iBAAiB,EAAEmC,+BAA+B,CAAC;YACvD4F,MAAI,CAACU,iBAAiB,CAClBR,IAAI,CAACxL,IAAI,EAAEkK,KAAK,EAAE/F,OAAO,EAAEoB,SAAS,EAAE4E,KAAK,EAAEF,SAAS,CAAC;;SAE9D,MAAM;UACLqB,MAAI,CAACU,iBAAiB,CAClBR,IAAI,CAACxL,IAAI,EAAEkK,KAAK,EAAE/F,OAAO,EAAEoB,SAAS,EAAE4E,KAAK,EAAEF,SAAS,CAAC;;OAE9D;MAlDD,OAAOC,KAAK,CAAC5I,MAAM,GAAG,CAAC;QAAAiK,KAAA;MAAA;MAmDvB,OAAOnB,QAAQ;IACjB;EAAC;IAAApL,GAAA;IAAAmB,KAAA,EAEO,SAAA6L,kBACJhM,IAAU,EAAEkK,KAAyB,EAAE/F,OAAyB,EAChEoB,SAA0B,EAAE4E,KAA+B,EAC3DF,SAAsB;MACxBjK,IAAI,CAAC8F,QAAQ,CAACjH,OAAO,CAAC,UAACoN,SAAS,EAAI;QAClC,IAAAC,qBAAA,GAAqBxP,mBAAmB,CAACuP,SAAS,CAACnN,IAAI,EAAEqF,OAAO,CAAC;UAAAgI,qBAAA,GAAA1J,cAAA,CAAAyJ,qBAAA;UAA1D1H,QAAQ,GAAA2H,qBAAA;QACf,IAAIhC,KAAK,CAAC3F,QAAQ,CAAC,IAAI,CAACyF,SAAS,CAACtE,GAAG,CAACsG,SAAS,CAACnN,IAAI,CAAC,EAAE;UACrD;;QAEF;QACA,IAAImN,SAAS,CAAC5K,EAAE,KAAK,OAAO,EAAE;UAC5B,IAAI4K,SAAS,CAACG,UAAU,CAACC,IAAI,CAAC,UAAAvN,IAAI,EAAG;YAC/B,OAAO,CAAC,CAAClC,SAAS,CAACkC,IAAI,EAAEyG,SAAS,EAAEpB,OAAO,CAAC;UAC9C,CAAC,CAAC,EAAE;YACNgG,KAAK,CAAC3F,QAAQ,CAAC,GAAG,IAAI;YACtB0F,KAAK,CAAC4B,IAAI,CAAC;cAACpB,QAAQ,EAAEvG,OAAO,CAACwG,cAAc;cAAE3K,IAAI,EAAEiM;YAAS,CAAC,CAAC;;SAElE;UAAO;UACJ,IAAIA,SAAS,CAACG,UAAU,CAACE,KAAK,CAAC,UAAAxN,IAAI,EAAG;YAChC,OAAO,CAAC,CAAClC,SAAS,CAACkC,IAAI,EAAEyG,SAAS,EAAEpB,OAAO,CAAC;UAC9C,CAAC,CAAC,EAAE;YACVgG,KAAK,CAAC3F,QAAQ,CAAC,GAAG,IAAI;YACtB0F,KAAK,CAAC4B,IAAI,CAAC;cAACpB,QAAQ,EAAEvG,OAAO,CAACwG,cAAc;cAAE3K,IAAI,EAAEiM;YAAS,CAAC,CAAC;;MAEnE,CAAC,CAAC;IACJ;IAEA;;;EAAA;IAAAjN,GAAA;IAAAmB,KAAA,EAGA,SAAAmF,QAAA,EAAO;MAAA,IAAAiH,MAAA;MACL5N,MAAM,CAACC,IAAI,CAAC,IAAI,CAACS,SAAS,CAAC,CACtBR,OAAO,CACJ,UAAAG,GAAG;QAAA,OAAIuN,MAAI,CAAClN,SAAS,CAACL,GAAG,CAAC,CAACH,OAAO,CAAC,UAAAY,MAAM;UAAA,OAAIA,MAAM,CAAC6F,OAAO,EAAE;QAAA,EAAC;MAAA,EAAC;IACzE;EAAC;IAAAtG,GAAA;IAAAmB,KAAA,EAEO,SAAA8C,uBAAuB5E,MAAsB;MAAA,IAAAmO,OAAA;MACnD7N,MAAM,CAACC,IAAI,CAACP,MAAM,CAAC,CAACQ,OAAO,CAAC,UAAAC,IAAI,EAAG;QACjC,IAAMmH,KAAK,GAAG5H,MAAM,CAACS,IAAI,CAAC;QAC1B,IAAA2N,eAAA,GAAqB3P,aAAa,CAACgC,IAAI,CAAC;UAAA4N,eAAA,GAAAjK,cAAA,CAAAgK,eAAA;UAAjCjI,QAAQ,GAAAkI,eAAA;QACf,IAAM1M,IAAI,GAAGwM,OAAI,CAAClP,KAAK,CAAC+F,KAAK,CAACmB,QAAQ,CAAC;QACvC,IAAIxE,IAAI,CAACE,UAAU,CAAC,OAAO,CAAC,IAAIF,IAAI,CAACE,UAAU,CAAC,OAAO,CAAC,CAACC,KAAK,EAAE;UAC9D,IAAMF,KAAK,GAAGD,IAAI,CAACE,UAAU,CAAC,OAAO,CAAC,CAACC,KAAiB;UACxD,IAAMwM,KAAK,GAAG1M,KAAK,CAACqB,MAAM,KAAK2E,KAAK,CAAChG,KAAK,CAACqB,MAAM,IAC7C2E,KAAK,CAAChG,KAAK,CAACqM,KAAK,CACb,UAACM,GAAG,EAAEnI,KAAK;YAAA,OAAKxE,KAAK,CAACwE,KAAK,CAAC,KAAK,CAAC,CAAC,IAAIxE,KAAK,CAACwE,KAAK,CAAC,KAAKmI,GAAG;UAAA,EAAC;UACpEnQ,IAAI,CAACoQ,MAAM,CACPF,KAAK,EACL;YAAA,OAAM,sBAAAhN,MAAA,CAAsBK,IAAI,CAAClB,IAAI,uDAAAa,MAAA,CACDM,KAAK,gBAAa,OAAAN,MAAA,CAC9CsG,KAAK,CAAChG,KAAK,MAAG;UAAA,EAAC;;QAE7B,IAAID,IAAI,CAACE,UAAU,CAAC,OAAO,CAAC,IAAIF,IAAI,CAACE,UAAU,CAAC,OAAO,CAAC,CAACC,KAAK,EAAE;UAC9D1D,IAAI,CAACoQ,MAAM,CACP5G,KAAK,CAAC5F,KAAK,KAAKL,IAAI,CAACE,UAAU,CAAC,OAAO,CAAC,CAACC,KAAe,EACxD;YAAA,OAAM,sBAAAR,MAAA,CAAsBK,IAAI,CAAClB,IAAI,qDACH,MAAAa,MAAA,CAC3BK,IAAI,CAACE,UAAU,CAAC,OAAO,CAAC,CAACC,KAAK,gBAAAR,MAAA,CAAasG,KAAK,CAAC5F,KAAK,CAAE;UAAA,EAAC;;MAExE,CAAC,CAAC;IACJ;EAAC;IAAArB,GAAA;IAAAmB,KAAA,EAEO,SAAA2C,UAAUzE,MAAsB;;MACtC,IAAMyO,MAAM,GAAmB,EAAE;MACjC,KAAK,IAAMC,SAAS,IAAI1O,MAAM,EAAE;QAC9B,IAAMoB,MAAM,GAAG,CAAAuN,EAAA,IAAAC,EAAA,OAAI,CAACzO,UAAU,cAAAyO,EAAA,uBAAAA,EAAA,CAAG5O,MAAM,cAAA2O,EAAA,uBAAAA,EAAA,CAAID,SAAS,CAAC;QACrD,IAAItN,MAAM,IAAI,IAAI,EAAE;UAClBqN,MAAM,CAACrN,MAAM,CAACX,IAAI,CAAC,GAAGT,MAAM,CAAC0O,SAAS,CAAC;SACxC,MAAM;UACLD,MAAM,CAACC,SAAS,CAAC,GAAG1O,MAAM,CAAC0O,SAAS,CAAC;;;MAGzC,OAAOD,MAAM;IACf;EAAC;IAAA9N,GAAA;IAAAmB,KAAA,EAEO,SAAA6C,YAAY3E,MAAsB;MAAA,IAAA6O,OAAA;MACxC,IAAMC,UAAU,GAAGxO,MAAM,CAACC,IAAI,CAACP,MAAM,CAAC,CAAC4M,MAAM,CAAC,UAAAnM,IAAI,EAAG;QACnD,IAAAsO,eAAA,GAAmBtQ,aAAa,CAACgC,IAAI,CAAC;UAAAuO,eAAA,GAAA5K,cAAA,CAAA2K,eAAA;UAA/B5I,QAAQ,GAAA6I,eAAA;QACf,OAAOH,OAAI,CAAC5P,KAAK,CAAC+F,KAAK,CAACmB,QAAQ,CAAC,IAAI,IAAI;MAC3C,CAAC,CAAC;MACF,IAAI2I,UAAU,CAAC7L,MAAM,GAAG,CAAC,EAAE;QACzB,MAAM,IAAIF,KAAK,CACX,4DAAAzB,MAAA,CACUwN,UAAU,iCAA8B,CAAC;;IAE3D;EAAC;IAAAnO,GAAA;IAAAmB,KAAA,EAEO,SAAA+C,WAAW/E,OAAiB;MAAA,IAAAmP,OAAA;MAClC,OAAOnP,OAAO,CAACqB,GAAG,CAAC,UAAAV,IAAI,EAAG;;QACxB,IAAMW,MAAM,GAAG,CAAAuN,EAAA,IAAAC,EAAA,GAAAK,OAAI,CAAC9O,UAAU,cAAAyO,EAAA,uBAAAA,EAAA,CAAG9O,OAAO,cAAA6O,EAAA,uBAAAA,EAAA,CAAIlO,IAAI,CAAC;QACjD,IAAIW,MAAM,IAAI,IAAI,EAAE;UAClB,OAAOA,MAAM,CAACX,IAAI;;QAEpB,OAAOA,IAAI;MACb,CAAC,EAAE,EAAE,CAAC;IACR;EAAC;IAAAE,GAAA;IAAAmB,KAAA,EAEO,SAAAgD,aAAahF,OAAiB;MAAA,IAAAoP,OAAA;MACpCpP,OAAO,CAACU,OAAO,CAAC,UAAAC,IAAI,EAAG;QACrB,IAAA0O,eAAA,GAAyB1Q,aAAa,CAACgC,IAAI,CAAC;UAAA2O,gBAAA,GAAAhL,cAAA,CAAA+K,eAAA;UAArCE,cAAc,GAAAD,gBAAA;QACrB,IAAI,CAACF,OAAI,CAACjQ,KAAK,CAAC+F,KAAK,CAACqK,cAAc,CAAC,EAAE;UACrC,MAAM,IAAItM,KAAK,gBAAAzB,MAAA,CAAgBb,IAAI,iCAA8B;;MAErE,CAAC,CAAC;IACJ;EAAC;EAAA,OAAAzB,aAAA;AAAA"},"metadata":{},"sourceType":"module","externalDependencies":[]}