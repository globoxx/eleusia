{"ast":null,"code":"import _toConsumableArray from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/toConsumableArray.js\";\n/**\r\n * @license\r\n * Copyright 2018 Google LLC. All Rights Reserved.\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n * http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n * =============================================================================\r\n */\nimport { ENGINE } from './engine';\nimport { Tensor, Variable } from './tensor';\nimport { convertToTensor, convertToTensorArray } from './tensor_util_env';\nimport * as util from './util';\n/**\r\n * Provided `f(x)`, returns another function `g(x, dy?)`, which gives the\r\n * gradient of `f(x)` with respect to `x`.\r\n *\r\n * If `dy` is provided, the gradient of `f(x).mul(dy).sum()` with respect to\r\n * `x` is computed instead. `f(x)` must take a single tensor `x` and return a\r\n * single tensor `y`. If `f()` takes multiple inputs, use `tf.grads` instead.\r\n *\r\n * ```js\r\n * // f(x) = x ^ 2\r\n * const f = x => x.square();\r\n * // f'(x) = 2x\r\n * const g = tf.grad(f);\r\n *\r\n * const x = tf.tensor1d([2, 3]);\r\n * g(x).print();\r\n * ```\r\n *\r\n * ```js\r\n * // f(x) = x ^ 3\r\n * const f = x => x.pow(tf.scalar(3, 'int32'));\r\n * // f'(x) = 3x ^ 2\r\n * const g = tf.grad(f);\r\n * // f''(x) = 6x\r\n * const gg = tf.grad(g);\r\n *\r\n * const x = tf.tensor1d([2, 3]);\r\n * gg(x).print();\r\n * ```\r\n *\r\n * @param f The function f(x), to compute gradient for.\r\n *\r\n * @doc {heading: 'Training', subheading: 'Gradients'}\r\n */\nfunction grad(f) {\n  util.assert(util.isFunction(f), function () {\n    return 'The f passed in grad(f) must be a function';\n  });\n  return function (x, dy) {\n    // x can be of any dtype, thus null as the last argument.\n    var $x = convertToTensor(x, 'x', 'tf.grad', 'string_or_numeric');\n    var $dy = dy != null ? convertToTensor(dy, 'dy', 'tf.grad') : null;\n    return ENGINE.tidy(function () {\n      var _ENGINE$gradients = ENGINE.gradients(function () {\n          return f($x);\n        }, [$x], $dy),\n        value = _ENGINE$gradients.value,\n        grads = _ENGINE$gradients.grads;\n      if ($dy != null) {\n        util.assertShapesMatch(value.shape, $dy.shape, 'The shape of dy passed in grad(f)(x, dy) must match the shape ' + 'returned by f(x)');\n      }\n      checkGrads(grads);\n      return grads[0];\n    });\n  };\n}\n/**\r\n * Provided `f(x1, x2,...)`, returns another function `g([x1, x2,...], dy?)`,\r\n * which gives an array of gradients of `f()` with respect to each input\r\n * [`x1`,`x2`,...].\r\n *\r\n * If `dy` is passed when calling `g()`, the gradient of\r\n * `f(x1,...).mul(dy).sum()` with respect to each input is computed instead.\r\n * The provided `f` must take one or more tensors and return a single tensor\r\n * `y`. If `f()` takes a single input, we recommend using `tf.grad` instead.\r\n *\r\n * ```js\r\n * // f(a, b) = a * b\r\n * const f = (a, b) => a.mul(b);\r\n * // df / da = b, df / db = a\r\n * const g = tf.grads(f);\r\n *\r\n * const a = tf.tensor1d([2, 3]);\r\n * const b = tf.tensor1d([-2, -3]);\r\n * const [da, db] = g([a, b]);\r\n * console.log('da');\r\n * da.print();\r\n * console.log('db');\r\n * db.print();\r\n * ```\r\n *\r\n * @param f The function `f(x1, x2,...)` to compute gradients for.\r\n *\r\n * @doc {heading: 'Training', subheading: 'Gradients'}\r\n */\nfunction grads(f) {\n  util.assert(util.isFunction(f), function () {\n    return 'The f passed in grads(f) must be a function';\n  });\n  return function (args, dy) {\n    util.assert(Array.isArray(args), function () {\n      return 'The args passed in grads(f)(args) must be an array ' + 'of `Tensor`s or `TensorLike`s';\n    });\n    // args can be of any dtype, thus null as the last argument.\n    var $args = convertToTensorArray(args, 'args', 'tf.grads', 'string_or_numeric');\n    var $dy = dy != null ? convertToTensor(dy, 'dy', 'tf.grads') : null;\n    return ENGINE.tidy(function () {\n      var _ENGINE$gradients2 = ENGINE.gradients(function () {\n          return f.apply(void 0, _toConsumableArray($args));\n        }, $args, $dy),\n        value = _ENGINE$gradients2.value,\n        grads = _ENGINE$gradients2.grads;\n      if ($dy != null) {\n        util.assertShapesMatch(value.shape, $dy.shape, 'The shape of dy passed in grads(f)([x1,...], dy) must ' + 'match the shape returned by f([x1,...])');\n      }\n      checkGrads(grads);\n      return grads;\n    });\n  };\n}\n/**\r\n * Like `tf.grad`, but also returns the value of `f()`. Useful when `f()`\r\n * returns a metric you want to show.\r\n *\r\n * The result is a rich object with the following properties:\r\n * - grad: The gradient of `f(x)` w.r.t. `x` (result of `tf.grad`).\r\n * - value: The value returned by `f(x)`.\r\n *\r\n * ```js\r\n * // f(x) = x ^ 2\r\n * const f = x => x.square();\r\n * // f'(x) = 2x\r\n * const g = tf.valueAndGrad(f);\r\n *\r\n * const x = tf.tensor1d([2, 3]);\r\n * const {value, grad} = g(x);\r\n *\r\n * console.log('value');\r\n * value.print();\r\n * console.log('grad');\r\n * grad.print();\r\n * ```\r\n *\r\n * @doc {heading: 'Training', subheading: 'Gradients'}\r\n */\nfunction valueAndGrad(f) {\n  util.assert(util.isFunction(f), function () {\n    return 'The f passed in valueAndGrad(f) must be a function';\n  });\n  return function (x, dy) {\n    util.assert(x instanceof Tensor, function () {\n      return 'The x passed in valueAndGrad(f)(x) must be a tensor';\n    });\n    util.assert(dy == null || dy instanceof Tensor, function () {\n      return 'The dy passed in valueAndGrad(f)(x, dy) must be a tensor';\n    });\n    var _ENGINE$gradients3 = ENGINE.gradients(function () {\n        return f(x);\n      }, [x], dy),\n      grads = _ENGINE$gradients3.grads,\n      value = _ENGINE$gradients3.value;\n    checkGrads(grads);\n    return {\n      grad: grads[0],\n      value: value\n    };\n  };\n}\n/**\r\n * Like `tf.grads`, but returns also the value of `f()`. Useful when `f()`\r\n * returns a metric you want to show.\r\n *\r\n * The result is a rich object with the following properties:\r\n * - grads: The gradients of `f()` w.r.t. each input (result of `tf.grads`).\r\n * - value: The value returned by `f(x)`.\r\n *\r\n * ```js\r\n * // f(a, b) = a * b\r\n * const f = (a, b) => a.mul(b);\r\n * // df/da = b, df/db = a\r\n * const g = tf.valueAndGrads(f);\r\n *\r\n * const a = tf.tensor1d([2, 3]);\r\n * const b = tf.tensor1d([-2, -3]);\r\n * const {value, grads} = g([a, b]);\r\n *\r\n * const [da, db] = grads;\r\n *\r\n * console.log('value');\r\n * value.print();\r\n *\r\n * console.log('da');\r\n * da.print();\r\n * console.log('db');\r\n * db.print();\r\n * ```\r\n *\r\n * @doc {heading: 'Training', subheading: 'Gradients'}\r\n */\nfunction valueAndGrads(f) {\n  util.assert(util.isFunction(f), function () {\n    return 'The f passed in valueAndGrads(f) must be a function';\n  });\n  return function (args, dy) {\n    util.assert(Array.isArray(args) && args.every(function (arg) {\n      return arg instanceof Tensor;\n    }), function () {\n      return 'The args passed in valueAndGrads(f)(args) must be array of ' + 'tensors';\n    });\n    util.assert(dy == null || dy instanceof Tensor, function () {\n      return 'The dy passed in valueAndGrads(f)(args, dy) must be a tensor';\n    });\n    var res = ENGINE.gradients(function () {\n      return f.apply(void 0, _toConsumableArray(args));\n    }, args, dy);\n    if (dy != null) {\n      util.assertShapesMatch(res.value.shape, dy.shape, 'The shape of dy passed in valueAndGrads(f)([x1,...], dy) must ' + 'match the shape returned by f([x1,...])');\n    }\n    checkGrads(res.grads);\n    return res;\n  };\n}\n/**\r\n * Computes and returns the gradient of f(x) with respect to the list of\r\n * trainable variables provided by `varList`. If no list is provided, it\r\n * defaults to all trainable variables.\r\n *\r\n * ```js\r\n * const a = tf.variable(tf.tensor1d([3, 4]));\r\n * const b = tf.variable(tf.tensor1d([5, 6]));\r\n * const x = tf.tensor1d([1, 2]);\r\n *\r\n * // f(a, b) = a * x ^ 2 + b * x\r\n * const f = () => a.mul(x.square()).add(b.mul(x)).sum();\r\n * // df/da = x ^ 2, df/db = x\r\n * const {value, grads} = tf.variableGrads(f);\r\n *\r\n * Object.keys(grads).forEach(varName => grads[varName].print());\r\n * ```\r\n *\r\n * @param f The function to execute. f() should return a scalar.\r\n * @param varList The list of variables to compute the gradients with respect\r\n *     to. Defaults to all trainable variables.\r\n * @returns An object with the following keys and values:\r\n *   - `value`: The value of the function `f`.\r\n *   - `grads`: A map from the names of the variables to the gradients.\r\n *     If the `varList` argument is provided explicitly and contains a subset of\r\n *     non-trainable variables, this map in the return value will contain keys\r\n *     that map the names of the non-trainable variables to `null`.\r\n *\r\n * @doc {heading: 'Training', subheading: 'Gradients'}\r\n */\nfunction variableGrads(f, varList) {\n  util.assert(util.isFunction(f), function () {\n    return 'The f passed in variableGrads(f) must be a function';\n  });\n  util.assert(varList == null || Array.isArray(varList) && varList.every(function (v) {\n    return v instanceof Variable;\n  }), function () {\n    return 'The varList passed in variableGrads(f, varList) must be an array ' + 'of variables';\n  });\n  var specifiedVarList = varList != null;\n  if (!specifiedVarList) {\n    // Get all of the trainable variables.\n    varList = [];\n    for (var varName in ENGINE.registeredVariables) {\n      varList.push(ENGINE.registeredVariables[varName]);\n    }\n  }\n  var specifiedNonTrainable = specifiedVarList ? varList.filter(function (variable) {\n    return !variable.trainable;\n  }) : null;\n  // Prune non-trainable variables.\n  var originalVarCount = varList.length;\n  varList = varList.filter(function (variable) {\n    return variable.trainable;\n  });\n  util.assert(varList.length > 0, function () {\n    return \"variableGrads() expects at least one of the input variables to \" + \"be trainable, but none of the \".concat(originalVarCount, \" variables is \") + \"trainable.\";\n  });\n  var allowNoGradients = true;\n  var _ENGINE$gradients4 = ENGINE.gradients(f, varList, null, allowNoGradients),\n    value = _ENGINE$gradients4.value,\n    grads = _ENGINE$gradients4.grads;\n  util.assert(grads.some(function (g) {\n    return g != null;\n  }), function () {\n    return 'Cannot find a connection between any variable and the result of ' + 'the loss function y=f(x). Please make sure the operations that ' + 'use variables are inside the function f passed to minimize().';\n  });\n  util.assert(value.rank === 0, function () {\n    return \"The f passed in variableGrads(f) must return a scalar, but it \" + \"returned a rank-\".concat(value.rank, \" tensor\");\n  });\n  var namedGrads = {};\n  varList.forEach(function (v, i) {\n    if (grads[i] != null) {\n      namedGrads[v.name] = grads[i];\n    }\n  });\n  if (specifiedNonTrainable != null) {\n    // If varList is explicitly provided and contains non-trainable values,\n    // add them to the returned gradients with `null` values.\n    specifiedNonTrainable.forEach(function (v) {\n      return namedGrads[v.name] = null;\n    });\n  }\n  return {\n    value: value,\n    grads: namedGrads\n  };\n}\n/**\r\n * Overrides the gradient computation of a function `f`.\r\n *\r\n * Takes a function\r\n * `f(...inputs, save) => {value: Tensor, gradFunc: (dy, saved) => Tensor[]}`\r\n * and returns another function `g(...inputs)` which takes the same inputs as\r\n * `f`. When called, `g` returns `f().value`. In backward mode, custom gradients\r\n * with respect to each input of `f` are computed using `f().gradFunc`.\r\n *\r\n * The `save` function passed to `f` should be used for saving tensors needed\r\n * in the gradient. And the `saved` passed to the `gradFunc` is a\r\n * `NamedTensorMap`, which contains those saved tensors.\r\n *\r\n * ```js\r\n * const customOp = tf.customGrad((x, save) => {\r\n *   // Save x to make sure it's available later for the gradient.\r\n *   save([x]);\r\n *   // Override gradient of our custom x ^ 2 op to be dy * abs(x);\r\n *   return {\r\n *     value: x.square(),\r\n *     // Note `saved.x` which points to the `x` we saved earlier.\r\n *     gradFunc: (dy, saved) => [dy.mul(saved[0].abs())]\r\n *   };\r\n * });\r\n *\r\n * const x = tf.tensor1d([-1, -2, 3]);\r\n * const dx = tf.grad(x => customOp(x));\r\n *\r\n * console.log(`f(x):`);\r\n * customOp(x).print();\r\n * console.log(`f'(x):`);\r\n * dx(x).print();\r\n * ```\r\n *\r\n * @param f The function to evaluate in forward mode, which should return\r\n *     `{value: Tensor, gradFunc: (dy, saved) => Tensor[]}`, where `gradFunc`\r\n *     returns the custom gradients of `f` with respect to its inputs.\r\n *\r\n * @doc {heading: 'Training', subheading: 'Gradients'}\r\n */\nfunction customGrad(f) {\n  return ENGINE.customGrad(f);\n}\nfunction checkGrads(grads) {\n  var numNullGradients = grads.filter(function (g) {\n    return g == null;\n  }).length;\n  if (numNullGradients > 0) {\n    throw new Error(\"Cannot compute gradient of y=f(x) with respect to x. Make sure that\\n    the f you passed encloses all operations that lead from x to y.\");\n  }\n}\nexport { customGrad, variableGrads, valueAndGrad, valueAndGrads, grad, grads };","map":{"version":3,"names":["ENGINE","Tensor","Variable","convertToTensor","convertToTensorArray","util","grad","f","assert","isFunction","x","dy","$x","$dy","tidy","_ENGINE$gradients","gradients","value","grads","assertShapesMatch","shape","checkGrads","args","Array","isArray","$args","_ENGINE$gradients2","apply","_toConsumableArray","valueAndGrad","_ENGINE$gradients3","valueAndGrads","every","arg","res","variableGrads","varList","v","specifiedVarList","varName","registeredVariables","push","specifiedNonTrainable","filter","variable","trainable","originalVarCount","length","concat","allowNoGradients","_ENGINE$gradients4","some","g","rank","namedGrads","forEach","i","name","customGrad","numNullGradients","Error"],"sources":["C:\\Users\\vince\\OneDrive\\Documents\\GitHub\\tfjs-core\\src\\gradients.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {CustomGradientFunc, ENGINE} from './engine';\nimport {Scalar, Tensor, Variable} from './tensor';\nimport {NamedTensorMap} from './tensor_types';\nimport {convertToTensor, convertToTensorArray} from './tensor_util_env';\nimport {TensorLike} from './types';\nimport * as util from './util';\n\n/**\n * Provided `f(x)`, returns another function `g(x, dy?)`, which gives the\n * gradient of `f(x)` with respect to `x`.\n *\n * If `dy` is provided, the gradient of `f(x).mul(dy).sum()` with respect to\n * `x` is computed instead. `f(x)` must take a single tensor `x` and return a\n * single tensor `y`. If `f()` takes multiple inputs, use `tf.grads` instead.\n *\n * ```js\n * // f(x) = x ^ 2\n * const f = x => x.square();\n * // f'(x) = 2x\n * const g = tf.grad(f);\n *\n * const x = tf.tensor1d([2, 3]);\n * g(x).print();\n * ```\n *\n * ```js\n * // f(x) = x ^ 3\n * const f = x => x.pow(tf.scalar(3, 'int32'));\n * // f'(x) = 3x ^ 2\n * const g = tf.grad(f);\n * // f''(x) = 6x\n * const gg = tf.grad(g);\n *\n * const x = tf.tensor1d([2, 3]);\n * gg(x).print();\n * ```\n *\n * @param f The function f(x), to compute gradient for.\n *\n * @doc {heading: 'Training', subheading: 'Gradients'}\n */\nfunction grad(f: (x: Tensor) => Tensor): (\n    x: TensorLike|Tensor, dy?: TensorLike|Tensor) => Tensor {\n  util.assert(\n      util.isFunction(f), () => 'The f passed in grad(f) must be a function');\n  return (x: TensorLike|Tensor, dy?: TensorLike|Tensor): Tensor => {\n    // x can be of any dtype, thus null as the last argument.\n    const $x = convertToTensor(x, 'x', 'tf.grad', 'string_or_numeric');\n    const $dy: Tensor =\n        (dy != null) ? convertToTensor(dy, 'dy', 'tf.grad') : null;\n    return ENGINE.tidy(() => {\n      const {value, grads} = ENGINE.gradients(() => f($x), [$x], $dy);\n      if ($dy != null) {\n        util.assertShapesMatch(\n            value.shape, $dy.shape,\n            'The shape of dy passed in grad(f)(x, dy) must match the shape ' +\n                'returned by f(x)');\n      }\n      checkGrads(grads);\n      return grads[0];\n    });\n  };\n}\n\n/**\n * Provided `f(x1, x2,...)`, returns another function `g([x1, x2,...], dy?)`,\n * which gives an array of gradients of `f()` with respect to each input\n * [`x1`,`x2`,...].\n *\n * If `dy` is passed when calling `g()`, the gradient of\n * `f(x1,...).mul(dy).sum()` with respect to each input is computed instead.\n * The provided `f` must take one or more tensors and return a single tensor\n * `y`. If `f()` takes a single input, we recommend using `tf.grad` instead.\n *\n * ```js\n * // f(a, b) = a * b\n * const f = (a, b) => a.mul(b);\n * // df / da = b, df / db = a\n * const g = tf.grads(f);\n *\n * const a = tf.tensor1d([2, 3]);\n * const b = tf.tensor1d([-2, -3]);\n * const [da, db] = g([a, b]);\n * console.log('da');\n * da.print();\n * console.log('db');\n * db.print();\n * ```\n *\n * @param f The function `f(x1, x2,...)` to compute gradients for.\n *\n * @doc {heading: 'Training', subheading: 'Gradients'}\n */\nfunction grads(f: (...args: Tensor[]) => Tensor): (\n    args: Array<Tensor|TensorLike>, dy?: Tensor|TensorLike) => Tensor[] {\n  util.assert(\n      util.isFunction(f), () => 'The f passed in grads(f) must be a function');\n  return (args: Array<Tensor|TensorLike>, dy?: Tensor|TensorLike): Tensor[] => {\n    util.assert(\n        Array.isArray(args),\n        () => 'The args passed in grads(f)(args) must be an array ' +\n            'of `Tensor`s or `TensorLike`s');\n    // args can be of any dtype, thus null as the last argument.\n    const $args =\n        convertToTensorArray(args, 'args', 'tf.grads', 'string_or_numeric');\n    const $dy: Tensor =\n        (dy != null) ? convertToTensor(dy, 'dy', 'tf.grads') : null;\n    return ENGINE.tidy(() => {\n      const {value, grads} = ENGINE.gradients(() => f(...$args), $args, $dy);\n      if ($dy != null) {\n        util.assertShapesMatch(\n            value.shape, $dy.shape,\n            'The shape of dy passed in grads(f)([x1,...], dy) must ' +\n                'match the shape returned by f([x1,...])');\n      }\n      checkGrads(grads);\n      return grads;\n    });\n  };\n}\n\n/**\n * Like `tf.grad`, but also returns the value of `f()`. Useful when `f()`\n * returns a metric you want to show.\n *\n * The result is a rich object with the following properties:\n * - grad: The gradient of `f(x)` w.r.t. `x` (result of `tf.grad`).\n * - value: The value returned by `f(x)`.\n *\n * ```js\n * // f(x) = x ^ 2\n * const f = x => x.square();\n * // f'(x) = 2x\n * const g = tf.valueAndGrad(f);\n *\n * const x = tf.tensor1d([2, 3]);\n * const {value, grad} = g(x);\n *\n * console.log('value');\n * value.print();\n * console.log('grad');\n * grad.print();\n * ```\n *\n * @doc {heading: 'Training', subheading: 'Gradients'}\n */\nfunction valueAndGrad<I extends Tensor, O extends Tensor>(f: (x: I) => O): (\n    x: I, dy?: O) => {\n  value: O;\n  grad: I;\n} {\n  util.assert(\n      util.isFunction(f),\n      () => 'The f passed in valueAndGrad(f) must be a function');\n  return (x: I, dy?: O) => {\n    util.assert(\n        x instanceof Tensor,\n        () => 'The x passed in valueAndGrad(f)(x) must be a tensor');\n    util.assert(\n        dy == null || dy instanceof Tensor,\n        () => 'The dy passed in valueAndGrad(f)(x, dy) must be a tensor');\n    const {grads, value} = ENGINE.gradients(() => f(x), [x], dy);\n    checkGrads(grads);\n    return {grad: grads[0] as I, value};\n  };\n}\n\n/**\n * Like `tf.grads`, but returns also the value of `f()`. Useful when `f()`\n * returns a metric you want to show.\n *\n * The result is a rich object with the following properties:\n * - grads: The gradients of `f()` w.r.t. each input (result of `tf.grads`).\n * - value: The value returned by `f(x)`.\n *\n * ```js\n * // f(a, b) = a * b\n * const f = (a, b) => a.mul(b);\n * // df/da = b, df/db = a\n * const g = tf.valueAndGrads(f);\n *\n * const a = tf.tensor1d([2, 3]);\n * const b = tf.tensor1d([-2, -3]);\n * const {value, grads} = g([a, b]);\n *\n * const [da, db] = grads;\n *\n * console.log('value');\n * value.print();\n *\n * console.log('da');\n * da.print();\n * console.log('db');\n * db.print();\n * ```\n *\n * @doc {heading: 'Training', subheading: 'Gradients'}\n */\nfunction valueAndGrads<O extends Tensor>(f: (...args: Tensor[]) => O): (\n    args: Tensor[], dy?: O) => {\n  grads: Tensor[];\n  value: O;\n} {\n  util.assert(\n      util.isFunction(f),\n      () => 'The f passed in valueAndGrads(f) must be a function');\n  return (args: Tensor[], dy?: O) => {\n    util.assert(\n        Array.isArray(args) && args.every(arg => arg instanceof Tensor),\n        () => 'The args passed in valueAndGrads(f)(args) must be array of ' +\n            'tensors');\n    util.assert(\n        dy == null || dy instanceof Tensor,\n        () => 'The dy passed in valueAndGrads(f)(args, dy) must be a tensor');\n    const res = ENGINE.gradients(() => f(...args), args, dy);\n    if (dy != null) {\n      util.assertShapesMatch(\n          res.value.shape, dy.shape,\n          'The shape of dy passed in valueAndGrads(f)([x1,...], dy) must ' +\n              'match the shape returned by f([x1,...])');\n    }\n    checkGrads(res.grads);\n    return res;\n  };\n}\n\n/**\n * Computes and returns the gradient of f(x) with respect to the list of\n * trainable variables provided by `varList`. If no list is provided, it\n * defaults to all trainable variables.\n *\n * ```js\n * const a = tf.variable(tf.tensor1d([3, 4]));\n * const b = tf.variable(tf.tensor1d([5, 6]));\n * const x = tf.tensor1d([1, 2]);\n *\n * // f(a, b) = a * x ^ 2 + b * x\n * const f = () => a.mul(x.square()).add(b.mul(x)).sum();\n * // df/da = x ^ 2, df/db = x\n * const {value, grads} = tf.variableGrads(f);\n *\n * Object.keys(grads).forEach(varName => grads[varName].print());\n * ```\n *\n * @param f The function to execute. f() should return a scalar.\n * @param varList The list of variables to compute the gradients with respect\n *     to. Defaults to all trainable variables.\n * @returns An object with the following keys and values:\n *   - `value`: The value of the function `f`.\n *   - `grads`: A map from the names of the variables to the gradients.\n *     If the `varList` argument is provided explicitly and contains a subset of\n *     non-trainable variables, this map in the return value will contain keys\n *     that map the names of the non-trainable variables to `null`.\n *\n * @doc {heading: 'Training', subheading: 'Gradients'}\n */\nfunction variableGrads(f: () => Scalar, varList?: Variable[]):\n    {value: Scalar, grads: NamedTensorMap} {\n  util.assert(\n      util.isFunction(f),\n      () => 'The f passed in variableGrads(f) must be a function');\n  util.assert(\n      varList == null ||\n          Array.isArray(varList) && varList.every(v => v instanceof Variable),\n      () =>\n          'The varList passed in variableGrads(f, varList) must be an array ' +\n          'of variables');\n\n  const specifiedVarList = varList != null;\n  if (!specifiedVarList) {\n    // Get all of the trainable variables.\n    varList = [];\n    for (const varName in ENGINE.registeredVariables) {\n      varList.push(ENGINE.registeredVariables[varName]);\n    }\n  }\n\n  const specifiedNonTrainable: Variable[] =\n      specifiedVarList ? varList.filter(variable => !variable.trainable) : null;\n\n  // Prune non-trainable variables.\n  const originalVarCount = varList.length;\n  varList = varList.filter(variable => variable.trainable);\n  util.assert(\n      varList.length > 0,\n      () => `variableGrads() expects at least one of the input variables to ` +\n          `be trainable, but none of the ${originalVarCount} variables is ` +\n          `trainable.`);\n\n  const allowNoGradients = true;\n  const {value, grads} = ENGINE.gradients(f, varList, null, allowNoGradients);\n\n  util.assert(\n      grads.some(g => g != null),\n      () => 'Cannot find a connection between any variable and the result of ' +\n          'the loss function y=f(x). Please make sure the operations that ' +\n          'use variables are inside the function f passed to minimize().');\n  util.assert(\n      value.rank === 0,\n      () => `The f passed in variableGrads(f) must return a scalar, but it ` +\n          `returned a rank-${value.rank} tensor`);\n\n  const namedGrads: NamedTensorMap = {};\n  varList.forEach((v, i) => {\n    if (grads[i] != null) {\n      namedGrads[v.name] = grads[i];\n    }\n  });\n  if (specifiedNonTrainable != null) {\n    // If varList is explicitly provided and contains non-trainable values,\n    // add them to the returned gradients with `null` values.\n    specifiedNonTrainable.forEach(v => namedGrads[v.name] = null);\n  }\n  return {value, grads: namedGrads};\n}\n\n/**\n * Overrides the gradient computation of a function `f`.\n *\n * Takes a function\n * `f(...inputs, save) => {value: Tensor, gradFunc: (dy, saved) => Tensor[]}`\n * and returns another function `g(...inputs)` which takes the same inputs as\n * `f`. When called, `g` returns `f().value`. In backward mode, custom gradients\n * with respect to each input of `f` are computed using `f().gradFunc`.\n *\n * The `save` function passed to `f` should be used for saving tensors needed\n * in the gradient. And the `saved` passed to the `gradFunc` is a\n * `NamedTensorMap`, which contains those saved tensors.\n *\n * ```js\n * const customOp = tf.customGrad((x, save) => {\n *   // Save x to make sure it's available later for the gradient.\n *   save([x]);\n *   // Override gradient of our custom x ^ 2 op to be dy * abs(x);\n *   return {\n *     value: x.square(),\n *     // Note `saved.x` which points to the `x` we saved earlier.\n *     gradFunc: (dy, saved) => [dy.mul(saved[0].abs())]\n *   };\n * });\n *\n * const x = tf.tensor1d([-1, -2, 3]);\n * const dx = tf.grad(x => customOp(x));\n *\n * console.log(`f(x):`);\n * customOp(x).print();\n * console.log(`f'(x):`);\n * dx(x).print();\n * ```\n *\n * @param f The function to evaluate in forward mode, which should return\n *     `{value: Tensor, gradFunc: (dy, saved) => Tensor[]}`, where `gradFunc`\n *     returns the custom gradients of `f` with respect to its inputs.\n *\n * @doc {heading: 'Training', subheading: 'Gradients'}\n */\nfunction customGrad<T extends Tensor>(f: CustomGradientFunc<T>):\n    (...args: Tensor[]) => T {\n  return ENGINE.customGrad(f);\n}\n\nfunction checkGrads(grads: Tensor[]) {\n  const numNullGradients = grads.filter(g => g == null).length;\n  if (numNullGradients > 0) {\n    throw new Error(\n        `Cannot compute gradient of y=f(x) with respect to x. Make sure that\n    the f you passed encloses all operations that lead from x to y.`);\n  }\n}\n\nexport {\n  customGrad,\n  variableGrads,\n  valueAndGrad,\n  valueAndGrads,\n  grad,\n  grads,\n};\n"],"mappings":";AAAA;;;;;;;;;;;;;;;;AAiBA,SAA4BA,MAAM,QAAO,UAAU;AACnD,SAAgBC,MAAM,EAAEC,QAAQ,QAAO,UAAU;AAEjD,SAAQC,eAAe,EAAEC,oBAAoB,QAAO,mBAAmB;AAEvE,OAAO,KAAKC,IAAI,MAAM,QAAQ;AAE9B;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAkCA,SAASC,IAAIA,CAACC,CAAwB;EAEpCF,IAAI,CAACG,MAAM,CACPH,IAAI,CAACI,UAAU,CAACF,CAAC,CAAC,EAAE;IAAA,OAAM,4CAA4C;EAAA,EAAC;EAC3E,OAAO,UAACG,CAAoB,EAAEC,EAAsB,EAAY;IAC9D;IACA,IAAMC,EAAE,GAAGT,eAAe,CAACO,CAAC,EAAE,GAAG,EAAE,SAAS,EAAE,mBAAmB,CAAC;IAClE,IAAMG,GAAG,GACJF,EAAE,IAAI,IAAI,GAAIR,eAAe,CAACQ,EAAE,EAAE,IAAI,EAAE,SAAS,CAAC,GAAG,IAAI;IAC9D,OAAOX,MAAM,CAACc,IAAI,CAAC,YAAK;MACtB,IAAAC,iBAAA,GAAuBf,MAAM,CAACgB,SAAS,CAAC;UAAA,OAAMT,CAAC,CAACK,EAAE,CAAC;QAAA,GAAE,CAACA,EAAE,CAAC,EAAEC,GAAG,CAAC;QAAxDI,KAAK,GAAAF,iBAAA,CAALE,KAAK;QAAEC,KAAK,GAAAH,iBAAA,CAALG,KAAK;MACnB,IAAIL,GAAG,IAAI,IAAI,EAAE;QACfR,IAAI,CAACc,iBAAiB,CAClBF,KAAK,CAACG,KAAK,EAAEP,GAAG,CAACO,KAAK,EACtB,gEAAgE,GAC5D,kBAAkB,CAAC;;MAE7BC,UAAU,CAACH,KAAK,CAAC;MACjB,OAAOA,KAAK,CAAC,CAAC,CAAC;IACjB,CAAC,CAAC;EACJ,CAAC;AACH;AAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA6BA,SAASA,KAAKA,CAACX,CAAgC;EAE7CF,IAAI,CAACG,MAAM,CACPH,IAAI,CAACI,UAAU,CAACF,CAAC,CAAC,EAAE;IAAA,OAAM,6CAA6C;EAAA,EAAC;EAC5E,OAAO,UAACe,IAA8B,EAAEX,EAAsB,EAAc;IAC1EN,IAAI,CAACG,MAAM,CACPe,KAAK,CAACC,OAAO,CAACF,IAAI,CAAC,EACnB;MAAA,OAAM,qDAAqD,GACvD,+BAA+B;IAAA,EAAC;IACxC;IACA,IAAMG,KAAK,GACPrB,oBAAoB,CAACkB,IAAI,EAAE,MAAM,EAAE,UAAU,EAAE,mBAAmB,CAAC;IACvE,IAAMT,GAAG,GACJF,EAAE,IAAI,IAAI,GAAIR,eAAe,CAACQ,EAAE,EAAE,IAAI,EAAE,UAAU,CAAC,GAAG,IAAI;IAC/D,OAAOX,MAAM,CAACc,IAAI,CAAC,YAAK;MACtB,IAAAY,kBAAA,GAAuB1B,MAAM,CAACgB,SAAS,CAAC;UAAA,OAAMT,CAAC,CAAAoB,KAAA,SAAAC,kBAAA,CAAIH,KAAK,EAAC;QAAA,GAAEA,KAAK,EAAEZ,GAAG,CAAC;QAA/DI,KAAK,GAAAS,kBAAA,CAALT,KAAK;QAAEC,KAAK,GAAAQ,kBAAA,CAALR,KAAK;MACnB,IAAIL,GAAG,IAAI,IAAI,EAAE;QACfR,IAAI,CAACc,iBAAiB,CAClBF,KAAK,CAACG,KAAK,EAAEP,GAAG,CAACO,KAAK,EACtB,wDAAwD,GACpD,yCAAyC,CAAC;;MAEpDC,UAAU,CAACH,KAAK,CAAC;MACjB,OAAOA,KAAK;IACd,CAAC,CAAC;EACJ,CAAC;AACH;AAEA;;;;;;;;;;;;;;;;;;;;;;;;;AAyBA,SAASW,YAAYA,CAAqCtB,CAAc;EAKtEF,IAAI,CAACG,MAAM,CACPH,IAAI,CAACI,UAAU,CAACF,CAAC,CAAC,EAClB;IAAA,OAAM,oDAAoD;EAAA,EAAC;EAC/D,OAAO,UAACG,CAAI,EAAEC,EAAM,EAAI;IACtBN,IAAI,CAACG,MAAM,CACPE,CAAC,YAAYT,MAAM,EACnB;MAAA,OAAM,qDAAqD;IAAA,EAAC;IAChEI,IAAI,CAACG,MAAM,CACPG,EAAE,IAAI,IAAI,IAAIA,EAAE,YAAYV,MAAM,EAClC;MAAA,OAAM,0DAA0D;IAAA,EAAC;IACrE,IAAA6B,kBAAA,GAAuB9B,MAAM,CAACgB,SAAS,CAAC;QAAA,OAAMT,CAAC,CAACG,CAAC,CAAC;MAAA,GAAE,CAACA,CAAC,CAAC,EAAEC,EAAE,CAAC;MAArDO,KAAK,GAAAY,kBAAA,CAALZ,KAAK;MAAED,KAAK,GAAAa,kBAAA,CAALb,KAAK;IACnBI,UAAU,CAACH,KAAK,CAAC;IACjB,OAAO;MAACZ,IAAI,EAAEY,KAAK,CAAC,CAAC,CAAM;MAAED,KAAK,EAALA;IAAK,CAAC;EACrC,CAAC;AACH;AAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA+BA,SAASc,aAAaA,CAAmBxB,CAA2B;EAKlEF,IAAI,CAACG,MAAM,CACPH,IAAI,CAACI,UAAU,CAACF,CAAC,CAAC,EAClB;IAAA,OAAM,qDAAqD;EAAA,EAAC;EAChE,OAAO,UAACe,IAAc,EAAEX,EAAM,EAAI;IAChCN,IAAI,CAACG,MAAM,CACPe,KAAK,CAACC,OAAO,CAACF,IAAI,CAAC,IAAIA,IAAI,CAACU,KAAK,CAAC,UAAAC,GAAG;MAAA,OAAIA,GAAG,YAAYhC,MAAM;IAAA,EAAC,EAC/D;MAAA,OAAM,6DAA6D,GAC/D,SAAS;IAAA,EAAC;IAClBI,IAAI,CAACG,MAAM,CACPG,EAAE,IAAI,IAAI,IAAIA,EAAE,YAAYV,MAAM,EAClC;MAAA,OAAM,8DAA8D;IAAA,EAAC;IACzE,IAAMiC,GAAG,GAAGlC,MAAM,CAACgB,SAAS,CAAC;MAAA,OAAMT,CAAC,CAAAoB,KAAA,SAAAC,kBAAA,CAAIN,IAAI,EAAC;IAAA,GAAEA,IAAI,EAAEX,EAAE,CAAC;IACxD,IAAIA,EAAE,IAAI,IAAI,EAAE;MACdN,IAAI,CAACc,iBAAiB,CAClBe,GAAG,CAACjB,KAAK,CAACG,KAAK,EAAET,EAAE,CAACS,KAAK,EACzB,gEAAgE,GAC5D,yCAAyC,CAAC;;IAEpDC,UAAU,CAACa,GAAG,CAAChB,KAAK,CAAC;IACrB,OAAOgB,GAAG;EACZ,CAAC;AACH;AAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA8BA,SAASC,aAAaA,CAAC5B,CAAe,EAAE6B,OAAoB;EAE1D/B,IAAI,CAACG,MAAM,CACPH,IAAI,CAACI,UAAU,CAACF,CAAC,CAAC,EAClB;IAAA,OAAM,qDAAqD;EAAA,EAAC;EAChEF,IAAI,CAACG,MAAM,CACP4B,OAAO,IAAI,IAAI,IACXb,KAAK,CAACC,OAAO,CAACY,OAAO,CAAC,IAAIA,OAAO,CAACJ,KAAK,CAAC,UAAAK,CAAC;IAAA,OAAIA,CAAC,YAAYnC,QAAQ;EAAA,EAAC,EACvE;IAAA,OACI,mEAAmE,GACnE,cAAc;EAAA,EAAC;EAEvB,IAAMoC,gBAAgB,GAAGF,OAAO,IAAI,IAAI;EACxC,IAAI,CAACE,gBAAgB,EAAE;IACrB;IACAF,OAAO,GAAG,EAAE;IACZ,KAAK,IAAMG,OAAO,IAAIvC,MAAM,CAACwC,mBAAmB,EAAE;MAChDJ,OAAO,CAACK,IAAI,CAACzC,MAAM,CAACwC,mBAAmB,CAACD,OAAO,CAAC,CAAC;;;EAIrD,IAAMG,qBAAqB,GACvBJ,gBAAgB,GAAGF,OAAO,CAACO,MAAM,CAAC,UAAAC,QAAQ;IAAA,OAAI,CAACA,QAAQ,CAACC,SAAS;EAAA,EAAC,GAAG,IAAI;EAE7E;EACA,IAAMC,gBAAgB,GAAGV,OAAO,CAACW,MAAM;EACvCX,OAAO,GAAGA,OAAO,CAACO,MAAM,CAAC,UAAAC,QAAQ;IAAA,OAAIA,QAAQ,CAACC,SAAS;EAAA,EAAC;EACxDxC,IAAI,CAACG,MAAM,CACP4B,OAAO,CAACW,MAAM,GAAG,CAAC,EAClB;IAAA,OAAM,qGAAAC,MAAA,CAC+BF,gBAAgB,mBAAgB,eACrD;EAAA,EAAC;EAErB,IAAMG,gBAAgB,GAAG,IAAI;EAC7B,IAAAC,kBAAA,GAAuBlD,MAAM,CAACgB,SAAS,CAACT,CAAC,EAAE6B,OAAO,EAAE,IAAI,EAAEa,gBAAgB,CAAC;IAApEhC,KAAK,GAAAiC,kBAAA,CAALjC,KAAK;IAAEC,KAAK,GAAAgC,kBAAA,CAALhC,KAAK;EAEnBb,IAAI,CAACG,MAAM,CACPU,KAAK,CAACiC,IAAI,CAAC,UAAAC,CAAC;IAAA,OAAIA,CAAC,IAAI,IAAI;EAAA,EAAC,EAC1B;IAAA,OAAM,kEAAkE,GACpE,iEAAiE,GACjE,+DAA+D;EAAA,EAAC;EACxE/C,IAAI,CAACG,MAAM,CACPS,KAAK,CAACoC,IAAI,KAAK,CAAC,EAChB;IAAA,OAAM,sFAAAL,MAAA,CACiB/B,KAAK,CAACoC,IAAI,YAAS;EAAA,EAAC;EAE/C,IAAMC,UAAU,GAAmB,EAAE;EACrClB,OAAO,CAACmB,OAAO,CAAC,UAAClB,CAAC,EAAEmB,CAAC,EAAI;IACvB,IAAItC,KAAK,CAACsC,CAAC,CAAC,IAAI,IAAI,EAAE;MACpBF,UAAU,CAACjB,CAAC,CAACoB,IAAI,CAAC,GAAGvC,KAAK,CAACsC,CAAC,CAAC;;EAEjC,CAAC,CAAC;EACF,IAAId,qBAAqB,IAAI,IAAI,EAAE;IACjC;IACA;IACAA,qBAAqB,CAACa,OAAO,CAAC,UAAAlB,CAAC;MAAA,OAAIiB,UAAU,CAACjB,CAAC,CAACoB,IAAI,CAAC,GAAG,IAAI;IAAA,EAAC;;EAE/D,OAAO;IAACxC,KAAK,EAALA,KAAK;IAAEC,KAAK,EAAEoC;EAAU,CAAC;AACnC;AAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAwCA,SAASI,UAAUA,CAAmBnD,CAAwB;EAE5D,OAAOP,MAAM,CAAC0D,UAAU,CAACnD,CAAC,CAAC;AAC7B;AAEA,SAASc,UAAUA,CAACH,KAAe;EACjC,IAAMyC,gBAAgB,GAAGzC,KAAK,CAACyB,MAAM,CAAC,UAAAS,CAAC;IAAA,OAAIA,CAAC,IAAI,IAAI;EAAA,EAAC,CAACL,MAAM;EAC5D,IAAIY,gBAAgB,GAAG,CAAC,EAAE;IACxB,MAAM,IAAIC,KAAK,4IAEkD;;AAErE;AAEA,SACEF,UAAU,EACVvB,aAAa,EACbN,YAAY,EACZE,aAAa,EACbzB,IAAI,EACJY,KAAK"},"metadata":{},"sourceType":"module","externalDependencies":[]}