{"ast":null,"code":"import _toConsumableArray from \"C:/Users/vince/OneDrive/Documents/GitHub/eleusia/node_modules/@babel/runtime/helpers/esm/toConsumableArray.js\";\n/**\r\n * @license\r\n * Copyright 2018 Google LLC\r\n *\r\n * Use of this source code is governed by an MIT-style\r\n * license that can be found in the LICENSE file or at\r\n * https://opensource.org/licenses/MIT.\r\n * =============================================================================\r\n */\n/**\r\n * deeplearn.js backend.\r\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { onesLike as coreOnesLike, scalar, tensor1d, tidy, where, zerosLike as coreZerosLike } from '@tensorflow/tfjs-core';\nimport { checkDataFormat } from '../common';\nimport { NotImplementedError, ValueError } from '../errors';\nimport * as math_utils from '../utils/math_utils';\nimport { imageDataFormat } from './common';\n// tslint:enable\n/* Setting and getting backend from deeplearn.js. */\n// Default deeplearn.js backend is WebGL (GPU).\nvar backend = 'webgl';\nexport function setBackend(requestedBackend) {\n  tfc.setBackend(requestedBackend);\n  backend = requestedBackend;\n}\nexport function getBackend() {\n  return backend;\n}\n/**\r\n * Indicates whether the backend is operating symbolically.\r\n *\r\n * This function will be used to determine how to interpret user code. If\r\n * it returns true, calls to the backend construct a symbolic graph; if\r\n * it returns false, calls to the backend execute immediately.\r\n */\nexport function isBackendSymbolic() {\n  return false;\n}\n/**\r\n * Get the number of elements in a Tensor.\r\n * @param x The Tensor.\r\n * @return Number of elements in `x`.\r\n */\nexport function countParams(x) {\n  var shape = x.shape;\n  if (shape.length > 0) {\n    return shape.reduce(function (a, b) {\n      return a * b;\n    });\n  } else {\n    // Scalar.\n    return 1;\n  }\n}\n/**\r\n * Casts a tensor to a different dtype and returns it.\r\n * @param x Input tensor.\r\n * @param dtype String: 'float32'|'int32'|'bool'.\r\n * @returns Tensor of the specified `dtype`.\r\n */\nexport function cast(x, dtype) {\n  return tfc.cast(x, dtype);\n}\n/**\r\n * Adds a 1-sized dimension at index \"axis\".\r\n * @param x Input tensor.\r\n * @param axis Position where to add the new axis.\r\n * @returns Result of the dimension expansion.\r\n */\nexport function expandDims(x) {\n  var axis = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : -1;\n  var outShape = x.shape.slice();\n  if (axis < 0) {\n    axis = outShape.length + axis + 1;\n  }\n  outShape.splice(axis, 0, 1);\n  return tfc.reshape(x, outShape);\n}\n/**\r\n * Repeats a 2D tensor.\r\n *\r\n * If `x` has shape `[samples, dim]` and `n` is 2, for example, the output\r\n * will have shape `[samples, 2, dim]`.\r\n *\r\n * @param x Input tensor.\r\n * @param n Integer, number of times to repeat.\r\n * @returns The result of the repeat operation.\r\n * @throws ValueError: If input tensor is not 2D.\r\n */\nexport function repeat(x, n) {\n  return tidy(function () {\n    if (x.shape.length !== 2) {\n      throw new ValueError(\"repeat() expects a rank-2 tensor, but received a \" + \"rank-\".concat(x.shape.length, \" tensor.\"));\n    }\n    var y = expandDims(x, 1);\n    return tile(y, [1, n, 1]);\n  });\n}\n/**\r\n * Flatten a Tensor into 1D.\r\n * @param x Input tensor.\r\n * @return The result of the flattening `x`.\r\n */\nexport function flatten(x) {\n  var newShape = [math_utils.arrayProd(x.shape)];\n  return tfc.reshape(x, newShape);\n}\n/**\r\n * Turn a nD tensor into a 2D tensor with same 0th dimension.\r\n * In other words, it flattens each data samples of a batch.\r\n *\r\n * @param x The tensor to flatten. The rank of this tensor is required to be 2\r\n *   or higher.\r\n * @return The result of the flattening.\r\n */\nexport function batchFlatten(x) {\n  if (x.rank <= 1) {\n    throw new ValueError(\"batchFlatten requires a minimum rank of 2. Got rank: \".concat(x.rank, \".\"));\n  }\n  var newShape = [x.shape[0], math_utils.arrayProd(x.shape, 1)];\n  return tfc.reshape(x, newShape);\n}\n/**\r\n * Do slicing along the first axis.\r\n * @param array input `tf.Tensor`.\r\n * @param start starting index, inclusive.\r\n * @param size size of the slice along the first axis.\r\n * @returns result of the slicing.\r\n * @throws ValueError: If `array` is of an unsupported subtype of `tf.Tensor`.\r\n */\nexport function sliceAlongFirstAxis(array, start, size) {\n  return tidy(function () {\n    switch (array.rank) {\n      case 1:\n        return tfc.slice1d(array, start, size);\n      case 2:\n        return tfc.slice2d(array, [start, 0], [size, array.shape[1]]);\n      case 3:\n        return tfc.slice3d(array, [start, 0, 0], [size, array.shape[1], array.shape[2]]);\n      case 4:\n        return tfc.slice4d(array, [start, 0, 0, 0], [size, array.shape[1], array.shape[2], array.shape[3]]);\n      case 5:\n        return tfc.slice(array, [start, 0, 0, 0, 0], [size, array.shape[1], array.shape[2], array.shape[3], array.shape[4]]);\n      case 6:\n        return tfc.slice(array, [start, 0, 0, 0, 0, 0], [size, array.shape[1], array.shape[2], array.shape[3], array.shape[4], array.shape[5]]);\n      default:\n        throw new ValueError(\"sliceAlongFirstAxis() received an unsupported tensor rank: \" + \"\".concat(array.rank));\n    }\n  });\n}\n/**\r\n * Do slicing along the last axis.\r\n * @param array input `tf.Tensor`.\r\n * @param start starting index, inclusive.\r\n * @param size size of the slice along the last axis.\r\n * @returns result of the slicing.\r\n * @throws ValueError: If `array` is of an unsupported subtype of `tf.Tensor`.\r\n */\nexport function sliceAlongLastAxis(array, start, size) {\n  return tidy(function () {\n    switch (array.rank) {\n      case 1:\n        return tfc.slice1d(array, start, size);\n      case 2:\n        return tfc.slice2d(array, [0, start], [array.shape[0], size]);\n      case 3:\n        return tfc.slice3d(array, [0, 0, start], [array.shape[0], array.shape[1], size]);\n      case 4:\n        return tfc.slice4d(array, [0, 0, 0, start], [array.shape[0], array.shape[1], array.shape[2], size]);\n      default:\n        throw new ValueError(\"sliceAlongLastAxis() received an unsupported tensor rank: \" + \"\".concat(array.rank));\n    }\n  });\n}\n/**\r\n * Do slicing along the sepcified axis.\r\n * @param array input `tf.Tensor`.\r\n * @param start starting index, inclusive.\r\n * @param size of the slice along the chosen axis.\r\n * @param choose an axis.\r\n * @returns result of the slicing.\r\n * @throws ValueError: If `array` is of an unsupported subtype of `tf.Tensor`.\r\n */\nexport function sliceAlongAxis(array, start, size, axis) {\n  return tidy(function () {\n    switch (array.rank) {\n      case 1:\n        return tfc.slice1d(array, start, size);\n      case 2:\n        switch (axis) {\n          case 1:\n            return sliceAlongFirstAxis(array, start, size);\n          case 2:\n            return sliceAlongLastAxis(array, start, size);\n          default:\n            throw new ValueError(\"The axis is not within the rank of the tensor \" + \"\".concat(axis));\n        }\n      case 3:\n        switch (axis) {\n          case 1:\n            return sliceAlongFirstAxis(array, start, size);\n          case 2:\n            return tfc.slice3d(array, [0, start, 0], [array.shape[0], size, array.shape[2]]);\n          case 3:\n            return sliceAlongLastAxis(array, start, size);\n          default:\n            throw new ValueError(\"The axis is not within the rank of the tensor \" + \"\".concat(axis));\n        }\n      case 4:\n        switch (axis) {\n          case 1:\n            return sliceAlongFirstAxis(array, start, size);\n          case 2:\n            return tfc.slice4d(array, [0, start, 0, 0], [array.shape[0], size, array.shape[2], array.shape[3]]);\n          case 3:\n            return tfc.slice4d(array, [0, 0, start, 0], [array.shape[0], array.shape[1], size, array.shape[3]]);\n          case 4:\n            return sliceAlongLastAxis(array, start, size);\n          default:\n            throw new ValueError(\"The axis is not within the rank of the tensor \" + \"\".concat(axis));\n        }\n      default:\n        throw new ValueError(\"sliceAlongLastAxis() received an unsupported tensor rank: \" + \"\".concat(array.rank));\n    }\n  });\n}\n/**\r\n * Concatenates a list of tensors alongside the specified axis.\r\n * @param tensors `Array` of tensors to concatenate.\r\n * @param axis Concatenation axis.\r\n * @returns The result of the concatenation.\r\n */\nexport function concatenate(tensors) {\n  var axis = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : -1;\n  var rank;\n  if (axis < 0) {\n    rank = tensors[0].rank;\n    if (rank !== 0) {\n      axis = rank;\n    } else {\n      axis = 0;\n    }\n  }\n  if (axis === tensors[0].rank) {\n    // Porting Note: This is necessary because tfc.concat() requires axis to be\n    //   in the interval [-rank, rank).\n    axis = -1;\n  }\n  // Porting Note: Sparse concat is not supported yet.\n  return tfc.concat(tensors, axis);\n}\n/**\r\n * Concatenate two arrays along the first dimension.\r\n * @param a The 1st `tf.Tensor` to concatenate.\r\n * @param b The 2nd `tf.Tensor` to concatenate.\r\n * @returns Result of the concatenation.\r\n * @throws ValueError: If `a` is of an unsupported subtype of `tf.Tensor`.\r\n */\nexport function concatAlongFirstAxis(a, b) {\n  switch (a.rank) {\n    case 1:\n      return tfc.concat1d([a, b]);\n    case 2:\n      return tfc.concat2d([a, b], 0);\n    case 3:\n      return tfc.concat3d([a, b], 0);\n    case 4:\n      return tfc.concat4d([a, b], 0);\n    default:\n      throw new ValueError(\"concatAlongFirstAxis() received an unsupported \" + \"tensor rank: \".concat(a.rank));\n  }\n}\n/**\r\n * Creates a tensor by tiling `x` by `n`.\r\n * @param x A tensor.\r\n * @param n An Array of integers or a single integer. If an Array, the length\r\n *   must be the same as the number of dimensions in `x`. If a single integer,\r\n *   it will be treated as an Array of length 1.\r\n */\nexport function tile(x, n) {\n  if (!Array.isArray(n)) {\n    n = [n];\n  }\n  if (x.rank !== n.length) {\n    throw new ValueError(\"The length of input n (\".concat(n.length, \") does not match \") + \"the number of dimensions in input x (\".concat(x.rank, \")\"));\n  }\n  return tfc.tile(x, n);\n}\n/* Creation of random tensors. */\n/**\r\n * Get a tensor with normal distribution of values.\r\n *\r\n * @param shape Shape of the tensor.\r\n * @param mean mean value of the normal distribution.\r\n * @param stddev standard deviation of the normal distribution.\r\n * @param dtype\r\n * @param seed\r\n * @return The normal tensor.\r\n */\nexport function randomNormal(shape) {\n  var mean = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0.0;\n  var stddev = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 1.0;\n  var dtype = arguments.length > 3 ? arguments[3] : undefined;\n  var seed = arguments.length > 4 ? arguments[4] : undefined;\n  return tfc.randomNormal(shape, mean, stddev, dtype, seed);\n}\n/* Linear Algebra */\n/**\r\n * Multiply two tensors and returns the result as a tensor.\r\n *\r\n * For 2D tensors, this is equivalent to matrix multiplication (matMul).\r\n * For tensors of higher ranks, it follows the Theano behavior,\r\n * (e.g. `(2, 3) * (4, 3, 5) -> (2, 4, 5)`).  From the Theano documentation:\r\n *\r\n * For N dimensions it is a sum product over the last axis of x and the\r\n * second-to-last of y:\r\n *\r\n * @param a A tensor of at least rank 2.\r\n * @param b A tensor of at least rank 2.\r\n * @param activation (optional) A string identifying the activation\r\n *   function.\r\n * @return Result of the dot operation.\r\n */\nexport function dot(a, b, activation, bias) {\n  if (a.rank < 2 || b.rank < 2) {\n    throw new NotImplementedError(\"dot requires both inputs to be rank >= 2\" + \" but got x shape = \".concat(a.shape, \" and y shape = \").concat(b.shape));\n  }\n  if (b.rank >= 3) {\n    var xLastDim = a.shape.slice(-1)[0];\n    var ySecondLastDim = b.shape.slice(-2)[0];\n    if (xLastDim !== ySecondLastDim) {\n      throw new NotImplementedError(\"If rank y >= 3, then the second last dim\" + \" of y must equal the last dim of x but got x shape = \".concat(a.shape, \" and \") + \" y shape = \".concat(b.shape));\n    }\n  }\n  // Handle basic 2D x 2D case.\n  if (a.rank === 2 && b.rank === 2) {\n    var transposeA = false;\n    var transposeB = false;\n    // tfc.fused.matMul only fuses certain activation functions. Unsupported\n    // activation functions are treated as 'linear' activations, which is\n    // equivalent to a no-op.\n    return tfc.fused.matMul({\n      a: a,\n      b: b,\n      transposeA: transposeA,\n      transposeB: transposeB,\n      bias: bias ? reshapeBias(a.rank, bias, imageDataFormat()) : null,\n      activation: activation\n    });\n  } else {\n    // Reshape x into the analogous 2D Tensor.\n    var aFirstDims = a.shape.slice(); // Holds all but the last dim of x.\n    var aLastDim = aFirstDims.pop();\n    a = tfc.reshape(a, [-1, aLastDim]);\n    // Reshape y into the analogous 2D Tensor, and keep track of the\n    // required dimensions to reproduce the output shape.\n    var bShape = b.shape.slice();\n    var bLastDim = bShape.pop();\n    var _ySecondLastDim = bShape.pop();\n    var yOtherDims = [].concat(_toConsumableArray(bShape), [bLastDim]);\n    // permutation should be like [r-2, 0, 1, 2, ... r-4, r-3, r-1]\n    // where r is the rank of y.\n    var perm = Array.from({\n      length: b.rank\n    }, function (_, i) {\n      if (i === 0) {\n        return b.rank - 2;\n      } else if (i <= b.rank - 2) {\n        return i - 1;\n      }\n      return i;\n    });\n    b = tfc.reshape(tfc.transpose(b, perm), [_ySecondLastDim, -1]);\n    // Multiply x and y as 2D Tensors, and then reshape back to original.\n    var outputShape = [].concat(_toConsumableArray(aFirstDims), _toConsumableArray(yOtherDims));\n    var _transposeA = false;\n    var _transposeB = false;\n    return tfc.reshape(tfc.fused.matMul({\n      a: a,\n      b: b,\n      transposeA: _transposeA,\n      transposeB: _transposeB,\n      bias: bias ? reshapeBias(a.rank, bias, imageDataFormat()) : null,\n      activation: activation\n    }), outputShape);\n  }\n}\n/**\r\n * Compute the sign Tensor of an input Tensor.\r\n *\r\n * Elements of the input `tf.Tensor` that are === 0 are mapped to 0.\r\n * Elements of the input `tf.Tensor` that are > 0 are mapped to 1.\r\n * Elements of the input `tf.Tensor` that are < 0 are mapped to -1.\r\n *\r\n * @param x Input `tf.Tensor`.\r\n * @return The sign `tf.Tensor`.\r\n */\nexport function sign(x) {\n  // TODO(cais): Move to the core.\n  return tidy(function () {\n    var zerosLikeX = coreZerosLike(x);\n    var onesLikeX = coreOnesLike(x);\n    return where(tfc.equal(x, zerosLikeX), zerosLikeX, where(tfc.greater(x, coreZerosLike(x)), onesLikeX, tfc.mul(-1, onesLikeX)));\n  });\n}\n/**\r\n * Computes the one-hot representation of an integer tensor.\r\n * @param indices nD integer tensor of shape\r\n *   `(batch_size, dim1, dim2, ... dim(n-1))`\r\n * @param numClasses Integer, number of classes to consider.\r\n * @returns (n + 1)D one hot representation of the input\r\n *   with shape `(batch_size, dim1, dim2, ... dim(n-1), num_classes)`\r\n */\nexport function oneHot(indices, numClasses) {\n  return tidy(function () {\n    if (indices.rank !== 1) {\n      throw new Error('Only 1D one-hot tensors are supported in the ' + 'deeplearn backend, at present.');\n    }\n    indices = tfc.cast(indices, 'int32');\n    return tfc.cast(tfc.oneHot(indices, numClasses), 'float32');\n  });\n}\n/* Elementary math functions. */\n/**\r\n * Retrieves the elements of indices `indices` in the tensor `reference`.\r\n * @param reference A tensor.\r\n * @param indices An integer tensor of indices or an `Array` of integers.\r\n * @param axis Axis along which to perform the gather operation.\r\n * @returns The result of the gathering as a tensor.\r\n */\nexport function gather(reference, indices, axis) {\n  return tidy(function () {\n    if (Array.isArray(indices)) {\n      indices = tensor1d(indices, 'int32');\n    } else {\n      indices = tfc.cast(indices, 'int32');\n    }\n    return tfc.gather(reference, indices, axis);\n  });\n}\n/**\r\n * Element-wise square.\r\n * @param x Input tensor.\r\n * @return element-wise x^2\r\n */\nexport function square(x) {\n  return tfc.mul(x, x);\n}\n/**\r\n * Element-wise exponentiation.\r\n *\r\n * Porting Note: In PyKeras, `a` (the exponent) is a Python integer, which\r\n *   takes advatnage of the backend's (e.g., TensorFlow's) automatic\r\n * conversion to tensor. Here we allow `a` to be either a number or a tensor.\r\n *\r\n * @param x The base tensor.\r\n * @param a The exponent, tensor or number. If a number, it is rounded to the\r\n *   nearest integer and converted to a tensor.\r\n * @returns A tensor of the same shape as `x`.\r\n */\nexport function pow(x, a) {\n  return tidy(function () {\n    if (typeof a === 'number') {\n      a = scalar(Math.round(a), 'int32');\n    }\n    if (a.dtype !== 'int32') {\n      throw new NotImplementedError(\"Non-int32 dtype (\".concat(a.dtype, \") is not supported by pow() yet\"));\n    }\n    return tfc.pow(x, a);\n  });\n}\n/**\r\n * Reshapes bias tensor according to rank of x.\r\n */\nfunction reshapeBias(xRank, bias, dataFormat) {\n  var biasShape = bias.shape;\n  if (bias.rank !== 1 && bias.rank !== xRank) {\n    throw new ValueError(\"Unexpected bias dimensions: \".concat(bias.rank) + \"; expected it to be 1 or \".concat(xRank));\n  }\n  if (xRank === 5) {\n    if (dataFormat === 'channelsFirst') {\n      if (biasShape.length === 1) {\n        return tfc.reshape(bias, [1, biasShape[0], 1, 1, 1]);\n      } else {\n        return tfc.reshape(bias, [1, biasShape[3], biasShape[0], biasShape[1], biasShape[2]]);\n      }\n    } else if (dataFormat === 'channelsLast') {\n      if (biasShape.length === 1) {\n        return tfc.reshape(bias, [1, 1, 1, 1, biasShape[0]]);\n      } else {\n        return tfc.reshape(bias, [1].concat(biasShape));\n      }\n    }\n  } else if (xRank === 4) {\n    if (dataFormat === 'channelsFirst') {\n      if (biasShape.length === 1) {\n        return tfc.reshape(bias, [1, biasShape[0], 1, 1]);\n      } else {\n        return tfc.reshape(bias, [1, biasShape[2], biasShape[0], biasShape[1]]);\n      }\n    } else if (dataFormat === 'channelsLast') {\n      if (biasShape.length === 1) {\n        return tfc.reshape(bias, [1, 1, 1, biasShape[0]]);\n      } else {\n        return tfc.reshape(bias, [1].concat(biasShape));\n      }\n    }\n  } else if (xRank === 3) {\n    if (dataFormat === 'channelsFirst') {\n      if (biasShape.length === 1) {\n        return tfc.reshape(bias, [1, biasShape[0], 1]);\n      } else {\n        return tfc.reshape(bias, [1, biasShape[1], biasShape[0]]);\n      }\n    } else if (dataFormat === 'channelsLast') {\n      if (biasShape.length === 1) {\n        return tfc.reshape(bias, [1, 1, biasShape[0]]);\n      } else {\n        return tfc.reshape(bias, [1].concat(biasShape));\n      }\n    }\n  } else if (xRank < 3) {\n    return bias;\n  }\n  throw new ValueError(\"Unsupported input rank by biasAdd: \".concat(bias.rank));\n}\n/* Neural-network operations. */\n/**\r\n * Add a bias to a tensor.\r\n *\r\n * @param x The tensor to add the bias to.\r\n * @param bias The bias to add to `x`. Must be 1D or the same rank as `x`.\r\n * @return Result of the bias adding.\r\n * @throws ValueError: If the rank of `bias` is incorrect.\r\n */\nexport function biasAdd(x, bias, dataFormat) {\n  return tidy(function () {\n    if (dataFormat == null) {\n      dataFormat = imageDataFormat();\n    }\n    checkDataFormat(dataFormat);\n    return tfc.add(x, reshapeBias(x.rank, bias, dataFormat));\n  });\n}\n/**\r\n * Exponential linear unit (ELU).\r\n * @param x A tensor or variable to compute the activation function for.\r\n * @param alpha: A scalar, a scaling factor for the negative section.\r\n * @return Output of the ELU operation.\r\n */\nexport function elu(x) {\n  var alpha = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 1;\n  // TODO(cais): Add support for alpha values other than 1.\n  if (alpha !== 1) {\n    throw new NotImplementedError(\"Support for alpha values other than 1 (\".concat(alpha, \") is not implemented \") + \"yet.\");\n  }\n  return tfc.elu(x);\n}\n/**\r\n * Softsign of a tensor.\r\n *\r\n * Defined as x / (abs(x) + 1), element-wise.\r\n *\r\n * @param x: Input.\r\n * @returns Output.\r\n */\nexport function softsign(x) {\n  return tidy(function () {\n    return tfc.div(x, tfc.add(tfc.abs(x), 1));\n  });\n}\n/**\r\n * Sets entries in `x` to zero at random, while scaling the entire tensor.\r\n *\r\n * @param x input tensor.\r\n * @param level fraction of the entries in the tensor that will be set to 0.\r\n * @param noiseShape shape of randomly generated keep/drop flags, must be\r\n *   broadcastable to the shape of `x`. Optional.\r\n * @param seed random seed to ensure determinism. Optional.\r\n * @returns Result of the dropout operation.\r\n */\nexport function dropout(x, level, noiseShape, seed) {\n  return tidy(function () {\n    return tfc.dropout(x, level, noiseShape, seed);\n  });\n}\n/**\r\n * Element-wise, segment-wise linear approximation of sigmoid.\r\n *\r\n * Returns `0.` if `x < -2.5`, `1.` if `x > 2.5`.\r\n * In `-2.5 <= x <= 2.5`, returns `0.2 * x + 0.5`.\r\n *\r\n * @param x Input tensor.\r\n * @returns Output tensor.\r\n */\nexport function hardSigmoid(x) {\n  return tidy(function () {\n    var y = tfc.add(.5, tfc.mul(.2, x));\n    return tfc.clipByValue(y, 0, 1);\n  });\n}\n/**\r\n * Invoke `x` in the training phase, and `alt` otherwise.\r\n *\r\n * Porting Note: We do not create placeholder tensors for the `training`\r\n * boolean flag here, because there is no such thing in the TF.js imperative\r\n * backend.\r\n *\r\n * @param x The function to invoke iff `training` is `true`.\r\n * @param alt The function to invoke iff `training` is `false`.\r\n * @param training Boolean flag for whether training phase is active.\r\n * @returns The return value of `x()` if `training` is `true`, or the return\r\n *   value of `alt()` if `training` is `false`.\r\n */\nexport function inTrainPhase(x, alt) {\n  var training = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : false;\n  return training ? x() : alt();\n}","map":{"version":3,"names":["tfc","onesLike","coreOnesLike","scalar","tensor1d","tidy","where","zerosLike","coreZerosLike","checkDataFormat","NotImplementedError","ValueError","math_utils","imageDataFormat","backend","setBackend","requestedBackend","getBackend","isBackendSymbolic","countParams","x","shape","length","reduce","a","b","cast","dtype","expandDims","axis","arguments","undefined","outShape","slice","splice","reshape","repeat","n","concat","y","tile","flatten","newShape","arrayProd","batchFlatten","rank","sliceAlongFirstAxis","array","start","size","slice1d","slice2d","slice3d","slice4d","sliceAlongLastAxis","sliceAlongAxis","concatenate","tensors","concatAlongFirstAxis","concat1d","concat2d","concat3d","concat4d","Array","isArray","randomNormal","mean","stddev","seed","dot","activation","bias","xLastDim","ySecondLastDim","transposeA","transposeB","fused","matMul","reshapeBias","aFirstDims","aLastDim","pop","bShape","bLastDim","yOtherDims","_toConsumableArray","perm","from","_","i","transpose","outputShape","sign","zerosLikeX","onesLikeX","equal","greater","mul","oneHot","indices","numClasses","Error","gather","reference","square","pow","Math","round","xRank","dataFormat","biasShape","biasAdd","add","elu","alpha","softsign","div","abs","dropout","level","noiseShape","hardSigmoid","clipByValue","inTrainPhase","alt","training"],"sources":["C:\\Users\\vince\\OneDrive\\Documents\\GitHub\\tfjs-layers\\src\\backend\\tfjs_backend.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * deeplearn.js backend.\n */\n\nimport * as tfc from '@tensorflow/tfjs-core';\nimport {onesLike as coreOnesLike, scalar, Tensor, Tensor1D, tensor1d, Tensor2D, Tensor3D, Tensor4D, Tensor5D, tidy, where, zerosLike as coreZerosLike} from '@tensorflow/tfjs-core';\nimport {checkDataFormat} from '../common';\nimport {NotImplementedError, ValueError} from '../errors';\nimport {DataFormat, Shape} from '../keras_format/common';\nimport {HasShape} from '../types';\nimport * as math_utils from '../utils/math_utils';\n\nimport {imageDataFormat} from './common';\n\n// tslint:enable\n\n/* Setting and getting backend from deeplearn.js. */\n\n// Default deeplearn.js backend is WebGL (GPU).\nlet backend: 'cpu'|'webgl' = 'webgl';\n\nexport function setBackend(requestedBackend: 'cpu'|'webgl') {\n  tfc.setBackend(requestedBackend);\n  backend = requestedBackend;\n}\n\nexport function getBackend(): 'cpu'|'webgl' {\n  return backend;\n}\n\n/**\n * Indicates whether the backend is operating symbolically.\n *\n * This function will be used to determine how to interpret user code. If\n * it returns true, calls to the backend construct a symbolic graph; if\n * it returns false, calls to the backend execute immediately.\n */\nexport function isBackendSymbolic(): boolean {\n  return false;\n}\n\n/**\n * Get the number of elements in a Tensor.\n * @param x The Tensor.\n * @return Number of elements in `x`.\n */\nexport function countParams(x: HasShape): number {\n  const shape = x.shape;\n  if (shape.length > 0) {\n    return shape.reduce((a: number, b: number) => a * b);\n  } else {\n    // Scalar.\n    return 1;\n  }\n}\n\n/**\n * Casts a tensor to a different dtype and returns it.\n * @param x Input tensor.\n * @param dtype String: 'float32'|'int32'|'bool'.\n * @returns Tensor of the specified `dtype`.\n */\nexport function cast(x: Tensor, dtype: tfc.DataType): Tensor {\n  return tfc.cast(x, dtype);\n}\n\n/**\n * Adds a 1-sized dimension at index \"axis\".\n * @param x Input tensor.\n * @param axis Position where to add the new axis.\n * @returns Result of the dimension expansion.\n */\nexport function expandDims(x: Tensor, axis = -1): Tensor {\n  const outShape = x.shape.slice();\n  if (axis < 0) {\n    axis = outShape.length + axis + 1;\n  }\n  outShape.splice(axis, 0, 1);\n  return tfc.reshape(x, outShape);\n}\n\n/**\n * Repeats a 2D tensor.\n *\n * If `x` has shape `[samples, dim]` and `n` is 2, for example, the output\n * will have shape `[samples, 2, dim]`.\n *\n * @param x Input tensor.\n * @param n Integer, number of times to repeat.\n * @returns The result of the repeat operation.\n * @throws ValueError: If input tensor is not 2D.\n */\nexport function repeat(x: Tensor, n: number): Tensor {\n  return tidy(() => {\n    if (x.shape.length !== 2) {\n      throw new ValueError(\n          `repeat() expects a rank-2 tensor, but received a ` +\n          `rank-${x.shape.length} tensor.`);\n    }\n    const y = expandDims(x, 1);\n    return tile(y, [1, n, 1]);\n  });\n}\n\n/**\n * Flatten a Tensor into 1D.\n * @param x Input tensor.\n * @return The result of the flattening `x`.\n */\nexport function flatten(x: Tensor): Tensor {\n  const newShape = [math_utils.arrayProd(x.shape)];\n  return tfc.reshape(x, newShape);\n}\n\n/**\n * Turn a nD tensor into a 2D tensor with same 0th dimension.\n * In other words, it flattens each data samples of a batch.\n *\n * @param x The tensor to flatten. The rank of this tensor is required to be 2\n *   or higher.\n * @return The result of the flattening.\n */\nexport function batchFlatten(x: Tensor): Tensor {\n  if (x.rank <= 1) {\n    throw new ValueError(\n        `batchFlatten requires a minimum rank of 2. Got rank: ${x.rank}.`);\n  }\n  const newShape = [x.shape[0], math_utils.arrayProd(x.shape, 1)];\n  return tfc.reshape(x, newShape);\n}\n\n/**\n * Do slicing along the first axis.\n * @param array input `tf.Tensor`.\n * @param start starting index, inclusive.\n * @param size size of the slice along the first axis.\n * @returns result of the slicing.\n * @throws ValueError: If `array` is of an unsupported subtype of `tf.Tensor`.\n */\nexport function sliceAlongFirstAxis(\n    array: Tensor, start: number, size: number): Tensor {\n  return tidy(() => {\n    switch (array.rank) {\n      case 1:\n        return tfc.slice1d(array as Tensor1D, start, size);\n      case 2:\n        return tfc.slice2d(\n            array as Tensor2D, [start, 0], [size, array.shape[1]]);\n      case 3:\n        return tfc.slice3d(\n            array as Tensor3D, [start, 0, 0],\n            [size, array.shape[1], array.shape[2]]);\n      case 4:\n        return tfc.slice4d(\n            array as Tensor4D, [start, 0, 0, 0],\n            [size, array.shape[1], array.shape[2], array.shape[3]]);\n      case 5:\n        return tfc.slice(array as Tensor5D, [start, 0, 0, 0, 0], [\n          size, array.shape[1], array.shape[2], array.shape[3], array.shape[4]\n        ]);\n      case 6:\n        return tfc.slice(array, [start, 0, 0, 0, 0, 0], [\n          size, array.shape[1], array.shape[2], array.shape[3], array.shape[4],\n          array.shape[5]\n        ]);\n      default:\n        throw new ValueError(\n            `sliceAlongFirstAxis() received an unsupported tensor rank: ` +\n            `${array.rank}`);\n    }\n  });\n}\n\n/**\n * Do slicing along the last axis.\n * @param array input `tf.Tensor`.\n * @param start starting index, inclusive.\n * @param size size of the slice along the last axis.\n * @returns result of the slicing.\n * @throws ValueError: If `array` is of an unsupported subtype of `tf.Tensor`.\n */\nexport function sliceAlongLastAxis(\n    array: Tensor, start: number, size: number): Tensor {\n  return tidy(() => {\n    switch (array.rank) {\n      case 1:\n        return tfc.slice1d(array as Tensor1D, start, size);\n      case 2:\n        return tfc.slice2d(\n            array as Tensor2D, [0, start], [array.shape[0], size]);\n      case 3:\n        return tfc.slice3d(\n            array as Tensor3D, [0, 0, start],\n            [array.shape[0], array.shape[1], size]);\n      case 4:\n        return tfc.slice4d(\n            array as Tensor4D, [0, 0, 0, start],\n            [array.shape[0], array.shape[1], array.shape[2], size]);\n      default:\n        throw new ValueError(\n            `sliceAlongLastAxis() received an unsupported tensor rank: ` +\n            `${array.rank}`);\n    }\n  });\n}\n\n/**\n * Do slicing along the sepcified axis.\n * @param array input `tf.Tensor`.\n * @param start starting index, inclusive.\n * @param size of the slice along the chosen axis.\n * @param choose an axis.\n * @returns result of the slicing.\n * @throws ValueError: If `array` is of an unsupported subtype of `tf.Tensor`.\n */\nexport function sliceAlongAxis(\n    array: Tensor, start: number, size: number, axis: number): Tensor {\n  return tidy(() => {\n    switch (array.rank) {\n      case 1:\n        return tfc.slice1d(array as Tensor1D, start, size);\n      case 2:\n        switch (axis) {\n          case 1:\n            return sliceAlongFirstAxis(array, start, size);\n          case 2:\n            return sliceAlongLastAxis(array, start, size);\n          default:\n            throw new ValueError(\n                `The axis is not within the rank of the tensor ` +\n                `${axis}`);\n        }\n      case 3:\n        switch (axis) {\n          case 1:\n            return sliceAlongFirstAxis(array, start, size);\n          case 2:\n            return tfc.slice3d(\n                array as Tensor3D, [0, start, 0],\n                [array.shape[0], size, array.shape[2]]);\n          case 3:\n            return sliceAlongLastAxis(array, start, size);\n          default:\n            throw new ValueError(\n                `The axis is not within the rank of the tensor ` +\n                `${axis}`);\n        }\n      case 4:\n        switch (axis) {\n          case 1:\n            return sliceAlongFirstAxis(array, start, size);\n          case 2:\n            return tfc.slice4d(\n                array as Tensor4D, [0, start, 0, 0],\n                [array.shape[0], size, array.shape[2], array.shape[3]]);\n          case 3:\n            return tfc.slice4d(\n                array as Tensor4D, [0, 0, start, 0],\n                [array.shape[0], array.shape[1], size, array.shape[3]]);\n          case 4:\n            return sliceAlongLastAxis(array, start, size);\n          default:\n            throw new ValueError(\n                `The axis is not within the rank of the tensor ` +\n                `${axis}`);\n        }\n      default:\n        throw new ValueError(\n            `sliceAlongLastAxis() received an unsupported tensor rank: ` +\n            `${array.rank}`);\n    }\n  });\n}\n\n/**\n * Concatenates a list of tensors alongside the specified axis.\n * @param tensors `Array` of tensors to concatenate.\n * @param axis Concatenation axis.\n * @returns The result of the concatenation.\n */\nexport function concatenate(tensors: Tensor[], axis = -1): Tensor {\n  let rank: number;\n  if (axis < 0) {\n    rank = tensors[0].rank;\n    if (rank !== 0) {\n      axis = rank;\n    } else {\n      axis = 0;\n    }\n  }\n  if (axis === tensors[0].rank) {\n    // Porting Note: This is necessary because tfc.concat() requires axis to be\n    //   in the interval [-rank, rank).\n    axis = -1;\n  }\n  // Porting Note: Sparse concat is not supported yet.\n  return tfc.concat(tensors, axis);\n}\n\n/**\n * Concatenate two arrays along the first dimension.\n * @param a The 1st `tf.Tensor` to concatenate.\n * @param b The 2nd `tf.Tensor` to concatenate.\n * @returns Result of the concatenation.\n * @throws ValueError: If `a` is of an unsupported subtype of `tf.Tensor`.\n */\nexport function concatAlongFirstAxis(a: Tensor, b: Tensor): Tensor {\n  switch (a.rank) {\n    case 1:\n      return tfc.concat1d([a as Tensor1D, b as Tensor1D]);\n    case 2:\n      return tfc.concat2d([a as Tensor2D, b as Tensor2D], 0);\n    case 3:\n      return tfc.concat3d([a as Tensor3D, b as Tensor3D], 0);\n    case 4:\n      return tfc.concat4d([a as Tensor4D, b as Tensor4D], 0);\n    default:\n      throw new ValueError(\n          `concatAlongFirstAxis() received an unsupported ` +\n          `tensor rank: ${a.rank}`);\n  }\n}\n\n/**\n * Creates a tensor by tiling `x` by `n`.\n * @param x A tensor.\n * @param n An Array of integers or a single integer. If an Array, the length\n *   must be the same as the number of dimensions in `x`. If a single integer,\n *   it will be treated as an Array of length 1.\n */\nexport function tile(x: Tensor, n: number|number[]): Tensor {\n  if (!Array.isArray(n)) {\n    n = [n];\n  }\n  if (x.rank !== n.length) {\n    throw new ValueError(\n        `The length of input n (${n.length}) does not match ` +\n        `the number of dimensions in input x (${x.rank})`);\n  }\n  return tfc.tile(x, n);\n}\n\n/* Creation of random tensors. */\n\n/**\n * Get a tensor with normal distribution of values.\n *\n * @param shape Shape of the tensor.\n * @param mean mean value of the normal distribution.\n * @param stddev standard deviation of the normal distribution.\n * @param dtype\n * @param seed\n * @return The normal tensor.\n */\nexport function randomNormal(\n    shape: Shape, mean = 0.0, stddev = 1.0, dtype?: 'float32'|'int32',\n    seed?: number): Tensor {\n  return tfc.randomNormal(shape, mean, stddev, dtype, seed);\n}\n\n/* Linear Algebra */\n\n/**\n * Multiply two tensors and returns the result as a tensor.\n *\n * For 2D tensors, this is equivalent to matrix multiplication (matMul).\n * For tensors of higher ranks, it follows the Theano behavior,\n * (e.g. `(2, 3) * (4, 3, 5) -> (2, 4, 5)`).  From the Theano documentation:\n *\n * For N dimensions it is a sum product over the last axis of x and the\n * second-to-last of y:\n *\n * @param a A tensor of at least rank 2.\n * @param b A tensor of at least rank 2.\n * @param activation (optional) A string identifying the activation\n *   function.\n * @return Result of the dot operation.\n */\nexport function dot(\n    a: Tensor, b: Tensor, activation?: tfc.fused.Activation,\n    bias?: Tensor): Tensor {\n  if ((a.rank < 2) || (b.rank < 2)) {\n    throw new NotImplementedError(\n        `dot requires both inputs to be rank >= 2` +\n        ` but got x shape = ${a.shape} and y shape = ${b.shape}`);\n  }\n  if (b.rank >= 3) {\n    const xLastDim = a.shape.slice(-1)[0];\n    const ySecondLastDim = b.shape.slice(-2)[0];\n    if (xLastDim !== ySecondLastDim) {\n      throw new NotImplementedError(\n          `If rank y >= 3, then the second last dim` +\n          ` of y must equal the last dim of x but got x shape = ${\n              a.shape} and ` +\n          ` y shape = ${b.shape}`);\n    }\n  }\n  // Handle basic 2D x 2D case.\n  if ((a.rank === 2) && (b.rank === 2)) {\n    const transposeA = false;\n    const transposeB = false;\n    // tfc.fused.matMul only fuses certain activation functions. Unsupported\n    // activation functions are treated as 'linear' activations, which is\n    // equivalent to a no-op.\n    return tfc.fused.matMul({\n      a,\n      b: b as Tensor2D,\n      transposeA,\n      transposeB,\n      bias: bias ? reshapeBias(a.rank, bias, imageDataFormat()) : null,\n      activation\n    });\n  } else {\n    // Reshape x into the analogous 2D Tensor.\n    const aFirstDims = a.shape.slice();  // Holds all but the last dim of x.\n    const aLastDim = aFirstDims.pop();\n    a = tfc.reshape(a, [-1, aLastDim]);\n\n    // Reshape y into the analogous 2D Tensor, and keep track of the\n    // required dimensions to reproduce the output shape.\n    const bShape = b.shape.slice();\n    const bLastDim = bShape.pop();\n    const ySecondLastDim = bShape.pop();\n    const yOtherDims = [...bShape, bLastDim];\n    // permutation should be like [r-2, 0, 1, 2, ... r-4, r-3, r-1]\n    // where r is the rank of y.\n    const perm = Array.from({length: b.rank}, (_, i) => {\n      if (i === 0) {\n        return b.rank - 2;\n      } else if (i <= b.rank - 2) {\n        return i - 1;\n      }\n      return i;\n    });\n    b = tfc.reshape(tfc.transpose(b, perm), [ySecondLastDim, -1]);\n\n    // Multiply x and y as 2D Tensors, and then reshape back to original.\n    const outputShape = [...aFirstDims, ...yOtherDims];\n    const transposeA = false;\n    const transposeB = false;\n    return tfc.reshape(\n        tfc.fused.matMul({\n          a,\n          b,\n          transposeA,\n          transposeB,\n          bias: bias ? reshapeBias(a.rank, bias, imageDataFormat()) : null,\n          activation\n        }),\n        outputShape);\n  }\n}\n\n/**\n * Compute the sign Tensor of an input Tensor.\n *\n * Elements of the input `tf.Tensor` that are === 0 are mapped to 0.\n * Elements of the input `tf.Tensor` that are > 0 are mapped to 1.\n * Elements of the input `tf.Tensor` that are < 0 are mapped to -1.\n *\n * @param x Input `tf.Tensor`.\n * @return The sign `tf.Tensor`.\n */\nexport function sign(x: Tensor): Tensor {\n  // TODO(cais): Move to the core.\n  return tidy(() => {\n    const zerosLikeX = coreZerosLike(x);\n    const onesLikeX = coreOnesLike(x);\n    return where(\n        tfc.equal(x, zerosLikeX), zerosLikeX,\n        where(\n            tfc.greater(x, coreZerosLike(x)), onesLikeX,\n            tfc.mul(-1, onesLikeX)));\n  });\n}\n\n/**\n * Computes the one-hot representation of an integer tensor.\n * @param indices nD integer tensor of shape\n *   `(batch_size, dim1, dim2, ... dim(n-1))`\n * @param numClasses Integer, number of classes to consider.\n * @returns (n + 1)D one hot representation of the input\n *   with shape `(batch_size, dim1, dim2, ... dim(n-1), num_classes)`\n */\nexport function oneHot(indices: Tensor, numClasses: number): Tensor {\n  return tidy(() => {\n    if (indices.rank !== 1) {\n      throw new Error(\n          'Only 1D one-hot tensors are supported in the ' +\n          'deeplearn backend, at present.');\n    }\n    indices = tfc.cast(indices, 'int32');\n    return tfc.cast(tfc.oneHot(indices as Tensor1D, numClasses), 'float32');\n  });\n}\n\n/* Elementary math functions. */\n\n/**\n * Retrieves the elements of indices `indices` in the tensor `reference`.\n * @param reference A tensor.\n * @param indices An integer tensor of indices or an `Array` of integers.\n * @param axis Axis along which to perform the gather operation.\n * @returns The result of the gathering as a tensor.\n */\nexport function gather(\n    reference: Tensor, indices: number[]|Tensor1D, axis?: number): Tensor {\n  return tidy(() => {\n    if (Array.isArray(indices)) {\n      indices = tensor1d(indices, 'int32');\n    } else {\n      indices = tfc.cast(indices, 'int32');\n    }\n    return tfc.gather(reference, indices, axis);\n  });\n}\n\n/**\n * Element-wise square.\n * @param x Input tensor.\n * @return element-wise x^2\n */\nexport function square(x: Tensor): Tensor {\n  return tfc.mul(x, x);\n}\n\n/**\n * Element-wise exponentiation.\n *\n * Porting Note: In PyKeras, `a` (the exponent) is a Python integer, which\n *   takes advatnage of the backend's (e.g., TensorFlow's) automatic\n * conversion to tensor. Here we allow `a` to be either a number or a tensor.\n *\n * @param x The base tensor.\n * @param a The exponent, tensor or number. If a number, it is rounded to the\n *   nearest integer and converted to a tensor.\n * @returns A tensor of the same shape as `x`.\n */\nexport function pow(x: Tensor, a: Tensor|number): Tensor {\n  return tidy(() => {\n    if (typeof (a) === 'number') {\n      a = scalar(Math.round(a), 'int32');\n    }\n    if (a.dtype !== 'int32') {\n      throw new NotImplementedError(\n          `Non-int32 dtype (${a.dtype}) is not supported by pow() yet`);\n    }\n    return tfc.pow(x, a);\n  });\n}\n\n/**\n * Reshapes bias tensor according to rank of x.\n */\nfunction reshapeBias(xRank: number, bias: Tensor, dataFormat: string) {\n  const biasShape = bias.shape;\n\n  if (bias.rank !== 1 && bias.rank !== xRank) {\n    throw new ValueError(\n        `Unexpected bias dimensions: ${bias.rank}` +\n        `; expected it to be 1 or ${xRank}`);\n  }\n\n  if (xRank === 5) {\n    if (dataFormat === 'channelsFirst') {\n      if (biasShape.length === 1) {\n        return tfc.reshape(bias, [1, biasShape[0], 1, 1, 1]);\n      } else {\n        return tfc.reshape(\n            bias, [1, biasShape[3], biasShape[0], biasShape[1], biasShape[2]]);\n      }\n    } else if (dataFormat === 'channelsLast') {\n      if (biasShape.length === 1) {\n        return tfc.reshape(bias, [1, 1, 1, 1, biasShape[0]]);\n      } else {\n        return tfc.reshape(bias, [1].concat(biasShape));\n      }\n    }\n  } else if (xRank === 4) {\n    if (dataFormat === 'channelsFirst') {\n      if (biasShape.length === 1) {\n        return tfc.reshape(bias, [1, biasShape[0], 1, 1]);\n      } else {\n        return tfc.reshape(bias, [1, biasShape[2], biasShape[0], biasShape[1]]);\n      }\n    } else if (dataFormat === 'channelsLast') {\n      if (biasShape.length === 1) {\n        return tfc.reshape(bias, [1, 1, 1, biasShape[0]]);\n      } else {\n        return tfc.reshape(bias, [1].concat(biasShape));\n      }\n    }\n  } else if (xRank === 3) {\n    if (dataFormat === 'channelsFirst') {\n      if (biasShape.length === 1) {\n        return tfc.reshape(bias, [1, biasShape[0], 1]);\n      } else {\n        return tfc.reshape(bias, [1, biasShape[1], biasShape[0]]);\n      }\n    } else if (dataFormat === 'channelsLast') {\n      if (biasShape.length === 1) {\n        return tfc.reshape(bias, [1, 1, biasShape[0]]);\n      } else {\n        return tfc.reshape(bias, [1].concat(biasShape));\n      }\n    }\n  } else if (xRank < 3) {\n    return bias;\n  }\n  throw new ValueError(`Unsupported input rank by biasAdd: ${bias.rank}`);\n}\n\n/* Neural-network operations. */\n\n/**\n * Add a bias to a tensor.\n *\n * @param x The tensor to add the bias to.\n * @param bias The bias to add to `x`. Must be 1D or the same rank as `x`.\n * @return Result of the bias adding.\n * @throws ValueError: If the rank of `bias` is incorrect.\n */\nexport function biasAdd(\n    x: Tensor, bias: Tensor, dataFormat?: DataFormat): Tensor {\n  return tidy(() => {\n    if (dataFormat == null) {\n      dataFormat = imageDataFormat();\n    }\n    checkDataFormat(dataFormat);\n\n    return tfc.add(x, reshapeBias(x.rank, bias, dataFormat));\n  });\n}\n\n/**\n * Exponential linear unit (ELU).\n * @param x A tensor or variable to compute the activation function for.\n * @param alpha: A scalar, a scaling factor for the negative section.\n * @return Output of the ELU operation.\n */\nexport function elu(x: Tensor, alpha = 1): Tensor {\n  // TODO(cais): Add support for alpha values other than 1.\n  if (alpha !== 1) {\n    throw new NotImplementedError(\n        `Support for alpha values other than 1 (${alpha}) is not implemented ` +\n        `yet.`);\n  }\n  return tfc.elu(x);\n}\n\n/**\n * Softsign of a tensor.\n *\n * Defined as x / (abs(x) + 1), element-wise.\n *\n * @param x: Input.\n * @returns Output.\n */\nexport function softsign(x: Tensor): Tensor {\n  return tidy(() => tfc.div(x, tfc.add(tfc.abs(x), 1)));\n}\n\n/**\n * Sets entries in `x` to zero at random, while scaling the entire tensor.\n *\n * @param x input tensor.\n * @param level fraction of the entries in the tensor that will be set to 0.\n * @param noiseShape shape of randomly generated keep/drop flags, must be\n *   broadcastable to the shape of `x`. Optional.\n * @param seed random seed to ensure determinism. Optional.\n * @returns Result of the dropout operation.\n */\nexport function dropout(\n    x: Tensor, level: number, noiseShape?: number[], seed?: number): Tensor {\n  return tidy(() => tfc.dropout(x, level, noiseShape, seed));\n}\n\n/**\n * Element-wise, segment-wise linear approximation of sigmoid.\n *\n * Returns `0.` if `x < -2.5`, `1.` if `x > 2.5`.\n * In `-2.5 <= x <= 2.5`, returns `0.2 * x + 0.5`.\n *\n * @param x Input tensor.\n * @returns Output tensor.\n */\nexport function hardSigmoid(x: Tensor): Tensor {\n  return tidy(() => {\n    const y = tfc.add(.5, tfc.mul(.2, x));\n    return tfc.clipByValue(y, 0, 1);\n  });\n}\n\n/**\n * Invoke `x` in the training phase, and `alt` otherwise.\n *\n * Porting Note: We do not create placeholder tensors for the `training`\n * boolean flag here, because there is no such thing in the TF.js imperative\n * backend.\n *\n * @param x The function to invoke iff `training` is `true`.\n * @param alt The function to invoke iff `training` is `false`.\n * @param training Boolean flag for whether training phase is active.\n * @returns The return value of `x()` if `training` is `true`, or the return\n *   value of `alt()` if `training` is `false`.\n */\nexport function inTrainPhase<T>(x: () => T, alt: () => T, training = false): T {\n  return training ? x() : alt();\n}\n"],"mappings":";AAAA;;;;;;;;;AAUA;;;AAIA,OAAO,KAAKA,GAAG,MAAM,uBAAuB;AAC5C,SAAQC,QAAQ,IAAIC,YAAY,EAAEC,MAAM,EAAoBC,QAAQ,EAA0CC,IAAI,EAAEC,KAAK,EAAEC,SAAS,IAAIC,aAAa,QAAO,uBAAuB;AACnL,SAAQC,eAAe,QAAO,WAAW;AACzC,SAAQC,mBAAmB,EAAEC,UAAU,QAAO,WAAW;AAGzD,OAAO,KAAKC,UAAU,MAAM,qBAAqB;AAEjD,SAAQC,eAAe,QAAO,UAAU;AAExC;AAEA;AAEA;AACA,IAAIC,OAAO,GAAkB,OAAO;AAEpC,OAAM,SAAUC,UAAUA,CAACC,gBAA+B;EACxDhB,GAAG,CAACe,UAAU,CAACC,gBAAgB,CAAC;EAChCF,OAAO,GAAGE,gBAAgB;AAC5B;AAEA,OAAM,SAAUC,UAAUA,CAAA;EACxB,OAAOH,OAAO;AAChB;AAEA;;;;;;;AAOA,OAAM,SAAUI,iBAAiBA,CAAA;EAC/B,OAAO,KAAK;AACd;AAEA;;;;;AAKA,OAAM,SAAUC,WAAWA,CAACC,CAAW;EACrC,IAAMC,KAAK,GAAGD,CAAC,CAACC,KAAK;EACrB,IAAIA,KAAK,CAACC,MAAM,GAAG,CAAC,EAAE;IACpB,OAAOD,KAAK,CAACE,MAAM,CAAC,UAACC,CAAS,EAAEC,CAAS;MAAA,OAAKD,CAAC,GAAGC,CAAC;IAAA,EAAC;GACrD,MAAM;IACL;IACA,OAAO,CAAC;;AAEZ;AAEA;;;;;;AAMA,OAAM,SAAUC,IAAIA,CAACN,CAAS,EAAEO,KAAmB;EACjD,OAAO3B,GAAG,CAAC0B,IAAI,CAACN,CAAC,EAAEO,KAAK,CAAC;AAC3B;AAEA;;;;;;AAMA,OAAM,SAAUC,UAAUA,CAACR,CAAS,EAAW;EAAA,IAATS,IAAI,GAAAC,SAAA,CAAAR,MAAA,QAAAQ,SAAA,QAAAC,SAAA,GAAAD,SAAA,MAAG,CAAC,CAAC;EAC7C,IAAME,QAAQ,GAAGZ,CAAC,CAACC,KAAK,CAACY,KAAK,EAAE;EAChC,IAAIJ,IAAI,GAAG,CAAC,EAAE;IACZA,IAAI,GAAGG,QAAQ,CAACV,MAAM,GAAGO,IAAI,GAAG,CAAC;;EAEnCG,QAAQ,CAACE,MAAM,CAACL,IAAI,EAAE,CAAC,EAAE,CAAC,CAAC;EAC3B,OAAO7B,GAAG,CAACmC,OAAO,CAACf,CAAC,EAAEY,QAAQ,CAAC;AACjC;AAEA;;;;;;;;;;;AAWA,OAAM,SAAUI,MAAMA,CAAChB,CAAS,EAAEiB,CAAS;EACzC,OAAOhC,IAAI,CAAC,YAAK;IACf,IAAIe,CAAC,CAACC,KAAK,CAACC,MAAM,KAAK,CAAC,EAAE;MACxB,MAAM,IAAIX,UAAU,CAChB,8DAAA2B,MAAA,CACQlB,CAAC,CAACC,KAAK,CAACC,MAAM,aAAU,CAAC;;IAEvC,IAAMiB,CAAC,GAAGX,UAAU,CAACR,CAAC,EAAE,CAAC,CAAC;IAC1B,OAAOoB,IAAI,CAACD,CAAC,EAAE,CAAC,CAAC,EAAEF,CAAC,EAAE,CAAC,CAAC,CAAC;EAC3B,CAAC,CAAC;AACJ;AAEA;;;;;AAKA,OAAM,SAAUI,OAAOA,CAACrB,CAAS;EAC/B,IAAMsB,QAAQ,GAAG,CAAC9B,UAAU,CAAC+B,SAAS,CAACvB,CAAC,CAACC,KAAK,CAAC,CAAC;EAChD,OAAOrB,GAAG,CAACmC,OAAO,CAACf,CAAC,EAAEsB,QAAQ,CAAC;AACjC;AAEA;;;;;;;;AAQA,OAAM,SAAUE,YAAYA,CAACxB,CAAS;EACpC,IAAIA,CAAC,CAACyB,IAAI,IAAI,CAAC,EAAE;IACf,MAAM,IAAIlC,UAAU,yDAAA2B,MAAA,CACwClB,CAAC,CAACyB,IAAI,OAAI;;EAExE,IAAMH,QAAQ,GAAG,CAACtB,CAAC,CAACC,KAAK,CAAC,CAAC,CAAC,EAAET,UAAU,CAAC+B,SAAS,CAACvB,CAAC,CAACC,KAAK,EAAE,CAAC,CAAC,CAAC;EAC/D,OAAOrB,GAAG,CAACmC,OAAO,CAACf,CAAC,EAAEsB,QAAQ,CAAC;AACjC;AAEA;;;;;;;;AAQA,OAAM,SAAUI,mBAAmBA,CAC/BC,KAAa,EAAEC,KAAa,EAAEC,IAAY;EAC5C,OAAO5C,IAAI,CAAC,YAAK;IACf,QAAQ0C,KAAK,CAACF,IAAI;MAChB,KAAK,CAAC;QACJ,OAAO7C,GAAG,CAACkD,OAAO,CAACH,KAAiB,EAAEC,KAAK,EAAEC,IAAI,CAAC;MACpD,KAAK,CAAC;QACJ,OAAOjD,GAAG,CAACmD,OAAO,CACdJ,KAAiB,EAAE,CAACC,KAAK,EAAE,CAAC,CAAC,EAAE,CAACC,IAAI,EAAEF,KAAK,CAAC1B,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC;MAC5D,KAAK,CAAC;QACJ,OAAOrB,GAAG,CAACoD,OAAO,CACdL,KAAiB,EAAE,CAACC,KAAK,EAAE,CAAC,EAAE,CAAC,CAAC,EAChC,CAACC,IAAI,EAAEF,KAAK,CAAC1B,KAAK,CAAC,CAAC,CAAC,EAAE0B,KAAK,CAAC1B,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC;MAC7C,KAAK,CAAC;QACJ,OAAOrB,GAAG,CAACqD,OAAO,CACdN,KAAiB,EAAE,CAACC,KAAK,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EACnC,CAACC,IAAI,EAAEF,KAAK,CAAC1B,KAAK,CAAC,CAAC,CAAC,EAAE0B,KAAK,CAAC1B,KAAK,CAAC,CAAC,CAAC,EAAE0B,KAAK,CAAC1B,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC;MAC7D,KAAK,CAAC;QACJ,OAAOrB,GAAG,CAACiC,KAAK,CAACc,KAAiB,EAAE,CAACC,KAAK,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CACvDC,IAAI,EAAEF,KAAK,CAAC1B,KAAK,CAAC,CAAC,CAAC,EAAE0B,KAAK,CAAC1B,KAAK,CAAC,CAAC,CAAC,EAAE0B,KAAK,CAAC1B,KAAK,CAAC,CAAC,CAAC,EAAE0B,KAAK,CAAC1B,KAAK,CAAC,CAAC,CAAC,CACrE,CAAC;MACJ,KAAK,CAAC;QACJ,OAAOrB,GAAG,CAACiC,KAAK,CAACc,KAAK,EAAE,CAACC,KAAK,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAC9CC,IAAI,EAAEF,KAAK,CAAC1B,KAAK,CAAC,CAAC,CAAC,EAAE0B,KAAK,CAAC1B,KAAK,CAAC,CAAC,CAAC,EAAE0B,KAAK,CAAC1B,KAAK,CAAC,CAAC,CAAC,EAAE0B,KAAK,CAAC1B,KAAK,CAAC,CAAC,CAAC,EACpE0B,KAAK,CAAC1B,KAAK,CAAC,CAAC,CAAC,CACf,CAAC;MACJ;QACE,MAAM,IAAIV,UAAU,CAChB,mEAAA2B,MAAA,CACGS,KAAK,CAACF,IAAI,CAAE,CAAC;IAAC;EAE3B,CAAC,CAAC;AACJ;AAEA;;;;;;;;AAQA,OAAM,SAAUS,kBAAkBA,CAC9BP,KAAa,EAAEC,KAAa,EAAEC,IAAY;EAC5C,OAAO5C,IAAI,CAAC,YAAK;IACf,QAAQ0C,KAAK,CAACF,IAAI;MAChB,KAAK,CAAC;QACJ,OAAO7C,GAAG,CAACkD,OAAO,CAACH,KAAiB,EAAEC,KAAK,EAAEC,IAAI,CAAC;MACpD,KAAK,CAAC;QACJ,OAAOjD,GAAG,CAACmD,OAAO,CACdJ,KAAiB,EAAE,CAAC,CAAC,EAAEC,KAAK,CAAC,EAAE,CAACD,KAAK,CAAC1B,KAAK,CAAC,CAAC,CAAC,EAAE4B,IAAI,CAAC,CAAC;MAC5D,KAAK,CAAC;QACJ,OAAOjD,GAAG,CAACoD,OAAO,CACdL,KAAiB,EAAE,CAAC,CAAC,EAAE,CAAC,EAAEC,KAAK,CAAC,EAChC,CAACD,KAAK,CAAC1B,KAAK,CAAC,CAAC,CAAC,EAAE0B,KAAK,CAAC1B,KAAK,CAAC,CAAC,CAAC,EAAE4B,IAAI,CAAC,CAAC;MAC7C,KAAK,CAAC;QACJ,OAAOjD,GAAG,CAACqD,OAAO,CACdN,KAAiB,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAEC,KAAK,CAAC,EACnC,CAACD,KAAK,CAAC1B,KAAK,CAAC,CAAC,CAAC,EAAE0B,KAAK,CAAC1B,KAAK,CAAC,CAAC,CAAC,EAAE0B,KAAK,CAAC1B,KAAK,CAAC,CAAC,CAAC,EAAE4B,IAAI,CAAC,CAAC;MAC7D;QACE,MAAM,IAAItC,UAAU,CAChB,kEAAA2B,MAAA,CACGS,KAAK,CAACF,IAAI,CAAE,CAAC;IAAC;EAE3B,CAAC,CAAC;AACJ;AAEA;;;;;;;;;AASA,OAAM,SAAUU,cAAcA,CAC1BR,KAAa,EAAEC,KAAa,EAAEC,IAAY,EAAEpB,IAAY;EAC1D,OAAOxB,IAAI,CAAC,YAAK;IACf,QAAQ0C,KAAK,CAACF,IAAI;MAChB,KAAK,CAAC;QACJ,OAAO7C,GAAG,CAACkD,OAAO,CAACH,KAAiB,EAAEC,KAAK,EAAEC,IAAI,CAAC;MACpD,KAAK,CAAC;QACJ,QAAQpB,IAAI;UACV,KAAK,CAAC;YACJ,OAAOiB,mBAAmB,CAACC,KAAK,EAAEC,KAAK,EAAEC,IAAI,CAAC;UAChD,KAAK,CAAC;YACJ,OAAOK,kBAAkB,CAACP,KAAK,EAAEC,KAAK,EAAEC,IAAI,CAAC;UAC/C;YACE,MAAM,IAAItC,UAAU,CAChB,sDAAA2B,MAAA,CACGT,IAAI,CAAE,CAAC;QAAC;MAErB,KAAK,CAAC;QACJ,QAAQA,IAAI;UACV,KAAK,CAAC;YACJ,OAAOiB,mBAAmB,CAACC,KAAK,EAAEC,KAAK,EAAEC,IAAI,CAAC;UAChD,KAAK,CAAC;YACJ,OAAOjD,GAAG,CAACoD,OAAO,CACdL,KAAiB,EAAE,CAAC,CAAC,EAAEC,KAAK,EAAE,CAAC,CAAC,EAChC,CAACD,KAAK,CAAC1B,KAAK,CAAC,CAAC,CAAC,EAAE4B,IAAI,EAAEF,KAAK,CAAC1B,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC;UAC7C,KAAK,CAAC;YACJ,OAAOiC,kBAAkB,CAACP,KAAK,EAAEC,KAAK,EAAEC,IAAI,CAAC;UAC/C;YACE,MAAM,IAAItC,UAAU,CAChB,sDAAA2B,MAAA,CACGT,IAAI,CAAE,CAAC;QAAC;MAErB,KAAK,CAAC;QACJ,QAAQA,IAAI;UACV,KAAK,CAAC;YACJ,OAAOiB,mBAAmB,CAACC,KAAK,EAAEC,KAAK,EAAEC,IAAI,CAAC;UAChD,KAAK,CAAC;YACJ,OAAOjD,GAAG,CAACqD,OAAO,CACdN,KAAiB,EAAE,CAAC,CAAC,EAAEC,KAAK,EAAE,CAAC,EAAE,CAAC,CAAC,EACnC,CAACD,KAAK,CAAC1B,KAAK,CAAC,CAAC,CAAC,EAAE4B,IAAI,EAAEF,KAAK,CAAC1B,KAAK,CAAC,CAAC,CAAC,EAAE0B,KAAK,CAAC1B,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC;UAC7D,KAAK,CAAC;YACJ,OAAOrB,GAAG,CAACqD,OAAO,CACdN,KAAiB,EAAE,CAAC,CAAC,EAAE,CAAC,EAAEC,KAAK,EAAE,CAAC,CAAC,EACnC,CAACD,KAAK,CAAC1B,KAAK,CAAC,CAAC,CAAC,EAAE0B,KAAK,CAAC1B,KAAK,CAAC,CAAC,CAAC,EAAE4B,IAAI,EAAEF,KAAK,CAAC1B,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC;UAC7D,KAAK,CAAC;YACJ,OAAOiC,kBAAkB,CAACP,KAAK,EAAEC,KAAK,EAAEC,IAAI,CAAC;UAC/C;YACE,MAAM,IAAItC,UAAU,CAChB,sDAAA2B,MAAA,CACGT,IAAI,CAAE,CAAC;QAAC;MAErB;QACE,MAAM,IAAIlB,UAAU,CAChB,kEAAA2B,MAAA,CACGS,KAAK,CAACF,IAAI,CAAE,CAAC;IAAC;EAE3B,CAAC,CAAC;AACJ;AAEA;;;;;;AAMA,OAAM,SAAUW,WAAWA,CAACC,OAAiB,EAAW;EAAA,IAAT5B,IAAI,GAAAC,SAAA,CAAAR,MAAA,QAAAQ,SAAA,QAAAC,SAAA,GAAAD,SAAA,MAAG,CAAC,CAAC;EACtD,IAAIe,IAAY;EAChB,IAAIhB,IAAI,GAAG,CAAC,EAAE;IACZgB,IAAI,GAAGY,OAAO,CAAC,CAAC,CAAC,CAACZ,IAAI;IACtB,IAAIA,IAAI,KAAK,CAAC,EAAE;MACdhB,IAAI,GAAGgB,IAAI;KACZ,MAAM;MACLhB,IAAI,GAAG,CAAC;;;EAGZ,IAAIA,IAAI,KAAK4B,OAAO,CAAC,CAAC,CAAC,CAACZ,IAAI,EAAE;IAC5B;IACA;IACAhB,IAAI,GAAG,CAAC,CAAC;;EAEX;EACA,OAAO7B,GAAG,CAACsC,MAAM,CAACmB,OAAO,EAAE5B,IAAI,CAAC;AAClC;AAEA;;;;;;;AAOA,OAAM,SAAU6B,oBAAoBA,CAAClC,CAAS,EAAEC,CAAS;EACvD,QAAQD,CAAC,CAACqB,IAAI;IACZ,KAAK,CAAC;MACJ,OAAO7C,GAAG,CAAC2D,QAAQ,CAAC,CAACnC,CAAa,EAAEC,CAAa,CAAC,CAAC;IACrD,KAAK,CAAC;MACJ,OAAOzB,GAAG,CAAC4D,QAAQ,CAAC,CAACpC,CAAa,EAAEC,CAAa,CAAC,EAAE,CAAC,CAAC;IACxD,KAAK,CAAC;MACJ,OAAOzB,GAAG,CAAC6D,QAAQ,CAAC,CAACrC,CAAa,EAAEC,CAAa,CAAC,EAAE,CAAC,CAAC;IACxD,KAAK,CAAC;MACJ,OAAOzB,GAAG,CAAC8D,QAAQ,CAAC,CAACtC,CAAa,EAAEC,CAAa,CAAC,EAAE,CAAC,CAAC;IACxD;MACE,MAAM,IAAId,UAAU,CAChB,oEAAA2B,MAAA,CACgBd,CAAC,CAACqB,IAAI,CAAE,CAAC;EAAC;AAEpC;AAEA;;;;;;;AAOA,OAAM,SAAUL,IAAIA,CAACpB,CAAS,EAAEiB,CAAkB;EAChD,IAAI,CAAC0B,KAAK,CAACC,OAAO,CAAC3B,CAAC,CAAC,EAAE;IACrBA,CAAC,GAAG,CAACA,CAAC,CAAC;;EAET,IAAIjB,CAAC,CAACyB,IAAI,KAAKR,CAAC,CAACf,MAAM,EAAE;IACvB,MAAM,IAAIX,UAAU,CAChB,0BAAA2B,MAAA,CAA0BD,CAAC,CAACf,MAAM,iEAAAgB,MAAA,CACMlB,CAAC,CAACyB,IAAI,MAAG,CAAC;;EAExD,OAAO7C,GAAG,CAACwC,IAAI,CAACpB,CAAC,EAAEiB,CAAC,CAAC;AACvB;AAEA;AAEA;;;;;;;;;;AAUA,OAAM,SAAU4B,YAAYA,CACxB5C,KAAY,EACC;EAAA,IADC6C,IAAI,GAAApC,SAAA,CAAAR,MAAA,QAAAQ,SAAA,QAAAC,SAAA,GAAAD,SAAA,MAAG,GAAG;EAAA,IAAEqC,MAAM,GAAArC,SAAA,CAAAR,MAAA,QAAAQ,SAAA,QAAAC,SAAA,GAAAD,SAAA,MAAG,GAAG;EAAA,IAAEH,KAAyB,GAAAG,SAAA,CAAAR,MAAA,OAAAQ,SAAA,MAAAC,SAAA;EAAA,IACjEqC,IAAa,GAAAtC,SAAA,CAAAR,MAAA,OAAAQ,SAAA,MAAAC,SAAA;EACf,OAAO/B,GAAG,CAACiE,YAAY,CAAC5C,KAAK,EAAE6C,IAAI,EAAEC,MAAM,EAAExC,KAAK,EAAEyC,IAAI,CAAC;AAC3D;AAEA;AAEA;;;;;;;;;;;;;;;;AAgBA,OAAM,SAAUC,GAAGA,CACf7C,CAAS,EAAEC,CAAS,EAAE6C,UAAiC,EACvDC,IAAa;EACf,IAAK/C,CAAC,CAACqB,IAAI,GAAG,CAAC,IAAMpB,CAAC,CAACoB,IAAI,GAAG,CAAE,EAAE;IAChC,MAAM,IAAInC,mBAAmB,CACzB,mEAAA4B,MAAA,CACsBd,CAAC,CAACH,KAAK,qBAAAiB,MAAA,CAAkBb,CAAC,CAACJ,KAAK,CAAE,CAAC;;EAE/D,IAAII,CAAC,CAACoB,IAAI,IAAI,CAAC,EAAE;IACf,IAAM2B,QAAQ,GAAGhD,CAAC,CAACH,KAAK,CAACY,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;IACrC,IAAMwC,cAAc,GAAGhD,CAAC,CAACJ,KAAK,CAACY,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;IAC3C,IAAIuC,QAAQ,KAAKC,cAAc,EAAE;MAC/B,MAAM,IAAI/D,mBAAmB,CACzB,qGAAA4B,MAAA,CAEId,CAAC,CAACH,KAAK,UAAO,iBAAAiB,MAAA,CACJb,CAAC,CAACJ,KAAK,CAAE,CAAC;;;EAGhC;EACA,IAAKG,CAAC,CAACqB,IAAI,KAAK,CAAC,IAAMpB,CAAC,CAACoB,IAAI,KAAK,CAAE,EAAE;IACpC,IAAM6B,UAAU,GAAG,KAAK;IACxB,IAAMC,UAAU,GAAG,KAAK;IACxB;IACA;IACA;IACA,OAAO3E,GAAG,CAAC4E,KAAK,CAACC,MAAM,CAAC;MACtBrD,CAAC,EAADA,CAAC;MACDC,CAAC,EAAEA,CAAa;MAChBiD,UAAU,EAAVA,UAAU;MACVC,UAAU,EAAVA,UAAU;MACVJ,IAAI,EAAEA,IAAI,GAAGO,WAAW,CAACtD,CAAC,CAACqB,IAAI,EAAE0B,IAAI,EAAE1D,eAAe,EAAE,CAAC,GAAG,IAAI;MAChEyD,UAAU,EAAVA;KACD,CAAC;GACH,MAAM;IACL;IACA,IAAMS,UAAU,GAAGvD,CAAC,CAACH,KAAK,CAACY,KAAK,EAAE,CAAC,CAAE;IACrC,IAAM+C,QAAQ,GAAGD,UAAU,CAACE,GAAG,EAAE;IACjCzD,CAAC,GAAGxB,GAAG,CAACmC,OAAO,CAACX,CAAC,EAAE,CAAC,CAAC,CAAC,EAAEwD,QAAQ,CAAC,CAAC;IAElC;IACA;IACA,IAAME,MAAM,GAAGzD,CAAC,CAACJ,KAAK,CAACY,KAAK,EAAE;IAC9B,IAAMkD,QAAQ,GAAGD,MAAM,CAACD,GAAG,EAAE;IAC7B,IAAMR,eAAc,GAAGS,MAAM,CAACD,GAAG,EAAE;IACnC,IAAMG,UAAU,MAAA9C,MAAA,CAAA+C,kBAAA,CAAOH,MAAM,IAAEC,QAAQ,EAAC;IACxC;IACA;IACA,IAAMG,IAAI,GAAGvB,KAAK,CAACwB,IAAI,CAAC;MAACjE,MAAM,EAAEG,CAAC,CAACoB;IAAI,CAAC,EAAE,UAAC2C,CAAC,EAAEC,CAAC,EAAI;MACjD,IAAIA,CAAC,KAAK,CAAC,EAAE;QACX,OAAOhE,CAAC,CAACoB,IAAI,GAAG,CAAC;OAClB,MAAM,IAAI4C,CAAC,IAAIhE,CAAC,CAACoB,IAAI,GAAG,CAAC,EAAE;QAC1B,OAAO4C,CAAC,GAAG,CAAC;;MAEd,OAAOA,CAAC;IACV,CAAC,CAAC;IACFhE,CAAC,GAAGzB,GAAG,CAACmC,OAAO,CAACnC,GAAG,CAAC0F,SAAS,CAACjE,CAAC,EAAE6D,IAAI,CAAC,EAAE,CAACb,eAAc,EAAE,CAAC,CAAC,CAAC,CAAC;IAE7D;IACA,IAAMkB,WAAW,MAAArD,MAAA,CAAA+C,kBAAA,CAAON,UAAU,GAAAM,kBAAA,CAAKD,UAAU,EAAC;IAClD,IAAMV,WAAU,GAAG,KAAK;IACxB,IAAMC,WAAU,GAAG,KAAK;IACxB,OAAO3E,GAAG,CAACmC,OAAO,CACdnC,GAAG,CAAC4E,KAAK,CAACC,MAAM,CAAC;MACfrD,CAAC,EAADA,CAAC;MACDC,CAAC,EAADA,CAAC;MACDiD,UAAU,EAAVA,WAAU;MACVC,UAAU,EAAVA,WAAU;MACVJ,IAAI,EAAEA,IAAI,GAAGO,WAAW,CAACtD,CAAC,CAACqB,IAAI,EAAE0B,IAAI,EAAE1D,eAAe,EAAE,CAAC,GAAG,IAAI;MAChEyD,UAAU,EAAVA;KACD,CAAC,EACFqB,WAAW,CAAC;;AAEpB;AAEA;;;;;;;;;;AAUA,OAAM,SAAUC,IAAIA,CAACxE,CAAS;EAC5B;EACA,OAAOf,IAAI,CAAC,YAAK;IACf,IAAMwF,UAAU,GAAGrF,aAAa,CAACY,CAAC,CAAC;IACnC,IAAM0E,SAAS,GAAG5F,YAAY,CAACkB,CAAC,CAAC;IACjC,OAAOd,KAAK,CACRN,GAAG,CAAC+F,KAAK,CAAC3E,CAAC,EAAEyE,UAAU,CAAC,EAAEA,UAAU,EACpCvF,KAAK,CACDN,GAAG,CAACgG,OAAO,CAAC5E,CAAC,EAAEZ,aAAa,CAACY,CAAC,CAAC,CAAC,EAAE0E,SAAS,EAC3C9F,GAAG,CAACiG,GAAG,CAAC,CAAC,CAAC,EAAEH,SAAS,CAAC,CAAC,CAAC;EAClC,CAAC,CAAC;AACJ;AAEA;;;;;;;;AAQA,OAAM,SAAUI,MAAMA,CAACC,OAAe,EAAEC,UAAkB;EACxD,OAAO/F,IAAI,CAAC,YAAK;IACf,IAAI8F,OAAO,CAACtD,IAAI,KAAK,CAAC,EAAE;MACtB,MAAM,IAAIwD,KAAK,CACX,+CAA+C,GAC/C,gCAAgC,CAAC;;IAEvCF,OAAO,GAAGnG,GAAG,CAAC0B,IAAI,CAACyE,OAAO,EAAE,OAAO,CAAC;IACpC,OAAOnG,GAAG,CAAC0B,IAAI,CAAC1B,GAAG,CAACkG,MAAM,CAACC,OAAmB,EAAEC,UAAU,CAAC,EAAE,SAAS,CAAC;EACzE,CAAC,CAAC;AACJ;AAEA;AAEA;;;;;;;AAOA,OAAM,SAAUE,MAAMA,CAClBC,SAAiB,EAAEJ,OAA0B,EAAEtE,IAAa;EAC9D,OAAOxB,IAAI,CAAC,YAAK;IACf,IAAI0D,KAAK,CAACC,OAAO,CAACmC,OAAO,CAAC,EAAE;MAC1BA,OAAO,GAAG/F,QAAQ,CAAC+F,OAAO,EAAE,OAAO,CAAC;KACrC,MAAM;MACLA,OAAO,GAAGnG,GAAG,CAAC0B,IAAI,CAACyE,OAAO,EAAE,OAAO,CAAC;;IAEtC,OAAOnG,GAAG,CAACsG,MAAM,CAACC,SAAS,EAAEJ,OAAO,EAAEtE,IAAI,CAAC;EAC7C,CAAC,CAAC;AACJ;AAEA;;;;;AAKA,OAAM,SAAU2E,MAAMA,CAACpF,CAAS;EAC9B,OAAOpB,GAAG,CAACiG,GAAG,CAAC7E,CAAC,EAAEA,CAAC,CAAC;AACtB;AAEA;;;;;;;;;;;;AAYA,OAAM,SAAUqF,GAAGA,CAACrF,CAAS,EAAEI,CAAgB;EAC7C,OAAOnB,IAAI,CAAC,YAAK;IACf,IAAI,OAAQmB,CAAE,KAAK,QAAQ,EAAE;MAC3BA,CAAC,GAAGrB,MAAM,CAACuG,IAAI,CAACC,KAAK,CAACnF,CAAC,CAAC,EAAE,OAAO,CAAC;;IAEpC,IAAIA,CAAC,CAACG,KAAK,KAAK,OAAO,EAAE;MACvB,MAAM,IAAIjB,mBAAmB,qBAAA4B,MAAA,CACLd,CAAC,CAACG,KAAK,qCAAkC;;IAEnE,OAAO3B,GAAG,CAACyG,GAAG,CAACrF,CAAC,EAAEI,CAAC,CAAC;EACtB,CAAC,CAAC;AACJ;AAEA;;;AAGA,SAASsD,WAAWA,CAAC8B,KAAa,EAAErC,IAAY,EAAEsC,UAAkB;EAClE,IAAMC,SAAS,GAAGvC,IAAI,CAAClD,KAAK;EAE5B,IAAIkD,IAAI,CAAC1B,IAAI,KAAK,CAAC,IAAI0B,IAAI,CAAC1B,IAAI,KAAK+D,KAAK,EAAE;IAC1C,MAAM,IAAIjG,UAAU,CAChB,+BAAA2B,MAAA,CAA+BiC,IAAI,CAAC1B,IAAI,gCAAAP,MAAA,CACZsE,KAAK,CAAE,CAAC;;EAG1C,IAAIA,KAAK,KAAK,CAAC,EAAE;IACf,IAAIC,UAAU,KAAK,eAAe,EAAE;MAClC,IAAIC,SAAS,CAACxF,MAAM,KAAK,CAAC,EAAE;QAC1B,OAAOtB,GAAG,CAACmC,OAAO,CAACoC,IAAI,EAAE,CAAC,CAAC,EAAEuC,SAAS,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC;OACrD,MAAM;QACL,OAAO9G,GAAG,CAACmC,OAAO,CACdoC,IAAI,EAAE,CAAC,CAAC,EAAEuC,SAAS,CAAC,CAAC,CAAC,EAAEA,SAAS,CAAC,CAAC,CAAC,EAAEA,SAAS,CAAC,CAAC,CAAC,EAAEA,SAAS,CAAC,CAAC,CAAC,CAAC,CAAC;;KAEzE,MAAM,IAAID,UAAU,KAAK,cAAc,EAAE;MACxC,IAAIC,SAAS,CAACxF,MAAM,KAAK,CAAC,EAAE;QAC1B,OAAOtB,GAAG,CAACmC,OAAO,CAACoC,IAAI,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAE,CAAC,EAAEuC,SAAS,CAAC,CAAC,CAAC,CAAC,CAAC;OACrD,MAAM;QACL,OAAO9G,GAAG,CAACmC,OAAO,CAACoC,IAAI,EAAE,CAAC,CAAC,CAAC,CAACjC,MAAM,CAACwE,SAAS,CAAC,CAAC;;;GAGpD,MAAM,IAAIF,KAAK,KAAK,CAAC,EAAE;IACtB,IAAIC,UAAU,KAAK,eAAe,EAAE;MAClC,IAAIC,SAAS,CAACxF,MAAM,KAAK,CAAC,EAAE;QAC1B,OAAOtB,GAAG,CAACmC,OAAO,CAACoC,IAAI,EAAE,CAAC,CAAC,EAAEuC,SAAS,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC;OAClD,MAAM;QACL,OAAO9G,GAAG,CAACmC,OAAO,CAACoC,IAAI,EAAE,CAAC,CAAC,EAAEuC,SAAS,CAAC,CAAC,CAAC,EAAEA,SAAS,CAAC,CAAC,CAAC,EAAEA,SAAS,CAAC,CAAC,CAAC,CAAC,CAAC;;KAE1E,MAAM,IAAID,UAAU,KAAK,cAAc,EAAE;MACxC,IAAIC,SAAS,CAACxF,MAAM,KAAK,CAAC,EAAE;QAC1B,OAAOtB,GAAG,CAACmC,OAAO,CAACoC,IAAI,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,EAAEuC,SAAS,CAAC,CAAC,CAAC,CAAC,CAAC;OAClD,MAAM;QACL,OAAO9G,GAAG,CAACmC,OAAO,CAACoC,IAAI,EAAE,CAAC,CAAC,CAAC,CAACjC,MAAM,CAACwE,SAAS,CAAC,CAAC;;;GAGpD,MAAM,IAAIF,KAAK,KAAK,CAAC,EAAE;IACtB,IAAIC,UAAU,KAAK,eAAe,EAAE;MAClC,IAAIC,SAAS,CAACxF,MAAM,KAAK,CAAC,EAAE;QAC1B,OAAOtB,GAAG,CAACmC,OAAO,CAACoC,IAAI,EAAE,CAAC,CAAC,EAAEuC,SAAS,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;OAC/C,MAAM;QACL,OAAO9G,GAAG,CAACmC,OAAO,CAACoC,IAAI,EAAE,CAAC,CAAC,EAAEuC,SAAS,CAAC,CAAC,CAAC,EAAEA,SAAS,CAAC,CAAC,CAAC,CAAC,CAAC;;KAE5D,MAAM,IAAID,UAAU,KAAK,cAAc,EAAE;MACxC,IAAIC,SAAS,CAACxF,MAAM,KAAK,CAAC,EAAE;QAC1B,OAAOtB,GAAG,CAACmC,OAAO,CAACoC,IAAI,EAAE,CAAC,CAAC,EAAE,CAAC,EAAEuC,SAAS,CAAC,CAAC,CAAC,CAAC,CAAC;OAC/C,MAAM;QACL,OAAO9G,GAAG,CAACmC,OAAO,CAACoC,IAAI,EAAE,CAAC,CAAC,CAAC,CAACjC,MAAM,CAACwE,SAAS,CAAC,CAAC;;;GAGpD,MAAM,IAAIF,KAAK,GAAG,CAAC,EAAE;IACpB,OAAOrC,IAAI;;EAEb,MAAM,IAAI5D,UAAU,uCAAA2B,MAAA,CAAuCiC,IAAI,CAAC1B,IAAI,EAAG;AACzE;AAEA;AAEA;;;;;;;;AAQA,OAAM,SAAUkE,OAAOA,CACnB3F,CAAS,EAAEmD,IAAY,EAAEsC,UAAuB;EAClD,OAAOxG,IAAI,CAAC,YAAK;IACf,IAAIwG,UAAU,IAAI,IAAI,EAAE;MACtBA,UAAU,GAAGhG,eAAe,EAAE;;IAEhCJ,eAAe,CAACoG,UAAU,CAAC;IAE3B,OAAO7G,GAAG,CAACgH,GAAG,CAAC5F,CAAC,EAAE0D,WAAW,CAAC1D,CAAC,CAACyB,IAAI,EAAE0B,IAAI,EAAEsC,UAAU,CAAC,CAAC;EAC1D,CAAC,CAAC;AACJ;AAEA;;;;;;AAMA,OAAM,SAAUI,GAAGA,CAAC7F,CAAS,EAAW;EAAA,IAAT8F,KAAK,GAAApF,SAAA,CAAAR,MAAA,QAAAQ,SAAA,QAAAC,SAAA,GAAAD,SAAA,MAAG,CAAC;EACtC;EACA,IAAIoF,KAAK,KAAK,CAAC,EAAE;IACf,MAAM,IAAIxG,mBAAmB,CACzB,0CAAA4B,MAAA,CAA0C4E,KAAK,mCACzC,CAAC;;EAEb,OAAOlH,GAAG,CAACiH,GAAG,CAAC7F,CAAC,CAAC;AACnB;AAEA;;;;;;;;AAQA,OAAM,SAAU+F,QAAQA,CAAC/F,CAAS;EAChC,OAAOf,IAAI,CAAC;IAAA,OAAML,GAAG,CAACoH,GAAG,CAAChG,CAAC,EAAEpB,GAAG,CAACgH,GAAG,CAAChH,GAAG,CAACqH,GAAG,CAACjG,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;EAAA,EAAC;AACvD;AAEA;;;;;;;;;;AAUA,OAAM,SAAUkG,OAAOA,CACnBlG,CAAS,EAAEmG,KAAa,EAAEC,UAAqB,EAAEpD,IAAa;EAChE,OAAO/D,IAAI,CAAC;IAAA,OAAML,GAAG,CAACsH,OAAO,CAAClG,CAAC,EAAEmG,KAAK,EAAEC,UAAU,EAAEpD,IAAI,CAAC;EAAA,EAAC;AAC5D;AAEA;;;;;;;;;AASA,OAAM,SAAUqD,WAAWA,CAACrG,CAAS;EACnC,OAAOf,IAAI,CAAC,YAAK;IACf,IAAMkC,CAAC,GAAGvC,GAAG,CAACgH,GAAG,CAAC,EAAE,EAAEhH,GAAG,CAACiG,GAAG,CAAC,EAAE,EAAE7E,CAAC,CAAC,CAAC;IACrC,OAAOpB,GAAG,CAAC0H,WAAW,CAACnF,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC;EACjC,CAAC,CAAC;AACJ;AAEA;;;;;;;;;;;;;AAaA,OAAM,SAAUoF,YAAYA,CAAIvG,CAAU,EAAEwG,GAAY,EAAkB;EAAA,IAAhBC,QAAQ,GAAA/F,SAAA,CAAAR,MAAA,QAAAQ,SAAA,QAAAC,SAAA,GAAAD,SAAA,MAAG,KAAK;EACxE,OAAO+F,QAAQ,GAAGzG,CAAC,EAAE,GAAGwG,GAAG,EAAE;AAC/B"},"metadata":{},"sourceType":"module","externalDependencies":[]}